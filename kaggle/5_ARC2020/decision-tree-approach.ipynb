{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add new variables and improve local score to 32 and 13\n",
    "- check ensemble of xgb, lgb and cat\n",
    "- https://www.kaggle.com/meaninglesslives/using-decision-trees-for-arc\n",
    "- https://www.kaggle.com/davidbnn92/task-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\n",
    "training_path = data_path / 'training'\n",
    "evaluation_path = data_path / 'evaluation'\n",
    "test_path = data_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattener(pred):\n",
    "    str_pred = str([row for row in pred])\n",
    "    str_pred = str_pred.replace(', ', '')\n",
    "    str_pred = str_pred.replace('[[', '|')\n",
    "    str_pred = str_pred.replace('][', '|')\n",
    "    str_pred = str_pred.replace(']]', '|')\n",
    "    return str_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder_path):\n",
    "    task_names_list = sorted(os.listdir(folder_path))\n",
    "    task_list = []\n",
    "    for task_name in task_names_list: \n",
    "        task_file = str(folder_path / task_name)\n",
    "        with open(task_file, 'r') as f:\n",
    "            task = json.load(f)\n",
    "            task_list.append(task)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['task_name'] = task_names_list\n",
    "    df['task'] = task_list\n",
    "    df['number_of_train_pairs'] = df['task'].apply(lambda x: len(x['train']))\n",
    "    df['number_of_test_pairs'] = df['task'].apply(lambda x: len(x['test']))\n",
    "    \n",
    "    # Compare image sizes\n",
    "    df['inputs_all_have_same_height'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['input']) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['inputs_all_have_same_width'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['input'][0]) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['inputs_all_have_same_shape'] = df['inputs_all_have_same_height'] * df['inputs_all_have_same_width']\n",
    "    df['input_height_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['input'])\n",
    "                     if (len(set([len(example['input']) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['input_width_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['input'][0])\n",
    "                     if (len(set([len(example['input'][0]) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['outputs_all_have_same_height'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['output']) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['outputs_all_have_same_width'] = df['task'].apply(\n",
    "        lambda task: int(len(set([len(example['output'][0]) for example in task['train']])) == 1)\n",
    "    )\n",
    "    df['outputs_all_have_same_shape'] = df['outputs_all_have_same_height'] * df['outputs_all_have_same_width']\n",
    "    df['output_height_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'])\n",
    "                     if (len(set([len(example['output']) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['output_width_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'][0])\n",
    "                     if (len(set([len(example['output'][0]) for example in task['train']])) == 1)\n",
    "                     else np.nan\n",
    "    )  \n",
    "    df['in_each_pair_shape_doesnt_change'] = df['task'].apply(\n",
    "        lambda task: np.prod([int(len(example['input'][0])==len(example['output'][0])\n",
    "                                  and len(example['input'])==len(example['output'])\n",
    "                                 ) for example in task['train']\n",
    "                            ])\n",
    "    )\n",
    "    df['in_each_pair_shape_ratio_is_the_same'] = df['task'].apply(\n",
    "        lambda task: (len(set([len(example['input'][0]) / len(example['output'][0])\n",
    "                                 for example in task['train']]))==1) * (\n",
    "                      len(set([len(example['input']) / len(example['output'])\n",
    "                                 for example in task['train']]))==1)\n",
    "    )\n",
    "    df['o/i_height_ratio_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output']) / len(task['train'][0]['input'])\n",
    "                     if (len(set([len(example['input']) / len(example['output'])\n",
    "                                 for example in task['train']]))==1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    df['o/i_width_ratio_if_constant'] = df['task'].apply(\n",
    "        lambda task: len(task['train'][0]['output'][0]) / len(task['train'][0]['input'][0])\n",
    "                     if (len(set([len(example['input'][0]) / len(example['output'][0])\n",
    "                                 for example in task['train']]))==1)\n",
    "                     else np.nan\n",
    "    )\n",
    "    \n",
    "    # my idea ---------\n",
    "    df[\"same_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input'])))== sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    \n",
    "    df[\"same_color_sum_in_edge\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(np.array(example['input'])[0,:]) +sum(np.array(example['input'])[:,0]) + \n",
    "                                    sum(np.array(example['input'])[-1,:]) +sum(np.array(example['input'])[:,-1])\n",
    "                                    == \n",
    "                                    sum(np.array(example['output'])[0,:]) +sum(np.array(example['output'])[:,0]) + \n",
    "                                    sum(np.array(example['output'])[-1,:]) +sum(np.array(example['output'])[:,-1])) for example in task['train']]))\n",
    "    \n",
    "    df[\"io_color_kind_diff\"] = df['task'].apply(lambda task: [len(np.unique(np.array(example['input']))) - len(np.unique(np.array(example['output']))) for example in task['train']])\n",
    "    df[\"io_color_kind_diff_constant\"] = df['io_color_kind_diff'].apply(lambda task: np.unique(np.array(task))[0] if len(np.unique(np.array(task)))==1 else -1)\n",
    "    df[\"output_not_include_0\"] = df['task'].apply(lambda task: np.all([np.all(np.array(example['output']) > 0) for example in task['train']]))\n",
    "    df[\"increase_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input']))) < sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    df[\"decrease_color_sum\"] = df['task'].apply(lambda task: \n",
    "                        np.all([int(sum(sum(np.array(example['input']))) > sum(sum(np.array(example['output'])))) for example in task['train']]))\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "training_descriptive_df = create_df(training_path)\n",
    "evaluation_descriptive_df = create_df(evaluation_path)\n",
    "test_descriptive_df = create_df(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(row):\n",
    "    # same shape and same color sum → xgboost\n",
    "    if row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.same_color_sum==1:\n",
    "        return 1\n",
    "    # same shape and increase color sum and include black in output　→ xgboost\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.increase_color_sum==1 and row.output_not_include_0 == 0:\n",
    "        return 2\n",
    "    # same shape and incrase color sum and no black in output\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.increase_color_sum==1 and row.output_not_include_0 == 1:\n",
    "        return 3\n",
    "    # same shape and decrease color sum → xgboost\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 1 and row[\"o/i_height_ratio_if_constant\"] ==1 and row[\"o/i_width_ratio_if_constant\"]==1 and row.decrease_color_sum==1:\n",
    "        return 4\n",
    "    # different shape and decrease color sum\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 0 and row.decrease_color_sum==1:\n",
    "        return 5\n",
    "    # different shape and increase color sum\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 0 and row.increase_color_sum==1:\n",
    "        return 6\n",
    "    # different shape and same color sum\n",
    "    elif row[\"in_each_pair_shape_doesnt_change\"] == 0 and row.same_color_sum==1:\n",
    "        return 7\n",
    "    # otherwise\n",
    "    else:\n",
    "        return 8\n",
    "training_descriptive_df[\"class\"] = training_descriptive_df.apply(lambda x: classification(x), axis=1)\n",
    "evaluation_descriptive_df[\"class\"] = evaluation_descriptive_df.apply(lambda x: classification(x), axis=1)\n",
    "test_descriptive_df[\"class\"] = test_descriptive_df.apply(lambda x: classification(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.3650\n",
      "5    0.2000\n",
      "8    0.1075\n",
      "4    0.1050\n",
      "6    0.0925\n",
      "1    0.0600\n",
      "3    0.0475\n",
      "7    0.0225\n",
      "Name: class, dtype: float64\n",
      "2    0.3650\n",
      "5    0.2000\n",
      "8    0.0950\n",
      "4    0.0950\n",
      "6    0.0925\n",
      "1    0.0700\n",
      "3    0.0600\n",
      "7    0.0225\n",
      "Name: class, dtype: float64\n",
      "2    0.31\n",
      "5    0.23\n",
      "4    0.18\n",
      "6    0.11\n",
      "8    0.09\n",
      "3    0.04\n",
      "1    0.03\n",
      "7    0.01\n",
      "Name: class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training_descriptive_df[\"class\"].value_counts(normalize=True))\n",
    "print(evaluation_descriptive_df[\"class\"].value_counts(normalize=True))\n",
    "print(test_descriptive_df[\"class\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours(color, cur_row, cur_col, nrows, ncols, radius):\n",
    "\n",
    "    if cur_row<=radius-1: top = -1\n",
    "    else: top = color[cur_row-radius][cur_col]\n",
    "        \n",
    "    if cur_row>=nrows-radius: bottom = -1\n",
    "    else: bottom = color[cur_row+radius][cur_col]\n",
    "        \n",
    "    if cur_col<=radius-1: left = -1\n",
    "    else: left = color[cur_row][cur_col-radius]\n",
    "        \n",
    "    if cur_col>=ncols-radius: right = -1\n",
    "    else: right = color[cur_row][cur_col+radius]\n",
    "        \n",
    "    return top, bottom, left, right\n",
    "\n",
    "def get_tl_tr(color, cur_row, cur_col, nrows, ncols, radius):\n",
    "        \n",
    "    if cur_row<=radius-1:\n",
    "        top_left = -1\n",
    "        top_right = -1\n",
    "    else:\n",
    "        if cur_col<=radius-1: top_left=-1\n",
    "        else: top_left = color[cur_row-radius][cur_col-radius]\n",
    "        if cur_col>=ncols-radius: top_right=-1\n",
    "        else: top_right = color[cur_row-radius][cur_col+radius]   \n",
    "        \n",
    "    return top_left, top_right\n",
    "\n",
    "def get_bl_br(color, cur_row, cur_col, nrows, ncols, radius):\n",
    "        \n",
    "    if cur_row>=nrows-radius:\n",
    "        bottom_left = -1\n",
    "        bottom_right = -1\n",
    "    else:\n",
    "        if cur_col<=radius-1: bottom_left=-1\n",
    "        else: bottom_left = color[cur_row+radius][cur_col-radius]\n",
    "        if cur_col>=ncols-radius: bottom_right=-1\n",
    "        else: bottom_right = color[cur_row+radius][cur_col+radius]   \n",
    "        \n",
    "    return bottom_left, bottom_right\n",
    "\n",
    "def diagonal(color, cur_row, cur_col, nrows, ncols, direction):\n",
    "    element = []\n",
    "    element.append(color[cur_row, cur_col])\n",
    "    if direction == \"upper-right\":\n",
    "        for i in range(-nrows,nrows):\n",
    "            if (cur_row + i < nrows and cur_row +i >=0) and (cur_col - i < ncols and cur_col - i >=0):\n",
    "                element.append(color[cur_row+i][cur_col-i])\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        for i in range(-nrows,nrows):  \n",
    "            if (cur_row + i < nrows and cur_row +i >=0) and (cur_col + i < ncols and cur_col + i >=0):\n",
    "                element.append(color[cur_row+i][cur_col+i])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    return np.array(element)\n",
    "\n",
    "def fliplr_check(color, cur_row, cur_col, r):\n",
    "    pixmap = color[cur_row-r:cur_row+r,cur_col-r:cur_col+r].copy()\n",
    "    if pixmap.shape[0] == pixmap.shape[1]:\n",
    "        return np.all(color[cur_row-r:cur_row+r,cur_col-r:cur_col+r] == np.fliplr(np.array(pixmap)))\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def flipud_check(color, cur_row, cur_col, r):\n",
    "    pixmap = color[cur_row-r:cur_row+r,cur_col-r:cur_col+r].copy()\n",
    "    if pixmap.shape[0] == pixmap.shape[1]:\n",
    "        return np.all(color[cur_row-r:cur_row+r,cur_col-r:cur_col+r] == np.flipud(np.array(pixmap)))\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def flip90_check(color, cur_row, cur_col, r):\n",
    "    pixmap = color[cur_row-r:cur_row+r,cur_col-r:cur_col+r].copy()\n",
    "    if pixmap.shape[0] == pixmap.shape[1]:\n",
    "        return np.all(color[cur_row-r:cur_row+r,cur_col-r:cur_col+r] == np.rot90(np.array(pixmap)))\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def make_features(input_color, nfeat): # for class 2 and 4\n",
    "    nrows, ncols = input_color.shape\n",
    "    feat = np.zeros((nrows*ncols,nfeat))\n",
    "    cur_idx = 0\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            feat[cur_idx,0] = i\n",
    "            feat[cur_idx,1] = j\n",
    "            feat[cur_idx,2] = input_color[i][j]\n",
    "            feat[cur_idx,3:7] = neighbours(input_color, i, j, nrows, ncols,1)\n",
    "            feat[cur_idx,7:9] = get_tl_tr(input_color, i, j, nrows, ncols,1)\n",
    "            feat[cur_idx,9] = len(np.unique(input_color[i,:]))\n",
    "            feat[cur_idx,10] = len(np.unique(input_color[:,j]))\n",
    "            feat[cur_idx,11] = (i+j)\n",
    "            feat[cur_idx,12] = len(np.unique(input_color[i-1:i+1,j-1:j+1]))\n",
    "            feat[cur_idx,13:15] = get_bl_br(input_color, i, j, nrows, ncols,1)\n",
    "            feat[cur_idx,15] = np.sum(input_color[i,:])\n",
    "            feat[cur_idx,16] = np.sum(input_color[:,j])\n",
    "            feat[cur_idx,17:21] = neighbours(input_color, i, j, nrows, ncols,2)\n",
    "            feat[cur_idx,21] = np.max(input_color[i,:])\n",
    "            feat[cur_idx,22] = np.min(input_color[i,:])\n",
    "            feat[cur_idx,23] = np.max(input_color[:,j])\n",
    "            feat[cur_idx,24] = np.min(input_color[:,j])\n",
    "            feat[cur_idx,25:29] = neighbours(input_color, i, j, nrows, ncols,3)\n",
    "            feat[cur_idx,29] = np.sum(input_color[i-1:i+1,j-1:j+1])\n",
    "            feat[cur_idx,30] = np.sum(input_color[i-2:i+2,j-2:j+2])\n",
    "            feat[cur_idx,31] = len(input_color[i-5:i+5,j-5:j+5])\n",
    "            cur_idx += 1\n",
    "        \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(task, mode='train'):\n",
    "    num_train_pairs = len(task[mode])\n",
    "    feat, target = [], []\n",
    "    \n",
    "    global local_neighb\n",
    "    for task_num in range(num_train_pairs):\n",
    "        input_color = np.array(task[mode][task_num]['input'])\n",
    "        target_color = task[mode][task_num]['output']\n",
    "        nrows, ncols = len(task[mode][task_num]['input']), len(task[mode][task_num]['input'][0])\n",
    "\n",
    "        target_rows, target_cols = len(task[mode][task_num]['output']), len(task[mode][task_num]['output'][0])\n",
    "        \n",
    "        if (target_rows!=nrows) or (target_cols!=ncols):\n",
    "            return None, None, 1\n",
    "\n",
    "        imsize = nrows*ncols\n",
    "        offset = imsize*task_num*3 #since we are using three types of aug\n",
    "        feat.extend(make_features(input_color, nfeat))\n",
    "        target.extend(np.array(target_color).reshape(-1,))\n",
    "            \n",
    "    return np.array(feat), np.array(target), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeat = 32\n",
    "local_neighb = 5\n",
    "def modelling(mode, kind):\n",
    "    print(mode)\n",
    "    count = 0\n",
    "    sample_sub = pd.read_csv(data_path/'sample_submission.csv')\n",
    "    sample_sub = sample_sub.set_index('output_id')\n",
    "    \n",
    "    valid_scores = {}\n",
    "    model_accuracies = {'ens': []}\n",
    "    pred_taskids = []\n",
    "    \n",
    "    if mode=='eval':\n",
    "        task_path = evaluation_path\n",
    "        #all_task_ids = list(evaluation_descriptive_df[evaluation_descriptive_df[\"class\"]==3][\"task_name\"]) # for particular class\n",
    "    elif mode=='train':\n",
    "        task_path = training_path\n",
    "        #all_task_ids = list(training_descriptive_df[training_descriptive_df[\"class\"]==3][\"task_name\"]) # for particular class\n",
    "    elif mode=='test':\n",
    "        task_path = test_path\n",
    "    all_task_ids = sorted(os.listdir(task_path))\n",
    "    # training -----\n",
    "    for task_id in all_task_ids:\n",
    "        task_file = str(task_path / task_id)\n",
    "        with open(task_file, 'r') as f:\n",
    "            task = json.load(f)\n",
    "\n",
    "        feat, target, not_valid = features(task)\n",
    "        if not_valid:\n",
    "            print('ignoring task', task_file)\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        if kind == \"xgb\":   \n",
    "            model = XGBClassifier(n_estimators=50, max_depth = 5, num_leaves=10, learning_rate=0.1, n_jobs=-1)\n",
    "        elif kind == \"lgb\":\n",
    "            model = LGBMClassifier(n_estimators=60, max_depth=4, n_jobs=-1, learning_rate=0.25)\n",
    "            #model = LGBMClassifier(n_estimators=60, max_depth=4, n_jobs=-1, learning_rate=0.25)\n",
    "        else:\n",
    "            model = CatBoostClassifier(n_estimators=50, max_depth = 6, learning_rate=0.25)\n",
    "            #model = CatBoostClassifier(n_estimators=50, max_depth = 6, learning_rate=0.25)\n",
    "        model.fit(feat, target, verbose=0)\n",
    "    # training on input pairs is done\n",
    "    \n",
    "    # test predictions begins here\n",
    "        num_test_pairs = len(task['test'])\n",
    "        for task_num in range(num_test_pairs):\n",
    "            cur_idx = 0\n",
    "            input_color = np.array(task['test'][task_num]['input'])\n",
    "            nrows, ncols = len(task['test'][task_num]['input']), len(task['test'][task_num]['input'][0])\n",
    "            feat = make_features(input_color, nfeat)\n",
    "            preds = model.predict(feat).reshape(nrows,ncols)\n",
    "        \n",
    "            if (mode=='train') or (mode=='eval'):\n",
    "                ens_acc = (np.array(task['test'][task_num]['output'])==preds).sum()/(nrows*ncols)\n",
    "                model_accuracies['ens'].append(ens_acc)\n",
    "                pred_taskids.append(f'{task_id[:-5]}_{task_num}')\n",
    "                #print('ensemble accuracy',(np.array(task['test'][task_num]['output'])==preds).sum()/(nrows*ncols))\n",
    "            \n",
    "            preds = preds.astype(int).tolist()\n",
    "            #plot_test(preds, task_id)\n",
    "            sample_sub.loc[f'{task_id[:-5]}_{task_num}','output'] = flattener(preds)\n",
    "    print(str(count)+\" tasks were ignored.\")\n",
    "    return sample_sub, model_accuracies, pred_taskids\n",
    "\n",
    "#_, train_xgb_accuracies, train_ids= modelling('train', 'xgb')\n",
    "#_, eval_xgb_accuracies, eval_ids  = modelling('eval', 'xgb')\n",
    "#_, train_lgb_accuracies, train_ids= modelling('train', 'lgb')\n",
    "#_, eval_lgb_accuracies, eval_ids  = modelling('eval', 'lgb')\n",
    "#_, train_cat_accuracies, train_ids= modelling('train', 'cat')\n",
    "#_, eval_cat_accuracies, eval_ids  = modelling('eval', 'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(train_xgb_accuracies, index=train_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete training tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)\n",
    "\n",
    "#df = pd.DataFrame(eval_xgb_accuracies, index=eval_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete evaluation tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(train_lgb_accuracies, index=train_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete training tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)\n",
    "\n",
    "#df = pd.DataFrame(eval_lgb_accuracies, index=eval_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete evaluation tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(train_cat_accuracies, index=train_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete training tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)\n",
    "\n",
    "#df = pd.DataFrame(eval_cat_accuracies, index=eval_ids)\n",
    "#for c in df.columns:\n",
    "#    print(f'for {c} no. of complete evaluation tasks is', (df.loc[:, c]==1).sum())\n",
    "#    print(df[df.loc[:, c]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeat = 32\n",
    "local_neighb = 5\n",
    "def ensemble(mode):\n",
    "    print(mode)\n",
    "    count = 0\n",
    "    sample_sub = pd.read_csv(data_path/'sample_submission.csv')\n",
    "    sample_sub = sample_sub.set_index('output_id')\n",
    "    \n",
    "    valid_scores = {}\n",
    "    model_accuracies = {'ens': []}\n",
    "    pred_taskids = []\n",
    "    \n",
    "    if mode=='eval':\n",
    "        task_path = evaluation_path\n",
    "        #all_task_ids = list(evaluation_descriptive_df[evaluation_descriptive_df[\"class\"]==3][\"task_name\"]) # for particular class\n",
    "    elif mode=='train':\n",
    "        task_path = training_path\n",
    "        #all_task_ids = list(training_descriptive_df[training_descriptive_df[\"class\"]==3][\"task_name\"]) # for particular class\n",
    "    elif mode=='test':\n",
    "        task_path = test_path\n",
    "    all_task_ids = sorted(os.listdir(task_path))\n",
    "    # training -----\n",
    "    for task_id in all_task_ids:\n",
    "        task_file = str(task_path / task_id)\n",
    "        with open(task_file, 'r') as f:\n",
    "            task = json.load(f)\n",
    "\n",
    "        feat, target, not_valid = features(task)\n",
    "        if not_valid:\n",
    "            print('ignoring task', task_file)\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        model_xgb = XGBClassifier(n_estimators=50, max_depth = 5, num_leaves=10, learning_rate=0.1, n_jobs=-1)\n",
    "        model_lgb = LGBMClassifier(n_estimators=60, max_depth=4, n_jobs=-1, learning_rate=0.25)\n",
    "        model_cat = CatBoostClassifier(n_estimators=50, max_depth = 6, learning_rate=0.25)\n",
    "        model_xgb.fit(feat, target, verbose=0)\n",
    "        model_lgb.fit(feat, target, verbose=0)\n",
    "        model_cat.fit(feat, target, verbose=0)\n",
    "    # training on input pairs is done\n",
    "    \n",
    "    # test predictions begins here\n",
    "        num_test_pairs = len(task['test'])\n",
    "        for task_num in range(num_test_pairs):\n",
    "            cur_idx = 0\n",
    "            input_color = np.array(task['test'][task_num]['input'])\n",
    "            nrows, ncols = len(task['test'][task_num]['input']), len(task['test'][task_num]['input'][0])\n",
    "            feat = make_features(input_color, nfeat)\n",
    "            preds_xgb = model_xgb.predict(feat).reshape(nrows,ncols)\n",
    "            preds_lgb = model_lgb.predict(feat).reshape(nrows,ncols)\n",
    "            preds_cat = model_cat.predict(feat).reshape(nrows,ncols)\n",
    "\n",
    "            if (mode=='train') or (mode=='eval'):\n",
    "                ens_acc = (np.array(task['test'][task_num]['output'])==preds).sum()/(nrows*ncols)\n",
    "                model_accuracies['ens'].append(ens_acc)\n",
    "                pred_taskids.append(f'{task_id[:-5]}_{task_num}')\n",
    "                #print('ensemble accuracy',(np.array(task['test'][task_num]['output'])==preds).sum()/(nrows*ncols))\n",
    "            \n",
    "            preds_list = [flattener(preds_xgb.astype(int).tolist()), flattener(preds_lgb.astype(int).tolist()), flattener(preds_cat.astype(int).tolist()),]\n",
    "            sample_sub.loc[f'{task_id[:-5]}_{task_num}','output'] = \" \".join(preds_list)\n",
    "    print(str(count)+\" tasks were ignored.\")\n",
    "    return sample_sub, model_accuracies, pred_taskids\n",
    "\n",
    "#_, train_xgb_accuracies, train_ids= ensemble('train')\n",
    "#_, eval_xgb_accuracies, eval_ids  = ensemble('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/00576224.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0692e18c.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0934a4d8.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0a1d4ef5.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0bb8deee.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0c786b71.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/0c9aba6e.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/12997ef3.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/136b0064.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/15696249.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/195ba7dc.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/1990f7a8.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/19bb5feb.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/1a2e2828.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/1a6449f1.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2037f2c7.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2072aba6.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/20818e16.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2697da3f.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2753e76c.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/27f8ce4f.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/281123b4.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2c0b0aff.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/2f0c5170.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/310f3251.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/3194b014.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/31d5ba1a.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/34b99a2b.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/351d6448.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/358ba94e.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/3979b1a8.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/3b4c2228.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/3d31c5b3.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/3ee1011a.json\n",
      "ignoring task /kaggle/input/abstraction-and-reasoning-challenge/test/414297c0.json\n",
      "35 tasks were ignored.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>output_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00576224_0</th>\n",
       "      <td>|32|78| |32|78| |00|00|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>009d5c81_0</th>\n",
       "      <td>|00000000000000|00000222222222|00000200020307|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00dbd492_0</th>\n",
       "      <td>|00000000000222220000|02222222220233320000|020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03560426_0</th>\n",
       "      <td>|8188000000|8188000000|8282000000|8202000000|0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05a7bcf2_0</th>\n",
       "      <td>|000000000020000000080000000000|00004444488888...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       output\n",
       "output_id                                                    \n",
       "00576224_0                           |32|78| |32|78| |00|00| \n",
       "009d5c81_0  |00000000000000|00000222222222|00000200020307|...\n",
       "00dbd492_0  |00000000000222220000|02222222220233320000|020...\n",
       "03560426_0  |8188000000|8188000000|8282000000|8202000000|0...\n",
       "05a7bcf2_0  |000000000020000000080000000000|00004444488888..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub, _, _ = ensemble('test')\n",
    "test_sub.to_csv('submission.csv')\n",
    "test_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
