{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf71e89b",
   "metadata": {
    "papermill": {
     "duration": 0.00483,
     "end_time": "2023-08-17T07:19:11.945137",
     "exception": false,
     "start_time": "2023-08-17T07:19:11.940307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- focus on nn modelling\n",
    "- save checkpoint files * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0092a",
   "metadata": {
    "papermill": {
     "duration": 0.003865,
     "end_time": "2023-08-17T07:19:11.953302",
     "exception": false,
     "start_time": "2023-08-17T07:19:11.949437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbd8945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T07:19:11.963573Z",
     "iopub.status.busy": "2023-08-17T07:19:11.963127Z",
     "iopub.status.idle": "2023-08-17T07:19:30.441578Z",
     "shell.execute_reply": "2023-08-17T07:19:30.440303Z"
    },
    "papermill": {
     "duration": 18.487482,
     "end_time": "2023-08-17T07:19:30.444741",
     "exception": false,
     "start_time": "2023-08-17T07:19:11.957259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "\n",
    "# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer\n",
    "\n",
    "#####\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 512))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(512, 512))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(512, last_num))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        return x\n",
    "#####\n",
    "\n",
    "        \n",
    "class MyModule(LightningModule):\n",
    "    def __init__(self, lr, num_columns, last_num = 1):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.num_columns = num_columns\n",
    "        self.last_num = last_num\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]))\n",
    "        self.model = MyModel(num_columns = self.num_columns, last_num = self.last_num)\n",
    "        self.log_outputs = {}\n",
    "        self.validation_step_outputs = []\n",
    "        self.train_step_outputs = []\n",
    "        \n",
    "    #####\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = self.lr)\n",
    "        return optimizer\n",
    "    #####\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)        \n",
    "        loss = self.loss_fn(preds, targets)        \n",
    "        self.train_step_outputs.append(loss)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        output = {\"targets\": targets.detach(), \"preds\": preds.detach(), \"loss\": loss.detach()}\n",
    "        self.validation_step_outputs.append(output)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        preds = self.forward(inputs)\n",
    "                \n",
    "        return preds\n",
    "    \n",
    "    def on_train_start(self) -> None:\n",
    "        self.print(f\"Train start\")\n",
    "        return super().on_train_start()\n",
    "    \n",
    "    def on_train_end(self) -> None:\n",
    "        self.print(\" \")\n",
    "        return super().on_train_end()\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        train_loss = torch.stack([x for x in self.train_step_outputs]).mean()\n",
    "        self.log_dict({\"loss\": train_loss})\n",
    "        self.log_outputs[\"loss\"] = train_loss\n",
    "        \n",
    "        train_loss     = self.log_outputs[\"loss\"]\n",
    "        valid_loss     = self.log_outputs[\"valid_loss\"]\n",
    "        self.print(f\"loss: {train_loss:.3f} - val_loss: {valid_loss:.3f}\")\n",
    "        \n",
    "        return super().on_train_epoch_end()\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        valid_loss = torch.stack([x[\"loss\"] for x in self.validation_step_outputs]).mean()\n",
    "        \n",
    "        self.log_dict({\"valid_loss\": valid_loss})\n",
    "        self.log_outputs[\"valid_loss\"] = valid_loss\n",
    "\n",
    "        return super().on_validation_epoch_end()\n",
    "    \n",
    "    \n",
    "class MyDataModule(LightningModule):\n",
    "    def __init__(self, train, test, target, feats, fold, batch_size = 32):\n",
    "        super(MyDataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.target = target\n",
    "        self.input_test = test\n",
    "        self.feats = feats\n",
    "        self.fold = fold\n",
    "        self.x_train = None\n",
    "        self.x_valid = None\n",
    "        self.y_train = None\n",
    "        self.y_valid = None\n",
    "        \n",
    "    #####\n",
    "    def split_train_valid_df(self):\n",
    "        skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "        for n, (tr_index, val_index) in enumerate(skf.split(self.train, self.target)):\n",
    "            if n == self.fold: \n",
    "                x_train = self.train.loc[tr_index].reset_index(drop=True)\n",
    "                x_valid = self.train.loc[val_index].reset_index(drop=True)\n",
    "                y_train = self.target.loc[tr_index].reset_index(drop=True)\n",
    "                y_valid = self.target.loc[val_index].reset_index(drop=True)\n",
    "        \n",
    "        return x_train, x_valid, y_train, y_valid\n",
    "    #####\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        x_tr, x_va, y_tr, y_va = self.split_train_valid_df()\n",
    "        self.x_train = x_tr\n",
    "        self.x_valid = x_va\n",
    "        self.y_train = y_tr \n",
    "        self.y_valid = y_va\n",
    "        self.test = self.input_test\n",
    "        \n",
    "    def get_dataframe(self, phase):\n",
    "        assert phase in [\"train\", \"valid\", \"test\"]\n",
    "        if phase == \"train\":\n",
    "            return self.x_train, self.y_train\n",
    "        elif phase == \"valid\":\n",
    "            return self.x_valid, self.y_valid\n",
    "        elif phase == \"test\":\n",
    "            return self.test, None\n",
    "        \n",
    "    def get_ds(self, phase):\n",
    "        x, y = self.get_dataframe(phase)\n",
    "        return MyDataset(df = x, target = y, feats = self.feats, phase = phase)\n",
    "        \n",
    "    def get_loader(self, phase):\n",
    "        assert phase in [\"train\", \"valid\", \"test\"]\n",
    "        ds = self.get_ds(phase = phase)\n",
    "        return DataLoader(ds, batch_size = self.batch_size, num_workers = 4,\n",
    "                        shuffle = True if phase == \"train\" else False,\n",
    "                        drop_last = True if phase == \"train\" else False)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.get_loader(\"train\")\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.get_loader(\"valid\")\n",
    "    \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, target, feats, phase = \"train\"):\n",
    "        self.phase = phase \n",
    "        self.feats = feats\n",
    "        self.data = df[feats]\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.phase in ['train', \"valid\"]:\n",
    "            return self.data.values[index].astype(float), self.target.values[index].astype(float)\n",
    "        elif self.phase == 'test':\n",
    "            return self.data.values[index].astype(float), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051254ed",
   "metadata": {
    "papermill": {
     "duration": 0.004351,
     "end_time": "2023-08-17T07:19:30.453697",
     "exception": false,
     "start_time": "2023-08-17T07:19:30.449346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4bd5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T07:19:30.464087Z",
     "iopub.status.busy": "2023-08-17T07:19:30.463632Z",
     "iopub.status.idle": "2023-08-17T07:19:30.916540Z",
     "shell.execute_reply": "2023-08-17T07:19:30.914518Z"
    },
    "papermill": {
     "duration": 0.463787,
     "end_time": "2023-08-17T07:19:30.921733",
     "exception": false,
     "start_time": "2023-08-17T07:19:30.457946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 72) (5, 72)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)\n",
    "    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "train['EJ'] = train['EJ'].map({'A': 0, 'B': 1})\n",
    "test['EJ']  = test['EJ'].map({'A': 0, 'B': 1})\n",
    "\n",
    "# process epsilon\n",
    "train = pd.merge(train, greeks, on = \"Id\", how = \"inner\")\n",
    "train_stratify = train[[\"Class\", \"Beta\", \"Delta\", \"Gamma\"]] \n",
    "train[\"Epsilon_ordinal\"] = train[\"Epsilon\"].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal() if x != \"Unknown\" else np.nan)\n",
    "\n",
    "org_features = [n for n in train.columns if n not in ['Class', 'Id', 'Alpha', \"Beta\", \"Gamma\", \"Delta\", \"Epsilon\"]]\n",
    "test_times = pd.DataFrame([train.Epsilon_ordinal.max() + 1] * len(test), columns = [\"Epsilon_ordinal\"])\n",
    "final_test = pd.concat((test, test_times), axis=1)\n",
    "\n",
    "# fill missing value\n",
    "train.fillna(-999, inplace=True)\n",
    "final_test.fillna(-999, inplace=True)\n",
    "\n",
    "# add pca columns\n",
    "pca_feat_num = 15\n",
    "pca_cols = [\"pca\"+str(i+1) for i in range(pca_feat_num)]\n",
    "pca = PCA(n_components=pca_feat_num,random_state=42)\n",
    "pca_train = pca.fit_transform(train[org_features])\n",
    "pca_test = pca.transform(final_test[org_features])\n",
    "pca_train = pd.DataFrame(pca_train, columns=pca_cols)\n",
    "pca_test = pd.DataFrame(pca_test, columns=pca_cols)\n",
    "train = pd.concat([train, pca_train],axis=1)\n",
    "final_test = pd.concat([final_test, pca_test],axis=1)\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "cons_feats = org_features + pca_cols\n",
    "normalize_train = scalar.fit_transform(train[cons_feats])\n",
    "normalize_train = pd.DataFrame(normalize_train, columns = cons_feats)\n",
    "normalize_test = scalar.transform(final_test[cons_feats])\n",
    "normalize_test = pd.DataFrame(normalize_test, columns = cons_feats)\n",
    "\n",
    "print(normalize_train.shape, normalize_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e91c2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T07:19:30.953760Z",
     "iopub.status.busy": "2023-08-17T07:19:30.952928Z",
     "iopub.status.idle": "2023-08-17T07:19:30.959710Z",
     "shell.execute_reply": "2023-08-17T07:19:30.958577Z"
    },
    "papermill": {
     "duration": 0.028155,
     "end_time": "2023-08-17T07:19:30.961936",
     "exception": false,
     "start_time": "2023-08-17T07:19:30.933781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"feats\": cons_feats,\n",
    "    \"cv\": MultilabelStratifiedKFold(5, shuffle=True, random_state=42),\n",
    "    \"n_splits\" : 5,\n",
    "    \"train_data\": normalize_train,\n",
    "    \"fold_y\": train_stratify,\n",
    "    \"test_data\": normalize_test,\n",
    "    \"target\": train.Class,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.01,\n",
    "    \"metric_function\" : balanced_log_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5536ae",
   "metadata": {
    "papermill": {
     "duration": 0.004408,
     "end_time": "2023-08-17T07:19:30.970670",
     "exception": false,
     "start_time": "2023-08-17T07:19:30.966262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570673d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T07:19:30.981485Z",
     "iopub.status.busy": "2023-08-17T07:19:30.981068Z",
     "iopub.status.idle": "2023-08-17T07:20:07.656940Z",
     "shell.execute_reply": "2023-08-17T07:20:07.655379Z"
    },
    "papermill": {
     "duration": 36.685016,
     "end_time": "2023-08-17T07:20:07.659901",
     "exception": false,
     "start_time": "2023-08-17T07:19:30.974885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "loss: 0.775 - val_loss: 0.932\n",
      "loss: 0.650 - val_loss: 0.777\n",
      "loss: 0.570 - val_loss: 0.860\n",
      "loss: 0.528 - val_loss: 1.045\n",
      "loss: 0.491 - val_loss: 1.424\n",
      " \n",
      "Train start\n",
      "loss: 0.943 - val_loss: 0.751\n",
      "loss: 0.722 - val_loss: 0.680\n",
      "loss: 0.606 - val_loss: 0.661\n",
      "loss: 0.553 - val_loss: 0.656\n",
      "loss: 0.507 - val_loss: 0.704\n",
      "loss: 0.464 - val_loss: 0.764\n",
      "loss: 0.444 - val_loss: 0.746\n",
      " \n",
      "Train start\n",
      "loss: 0.850 - val_loss: 0.840\n",
      "loss: 0.682 - val_loss: 0.781\n",
      "loss: 0.575 - val_loss: 0.746\n",
      "loss: 0.525 - val_loss: 0.696\n",
      "loss: 0.489 - val_loss: 0.717\n",
      "loss: 0.462 - val_loss: 0.706\n",
      "loss: 0.430 - val_loss: 0.746\n",
      " \n",
      "Train start\n",
      "loss: 0.928 - val_loss: 0.888\n",
      "loss: 0.710 - val_loss: 0.750\n",
      "loss: 0.600 - val_loss: 0.796\n",
      "loss: 0.539 - val_loss: 0.748\n",
      "loss: 0.490 - val_loss: 0.751\n",
      "loss: 0.461 - val_loss: 0.758\n",
      "loss: 0.431 - val_loss: 0.761\n",
      " \n",
      "Train start\n",
      "loss: 0.837 - val_loss: 0.694\n",
      "loss: 0.650 - val_loss: 0.624\n",
      "loss: 0.563 - val_loss: 0.593\n",
      "loss: 0.510 - val_loss: 0.600\n",
      "loss: 0.480 - val_loss: 0.558\n",
      "loss: 0.466 - val_loss: 0.591\n",
      "loss: 0.445 - val_loss: 0.619\n",
      "loss: 0.425 - val_loss: 0.594\n",
      " \n"
     ]
    }
   ],
   "source": [
    "seed_everything(42, workers=True)\n",
    "    \n",
    "for fold in range(config[\"n_splits\"]):\n",
    "    callbacks = []\n",
    "    es_callback = EarlyStopping(monitor='valid_loss', patience=3)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"valid_loss\", dirpath=f\"./checkpoints-{fold}\", filename=f\"model-{fold}\", save_top_k=1, mode=\"min\",)\n",
    "    callbacks.append(es_callback)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # train\n",
    "    trainer = Trainer(max_epochs = config[\"epochs\"], callbacks=callbacks, enable_progress_bar = False, log_every_n_steps = 10)\n",
    "    model = MyModule(lr = config[\"lr\"], num_columns = len(config[\"feats\"]), last_num = 1).to(\"cpu\", dtype=float)\n",
    "    data_module = MyDataModule(train = config[\"train_data\"], test = config[\"test_data\"], feats = config[\"feats\"], \n",
    "                           fold = fold, target = config[\"target\"], batch_size = config[\"batch_size\"])\n",
    "    trainer.fit(model, datamodule = data_module)\n",
    "    \n",
    "    valid_loader = data_module.get_loader(\"valid\")\n",
    "    test_loader = data_module.get_loader(\"test\")\n",
    "    if fold == 0:\n",
    "        valid_preds = trainer.predict(model, dataloaders = valid_loader, ckpt_path = \"best\")\n",
    "        test_preds = trainer.predict(model, dataloaders = test_loader, ckpt_path = \"best\")\n",
    "\n",
    "        valid_preds = torch.cat(valid_preds)\n",
    "        test_preds = torch.cat(test_preds)\n",
    "        valid_target = data_module.y_valid.values        \n",
    "    else:\n",
    "        tmp_preds = trainer.predict(model, dataloaders = valid_loader, ckpt_path = \"best\")\n",
    "        tmp_test_preds = trainer.predict(model, dataloaders = test_loader, ckpt_path = \"best\")\n",
    "\n",
    "        tmp_preds = torch.cat(tmp_preds)\n",
    "        tmp_test_preds = torch.cat(tmp_test_preds)\n",
    "        tmp_target = data_module.y_valid.values\n",
    "        \n",
    "        valid_target = np.hstack((valid_target, tmp_target))\n",
    "        valid_preds = torch.cat((valid_preds, tmp_preds))        \n",
    "        \n",
    "        test_preds += tmp_test_preds\n",
    "\n",
    "test_preds /= config[\"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f3b630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T07:20:07.677751Z",
     "iopub.status.busy": "2023-08-17T07:20:07.677322Z",
     "iopub.status.idle": "2023-08-17T07:20:07.692680Z",
     "shell.execute_reply": "2023-08-17T07:20:07.691487Z"
    },
    "papermill": {
     "duration": 0.027293,
     "end_time": "2023-08-17T07:20:07.695092",
     "exception": false,
     "start_time": "2023-08-17T07:20:07.667799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37644220842761844\n"
     ]
    }
   ],
   "source": [
    "print(config[\"metric_function\"](valid_target, torch.sigmoid(valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae51ce",
   "metadata": {
    "papermill": {
     "duration": 0.007626,
     "end_time": "2023-08-17T07:20:07.710707",
     "exception": false,
     "start_time": "2023-08-17T07:20:07.703081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 72.412219,
   "end_time": "2023-08-17T07:20:10.473749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-17T07:18:58.061530",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
