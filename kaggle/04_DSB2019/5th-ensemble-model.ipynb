{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- include correlation in post_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, mean_squared_error\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "np.set_printoptions(precision=8)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def qwk(a1, a2):\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "    e = e / a1.shape[0]\n",
    "    return np.round(1 - o / e, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "        return -qwk(y, X_p)\n",
    "        \n",
    "    def fit(self, X, y,random_flg=False):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        if random_flg:\n",
    "            initial_coef = [np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7), np.random.uniform(0.8,0.9)]\n",
    "        else:\n",
    "            initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        \n",
    "    def predict(self, X, coef):\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stract_hists(feature, train, test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5, label = \"train\")\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5, label = \"test\")\n",
    "        plt.show()\n",
    "    return msre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n",
    "    train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n",
    "    test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n",
    "    #specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n",
    "    sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print(\"Finish reading\")\n",
    "    return train, train_labels, test, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data(train, train_labels):\n",
    "    keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "    train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "    train = train[train.installation_id.isin(train_labels.installation_id.unique())]\n",
    "    assess_title = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
    "       'Cauldron Filler (Assessment)', 'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)']\n",
    "    additional_remove_index = []\n",
    "    for i, session in train.groupby('installation_id', sort=False):\n",
    "        last_row = session.index[-1]\n",
    "        session = session[session.title.isin(assess_title)]\n",
    "        first_row = session.index[-1] + 1\n",
    "        for j in range(first_row, last_row+1):\n",
    "            additional_remove_index.append(j)                \n",
    "    train = train[~train.index.isin(additional_remove_index)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test):\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    list_of_title_eventcode = list(set(train['title_event_code'].unique()).union(set(test['title_event_code'].unique())))\n",
    "    \n",
    "    list_of_eventid = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"level\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    test[\"level\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"round\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "    test[\"round\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)         \n",
    "        \n",
    "    return train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    last_activity = 0\n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n",
    "    title_eventcode_count = {str(ele): 0 for ele in list_of_title_eventcode}\n",
    "    eventid_count = {str(ele): 0 for ele in list_of_eventid}\n",
    "    user_world_count = {\"world_\"+str(wor) : 0 for wor in activities_world.values()}\n",
    "    \n",
    "    last_session_time_sec = 0\n",
    "    all_assessments = []\n",
    "    accuracy_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0 \n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_game_level = np.array([]); tree_game_level = np.array([]); magma_game_level = np.array([])\n",
    "    crys_game_round = np.array([]); tree_game_round = np.array([]); magma_game_round = np.array([])\n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    durations_game = []; durations_activity = []\n",
    "    \n",
    "    for i, session in user_sample.groupby('game_session', sort=False):  \n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "        \n",
    "        if session_type != 'Assessment':\n",
    "            time_spent = int(session['game_time'].iloc[-1] / 1000)\n",
    "            time_spent_each_act[activities_labels[session_title]] += time_spent   \n",
    "            \n",
    "            if session_type == \"Game\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_game_true += true\n",
    "                    crys_game_false += false\n",
    "                    crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    crys_game_level = np.concatenate([crys_game_level, session[\"level\"]], axis=0)\n",
    "                    crys_game_round = np.concatenate([crys_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_game_true += true\n",
    "                    tree_game_false += false\n",
    "                    tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    tree_game_level = np.concatenate([tree_game_level, session[\"level\"]], axis=0)\n",
    "                    tree_game_round = np.concatenate([tree_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_game_true += true\n",
    "                    magma_game_false += false\n",
    "                    magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    magma_game_level = np.concatenate([magma_game_level, session[\"level\"]], axis=0)\n",
    "                    magma_game_round = np.concatenate([magma_game_round, session[\"round\"]], axis=0)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            if session_type == \"Activity\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_act_true += true\n",
    "                    crys_act_false += false\n",
    "                    crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_act_true += true\n",
    "                    tree_act_false += false\n",
    "                    tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_act_true += true\n",
    "                    magma_act_false += false\n",
    "                    magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1): # test set or session in train_label\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum() # true in target assess\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum() # false in target assessment\n",
    "            \n",
    "            # from start of installation_id to the start of target assessment ------------------------\n",
    "            features = user_activities_count.copy() # appearance of each type without duplicates\n",
    "            features.update(time_spent_each_act.copy()) # cumulative gameplay time in each title\n",
    "            features.update(title_eventcode_count.copy()) # apperance of combi of title and event_code\n",
    "            features.update(eventid_count.copy()) # apperance of eventid\n",
    "            features.update(user_world_count.copy()) # appearance of world with duplicates\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            features[\"misses\"] = miss\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    " \n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(crys_game_level) if len(crys_game_level[~np.isnan(crys_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(crys_game_round) if len(crys_game_round[~np.isnan(crys_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(tree_game_level) if len(tree_game_level[~np.isnan(tree_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(tree_game_round) if len(tree_game_round[~np.isnan(tree_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(magma_game_level) if len(magma_game_level[~np.isnan(magma_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(magma_game_round) if len(magma_game_round[~np.isnan(magma_game_round)]) >=1 else 0\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['activity_last_duration'] = 0\n",
    "                features['activity_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['activity_last_duration'] = durations_activity[-1]\n",
    "                features['activity_max_duration'] = np.max(durations_activity)\n",
    "            \n",
    "            # unique type --------------------------------------------------------\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['session_title'] = session_title\n",
    "            \n",
    "            # nums in target assessment data ------------------------------------------\n",
    "            if durations == []: #span of timestamp in target assessment\n",
    "                features['duration_mean'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds) \n",
    "            \n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[str(features['accuracy_group'])] += 1\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            \n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "                        \n",
    "        n_of_title_eventcode = Counter(session['title_event_code']) \n",
    "        for key in n_of_title_eventcode.keys():\n",
    "            title_eventcode_count[str(key)] += n_of_title_eventcode[key]\n",
    "            \n",
    "        miss += np.sum(session[\"misses\"])\n",
    "        \n",
    "        n_of_eventid = Counter(session['event_id']) \n",
    "        for key in n_of_eventid.keys():\n",
    "            eventid_count[str(key)] += n_of_eventid[key]\n",
    "                        \n",
    "        user_world_count[\"world_\"+str(session_world)] += session.shape[0]\n",
    "\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(train, test):\n",
    "    new_train = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        new_train += get_data(user_sample)\n",
    "    new_train = pd.DataFrame(new_train)\n",
    "    print(new_train.shape)\n",
    "    del train\n",
    "    \n",
    "    new_test = []\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        a = get_data(user_sample, test_set=True)\n",
    "        new_test.append(a)   \n",
    "    new_test = pd.DataFrame(new_test)\n",
    "    print(new_test.shape)\n",
    "    del test\n",
    "    \n",
    "    return new_train, new_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(new_train, new_test, stand_flg = False, corr_flg = False):\n",
    "    X_train = new_train.copy()\n",
    "    X_test = new_test.copy()\n",
    "    y_train = new_train.accuracy_group\n",
    "    \n",
    "    if stand_flg == True:\n",
    "        features = [i for i in new_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "        categoricals = ['session_title']\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(X_train[[cat]])\n",
    "                test_cats = enc.transform(X_test[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                X_train = pd.concat([X_train, train_cats], axis=1)\n",
    "                X_test = pd.concat([X_test, test_cats], axis=1)\n",
    "            scalar = MinMaxScaler()\n",
    "            X_train[features] = scalar.fit_transform(X_train[features])\n",
    "            X_test[features] = scalar.transform(X_test[features])\n",
    "        X_train = X_train.drop([\"session_title\"], axis=1)\n",
    "        X_test = X_test.drop([\"session_title\"], axis=1)\n",
    "    \n",
    "    X_train = X_train.drop(['accuracy_group'],axis=1) \n",
    "    X_test = X_test.drop([\"installation_id\", \"accuracy_group\"], axis=1)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = []\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "            \n",
    "    if corr_flg == True:\n",
    "        correlations = new_train.corr().abs()\n",
    "        correlations = correlations.mask(np.tril(np.ones(correlations.shape)).astype(np.bool))\n",
    "        correlations = correlations.stack().reset_index()\n",
    "        corr_columns = [\"level_0\", \"level_1\", \"value\"]\n",
    "        correlations.columns = corr_columns\n",
    "        correlations = correlations.sort_values(\"value\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        high_corr = correlations[correlations[\"value\"] >= 0.995]\n",
    "\n",
    "        high_corr_features = []\n",
    "        for i in range(high_corr.shape[0]):\n",
    "            if high_corr.iloc[i][\"level_0\"] not in high_corr_features and high_corr.iloc[i][\"level_1\"] not in high_corr_features:\n",
    "                high_corr_features.append(high_corr.iloc[i][\"level_0\"])\n",
    "        \n",
    "        for i in high_corr_features:\n",
    "            if i not in remove_features:\n",
    "                remove_features.append(i)  \n",
    "                \n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "    X_test = X_test.drop(remove_features, axis=1)\n",
    "    X_test = X_test[sorted(X_test.columns.tolist())]\n",
    "    print(\"train: \", X_train.shape)\n",
    "    print(\"test: \", X_test.shape)\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n"
     ]
    }
   ],
   "source": [
    "train, train_labels, test, sample_submission = read_data()\n",
    "train = remove_data(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid = encode_title(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297c61c4da741498f0e703e35b10ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=3614, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(17690, 860)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e040f577e0324b02834b9971ad19c305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=1000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1000, 860)\n"
     ]
    }
   ],
   "source": [
    "new_train, new_test = make_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (17690, 834)\n",
      "test:  (1000, 833)\n",
      "train:  (17690, 838)\n",
      "test:  (1000, 837)\n"
     ]
    }
   ],
   "source": [
    "X_train_lgb, y_train_lgb, X_test_lgb = post_process(new_train, new_test)\n",
    "X_train_lr, y_train_lr, X_test_lr = post_process(new_train, new_test, stand_flg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lgb.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_lgb.columns]\n",
    "X_train_lr.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_lr.columns]\n",
    "X_test_lgb.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_lgb.columns]\n",
    "X_test_lr.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_lr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(model_name, X_tr, y_tr, X_te):\n",
    "    n_folds=5\n",
    "    skf=StratifiedKFold(n_splits = n_folds)\n",
    "    coefficients = []\n",
    "    train_qwk_scores = []; test_qwk_scores = []; train_qwk_dist = []; test_qwk_dist = []\n",
    "    pred_value = np.zeros([X_te.shape[0]])\n",
    "\n",
    "    lgbm_params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"tree_learner\": \"serial\",\n",
    "        \"max_depth\" : 4,\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"num_leaves\" : 13,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        }\n",
    "\n",
    "    for i , (train_index, test_index) in enumerate(skf.split(X_tr, y_tr)):\n",
    "        optR = OptimizedRounder()\n",
    "        X_train2 = X_tr.iloc[train_index,:]\n",
    "        y_train2 = y_tr.iloc[train_index]\n",
    "        X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "        X_test2 = X_tr.iloc[test_index,:]\n",
    "        y_test2 = y_tr.iloc[test_index]\n",
    "        test2 = pd.concat([X_test2, y_test2], axis=1)\n",
    "        test2 = test2.groupby('installation_id').apply(lambda x: x.sample(1, random_state=1223)).reset_index(drop=True)\n",
    "        X_test2 = test2.drop([\"accuracy_group\", \"installation_id\"], axis=1)\n",
    "        y_test2 = test2[\"accuracy_group\"]\n",
    "    \n",
    "        if model_name == \"lgb\":\n",
    "            lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "            lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "    \n",
    "            clf = lgb.train(\n",
    "                lgbm_params, lgb_train,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                num_boost_round=100000,\n",
    "                early_stopping_rounds=10,\n",
    "        )\n",
    "            train_predict = clf.predict(X_train2, num_iteration = clf.best_iteration)\n",
    "            valid_predict = clf.predict(X_test2, num_iteration = clf.best_iteration)\n",
    "            pred_value += clf.predict(X_te, num_iteration = clf.best_iteration) / n_folds\n",
    "        \n",
    "        elif model_name == \"lr\":    \n",
    "            clf = LinearRegression()\n",
    "            clf.fit(X_train2, y_train2) \n",
    "            train_predict = clf.predict(X_train2)\n",
    "            valid_predict = clf.predict(X_test2)\n",
    "            pred_value += clf.predict(X_te) / n_folds\n",
    "        \n",
    "        elif model_name == \"nn\":\n",
    "            verbosity = 100\n",
    "            clf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train2.shape[1],)),\n",
    "                tf.keras.layers.Dense(200, activation='relu'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(100, activation='tanh'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                #tf.keras.layers.Dense(50, activation='relu'),\n",
    "                #tf.keras.layers.LayerNormalization(),\n",
    "                #tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(25, activation='relu'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(1, activation='relu')\n",
    "            ])\n",
    "            clf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "            save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "            \n",
    "            clf.fit(X_train2, \n",
    "                y_train2, \n",
    "                validation_data=(X_test2, y_test2),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "            clf.load_weights('./nn_model.w8')\n",
    "            train_predict = clf.predict(X_train2)\n",
    "            valid_predict = clf.predict(X_test2)\n",
    "            test_coefficients = np.mean(coefficients, axis=0)\n",
    "            pred_value += clf.predict(X_te).reshape(X_te.shape[0],) / n_folds\n",
    "    \n",
    "        optR.fit(valid_predict.reshape(-1,), y_test2)\n",
    "        tmp_coefficients = optR.coefficients()\n",
    "        print(\"fold_\"+str(i)+\" coefficients: \", tmp_coefficients)\n",
    "        opt_train_preds = optR.predict(train_predict.reshape(-1, ), tmp_coefficients)\n",
    "        train_qwk_score = qwk(y_train2, opt_train_preds)\n",
    "        opt_test_preds = optR.predict(valid_predict.reshape(-1, ), tmp_coefficients)\n",
    "        test_qwk_score = qwk(y_test2, opt_test_preds)\n",
    "        train_qwk_scores.append(train_qwk_score)\n",
    "        test_qwk_scores.append(test_qwk_score)\n",
    "        coefficients.append(tmp_coefficients)\n",
    "    \n",
    "        train_qwk_d = qwk(y_train2, eval_qwk_lgb_regr(train_predict, new_train))\n",
    "        test_qwk_d = qwk(y_test2, eval_qwk_lgb_regr(valid_predict, new_train))\n",
    "        train_qwk_dist.append(train_qwk_d)\n",
    "        test_qwk_dist.append(test_qwk_d)\n",
    "        \n",
    "    print(\"training qwk     : \", train_qwk_scores, np.mean(train_qwk_scores))\n",
    "    print(\"validation qwk   : \", test_qwk_scores, np.mean(test_qwk_scores))\n",
    "    print(\"train qwk by dist: \", train_qwk_dist, np.mean(train_qwk_dist))\n",
    "    print(\"valid qwk by dist: \", test_qwk_dist, np.mean(test_qwk_dist))\n",
    "\n",
    "    return pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's rmse: 1.25323\tvalid_1's rmse: 1.2734\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24974\tvalid_1's rmse: 1.27038\n",
      "[3]\ttraining's rmse: 1.24632\tvalid_1's rmse: 1.26739\n",
      "[4]\ttraining's rmse: 1.24296\tvalid_1's rmse: 1.26454\n",
      "[5]\ttraining's rmse: 1.23965\tvalid_1's rmse: 1.26166\n",
      "[6]\ttraining's rmse: 1.2364\tvalid_1's rmse: 1.2589\n",
      "[7]\ttraining's rmse: 1.23321\tvalid_1's rmse: 1.25613\n",
      "[8]\ttraining's rmse: 1.23007\tvalid_1's rmse: 1.25347\n",
      "[9]\ttraining's rmse: 1.22698\tvalid_1's rmse: 1.2508\n",
      "[10]\ttraining's rmse: 1.22395\tvalid_1's rmse: 1.24823\n",
      "[11]\ttraining's rmse: 1.22097\tvalid_1's rmse: 1.24566\n",
      "[12]\ttraining's rmse: 1.21805\tvalid_1's rmse: 1.24318\n",
      "[13]\ttraining's rmse: 1.2153\tvalid_1's rmse: 1.24079\n",
      "[14]\ttraining's rmse: 1.21259\tvalid_1's rmse: 1.23839\n",
      "[15]\ttraining's rmse: 1.2098\tvalid_1's rmse: 1.23596\n",
      "[16]\ttraining's rmse: 1.20707\tvalid_1's rmse: 1.23333\n",
      "[17]\ttraining's rmse: 1.20431\tvalid_1's rmse: 1.23102\n",
      "[18]\ttraining's rmse: 1.20166\tvalid_1's rmse: 1.22843\n",
      "[19]\ttraining's rmse: 1.199\tvalid_1's rmse: 1.22618\n",
      "[20]\ttraining's rmse: 1.19654\tvalid_1's rmse: 1.22406\n",
      "[21]\ttraining's rmse: 1.19396\tvalid_1's rmse: 1.22188\n",
      "[22]\ttraining's rmse: 1.19147\tvalid_1's rmse: 1.21947\n",
      "[23]\ttraining's rmse: 1.18897\tvalid_1's rmse: 1.21745\n",
      "[24]\ttraining's rmse: 1.18657\tvalid_1's rmse: 1.2151\n",
      "[25]\ttraining's rmse: 1.18415\tvalid_1's rmse: 1.21307\n",
      "[26]\ttraining's rmse: 1.18182\tvalid_1's rmse: 1.21082\n",
      "[27]\ttraining's rmse: 1.17953\tvalid_1's rmse: 1.20888\n",
      "[28]\ttraining's rmse: 1.17722\tvalid_1's rmse: 1.20699\n",
      "[29]\ttraining's rmse: 1.17501\tvalid_1's rmse: 1.20482\n",
      "[30]\ttraining's rmse: 1.17278\tvalid_1's rmse: 1.20303\n",
      "[31]\ttraining's rmse: 1.17064\tvalid_1's rmse: 1.20092\n",
      "[32]\ttraining's rmse: 1.16848\tvalid_1's rmse: 1.19916\n",
      "[33]\ttraining's rmse: 1.16641\tvalid_1's rmse: 1.19711\n",
      "[34]\ttraining's rmse: 1.16432\tvalid_1's rmse: 1.19547\n",
      "[35]\ttraining's rmse: 1.16231\tvalid_1's rmse: 1.1938\n",
      "[36]\ttraining's rmse: 1.16034\tvalid_1's rmse: 1.19192\n",
      "[37]\ttraining's rmse: 1.15835\tvalid_1's rmse: 1.19028\n",
      "[38]\ttraining's rmse: 1.15652\tvalid_1's rmse: 1.18871\n",
      "[39]\ttraining's rmse: 1.15464\tvalid_1's rmse: 1.18686\n",
      "[40]\ttraining's rmse: 1.15275\tvalid_1's rmse: 1.18532\n",
      "[41]\ttraining's rmse: 1.15089\tvalid_1's rmse: 1.18385\n",
      "[42]\ttraining's rmse: 1.1491\tvalid_1's rmse: 1.1821\n",
      "[43]\ttraining's rmse: 1.14735\tvalid_1's rmse: 1.1806\n",
      "[44]\ttraining's rmse: 1.14562\tvalid_1's rmse: 1.17887\n",
      "[45]\ttraining's rmse: 1.14388\tvalid_1's rmse: 1.17741\n",
      "[46]\ttraining's rmse: 1.1422\tvalid_1's rmse: 1.17581\n",
      "[47]\ttraining's rmse: 1.14052\tvalid_1's rmse: 1.1745\n",
      "[48]\ttraining's rmse: 1.13896\tvalid_1's rmse: 1.17318\n",
      "[49]\ttraining's rmse: 1.13731\tvalid_1's rmse: 1.17179\n",
      "[50]\ttraining's rmse: 1.13574\tvalid_1's rmse: 1.17022\n",
      "[51]\ttraining's rmse: 1.13414\tvalid_1's rmse: 1.16895\n",
      "[52]\ttraining's rmse: 1.1327\tvalid_1's rmse: 1.16782\n",
      "[53]\ttraining's rmse: 1.13131\tvalid_1's rmse: 1.16665\n",
      "[54]\ttraining's rmse: 1.12983\tvalid_1's rmse: 1.16549\n",
      "[55]\ttraining's rmse: 1.12848\tvalid_1's rmse: 1.16429\n",
      "[56]\ttraining's rmse: 1.12703\tvalid_1's rmse: 1.16293\n",
      "[57]\ttraining's rmse: 1.12572\tvalid_1's rmse: 1.16178\n",
      "[58]\ttraining's rmse: 1.12428\tvalid_1's rmse: 1.1606\n",
      "[59]\ttraining's rmse: 1.12301\tvalid_1's rmse: 1.15953\n",
      "[60]\ttraining's rmse: 1.12161\tvalid_1's rmse: 1.15836\n",
      "[61]\ttraining's rmse: 1.12038\tvalid_1's rmse: 1.15732\n",
      "[62]\ttraining's rmse: 1.11904\tvalid_1's rmse: 1.15599\n",
      "[63]\ttraining's rmse: 1.11785\tvalid_1's rmse: 1.15498\n",
      "[64]\ttraining's rmse: 1.11653\tvalid_1's rmse: 1.15398\n",
      "[65]\ttraining's rmse: 1.11538\tvalid_1's rmse: 1.15298\n",
      "[66]\ttraining's rmse: 1.1141\tvalid_1's rmse: 1.15206\n",
      "[67]\ttraining's rmse: 1.11298\tvalid_1's rmse: 1.15105\n",
      "[68]\ttraining's rmse: 1.11173\tvalid_1's rmse: 1.14983\n",
      "[69]\ttraining's rmse: 1.1105\tvalid_1's rmse: 1.14889\n",
      "[70]\ttraining's rmse: 1.10941\tvalid_1's rmse: 1.14791\n",
      "[71]\ttraining's rmse: 1.10821\tvalid_1's rmse: 1.14668\n",
      "[72]\ttraining's rmse: 1.10717\tvalid_1's rmse: 1.14573\n",
      "[73]\ttraining's rmse: 1.10595\tvalid_1's rmse: 1.1449\n",
      "[74]\ttraining's rmse: 1.10494\tvalid_1's rmse: 1.14405\n",
      "[75]\ttraining's rmse: 1.10374\tvalid_1's rmse: 1.14314\n",
      "[76]\ttraining's rmse: 1.10261\tvalid_1's rmse: 1.14196\n",
      "[77]\ttraining's rmse: 1.10164\tvalid_1's rmse: 1.14113\n",
      "[78]\ttraining's rmse: 1.10051\tvalid_1's rmse: 1.14033\n",
      "[79]\ttraining's rmse: 1.09956\tvalid_1's rmse: 1.13951\n",
      "[80]\ttraining's rmse: 1.09849\tvalid_1's rmse: 1.1384\n",
      "[81]\ttraining's rmse: 1.09757\tvalid_1's rmse: 1.13764\n",
      "[82]\ttraining's rmse: 1.09649\tvalid_1's rmse: 1.13683\n",
      "[83]\ttraining's rmse: 1.09546\tvalid_1's rmse: 1.13602\n",
      "[84]\ttraining's rmse: 1.09458\tvalid_1's rmse: 1.13528\n",
      "[85]\ttraining's rmse: 1.09357\tvalid_1's rmse: 1.13432\n",
      "[86]\ttraining's rmse: 1.09259\tvalid_1's rmse: 1.13363\n",
      "[87]\ttraining's rmse: 1.09173\tvalid_1's rmse: 1.13291\n",
      "[88]\ttraining's rmse: 1.09071\tvalid_1's rmse: 1.13219\n",
      "[89]\ttraining's rmse: 1.08975\tvalid_1's rmse: 1.13126\n",
      "[90]\ttraining's rmse: 1.08882\tvalid_1's rmse: 1.13057\n",
      "[91]\ttraining's rmse: 1.08802\tvalid_1's rmse: 1.12991\n",
      "[92]\ttraining's rmse: 1.0871\tvalid_1's rmse: 1.12921\n",
      "[93]\ttraining's rmse: 1.08619\tvalid_1's rmse: 1.12828\n",
      "[94]\ttraining's rmse: 1.08542\tvalid_1's rmse: 1.1276\n",
      "[95]\ttraining's rmse: 1.08458\tvalid_1's rmse: 1.12694\n",
      "[96]\ttraining's rmse: 1.08383\tvalid_1's rmse: 1.12636\n",
      "[97]\ttraining's rmse: 1.08296\tvalid_1's rmse: 1.12546\n",
      "[98]\ttraining's rmse: 1.08204\tvalid_1's rmse: 1.12484\n",
      "[99]\ttraining's rmse: 1.08125\tvalid_1's rmse: 1.12422\n",
      "[100]\ttraining's rmse: 1.08053\tvalid_1's rmse: 1.12362\n",
      "[101]\ttraining's rmse: 1.07973\tvalid_1's rmse: 1.12302\n",
      "[102]\ttraining's rmse: 1.0789\tvalid_1's rmse: 1.12214\n",
      "[103]\ttraining's rmse: 1.07822\tvalid_1's rmse: 1.12163\n",
      "[104]\ttraining's rmse: 1.07748\tvalid_1's rmse: 1.12111\n",
      "[105]\ttraining's rmse: 1.07681\tvalid_1's rmse: 1.12064\n",
      "[106]\ttraining's rmse: 1.07597\tvalid_1's rmse: 1.11991\n",
      "[107]\ttraining's rmse: 1.07519\tvalid_1's rmse: 1.11912\n",
      "[108]\ttraining's rmse: 1.07431\tvalid_1's rmse: 1.11849\n",
      "[109]\ttraining's rmse: 1.07361\tvalid_1's rmse: 1.11801\n",
      "[110]\ttraining's rmse: 1.07298\tvalid_1's rmse: 1.1175\n",
      "[111]\ttraining's rmse: 1.07231\tvalid_1's rmse: 1.11694\n",
      "[112]\ttraining's rmse: 1.07158\tvalid_1's rmse: 1.11612\n",
      "[113]\ttraining's rmse: 1.07097\tvalid_1's rmse: 1.11565\n",
      "[114]\ttraining's rmse: 1.07033\tvalid_1's rmse: 1.11515\n",
      "[115]\ttraining's rmse: 1.06957\tvalid_1's rmse: 1.1146\n",
      "[116]\ttraining's rmse: 1.06888\tvalid_1's rmse: 1.11407\n",
      "[117]\ttraining's rmse: 1.06826\tvalid_1's rmse: 1.11358\n",
      "[118]\ttraining's rmse: 1.06766\tvalid_1's rmse: 1.11315\n",
      "[119]\ttraining's rmse: 1.06692\tvalid_1's rmse: 1.11252\n",
      "[120]\ttraining's rmse: 1.06625\tvalid_1's rmse: 1.11184\n",
      "[121]\ttraining's rmse: 1.06548\tvalid_1's rmse: 1.11131\n",
      "[122]\ttraining's rmse: 1.06491\tvalid_1's rmse: 1.11089\n",
      "[123]\ttraining's rmse: 1.06435\tvalid_1's rmse: 1.11036\n",
      "[124]\ttraining's rmse: 1.06371\tvalid_1's rmse: 1.1097\n",
      "[125]\ttraining's rmse: 1.06297\tvalid_1's rmse: 1.1092\n",
      "[126]\ttraining's rmse: 1.06241\tvalid_1's rmse: 1.10884\n",
      "[127]\ttraining's rmse: 1.06186\tvalid_1's rmse: 1.1084\n",
      "[128]\ttraining's rmse: 1.06112\tvalid_1's rmse: 1.10775\n",
      "[129]\ttraining's rmse: 1.06061\tvalid_1's rmse: 1.10742\n",
      "[130]\ttraining's rmse: 1.06005\tvalid_1's rmse: 1.10696\n",
      "[131]\ttraining's rmse: 1.05954\tvalid_1's rmse: 1.10664\n",
      "[132]\ttraining's rmse: 1.05894\tvalid_1's rmse: 1.10598\n",
      "[133]\ttraining's rmse: 1.05823\tvalid_1's rmse: 1.10536\n",
      "[134]\ttraining's rmse: 1.05773\tvalid_1's rmse: 1.10499\n",
      "[135]\ttraining's rmse: 1.05721\tvalid_1's rmse: 1.10462\n",
      "[136]\ttraining's rmse: 1.05673\tvalid_1's rmse: 1.1043\n",
      "[137]\ttraining's rmse: 1.05624\tvalid_1's rmse: 1.10392\n",
      "[138]\ttraining's rmse: 1.05574\tvalid_1's rmse: 1.10352\n",
      "[139]\ttraining's rmse: 1.0551\tvalid_1's rmse: 1.10305\n",
      "[140]\ttraining's rmse: 1.05456\tvalid_1's rmse: 1.10254\n",
      "[141]\ttraining's rmse: 1.05402\tvalid_1's rmse: 1.10205\n",
      "[142]\ttraining's rmse: 1.05358\tvalid_1's rmse: 1.10173\n",
      "[143]\ttraining's rmse: 1.05293\tvalid_1's rmse: 1.10117\n",
      "[144]\ttraining's rmse: 1.05247\tvalid_1's rmse: 1.10081\n",
      "[145]\ttraining's rmse: 1.05187\tvalid_1's rmse: 1.10041\n",
      "[146]\ttraining's rmse: 1.05137\tvalid_1's rmse: 1.09994\n",
      "[147]\ttraining's rmse: 1.05092\tvalid_1's rmse: 1.09961\n",
      "[148]\ttraining's rmse: 1.0505\tvalid_1's rmse: 1.09927\n",
      "[149]\ttraining's rmse: 1.04989\tvalid_1's rmse: 1.09871\n",
      "[150]\ttraining's rmse: 1.04949\tvalid_1's rmse: 1.09838\n",
      "[151]\ttraining's rmse: 1.049\tvalid_1's rmse: 1.09795\n",
      "[152]\ttraining's rmse: 1.04853\tvalid_1's rmse: 1.09751\n",
      "[153]\ttraining's rmse: 1.04814\tvalid_1's rmse: 1.09725\n",
      "[154]\ttraining's rmse: 1.04773\tvalid_1's rmse: 1.09695\n",
      "[155]\ttraining's rmse: 1.04734\tvalid_1's rmse: 1.09669\n",
      "[156]\ttraining's rmse: 1.04678\tvalid_1's rmse: 1.09645\n",
      "[157]\ttraining's rmse: 1.0463\tvalid_1's rmse: 1.09607\n",
      "[158]\ttraining's rmse: 1.04585\tvalid_1's rmse: 1.0957\n",
      "[159]\ttraining's rmse: 1.04548\tvalid_1's rmse: 1.09539\n",
      "[160]\ttraining's rmse: 1.04506\tvalid_1's rmse: 1.09501\n",
      "[161]\ttraining's rmse: 1.04466\tvalid_1's rmse: 1.09474\n",
      "[162]\ttraining's rmse: 1.0443\tvalid_1's rmse: 1.0944\n",
      "[163]\ttraining's rmse: 1.04376\tvalid_1's rmse: 1.09389\n",
      "[164]\ttraining's rmse: 1.04331\tvalid_1's rmse: 1.09354\n",
      "[165]\ttraining's rmse: 1.04294\tvalid_1's rmse: 1.09335\n",
      "[166]\ttraining's rmse: 1.04244\tvalid_1's rmse: 1.09301\n",
      "[167]\ttraining's rmse: 1.04203\tvalid_1's rmse: 1.09262\n",
      "[168]\ttraining's rmse: 1.04169\tvalid_1's rmse: 1.09245\n",
      "[169]\ttraining's rmse: 1.04127\tvalid_1's rmse: 1.09206\n",
      "[170]\ttraining's rmse: 1.04094\tvalid_1's rmse: 1.0919\n",
      "[171]\ttraining's rmse: 1.04052\tvalid_1's rmse: 1.09154\n",
      "[172]\ttraining's rmse: 1.04003\tvalid_1's rmse: 1.09133\n",
      "[173]\ttraining's rmse: 1.03968\tvalid_1's rmse: 1.09108\n",
      "[174]\ttraining's rmse: 1.03919\tvalid_1's rmse: 1.09071\n",
      "[175]\ttraining's rmse: 1.03887\tvalid_1's rmse: 1.09058\n",
      "[176]\ttraining's rmse: 1.03834\tvalid_1's rmse: 1.09005\n",
      "[177]\ttraining's rmse: 1.03799\tvalid_1's rmse: 1.08984\n",
      "[178]\ttraining's rmse: 1.03763\tvalid_1's rmse: 1.08952\n",
      "[179]\ttraining's rmse: 1.03729\tvalid_1's rmse: 1.08933\n",
      "[180]\ttraining's rmse: 1.03689\tvalid_1's rmse: 1.08894\n",
      "[181]\ttraining's rmse: 1.03658\tvalid_1's rmse: 1.08874\n",
      "[182]\ttraining's rmse: 1.03619\tvalid_1's rmse: 1.08835\n",
      "[183]\ttraining's rmse: 1.03581\tvalid_1's rmse: 1.08798\n",
      "[184]\ttraining's rmse: 1.03544\tvalid_1's rmse: 1.08765\n",
      "[185]\ttraining's rmse: 1.03506\tvalid_1's rmse: 1.08728\n",
      "[186]\ttraining's rmse: 1.0345\tvalid_1's rmse: 1.0867\n",
      "[187]\ttraining's rmse: 1.03414\tvalid_1's rmse: 1.08637\n",
      "[188]\ttraining's rmse: 1.03379\tvalid_1's rmse: 1.08602\n",
      "[189]\ttraining's rmse: 1.03324\tvalid_1's rmse: 1.08548\n",
      "[190]\ttraining's rmse: 1.03292\tvalid_1's rmse: 1.08533\n",
      "[191]\ttraining's rmse: 1.03239\tvalid_1's rmse: 1.08481\n",
      "[192]\ttraining's rmse: 1.03205\tvalid_1's rmse: 1.08456\n",
      "[193]\ttraining's rmse: 1.03171\tvalid_1's rmse: 1.08438\n",
      "[194]\ttraining's rmse: 1.03139\tvalid_1's rmse: 1.0842\n",
      "[195]\ttraining's rmse: 1.03106\tvalid_1's rmse: 1.08396\n",
      "[196]\ttraining's rmse: 1.03072\tvalid_1's rmse: 1.08366\n",
      "[197]\ttraining's rmse: 1.03039\tvalid_1's rmse: 1.08348\n",
      "[198]\ttraining's rmse: 1.03007\tvalid_1's rmse: 1.08317\n",
      "[199]\ttraining's rmse: 1.02972\tvalid_1's rmse: 1.08292\n",
      "[200]\ttraining's rmse: 1.02939\tvalid_1's rmse: 1.08258\n",
      "[201]\ttraining's rmse: 1.02907\tvalid_1's rmse: 1.0824\n",
      "[202]\ttraining's rmse: 1.02874\tvalid_1's rmse: 1.08227\n",
      "[203]\ttraining's rmse: 1.02843\tvalid_1's rmse: 1.082\n",
      "[204]\ttraining's rmse: 1.0281\tvalid_1's rmse: 1.0818\n",
      "[205]\ttraining's rmse: 1.02761\tvalid_1's rmse: 1.08131\n",
      "[206]\ttraining's rmse: 1.02732\tvalid_1's rmse: 1.08117\n",
      "[207]\ttraining's rmse: 1.02702\tvalid_1's rmse: 1.08103\n",
      "[208]\ttraining's rmse: 1.02668\tvalid_1's rmse: 1.08086\n",
      "[209]\ttraining's rmse: 1.02634\tvalid_1's rmse: 1.0807\n",
      "[210]\ttraining's rmse: 1.02587\tvalid_1's rmse: 1.08019\n",
      "[211]\ttraining's rmse: 1.02556\tvalid_1's rmse: 1.08011\n",
      "[212]\ttraining's rmse: 1.0251\tvalid_1's rmse: 1.07964\n",
      "[213]\ttraining's rmse: 1.02477\tvalid_1's rmse: 1.07952\n",
      "[214]\ttraining's rmse: 1.02444\tvalid_1's rmse: 1.07942\n",
      "[215]\ttraining's rmse: 1.02415\tvalid_1's rmse: 1.07929\n",
      "[216]\ttraining's rmse: 1.02383\tvalid_1's rmse: 1.07918\n",
      "[217]\ttraining's rmse: 1.02339\tvalid_1's rmse: 1.07871\n",
      "[218]\ttraining's rmse: 1.02307\tvalid_1's rmse: 1.07856\n",
      "[219]\ttraining's rmse: 1.02277\tvalid_1's rmse: 1.07844\n",
      "[220]\ttraining's rmse: 1.02233\tvalid_1's rmse: 1.078\n",
      "[221]\ttraining's rmse: 1.02202\tvalid_1's rmse: 1.07786\n",
      "[222]\ttraining's rmse: 1.02173\tvalid_1's rmse: 1.07774\n",
      "[223]\ttraining's rmse: 1.0213\tvalid_1's rmse: 1.07731\n",
      "[224]\ttraining's rmse: 1.02105\tvalid_1's rmse: 1.07718\n",
      "[225]\ttraining's rmse: 1.02074\tvalid_1's rmse: 1.07704\n",
      "[226]\ttraining's rmse: 1.02033\tvalid_1's rmse: 1.07659\n",
      "[227]\ttraining's rmse: 1.02006\tvalid_1's rmse: 1.07644\n",
      "[228]\ttraining's rmse: 1.01976\tvalid_1's rmse: 1.0763\n",
      "[229]\ttraining's rmse: 1.01946\tvalid_1's rmse: 1.07609\n",
      "[230]\ttraining's rmse: 1.01917\tvalid_1's rmse: 1.07598\n",
      "[231]\ttraining's rmse: 1.01876\tvalid_1's rmse: 1.07559\n",
      "[232]\ttraining's rmse: 1.01846\tvalid_1's rmse: 1.07547\n",
      "[233]\ttraining's rmse: 1.01817\tvalid_1's rmse: 1.07534\n",
      "[234]\ttraining's rmse: 1.01789\tvalid_1's rmse: 1.07525\n",
      "[235]\ttraining's rmse: 1.0175\tvalid_1's rmse: 1.07488\n",
      "[236]\ttraining's rmse: 1.01722\tvalid_1's rmse: 1.07475\n",
      "[237]\ttraining's rmse: 1.01698\tvalid_1's rmse: 1.07464\n",
      "[238]\ttraining's rmse: 1.0167\tvalid_1's rmse: 1.07452\n",
      "[239]\ttraining's rmse: 1.01643\tvalid_1's rmse: 1.07445\n",
      "[240]\ttraining's rmse: 1.01605\tvalid_1's rmse: 1.07405\n",
      "[241]\ttraining's rmse: 1.01578\tvalid_1's rmse: 1.07392\n",
      "[242]\ttraining's rmse: 1.01544\tvalid_1's rmse: 1.07363\n",
      "[243]\ttraining's rmse: 1.01518\tvalid_1's rmse: 1.07355\n",
      "[244]\ttraining's rmse: 1.01491\tvalid_1's rmse: 1.0734\n",
      "[245]\ttraining's rmse: 1.01465\tvalid_1's rmse: 1.07327\n",
      "[246]\ttraining's rmse: 1.01443\tvalid_1's rmse: 1.0731\n",
      "[247]\ttraining's rmse: 1.01406\tvalid_1's rmse: 1.07273\n",
      "[248]\ttraining's rmse: 1.01384\tvalid_1's rmse: 1.07264\n",
      "[249]\ttraining's rmse: 1.01358\tvalid_1's rmse: 1.07253\n",
      "[250]\ttraining's rmse: 1.01332\tvalid_1's rmse: 1.07249\n",
      "[251]\ttraining's rmse: 1.01307\tvalid_1's rmse: 1.07236\n",
      "[252]\ttraining's rmse: 1.01278\tvalid_1's rmse: 1.07221\n",
      "[253]\ttraining's rmse: 1.01246\tvalid_1's rmse: 1.07193\n",
      "[254]\ttraining's rmse: 1.0122\tvalid_1's rmse: 1.07188\n",
      "[255]\ttraining's rmse: 1.01194\tvalid_1's rmse: 1.07183\n",
      "[256]\ttraining's rmse: 1.01171\tvalid_1's rmse: 1.07161\n",
      "[257]\ttraining's rmse: 1.01146\tvalid_1's rmse: 1.07159\n",
      "[258]\ttraining's rmse: 1.01118\tvalid_1's rmse: 1.07145\n",
      "[259]\ttraining's rmse: 1.01093\tvalid_1's rmse: 1.07133\n",
      "[260]\ttraining's rmse: 1.01071\tvalid_1's rmse: 1.0712\n",
      "[261]\ttraining's rmse: 1.01043\tvalid_1's rmse: 1.07107\n",
      "[262]\ttraining's rmse: 1.01013\tvalid_1's rmse: 1.07079\n",
      "[263]\ttraining's rmse: 1.00989\tvalid_1's rmse: 1.07075\n",
      "[264]\ttraining's rmse: 1.00964\tvalid_1's rmse: 1.07071\n",
      "[265]\ttraining's rmse: 1.00942\tvalid_1's rmse: 1.07051\n",
      "[266]\ttraining's rmse: 1.00919\tvalid_1's rmse: 1.0705\n",
      "[267]\ttraining's rmse: 1.00884\tvalid_1's rmse: 1.07016\n",
      "[268]\ttraining's rmse: 1.00861\tvalid_1's rmse: 1.07001\n",
      "[269]\ttraining's rmse: 1.00827\tvalid_1's rmse: 1.06966\n",
      "[270]\ttraining's rmse: 1.00803\tvalid_1's rmse: 1.0696\n",
      "[271]\ttraining's rmse: 1.00781\tvalid_1's rmse: 1.06952\n",
      "[272]\ttraining's rmse: 1.00759\tvalid_1's rmse: 1.0694\n",
      "[273]\ttraining's rmse: 1.00726\tvalid_1's rmse: 1.06909\n",
      "[274]\ttraining's rmse: 1.00698\tvalid_1's rmse: 1.06882\n",
      "[275]\ttraining's rmse: 1.00677\tvalid_1's rmse: 1.06865\n",
      "[276]\ttraining's rmse: 1.00654\tvalid_1's rmse: 1.06856\n",
      "[277]\ttraining's rmse: 1.00631\tvalid_1's rmse: 1.06846\n",
      "[278]\ttraining's rmse: 1.0061\tvalid_1's rmse: 1.06834\n",
      "[279]\ttraining's rmse: 1.00587\tvalid_1's rmse: 1.06828\n",
      "[280]\ttraining's rmse: 1.00565\tvalid_1's rmse: 1.06825\n",
      "[281]\ttraining's rmse: 1.00545\tvalid_1's rmse: 1.06818\n",
      "[282]\ttraining's rmse: 1.00525\tvalid_1's rmse: 1.0681\n",
      "[283]\ttraining's rmse: 1.00493\tvalid_1's rmse: 1.06778\n",
      "[284]\ttraining's rmse: 1.00472\tvalid_1's rmse: 1.06771\n",
      "[285]\ttraining's rmse: 1.0045\tvalid_1's rmse: 1.0676\n",
      "[286]\ttraining's rmse: 1.00431\tvalid_1's rmse: 1.06749\n",
      "[287]\ttraining's rmse: 1.00408\tvalid_1's rmse: 1.0674\n",
      "[288]\ttraining's rmse: 1.00379\tvalid_1's rmse: 1.0671\n",
      "[289]\ttraining's rmse: 1.00358\tvalid_1's rmse: 1.06709\n",
      "[290]\ttraining's rmse: 1.00326\tvalid_1's rmse: 1.06683\n",
      "[291]\ttraining's rmse: 1.00306\tvalid_1's rmse: 1.06676\n",
      "[292]\ttraining's rmse: 1.00285\tvalid_1's rmse: 1.06669\n",
      "[293]\ttraining's rmse: 1.0026\tvalid_1's rmse: 1.06645\n",
      "[294]\ttraining's rmse: 1.00237\tvalid_1's rmse: 1.06626\n",
      "[295]\ttraining's rmse: 1.00216\tvalid_1's rmse: 1.06615\n",
      "[296]\ttraining's rmse: 1.00197\tvalid_1's rmse: 1.06605\n",
      "[297]\ttraining's rmse: 1.00178\tvalid_1's rmse: 1.06592\n",
      "[298]\ttraining's rmse: 1.00147\tvalid_1's rmse: 1.06567\n",
      "[299]\ttraining's rmse: 1.00119\tvalid_1's rmse: 1.06543\n",
      "[300]\ttraining's rmse: 1.00099\tvalid_1's rmse: 1.06531\n",
      "[301]\ttraining's rmse: 1.00079\tvalid_1's rmse: 1.06528\n",
      "[302]\ttraining's rmse: 1.00059\tvalid_1's rmse: 1.06518\n",
      "[303]\ttraining's rmse: 1.00039\tvalid_1's rmse: 1.06509\n",
      "[304]\ttraining's rmse: 1.00018\tvalid_1's rmse: 1.065\n",
      "[305]\ttraining's rmse: 0.999994\tvalid_1's rmse: 1.06492\n",
      "[306]\ttraining's rmse: 0.999782\tvalid_1's rmse: 1.06491\n",
      "[307]\ttraining's rmse: 0.999501\tvalid_1's rmse: 1.06468\n",
      "[308]\ttraining's rmse: 0.999297\tvalid_1's rmse: 1.06466\n",
      "[309]\ttraining's rmse: 0.999094\tvalid_1's rmse: 1.06459\n",
      "[310]\ttraining's rmse: 0.998826\tvalid_1's rmse: 1.06436\n",
      "[311]\ttraining's rmse: 0.998647\tvalid_1's rmse: 1.06425\n",
      "[312]\ttraining's rmse: 0.998453\tvalid_1's rmse: 1.06416\n",
      "[313]\ttraining's rmse: 0.998248\tvalid_1's rmse: 1.0641\n",
      "[314]\ttraining's rmse: 0.998044\tvalid_1's rmse: 1.0641\n",
      "[315]\ttraining's rmse: 0.997775\tvalid_1's rmse: 1.06389\n",
      "[316]\ttraining's rmse: 0.9976\tvalid_1's rmse: 1.06381\n",
      "[317]\ttraining's rmse: 0.997342\tvalid_1's rmse: 1.06359\n",
      "[318]\ttraining's rmse: 0.997164\tvalid_1's rmse: 1.06356\n",
      "[319]\ttraining's rmse: 0.996962\tvalid_1's rmse: 1.06352\n",
      "[320]\ttraining's rmse: 0.996776\tvalid_1's rmse: 1.06345\n",
      "[321]\ttraining's rmse: 0.996516\tvalid_1's rmse: 1.06325\n",
      "[322]\ttraining's rmse: 0.996313\tvalid_1's rmse: 1.06323\n",
      "[323]\ttraining's rmse: 0.996142\tvalid_1's rmse: 1.06314\n",
      "[324]\ttraining's rmse: 0.995884\tvalid_1's rmse: 1.06287\n",
      "[325]\ttraining's rmse: 0.995718\tvalid_1's rmse: 1.06278\n",
      "[326]\ttraining's rmse: 0.995532\tvalid_1's rmse: 1.06271\n",
      "[327]\ttraining's rmse: 0.995367\tvalid_1's rmse: 1.06263\n",
      "[328]\ttraining's rmse: 0.995171\tvalid_1's rmse: 1.06263\n",
      "[329]\ttraining's rmse: 0.995006\tvalid_1's rmse: 1.06256\n",
      "[330]\ttraining's rmse: 0.994758\tvalid_1's rmse: 1.06236\n",
      "[331]\ttraining's rmse: 0.99457\tvalid_1's rmse: 1.06238\n",
      "[332]\ttraining's rmse: 0.994324\tvalid_1's rmse: 1.06218\n",
      "[333]\ttraining's rmse: 0.994137\tvalid_1's rmse: 1.06214\n",
      "[334]\ttraining's rmse: 0.993968\tvalid_1's rmse: 1.06206\n",
      "[335]\ttraining's rmse: 0.993815\tvalid_1's rmse: 1.06197\n",
      "[336]\ttraining's rmse: 0.993637\tvalid_1's rmse: 1.06196\n",
      "[337]\ttraining's rmse: 0.993453\tvalid_1's rmse: 1.06193\n",
      "[338]\ttraining's rmse: 0.993209\tvalid_1's rmse: 1.0617\n",
      "[339]\ttraining's rmse: 0.992975\tvalid_1's rmse: 1.0615\n",
      "[340]\ttraining's rmse: 0.992789\tvalid_1's rmse: 1.0615\n",
      "[341]\ttraining's rmse: 0.992628\tvalid_1's rmse: 1.0614\n",
      "[342]\ttraining's rmse: 0.992447\tvalid_1's rmse: 1.06136\n",
      "[343]\ttraining's rmse: 0.992225\tvalid_1's rmse: 1.0613\n",
      "[344]\ttraining's rmse: 0.99208\tvalid_1's rmse: 1.06128\n",
      "[345]\ttraining's rmse: 0.991844\tvalid_1's rmse: 1.0611\n",
      "[346]\ttraining's rmse: 0.991667\tvalid_1's rmse: 1.06107\n",
      "[347]\ttraining's rmse: 0.991497\tvalid_1's rmse: 1.06104\n",
      "[348]\ttraining's rmse: 0.99128\tvalid_1's rmse: 1.06091\n",
      "[349]\ttraining's rmse: 0.991117\tvalid_1's rmse: 1.06089\n",
      "[350]\ttraining's rmse: 0.990905\tvalid_1's rmse: 1.06075\n",
      "[351]\ttraining's rmse: 0.990734\tvalid_1's rmse: 1.06074\n",
      "[352]\ttraining's rmse: 0.990515\tvalid_1's rmse: 1.06051\n",
      "[353]\ttraining's rmse: 0.990375\tvalid_1's rmse: 1.0604\n",
      "[354]\ttraining's rmse: 0.990192\tvalid_1's rmse: 1.06041\n",
      "[355]\ttraining's rmse: 0.98997\tvalid_1's rmse: 1.06026\n",
      "[356]\ttraining's rmse: 0.989817\tvalid_1's rmse: 1.06017\n",
      "[357]\ttraining's rmse: 0.989609\tvalid_1's rmse: 1.06012\n",
      "[358]\ttraining's rmse: 0.989441\tvalid_1's rmse: 1.06015\n",
      "[359]\ttraining's rmse: 0.989283\tvalid_1's rmse: 1.06003\n",
      "[360]\ttraining's rmse: 0.989102\tvalid_1's rmse: 1.06003\n",
      "[361]\ttraining's rmse: 0.988891\tvalid_1's rmse: 1.05981\n",
      "[362]\ttraining's rmse: 0.988723\tvalid_1's rmse: 1.05978\n",
      "[363]\ttraining's rmse: 0.988522\tvalid_1's rmse: 1.05973\n",
      "[364]\ttraining's rmse: 0.988314\tvalid_1's rmse: 1.05961\n",
      "[365]\ttraining's rmse: 0.988146\tvalid_1's rmse: 1.05961\n",
      "[366]\ttraining's rmse: 0.987942\tvalid_1's rmse: 1.05945\n",
      "[367]\ttraining's rmse: 0.987799\tvalid_1's rmse: 1.05949\n",
      "[368]\ttraining's rmse: 0.987635\tvalid_1's rmse: 1.05945\n",
      "[369]\ttraining's rmse: 0.987422\tvalid_1's rmse: 1.05925\n",
      "[370]\ttraining's rmse: 0.987248\tvalid_1's rmse: 1.05925\n",
      "[371]\ttraining's rmse: 0.987054\tvalid_1's rmse: 1.05925\n",
      "[372]\ttraining's rmse: 0.986909\tvalid_1's rmse: 1.0592\n",
      "[373]\ttraining's rmse: 0.986711\tvalid_1's rmse: 1.05906\n",
      "[374]\ttraining's rmse: 0.986551\tvalid_1's rmse: 1.05909\n",
      "[375]\ttraining's rmse: 0.986341\tvalid_1's rmse: 1.05887\n",
      "[376]\ttraining's rmse: 0.986183\tvalid_1's rmse: 1.05885\n",
      "[377]\ttraining's rmse: 0.986017\tvalid_1's rmse: 1.05886\n",
      "[378]\ttraining's rmse: 0.985876\tvalid_1's rmse: 1.05881\n",
      "[379]\ttraining's rmse: 0.985679\tvalid_1's rmse: 1.05868\n",
      "[380]\ttraining's rmse: 0.985494\tvalid_1's rmse: 1.05865\n",
      "[381]\ttraining's rmse: 0.985296\tvalid_1's rmse: 1.05845\n",
      "[382]\ttraining's rmse: 0.985151\tvalid_1's rmse: 1.05837\n",
      "[383]\ttraining's rmse: 0.984997\tvalid_1's rmse: 1.05836\n",
      "[384]\ttraining's rmse: 0.984842\tvalid_1's rmse: 1.05837\n",
      "[385]\ttraining's rmse: 0.984674\tvalid_1's rmse: 1.05835\n",
      "[386]\ttraining's rmse: 0.984541\tvalid_1's rmse: 1.05831\n",
      "[387]\ttraining's rmse: 0.984387\tvalid_1's rmse: 1.05824\n",
      "[388]\ttraining's rmse: 0.984241\tvalid_1's rmse: 1.05818\n",
      "[389]\ttraining's rmse: 0.984057\tvalid_1's rmse: 1.05811\n",
      "[390]\ttraining's rmse: 0.98387\tvalid_1's rmse: 1.05793\n",
      "[391]\ttraining's rmse: 0.983689\tvalid_1's rmse: 1.05788\n",
      "[392]\ttraining's rmse: 0.983548\tvalid_1's rmse: 1.0579\n",
      "[393]\ttraining's rmse: 0.983385\tvalid_1's rmse: 1.05788\n",
      "[394]\ttraining's rmse: 0.983214\tvalid_1's rmse: 1.05767\n",
      "[395]\ttraining's rmse: 0.983085\tvalid_1's rmse: 1.05757\n",
      "[396]\ttraining's rmse: 0.982912\tvalid_1's rmse: 1.05755\n",
      "[397]\ttraining's rmse: 0.982753\tvalid_1's rmse: 1.05751\n",
      "[398]\ttraining's rmse: 0.982607\tvalid_1's rmse: 1.05741\n",
      "[399]\ttraining's rmse: 0.982427\tvalid_1's rmse: 1.05735\n",
      "[400]\ttraining's rmse: 0.982287\tvalid_1's rmse: 1.05732\n",
      "[401]\ttraining's rmse: 0.982121\tvalid_1's rmse: 1.05714\n",
      "[402]\ttraining's rmse: 0.981977\tvalid_1's rmse: 1.05715\n",
      "[403]\ttraining's rmse: 0.981835\tvalid_1's rmse: 1.05706\n",
      "[404]\ttraining's rmse: 0.981649\tvalid_1's rmse: 1.05693\n",
      "[405]\ttraining's rmse: 0.981471\tvalid_1's rmse: 1.05692\n",
      "[406]\ttraining's rmse: 0.981321\tvalid_1's rmse: 1.05682\n",
      "[407]\ttraining's rmse: 0.981176\tvalid_1's rmse: 1.0568\n",
      "[408]\ttraining's rmse: 0.981055\tvalid_1's rmse: 1.0568\n",
      "[409]\ttraining's rmse: 0.980882\tvalid_1's rmse: 1.05677\n",
      "[410]\ttraining's rmse: 0.980791\tvalid_1's rmse: 1.05669\n",
      "[411]\ttraining's rmse: 0.980665\tvalid_1's rmse: 1.0567\n",
      "[412]\ttraining's rmse: 0.980533\tvalid_1's rmse: 1.05668\n",
      "[413]\ttraining's rmse: 0.980357\tvalid_1's rmse: 1.05653\n",
      "[414]\ttraining's rmse: 0.980222\tvalid_1's rmse: 1.05653\n",
      "[415]\ttraining's rmse: 0.980071\tvalid_1's rmse: 1.05651\n",
      "[416]\ttraining's rmse: 0.979898\tvalid_1's rmse: 1.05641\n",
      "[417]\ttraining's rmse: 0.979771\tvalid_1's rmse: 1.05638\n",
      "[418]\ttraining's rmse: 0.97965\tvalid_1's rmse: 1.05628\n",
      "[419]\ttraining's rmse: 0.979484\tvalid_1's rmse: 1.05627\n",
      "[420]\ttraining's rmse: 0.97937\tvalid_1's rmse: 1.05623\n",
      "[421]\ttraining's rmse: 0.979222\tvalid_1's rmse: 1.0562\n",
      "[422]\ttraining's rmse: 0.979092\tvalid_1's rmse: 1.05612\n",
      "[423]\ttraining's rmse: 0.978923\tvalid_1's rmse: 1.05596\n",
      "[424]\ttraining's rmse: 0.978782\tvalid_1's rmse: 1.05596\n",
      "[425]\ttraining's rmse: 0.97864\tvalid_1's rmse: 1.05584\n",
      "[426]\ttraining's rmse: 0.978514\tvalid_1's rmse: 1.05573\n",
      "[427]\ttraining's rmse: 0.978353\tvalid_1's rmse: 1.05572\n",
      "[428]\ttraining's rmse: 0.978203\tvalid_1's rmse: 1.05559\n",
      "[429]\ttraining's rmse: 0.978074\tvalid_1's rmse: 1.05551\n",
      "[430]\ttraining's rmse: 0.977938\tvalid_1's rmse: 1.05552\n",
      "[431]\ttraining's rmse: 0.977784\tvalid_1's rmse: 1.05547\n",
      "[432]\ttraining's rmse: 0.977624\tvalid_1's rmse: 1.0554\n",
      "[433]\ttraining's rmse: 0.977505\tvalid_1's rmse: 1.05541\n",
      "[434]\ttraining's rmse: 0.977382\tvalid_1's rmse: 1.05536\n",
      "[435]\ttraining's rmse: 0.977261\tvalid_1's rmse: 1.05528\n",
      "[436]\ttraining's rmse: 0.977087\tvalid_1's rmse: 1.05523\n",
      "[437]\ttraining's rmse: 0.976968\tvalid_1's rmse: 1.05522\n",
      "[438]\ttraining's rmse: 0.976856\tvalid_1's rmse: 1.05512\n",
      "[439]\ttraining's rmse: 0.976775\tvalid_1's rmse: 1.05503\n",
      "[440]\ttraining's rmse: 0.976621\tvalid_1's rmse: 1.05498\n",
      "[441]\ttraining's rmse: 0.976451\tvalid_1's rmse: 1.05494\n",
      "[442]\ttraining's rmse: 0.976322\tvalid_1's rmse: 1.05493\n",
      "[443]\ttraining's rmse: 0.97618\tvalid_1's rmse: 1.05487\n",
      "[444]\ttraining's rmse: 0.97605\tvalid_1's rmse: 1.05477\n",
      "[445]\ttraining's rmse: 0.975937\tvalid_1's rmse: 1.05472\n",
      "[446]\ttraining's rmse: 0.975787\tvalid_1's rmse: 1.05472\n",
      "[447]\ttraining's rmse: 0.97564\tvalid_1's rmse: 1.05468\n",
      "[448]\ttraining's rmse: 0.975525\tvalid_1's rmse: 1.05457\n",
      "[449]\ttraining's rmse: 0.975376\tvalid_1's rmse: 1.05452\n",
      "[450]\ttraining's rmse: 0.9752\tvalid_1's rmse: 1.0544\n",
      "[451]\ttraining's rmse: 0.975096\tvalid_1's rmse: 1.05437\n",
      "[452]\ttraining's rmse: 0.975021\tvalid_1's rmse: 1.05435\n",
      "[453]\ttraining's rmse: 0.974886\tvalid_1's rmse: 1.05435\n",
      "[454]\ttraining's rmse: 0.974785\tvalid_1's rmse: 1.05435\n",
      "[455]\ttraining's rmse: 0.974644\tvalid_1's rmse: 1.05431\n",
      "[456]\ttraining's rmse: 0.974521\tvalid_1's rmse: 1.05419\n",
      "[457]\ttraining's rmse: 0.974346\tvalid_1's rmse: 1.05405\n",
      "[458]\ttraining's rmse: 0.974217\tvalid_1's rmse: 1.05393\n",
      "[459]\ttraining's rmse: 0.974085\tvalid_1's rmse: 1.05396\n",
      "[460]\ttraining's rmse: 0.973951\tvalid_1's rmse: 1.05389\n",
      "[461]\ttraining's rmse: 0.973804\tvalid_1's rmse: 1.05377\n",
      "[462]\ttraining's rmse: 0.973697\tvalid_1's rmse: 1.05373\n",
      "[463]\ttraining's rmse: 0.973568\tvalid_1's rmse: 1.05376\n",
      "[464]\ttraining's rmse: 0.973426\tvalid_1's rmse: 1.05375\n",
      "[465]\ttraining's rmse: 0.973316\tvalid_1's rmse: 1.0537\n",
      "[466]\ttraining's rmse: 0.97318\tvalid_1's rmse: 1.05366\n",
      "[467]\ttraining's rmse: 0.973083\tvalid_1's rmse: 1.05369\n",
      "[468]\ttraining's rmse: 0.97297\tvalid_1's rmse: 1.05362\n",
      "[469]\ttraining's rmse: 0.972837\tvalid_1's rmse: 1.05358\n",
      "[470]\ttraining's rmse: 0.972745\tvalid_1's rmse: 1.05354\n",
      "[471]\ttraining's rmse: 0.972632\tvalid_1's rmse: 1.05343\n",
      "[472]\ttraining's rmse: 0.97253\tvalid_1's rmse: 1.0534\n",
      "[473]\ttraining's rmse: 0.972365\tvalid_1's rmse: 1.05329\n",
      "[474]\ttraining's rmse: 0.972241\tvalid_1's rmse: 1.05331\n",
      "[475]\ttraining's rmse: 0.972147\tvalid_1's rmse: 1.05321\n",
      "[476]\ttraining's rmse: 0.972017\tvalid_1's rmse: 1.05317\n",
      "[477]\ttraining's rmse: 0.971891\tvalid_1's rmse: 1.05317\n",
      "[478]\ttraining's rmse: 0.971793\tvalid_1's rmse: 1.05309\n",
      "[479]\ttraining's rmse: 0.971705\tvalid_1's rmse: 1.05306\n",
      "[480]\ttraining's rmse: 0.971545\tvalid_1's rmse: 1.05294\n",
      "[481]\ttraining's rmse: 0.971409\tvalid_1's rmse: 1.05281\n",
      "[482]\ttraining's rmse: 0.971338\tvalid_1's rmse: 1.05279\n",
      "[483]\ttraining's rmse: 0.971222\tvalid_1's rmse: 1.05275\n",
      "[484]\ttraining's rmse: 0.97111\tvalid_1's rmse: 1.05271\n",
      "[485]\ttraining's rmse: 0.970981\tvalid_1's rmse: 1.05272\n",
      "[486]\ttraining's rmse: 0.970865\tvalid_1's rmse: 1.05263\n",
      "[487]\ttraining's rmse: 0.97073\tvalid_1's rmse: 1.05261\n",
      "[488]\ttraining's rmse: 0.970618\tvalid_1's rmse: 1.05258\n",
      "[489]\ttraining's rmse: 0.970498\tvalid_1's rmse: 1.05257\n",
      "[490]\ttraining's rmse: 0.970353\tvalid_1's rmse: 1.05246\n",
      "[491]\ttraining's rmse: 0.970246\tvalid_1's rmse: 1.0524\n",
      "[492]\ttraining's rmse: 0.970142\tvalid_1's rmse: 1.05234\n",
      "[493]\ttraining's rmse: 0.970018\tvalid_1's rmse: 1.0523\n",
      "[494]\ttraining's rmse: 0.969903\tvalid_1's rmse: 1.05218\n",
      "[495]\ttraining's rmse: 0.969819\tvalid_1's rmse: 1.05215\n",
      "[496]\ttraining's rmse: 0.969655\tvalid_1's rmse: 1.05205\n",
      "[497]\ttraining's rmse: 0.969542\tvalid_1's rmse: 1.05199\n",
      "[498]\ttraining's rmse: 0.969414\tvalid_1's rmse: 1.05199\n",
      "[499]\ttraining's rmse: 0.969294\tvalid_1's rmse: 1.05199\n",
      "[500]\ttraining's rmse: 0.9692\tvalid_1's rmse: 1.05198\n",
      "[501]\ttraining's rmse: 0.969078\tvalid_1's rmse: 1.05191\n",
      "[502]\ttraining's rmse: 0.968964\tvalid_1's rmse: 1.05181\n",
      "[503]\ttraining's rmse: 0.968825\tvalid_1's rmse: 1.05171\n",
      "[504]\ttraining's rmse: 0.968742\tvalid_1's rmse: 1.05168\n",
      "[505]\ttraining's rmse: 0.968644\tvalid_1's rmse: 1.05164\n",
      "[506]\ttraining's rmse: 0.968529\tvalid_1's rmse: 1.05163\n",
      "[507]\ttraining's rmse: 0.968425\tvalid_1's rmse: 1.05156\n",
      "[508]\ttraining's rmse: 0.968318\tvalid_1's rmse: 1.05152\n",
      "[509]\ttraining's rmse: 0.968203\tvalid_1's rmse: 1.05151\n",
      "[510]\ttraining's rmse: 0.968084\tvalid_1's rmse: 1.0515\n",
      "[511]\ttraining's rmse: 0.967993\tvalid_1's rmse: 1.05148\n",
      "[512]\ttraining's rmse: 0.967899\tvalid_1's rmse: 1.05147\n",
      "[513]\ttraining's rmse: 0.9678\tvalid_1's rmse: 1.05143\n",
      "[514]\ttraining's rmse: 0.967651\tvalid_1's rmse: 1.05134\n",
      "[515]\ttraining's rmse: 0.967537\tvalid_1's rmse: 1.05133\n",
      "[516]\ttraining's rmse: 0.967459\tvalid_1's rmse: 1.05132\n",
      "[517]\ttraining's rmse: 0.967334\tvalid_1's rmse: 1.05122\n",
      "[518]\ttraining's rmse: 0.967201\tvalid_1's rmse: 1.05114\n",
      "[519]\ttraining's rmse: 0.967125\tvalid_1's rmse: 1.05113\n",
      "[520]\ttraining's rmse: 0.967016\tvalid_1's rmse: 1.05109\n",
      "[521]\ttraining's rmse: 0.966906\tvalid_1's rmse: 1.05106\n",
      "[522]\ttraining's rmse: 0.966791\tvalid_1's rmse: 1.05104\n",
      "[523]\ttraining's rmse: 0.966657\tvalid_1's rmse: 1.05096\n",
      "[524]\ttraining's rmse: 0.966548\tvalid_1's rmse: 1.05092\n",
      "[525]\ttraining's rmse: 0.966421\tvalid_1's rmse: 1.05092\n",
      "[526]\ttraining's rmse: 0.966325\tvalid_1's rmse: 1.05086\n",
      "[527]\ttraining's rmse: 0.966208\tvalid_1's rmse: 1.05085\n",
      "[528]\ttraining's rmse: 0.966108\tvalid_1's rmse: 1.0508\n",
      "[529]\ttraining's rmse: 0.966021\tvalid_1's rmse: 1.05078\n",
      "[530]\ttraining's rmse: 0.965916\tvalid_1's rmse: 1.05077\n",
      "[531]\ttraining's rmse: 0.96581\tvalid_1's rmse: 1.05077\n",
      "[532]\ttraining's rmse: 0.965683\tvalid_1's rmse: 1.0507\n",
      "[533]\ttraining's rmse: 0.965574\tvalid_1's rmse: 1.05062\n",
      "[534]\ttraining's rmse: 0.965455\tvalid_1's rmse: 1.0505\n",
      "[535]\ttraining's rmse: 0.965381\tvalid_1's rmse: 1.05049\n",
      "[536]\ttraining's rmse: 0.965289\tvalid_1's rmse: 1.05037\n",
      "[537]\ttraining's rmse: 0.965168\tvalid_1's rmse: 1.05036\n",
      "[538]\ttraining's rmse: 0.965108\tvalid_1's rmse: 1.05036\n",
      "[539]\ttraining's rmse: 0.964997\tvalid_1's rmse: 1.05034\n",
      "[540]\ttraining's rmse: 0.964912\tvalid_1's rmse: 1.05033\n",
      "[541]\ttraining's rmse: 0.964838\tvalid_1's rmse: 1.05029\n",
      "[542]\ttraining's rmse: 0.964751\tvalid_1's rmse: 1.05024\n",
      "[543]\ttraining's rmse: 0.964631\tvalid_1's rmse: 1.05023\n",
      "[544]\ttraining's rmse: 0.964513\tvalid_1's rmse: 1.05023\n",
      "[545]\ttraining's rmse: 0.964407\tvalid_1's rmse: 1.0502\n",
      "[546]\ttraining's rmse: 0.964281\tvalid_1's rmse: 1.05015\n",
      "[547]\ttraining's rmse: 0.964196\tvalid_1's rmse: 1.05011\n",
      "[548]\ttraining's rmse: 0.964084\tvalid_1's rmse: 1.05011\n",
      "[549]\ttraining's rmse: 0.963976\tvalid_1's rmse: 1.05008\n",
      "[550]\ttraining's rmse: 0.963855\tvalid_1's rmse: 1.05004\n",
      "[551]\ttraining's rmse: 0.963736\tvalid_1's rmse: 1.05001\n",
      "[552]\ttraining's rmse: 0.963652\tvalid_1's rmse: 1.05005\n",
      "[553]\ttraining's rmse: 0.963511\tvalid_1's rmse: 1.04998\n",
      "[554]\ttraining's rmse: 0.963409\tvalid_1's rmse: 1.04995\n",
      "[555]\ttraining's rmse: 0.963318\tvalid_1's rmse: 1.04991\n",
      "[556]\ttraining's rmse: 0.963225\tvalid_1's rmse: 1.04991\n",
      "[557]\ttraining's rmse: 0.963136\tvalid_1's rmse: 1.04985\n",
      "[558]\ttraining's rmse: 0.963041\tvalid_1's rmse: 1.04979\n",
      "[559]\ttraining's rmse: 0.962891\tvalid_1's rmse: 1.04974\n",
      "[560]\ttraining's rmse: 0.962776\tvalid_1's rmse: 1.04973\n",
      "[561]\ttraining's rmse: 0.962719\tvalid_1's rmse: 1.04974\n",
      "[562]\ttraining's rmse: 0.962618\tvalid_1's rmse: 1.04971\n",
      "[563]\ttraining's rmse: 0.962514\tvalid_1's rmse: 1.04962\n",
      "[564]\ttraining's rmse: 0.962431\tvalid_1's rmse: 1.04962\n",
      "[565]\ttraining's rmse: 0.962361\tvalid_1's rmse: 1.04958\n",
      "[566]\ttraining's rmse: 0.962224\tvalid_1's rmse: 1.0495\n",
      "[567]\ttraining's rmse: 0.962135\tvalid_1's rmse: 1.04947\n",
      "[568]\ttraining's rmse: 0.962017\tvalid_1's rmse: 1.04946\n",
      "[569]\ttraining's rmse: 0.961937\tvalid_1's rmse: 1.04937\n",
      "[570]\ttraining's rmse: 0.961833\tvalid_1's rmse: 1.04934\n",
      "[571]\ttraining's rmse: 0.961699\tvalid_1's rmse: 1.04934\n",
      "[572]\ttraining's rmse: 0.961601\tvalid_1's rmse: 1.04929\n",
      "[573]\ttraining's rmse: 0.961513\tvalid_1's rmse: 1.04924\n",
      "[574]\ttraining's rmse: 0.96143\tvalid_1's rmse: 1.0492\n",
      "[575]\ttraining's rmse: 0.96134\tvalid_1's rmse: 1.04926\n",
      "[576]\ttraining's rmse: 0.961235\tvalid_1's rmse: 1.04926\n",
      "[577]\ttraining's rmse: 0.961134\tvalid_1's rmse: 1.04917\n",
      "[578]\ttraining's rmse: 0.961056\tvalid_1's rmse: 1.04918\n",
      "[579]\ttraining's rmse: 0.96094\tvalid_1's rmse: 1.04914\n",
      "[580]\ttraining's rmse: 0.960832\tvalid_1's rmse: 1.04918\n",
      "[581]\ttraining's rmse: 0.960751\tvalid_1's rmse: 1.04914\n",
      "[582]\ttraining's rmse: 0.960684\tvalid_1's rmse: 1.0491\n",
      "[583]\ttraining's rmse: 0.96059\tvalid_1's rmse: 1.04909\n",
      "[584]\ttraining's rmse: 0.960489\tvalid_1's rmse: 1.04906\n",
      "[585]\ttraining's rmse: 0.96036\tvalid_1's rmse: 1.04907\n",
      "[586]\ttraining's rmse: 0.960203\tvalid_1's rmse: 1.049\n",
      "[587]\ttraining's rmse: 0.960108\tvalid_1's rmse: 1.04896\n",
      "[588]\ttraining's rmse: 0.959991\tvalid_1's rmse: 1.04899\n",
      "[589]\ttraining's rmse: 0.959881\tvalid_1's rmse: 1.04896\n",
      "[590]\ttraining's rmse: 0.959803\tvalid_1's rmse: 1.04892\n",
      "[591]\ttraining's rmse: 0.959699\tvalid_1's rmse: 1.04891\n",
      "[592]\ttraining's rmse: 0.959584\tvalid_1's rmse: 1.04884\n",
      "[593]\ttraining's rmse: 0.959487\tvalid_1's rmse: 1.04884\n",
      "[594]\ttraining's rmse: 0.959407\tvalid_1's rmse: 1.0488\n",
      "[595]\ttraining's rmse: 0.959304\tvalid_1's rmse: 1.04879\n",
      "[596]\ttraining's rmse: 0.959229\tvalid_1's rmse: 1.04886\n",
      "[597]\ttraining's rmse: 0.959109\tvalid_1's rmse: 1.04879\n",
      "[598]\ttraining's rmse: 0.959013\tvalid_1's rmse: 1.04875\n",
      "[599]\ttraining's rmse: 0.95889\tvalid_1's rmse: 1.04876\n",
      "[600]\ttraining's rmse: 0.958803\tvalid_1's rmse: 1.04873\n",
      "[601]\ttraining's rmse: 0.958704\tvalid_1's rmse: 1.04865\n",
      "[602]\ttraining's rmse: 0.958605\tvalid_1's rmse: 1.04869\n",
      "[603]\ttraining's rmse: 0.958532\tvalid_1's rmse: 1.04875\n",
      "[604]\ttraining's rmse: 0.958455\tvalid_1's rmse: 1.04879\n",
      "[605]\ttraining's rmse: 0.958339\tvalid_1's rmse: 1.0488\n",
      "[606]\ttraining's rmse: 0.958229\tvalid_1's rmse: 1.04872\n",
      "[607]\ttraining's rmse: 0.958157\tvalid_1's rmse: 1.04872\n",
      "[608]\ttraining's rmse: 0.958073\tvalid_1's rmse: 1.04865\n",
      "[609]\ttraining's rmse: 0.957982\tvalid_1's rmse: 1.04861\n",
      "[610]\ttraining's rmse: 0.957888\tvalid_1's rmse: 1.04858\n",
      "[611]\ttraining's rmse: 0.957787\tvalid_1's rmse: 1.04858\n",
      "[612]\ttraining's rmse: 0.95773\tvalid_1's rmse: 1.04854\n",
      "[613]\ttraining's rmse: 0.957634\tvalid_1's rmse: 1.04845\n",
      "[614]\ttraining's rmse: 0.957561\tvalid_1's rmse: 1.04844\n",
      "[615]\ttraining's rmse: 0.957449\tvalid_1's rmse: 1.04845\n",
      "[616]\ttraining's rmse: 0.957384\tvalid_1's rmse: 1.04845\n",
      "[617]\ttraining's rmse: 0.957277\tvalid_1's rmse: 1.04837\n",
      "[618]\ttraining's rmse: 0.957231\tvalid_1's rmse: 1.04832\n",
      "[619]\ttraining's rmse: 0.957118\tvalid_1's rmse: 1.04833\n",
      "[620]\ttraining's rmse: 0.957048\tvalid_1's rmse: 1.04838\n",
      "[621]\ttraining's rmse: 0.956984\tvalid_1's rmse: 1.04838\n",
      "[622]\ttraining's rmse: 0.956878\tvalid_1's rmse: 1.04828\n",
      "[623]\ttraining's rmse: 0.956788\tvalid_1's rmse: 1.04829\n",
      "[624]\ttraining's rmse: 0.956718\tvalid_1's rmse: 1.0483\n",
      "[625]\ttraining's rmse: 0.956608\tvalid_1's rmse: 1.04823\n",
      "[626]\ttraining's rmse: 0.956492\tvalid_1's rmse: 1.04818\n",
      "[627]\ttraining's rmse: 0.956385\tvalid_1's rmse: 1.04815\n",
      "[628]\ttraining's rmse: 0.95628\tvalid_1's rmse: 1.04816\n",
      "[629]\ttraining's rmse: 0.956235\tvalid_1's rmse: 1.04812\n",
      "[630]\ttraining's rmse: 0.95615\tvalid_1's rmse: 1.04813\n",
      "[631]\ttraining's rmse: 0.956047\tvalid_1's rmse: 1.04815\n",
      "[632]\ttraining's rmse: 0.955961\tvalid_1's rmse: 1.04811\n",
      "[633]\ttraining's rmse: 0.955853\tvalid_1's rmse: 1.04813\n",
      "[634]\ttraining's rmse: 0.955784\tvalid_1's rmse: 1.04819\n",
      "[635]\ttraining's rmse: 0.955688\tvalid_1's rmse: 1.04817\n",
      "[636]\ttraining's rmse: 0.955624\tvalid_1's rmse: 1.04811\n",
      "[637]\ttraining's rmse: 0.955521\tvalid_1's rmse: 1.04806\n",
      "[638]\ttraining's rmse: 0.955422\tvalid_1's rmse: 1.04808\n",
      "[639]\ttraining's rmse: 0.955321\tvalid_1's rmse: 1.04801\n",
      "[640]\ttraining's rmse: 0.955214\tvalid_1's rmse: 1.04796\n",
      "[641]\ttraining's rmse: 0.955152\tvalid_1's rmse: 1.04796\n",
      "[642]\ttraining's rmse: 0.955092\tvalid_1's rmse: 1.04798\n",
      "[643]\ttraining's rmse: 0.955041\tvalid_1's rmse: 1.04798\n",
      "[644]\ttraining's rmse: 0.954982\tvalid_1's rmse: 1.04796\n",
      "[645]\ttraining's rmse: 0.954886\tvalid_1's rmse: 1.04795\n",
      "[646]\ttraining's rmse: 0.954779\tvalid_1's rmse: 1.04793\n",
      "[647]\ttraining's rmse: 0.954679\tvalid_1's rmse: 1.04789\n",
      "[648]\ttraining's rmse: 0.954583\tvalid_1's rmse: 1.04789\n",
      "[649]\ttraining's rmse: 0.954489\tvalid_1's rmse: 1.04789\n",
      "[650]\ttraining's rmse: 0.954385\tvalid_1's rmse: 1.04787\n",
      "[651]\ttraining's rmse: 0.954285\tvalid_1's rmse: 1.04784\n",
      "[652]\ttraining's rmse: 0.954224\tvalid_1's rmse: 1.04779\n",
      "[653]\ttraining's rmse: 0.95412\tvalid_1's rmse: 1.04769\n",
      "[654]\ttraining's rmse: 0.95407\tvalid_1's rmse: 1.04768\n",
      "[655]\ttraining's rmse: 0.953959\tvalid_1's rmse: 1.04764\n",
      "[656]\ttraining's rmse: 0.953892\tvalid_1's rmse: 1.04759\n",
      "[657]\ttraining's rmse: 0.953781\tvalid_1's rmse: 1.04758\n",
      "[658]\ttraining's rmse: 0.95369\tvalid_1's rmse: 1.04753\n",
      "[659]\ttraining's rmse: 0.953603\tvalid_1's rmse: 1.04744\n",
      "[660]\ttraining's rmse: 0.953512\tvalid_1's rmse: 1.04747\n",
      "[661]\ttraining's rmse: 0.953412\tvalid_1's rmse: 1.04743\n",
      "[662]\ttraining's rmse: 0.953358\tvalid_1's rmse: 1.04741\n",
      "[663]\ttraining's rmse: 0.953302\tvalid_1's rmse: 1.04739\n",
      "[664]\ttraining's rmse: 0.953199\tvalid_1's rmse: 1.04743\n",
      "[665]\ttraining's rmse: 0.95311\tvalid_1's rmse: 1.04741\n",
      "[666]\ttraining's rmse: 0.95301\tvalid_1's rmse: 1.04739\n",
      "[667]\ttraining's rmse: 0.952907\tvalid_1's rmse: 1.04728\n",
      "[668]\ttraining's rmse: 0.952805\tvalid_1's rmse: 1.04722\n",
      "[669]\ttraining's rmse: 0.952784\tvalid_1's rmse: 1.04718\n",
      "[670]\ttraining's rmse: 0.952716\tvalid_1's rmse: 1.04715\n",
      "[671]\ttraining's rmse: 0.952657\tvalid_1's rmse: 1.0471\n",
      "[672]\ttraining's rmse: 0.952609\tvalid_1's rmse: 1.04709\n",
      "[673]\ttraining's rmse: 0.952518\tvalid_1's rmse: 1.04709\n",
      "[674]\ttraining's rmse: 0.952432\tvalid_1's rmse: 1.04713\n",
      "[675]\ttraining's rmse: 0.952332\tvalid_1's rmse: 1.04707\n",
      "[676]\ttraining's rmse: 0.952275\tvalid_1's rmse: 1.04706\n",
      "[677]\ttraining's rmse: 0.952169\tvalid_1's rmse: 1.04704\n",
      "[678]\ttraining's rmse: 0.952076\tvalid_1's rmse: 1.04705\n",
      "[679]\ttraining's rmse: 0.952037\tvalid_1's rmse: 1.04703\n",
      "[680]\ttraining's rmse: 0.951951\tvalid_1's rmse: 1.04699\n",
      "[681]\ttraining's rmse: 0.951893\tvalid_1's rmse: 1.04699\n",
      "[682]\ttraining's rmse: 0.951804\tvalid_1's rmse: 1.04691\n",
      "[683]\ttraining's rmse: 0.951724\tvalid_1's rmse: 1.04693\n",
      "[684]\ttraining's rmse: 0.95167\tvalid_1's rmse: 1.04691\n",
      "[685]\ttraining's rmse: 0.951574\tvalid_1's rmse: 1.04684\n",
      "[686]\ttraining's rmse: 0.951488\tvalid_1's rmse: 1.04681\n",
      "[687]\ttraining's rmse: 0.951451\tvalid_1's rmse: 1.0468\n",
      "[688]\ttraining's rmse: 0.951364\tvalid_1's rmse: 1.04683\n",
      "[689]\ttraining's rmse: 0.951282\tvalid_1's rmse: 1.04678\n",
      "[690]\ttraining's rmse: 0.951182\tvalid_1's rmse: 1.04672\n",
      "[691]\ttraining's rmse: 0.951125\tvalid_1's rmse: 1.04671\n",
      "[692]\ttraining's rmse: 0.951041\tvalid_1's rmse: 1.04672\n",
      "[693]\ttraining's rmse: 0.951014\tvalid_1's rmse: 1.04671\n",
      "[694]\ttraining's rmse: 0.950892\tvalid_1's rmse: 1.04671\n",
      "[695]\ttraining's rmse: 0.9508\tvalid_1's rmse: 1.04664\n",
      "[696]\ttraining's rmse: 0.950752\tvalid_1's rmse: 1.04664\n",
      "[697]\ttraining's rmse: 0.95065\tvalid_1's rmse: 1.04659\n",
      "[698]\ttraining's rmse: 0.950545\tvalid_1's rmse: 1.04657\n",
      "[699]\ttraining's rmse: 0.950451\tvalid_1's rmse: 1.04649\n",
      "[700]\ttraining's rmse: 0.950369\tvalid_1's rmse: 1.04649\n",
      "[701]\ttraining's rmse: 0.950313\tvalid_1's rmse: 1.0465\n",
      "[702]\ttraining's rmse: 0.950222\tvalid_1's rmse: 1.04643\n",
      "[703]\ttraining's rmse: 0.950143\tvalid_1's rmse: 1.04641\n",
      "[704]\ttraining's rmse: 0.95004\tvalid_1's rmse: 1.04641\n",
      "[705]\ttraining's rmse: 0.949926\tvalid_1's rmse: 1.04644\n",
      "[706]\ttraining's rmse: 0.949851\tvalid_1's rmse: 1.04639\n",
      "[707]\ttraining's rmse: 0.949819\tvalid_1's rmse: 1.04639\n",
      "[708]\ttraining's rmse: 0.949767\tvalid_1's rmse: 1.04636\n",
      "[709]\ttraining's rmse: 0.94969\tvalid_1's rmse: 1.04637\n",
      "[710]\ttraining's rmse: 0.949599\tvalid_1's rmse: 1.04631\n",
      "[711]\ttraining's rmse: 0.949519\tvalid_1's rmse: 1.04631\n",
      "[712]\ttraining's rmse: 0.94942\tvalid_1's rmse: 1.04635\n",
      "[713]\ttraining's rmse: 0.949331\tvalid_1's rmse: 1.04628\n",
      "[714]\ttraining's rmse: 0.949216\tvalid_1's rmse: 1.04629\n",
      "[715]\ttraining's rmse: 0.949131\tvalid_1's rmse: 1.04626\n",
      "[716]\ttraining's rmse: 0.949037\tvalid_1's rmse: 1.04624\n",
      "[717]\ttraining's rmse: 0.948956\tvalid_1's rmse: 1.04623\n",
      "[718]\ttraining's rmse: 0.948919\tvalid_1's rmse: 1.04621\n",
      "[719]\ttraining's rmse: 0.948865\tvalid_1's rmse: 1.04621\n",
      "[720]\ttraining's rmse: 0.948767\tvalid_1's rmse: 1.04622\n",
      "[721]\ttraining's rmse: 0.948717\tvalid_1's rmse: 1.04622\n",
      "[722]\ttraining's rmse: 0.948691\tvalid_1's rmse: 1.0462\n",
      "[723]\ttraining's rmse: 0.948642\tvalid_1's rmse: 1.04619\n",
      "[724]\ttraining's rmse: 0.948545\tvalid_1's rmse: 1.04615\n",
      "[725]\ttraining's rmse: 0.948459\tvalid_1's rmse: 1.04609\n",
      "[726]\ttraining's rmse: 0.948396\tvalid_1's rmse: 1.04615\n",
      "[727]\ttraining's rmse: 0.948314\tvalid_1's rmse: 1.04616\n",
      "[728]\ttraining's rmse: 0.94823\tvalid_1's rmse: 1.04613\n",
      "[729]\ttraining's rmse: 0.94818\tvalid_1's rmse: 1.04613\n",
      "[730]\ttraining's rmse: 0.948085\tvalid_1's rmse: 1.04611\n",
      "[731]\ttraining's rmse: 0.947973\tvalid_1's rmse: 1.04611\n",
      "[732]\ttraining's rmse: 0.947944\tvalid_1's rmse: 1.04611\n",
      "[733]\ttraining's rmse: 0.947862\tvalid_1's rmse: 1.04607\n",
      "[734]\ttraining's rmse: 0.947781\tvalid_1's rmse: 1.04607\n",
      "[735]\ttraining's rmse: 0.94775\tvalid_1's rmse: 1.04607\n",
      "[736]\ttraining's rmse: 0.947665\tvalid_1's rmse: 1.04603\n",
      "[737]\ttraining's rmse: 0.947612\tvalid_1's rmse: 1.04604\n",
      "[738]\ttraining's rmse: 0.947528\tvalid_1's rmse: 1.04599\n",
      "[739]\ttraining's rmse: 0.947423\tvalid_1's rmse: 1.04601\n",
      "[740]\ttraining's rmse: 0.947377\tvalid_1's rmse: 1.04601\n",
      "[741]\ttraining's rmse: 0.947297\tvalid_1's rmse: 1.04598\n",
      "[742]\ttraining's rmse: 0.947203\tvalid_1's rmse: 1.04592\n",
      "[743]\ttraining's rmse: 0.947154\tvalid_1's rmse: 1.04589\n",
      "[744]\ttraining's rmse: 0.947111\tvalid_1's rmse: 1.04582\n",
      "[745]\ttraining's rmse: 0.947046\tvalid_1's rmse: 1.04583\n",
      "[746]\ttraining's rmse: 0.946964\tvalid_1's rmse: 1.04578\n",
      "[747]\ttraining's rmse: 0.946947\tvalid_1's rmse: 1.04578\n",
      "[748]\ttraining's rmse: 0.946869\tvalid_1's rmse: 1.04575\n",
      "[749]\ttraining's rmse: 0.946787\tvalid_1's rmse: 1.04572\n",
      "[750]\ttraining's rmse: 0.946704\tvalid_1's rmse: 1.04572\n",
      "[751]\ttraining's rmse: 0.946637\tvalid_1's rmse: 1.04573\n",
      "[752]\ttraining's rmse: 0.94653\tvalid_1's rmse: 1.04574\n",
      "[753]\ttraining's rmse: 0.946487\tvalid_1's rmse: 1.04574\n",
      "[754]\ttraining's rmse: 0.946411\tvalid_1's rmse: 1.0457\n",
      "[755]\ttraining's rmse: 0.946313\tvalid_1's rmse: 1.04569\n",
      "[756]\ttraining's rmse: 0.946228\tvalid_1's rmse: 1.04568\n",
      "[757]\ttraining's rmse: 0.946187\tvalid_1's rmse: 1.04566\n",
      "[758]\ttraining's rmse: 0.946152\tvalid_1's rmse: 1.04564\n",
      "[759]\ttraining's rmse: 0.946128\tvalid_1's rmse: 1.04563\n",
      "[760]\ttraining's rmse: 0.946036\tvalid_1's rmse: 1.0456\n",
      "[761]\ttraining's rmse: 0.945968\tvalid_1's rmse: 1.0456\n",
      "[762]\ttraining's rmse: 0.945922\tvalid_1's rmse: 1.04557\n",
      "[763]\ttraining's rmse: 0.945881\tvalid_1's rmse: 1.04551\n",
      "[764]\ttraining's rmse: 0.9458\tvalid_1's rmse: 1.04548\n",
      "[765]\ttraining's rmse: 0.945767\tvalid_1's rmse: 1.04548\n",
      "[766]\ttraining's rmse: 0.945676\tvalid_1's rmse: 1.04542\n",
      "[767]\ttraining's rmse: 0.945608\tvalid_1's rmse: 1.04544\n",
      "[768]\ttraining's rmse: 0.945544\tvalid_1's rmse: 1.04545\n",
      "[769]\ttraining's rmse: 0.94548\tvalid_1's rmse: 1.04547\n",
      "[770]\ttraining's rmse: 0.945464\tvalid_1's rmse: 1.04547\n",
      "[771]\ttraining's rmse: 0.945385\tvalid_1's rmse: 1.04549\n",
      "[772]\ttraining's rmse: 0.945311\tvalid_1's rmse: 1.04545\n",
      "[773]\ttraining's rmse: 0.945257\tvalid_1's rmse: 1.04547\n",
      "[774]\ttraining's rmse: 0.945178\tvalid_1's rmse: 1.04543\n",
      "[775]\ttraining's rmse: 0.945099\tvalid_1's rmse: 1.04536\n",
      "[776]\ttraining's rmse: 0.945033\tvalid_1's rmse: 1.04536\n",
      "[777]\ttraining's rmse: 0.945005\tvalid_1's rmse: 1.04536\n",
      "[778]\ttraining's rmse: 0.94492\tvalid_1's rmse: 1.04531\n",
      "[779]\ttraining's rmse: 0.944874\tvalid_1's rmse: 1.04531\n",
      "[780]\ttraining's rmse: 0.944783\tvalid_1's rmse: 1.04529\n",
      "[781]\ttraining's rmse: 0.944768\tvalid_1's rmse: 1.04529\n",
      "[782]\ttraining's rmse: 0.944699\tvalid_1's rmse: 1.04529\n",
      "[783]\ttraining's rmse: 0.944656\tvalid_1's rmse: 1.04526\n",
      "[784]\ttraining's rmse: 0.944609\tvalid_1's rmse: 1.04523\n",
      "[785]\ttraining's rmse: 0.944563\tvalid_1's rmse: 1.0452\n",
      "[786]\ttraining's rmse: 0.944528\tvalid_1's rmse: 1.04517\n",
      "[787]\ttraining's rmse: 0.944466\tvalid_1's rmse: 1.04518\n",
      "[788]\ttraining's rmse: 0.944373\tvalid_1's rmse: 1.04512\n",
      "[789]\ttraining's rmse: 0.944331\tvalid_1's rmse: 1.04512\n",
      "[790]\ttraining's rmse: 0.944258\tvalid_1's rmse: 1.0451\n",
      "[791]\ttraining's rmse: 0.944192\tvalid_1's rmse: 1.04507\n",
      "[792]\ttraining's rmse: 0.94412\tvalid_1's rmse: 1.04506\n",
      "[793]\ttraining's rmse: 0.944061\tvalid_1's rmse: 1.04504\n",
      "[794]\ttraining's rmse: 0.943984\tvalid_1's rmse: 1.04499\n",
      "[795]\ttraining's rmse: 0.9439\tvalid_1's rmse: 1.04497\n",
      "[796]\ttraining's rmse: 0.943847\tvalid_1's rmse: 1.04497\n",
      "[797]\ttraining's rmse: 0.943832\tvalid_1's rmse: 1.04497\n",
      "[798]\ttraining's rmse: 0.943774\tvalid_1's rmse: 1.04496\n",
      "[799]\ttraining's rmse: 0.943728\tvalid_1's rmse: 1.04495\n",
      "[800]\ttraining's rmse: 0.943647\tvalid_1's rmse: 1.04493\n",
      "[801]\ttraining's rmse: 0.943587\tvalid_1's rmse: 1.04493\n",
      "[802]\ttraining's rmse: 0.943515\tvalid_1's rmse: 1.04494\n",
      "[803]\ttraining's rmse: 0.943444\tvalid_1's rmse: 1.0449\n",
      "[804]\ttraining's rmse: 0.94339\tvalid_1's rmse: 1.0449\n",
      "[805]\ttraining's rmse: 0.943346\tvalid_1's rmse: 1.04487\n",
      "[806]\ttraining's rmse: 0.943313\tvalid_1's rmse: 1.04485\n",
      "[807]\ttraining's rmse: 0.943286\tvalid_1's rmse: 1.04484\n",
      "[808]\ttraining's rmse: 0.943201\tvalid_1's rmse: 1.04482\n",
      "[809]\ttraining's rmse: 0.943125\tvalid_1's rmse: 1.04482\n",
      "[810]\ttraining's rmse: 0.943053\tvalid_1's rmse: 1.0448\n",
      "[811]\ttraining's rmse: 0.943006\tvalid_1's rmse: 1.04479\n",
      "[812]\ttraining's rmse: 0.942939\tvalid_1's rmse: 1.04485\n",
      "[813]\ttraining's rmse: 0.942862\tvalid_1's rmse: 1.04485\n",
      "[814]\ttraining's rmse: 0.942802\tvalid_1's rmse: 1.04484\n",
      "[815]\ttraining's rmse: 0.942744\tvalid_1's rmse: 1.04482\n",
      "[816]\ttraining's rmse: 0.942669\tvalid_1's rmse: 1.04478\n",
      "[817]\ttraining's rmse: 0.942593\tvalid_1's rmse: 1.04475\n",
      "[818]\ttraining's rmse: 0.9425\tvalid_1's rmse: 1.04475\n",
      "[819]\ttraining's rmse: 0.942409\tvalid_1's rmse: 1.04474\n",
      "[820]\ttraining's rmse: 0.942377\tvalid_1's rmse: 1.04471\n",
      "[821]\ttraining's rmse: 0.942315\tvalid_1's rmse: 1.04474\n",
      "[822]\ttraining's rmse: 0.942271\tvalid_1's rmse: 1.04472\n",
      "[823]\ttraining's rmse: 0.942192\tvalid_1's rmse: 1.04472\n",
      "[824]\ttraining's rmse: 0.942151\tvalid_1's rmse: 1.04472\n",
      "[825]\ttraining's rmse: 0.942107\tvalid_1's rmse: 1.04471\n",
      "[826]\ttraining's rmse: 0.942038\tvalid_1's rmse: 1.04469\n",
      "[827]\ttraining's rmse: 0.941996\tvalid_1's rmse: 1.04472\n",
      "[828]\ttraining's rmse: 0.941903\tvalid_1's rmse: 1.04476\n",
      "[829]\ttraining's rmse: 0.941847\tvalid_1's rmse: 1.04474\n",
      "[830]\ttraining's rmse: 0.941764\tvalid_1's rmse: 1.04471\n",
      "[831]\ttraining's rmse: 0.941733\tvalid_1's rmse: 1.04469\n",
      "[832]\ttraining's rmse: 0.941694\tvalid_1's rmse: 1.04471\n",
      "[833]\ttraining's rmse: 0.941661\tvalid_1's rmse: 1.04467\n",
      "[834]\ttraining's rmse: 0.941597\tvalid_1's rmse: 1.04469\n",
      "[835]\ttraining's rmse: 0.941524\tvalid_1's rmse: 1.04469\n",
      "[836]\ttraining's rmse: 0.941444\tvalid_1's rmse: 1.04469\n",
      "[837]\ttraining's rmse: 0.9414\tvalid_1's rmse: 1.04471\n",
      "[838]\ttraining's rmse: 0.941362\tvalid_1's rmse: 1.04471\n",
      "[839]\ttraining's rmse: 0.941313\tvalid_1's rmse: 1.04472\n",
      "[840]\ttraining's rmse: 0.941258\tvalid_1's rmse: 1.04471\n",
      "[841]\ttraining's rmse: 0.941203\tvalid_1's rmse: 1.04468\n",
      "[842]\ttraining's rmse: 0.941166\tvalid_1's rmse: 1.04469\n",
      "[843]\ttraining's rmse: 0.941099\tvalid_1's rmse: 1.0447\n",
      "Early stopping, best iteration is:\n",
      "[833]\ttraining's rmse: 0.941661\tvalid_1's rmse: 1.04467\n",
      "fold_0 coefficients:  [0.55808353 1.62205742 2.05790957]\n",
      "[1]\ttraining's rmse: 1.25322\tvalid_1's rmse: 1.28172\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24976\tvalid_1's rmse: 1.27832\n",
      "[3]\ttraining's rmse: 1.24638\tvalid_1's rmse: 1.27499\n",
      "[4]\ttraining's rmse: 1.24303\tvalid_1's rmse: 1.27163\n",
      "[5]\ttraining's rmse: 1.23976\tvalid_1's rmse: 1.26841\n",
      "[6]\ttraining's rmse: 1.23655\tvalid_1's rmse: 1.26521\n",
      "[7]\ttraining's rmse: 1.23337\tvalid_1's rmse: 1.26196\n",
      "[8]\ttraining's rmse: 1.23027\tvalid_1's rmse: 1.25891\n",
      "[9]\ttraining's rmse: 1.22722\tvalid_1's rmse: 1.25586\n",
      "[10]\ttraining's rmse: 1.22413\tvalid_1's rmse: 1.25287\n",
      "[11]\ttraining's rmse: 1.2214\tvalid_1's rmse: 1.25022\n",
      "[12]\ttraining's rmse: 1.21847\tvalid_1's rmse: 1.24723\n",
      "[13]\ttraining's rmse: 1.21583\tvalid_1's rmse: 1.24463\n",
      "[14]\ttraining's rmse: 1.21294\tvalid_1's rmse: 1.24182\n",
      "[15]\ttraining's rmse: 1.2101\tvalid_1's rmse: 1.23911\n",
      "[16]\ttraining's rmse: 1.20758\tvalid_1's rmse: 1.23663\n",
      "[17]\ttraining's rmse: 1.20511\tvalid_1's rmse: 1.2342\n",
      "[18]\ttraining's rmse: 1.2024\tvalid_1's rmse: 1.23162\n",
      "[19]\ttraining's rmse: 1.20002\tvalid_1's rmse: 1.22922\n",
      "[20]\ttraining's rmse: 1.1974\tvalid_1's rmse: 1.22667\n",
      "[21]\ttraining's rmse: 1.19509\tvalid_1's rmse: 1.22439\n",
      "[22]\ttraining's rmse: 1.1926\tvalid_1's rmse: 1.22187\n",
      "[23]\ttraining's rmse: 1.19037\tvalid_1's rmse: 1.21966\n",
      "[24]\ttraining's rmse: 1.1879\tvalid_1's rmse: 1.21726\n",
      "[25]\ttraining's rmse: 1.18575\tvalid_1's rmse: 1.21513\n",
      "[26]\ttraining's rmse: 1.18335\tvalid_1's rmse: 1.21282\n",
      "[27]\ttraining's rmse: 1.18126\tvalid_1's rmse: 1.21076\n",
      "[28]\ttraining's rmse: 1.17894\tvalid_1's rmse: 1.20852\n",
      "[29]\ttraining's rmse: 1.17693\tvalid_1's rmse: 1.20648\n",
      "[30]\ttraining's rmse: 1.17471\tvalid_1's rmse: 1.20428\n",
      "[31]\ttraining's rmse: 1.17277\tvalid_1's rmse: 1.20236\n",
      "[32]\ttraining's rmse: 1.1706\tvalid_1's rmse: 1.20021\n",
      "[33]\ttraining's rmse: 1.16872\tvalid_1's rmse: 1.1984\n",
      "[34]\ttraining's rmse: 1.16688\tvalid_1's rmse: 1.19656\n",
      "[35]\ttraining's rmse: 1.16479\tvalid_1's rmse: 1.19446\n",
      "[36]\ttraining's rmse: 1.16301\tvalid_1's rmse: 1.1927\n",
      "[37]\ttraining's rmse: 1.16099\tvalid_1's rmse: 1.19064\n",
      "[38]\ttraining's rmse: 1.15927\tvalid_1's rmse: 1.18892\n",
      "[39]\ttraining's rmse: 1.15732\tvalid_1's rmse: 1.18695\n",
      "[40]\ttraining's rmse: 1.15565\tvalid_1's rmse: 1.18536\n",
      "[41]\ttraining's rmse: 1.15375\tvalid_1's rmse: 1.18344\n",
      "[42]\ttraining's rmse: 1.15215\tvalid_1's rmse: 1.18194\n",
      "[43]\ttraining's rmse: 1.15036\tvalid_1's rmse: 1.18019\n",
      "[44]\ttraining's rmse: 1.14881\tvalid_1's rmse: 1.17873\n",
      "[45]\ttraining's rmse: 1.14703\tvalid_1's rmse: 1.17692\n",
      "[46]\ttraining's rmse: 1.14554\tvalid_1's rmse: 1.1754\n",
      "[47]\ttraining's rmse: 1.14374\tvalid_1's rmse: 1.1736\n",
      "[48]\ttraining's rmse: 1.14228\tvalid_1's rmse: 1.17215\n",
      "[49]\ttraining's rmse: 1.1406\tvalid_1's rmse: 1.17061\n",
      "[50]\ttraining's rmse: 1.13919\tvalid_1's rmse: 1.16919\n",
      "[51]\ttraining's rmse: 1.13749\tvalid_1's rmse: 1.16757\n",
      "[52]\ttraining's rmse: 1.13613\tvalid_1's rmse: 1.16632\n",
      "[53]\ttraining's rmse: 1.13449\tvalid_1's rmse: 1.16465\n",
      "[54]\ttraining's rmse: 1.13309\tvalid_1's rmse: 1.16333\n",
      "[55]\ttraining's rmse: 1.13158\tvalid_1's rmse: 1.16184\n",
      "[56]\ttraining's rmse: 1.13031\tvalid_1's rmse: 1.16055\n",
      "[57]\ttraining's rmse: 1.12876\tvalid_1's rmse: 1.15904\n",
      "[58]\ttraining's rmse: 1.12731\tvalid_1's rmse: 1.15767\n",
      "[59]\ttraining's rmse: 1.1258\tvalid_1's rmse: 1.15622\n",
      "[60]\ttraining's rmse: 1.1244\tvalid_1's rmse: 1.15491\n",
      "[61]\ttraining's rmse: 1.12292\tvalid_1's rmse: 1.15353\n",
      "[62]\ttraining's rmse: 1.1218\tvalid_1's rmse: 1.15234\n",
      "[63]\ttraining's rmse: 1.12046\tvalid_1's rmse: 1.15096\n",
      "[64]\ttraining's rmse: 1.11928\tvalid_1's rmse: 1.14983\n",
      "[65]\ttraining's rmse: 1.11797\tvalid_1's rmse: 1.14862\n",
      "[66]\ttraining's rmse: 1.1166\tvalid_1's rmse: 1.14738\n",
      "[67]\ttraining's rmse: 1.11547\tvalid_1's rmse: 1.14637\n",
      "[68]\ttraining's rmse: 1.11442\tvalid_1's rmse: 1.14537\n",
      "[69]\ttraining's rmse: 1.11311\tvalid_1's rmse: 1.14408\n",
      "[70]\ttraining's rmse: 1.11202\tvalid_1's rmse: 1.14309\n",
      "[71]\ttraining's rmse: 1.11081\tvalid_1's rmse: 1.14209\n",
      "[72]\ttraining's rmse: 1.10952\tvalid_1's rmse: 1.1409\n",
      "[73]\ttraining's rmse: 1.10848\tvalid_1's rmse: 1.13996\n",
      "[74]\ttraining's rmse: 1.10753\tvalid_1's rmse: 1.13899\n",
      "[75]\ttraining's rmse: 1.10652\tvalid_1's rmse: 1.13806\n",
      "[76]\ttraining's rmse: 1.10532\tvalid_1's rmse: 1.13699\n",
      "[77]\ttraining's rmse: 1.10395\tvalid_1's rmse: 1.13581\n",
      "[78]\ttraining's rmse: 1.10275\tvalid_1's rmse: 1.13473\n",
      "[79]\ttraining's rmse: 1.1018\tvalid_1's rmse: 1.13373\n",
      "[80]\ttraining's rmse: 1.10066\tvalid_1's rmse: 1.13257\n",
      "[81]\ttraining's rmse: 1.09975\tvalid_1's rmse: 1.13154\n",
      "[82]\ttraining's rmse: 1.09884\tvalid_1's rmse: 1.13059\n",
      "[83]\ttraining's rmse: 1.09756\tvalid_1's rmse: 1.12967\n",
      "[84]\ttraining's rmse: 1.09647\tvalid_1's rmse: 1.12848\n",
      "[85]\ttraining's rmse: 1.09561\tvalid_1's rmse: 1.12764\n",
      "[86]\ttraining's rmse: 1.09475\tvalid_1's rmse: 1.12679\n",
      "[87]\ttraining's rmse: 1.09371\tvalid_1's rmse: 1.12595\n",
      "[88]\ttraining's rmse: 1.09271\tvalid_1's rmse: 1.12499\n",
      "[89]\ttraining's rmse: 1.0919\tvalid_1's rmse: 1.12426\n",
      "[90]\ttraining's rmse: 1.09107\tvalid_1's rmse: 1.12348\n",
      "[91]\ttraining's rmse: 1.09008\tvalid_1's rmse: 1.12244\n",
      "[92]\ttraining's rmse: 1.08913\tvalid_1's rmse: 1.12154\n",
      "[93]\ttraining's rmse: 1.08828\tvalid_1's rmse: 1.12071\n",
      "[94]\ttraining's rmse: 1.08753\tvalid_1's rmse: 1.12001\n",
      "[95]\ttraining's rmse: 1.08662\tvalid_1's rmse: 1.11914\n",
      "[96]\ttraining's rmse: 1.08588\tvalid_1's rmse: 1.11851\n",
      "[97]\ttraining's rmse: 1.08508\tvalid_1's rmse: 1.1177\n",
      "[98]\ttraining's rmse: 1.08434\tvalid_1's rmse: 1.11703\n",
      "[99]\ttraining's rmse: 1.08364\tvalid_1's rmse: 1.11634\n",
      "[100]\ttraining's rmse: 1.08274\tvalid_1's rmse: 1.11565\n",
      "[101]\ttraining's rmse: 1.08189\tvalid_1's rmse: 1.1148\n",
      "[102]\ttraining's rmse: 1.08112\tvalid_1's rmse: 1.11404\n",
      "[103]\ttraining's rmse: 1.08042\tvalid_1's rmse: 1.11344\n",
      "[104]\ttraining's rmse: 1.07977\tvalid_1's rmse: 1.11278\n",
      "[105]\ttraining's rmse: 1.07888\tvalid_1's rmse: 1.11211\n",
      "[106]\ttraining's rmse: 1.07807\tvalid_1's rmse: 1.11137\n",
      "[107]\ttraining's rmse: 1.07736\tvalid_1's rmse: 1.11067\n",
      "[108]\ttraining's rmse: 1.07674\tvalid_1's rmse: 1.11\n",
      "[109]\ttraining's rmse: 1.07591\tvalid_1's rmse: 1.10932\n",
      "[110]\ttraining's rmse: 1.07521\tvalid_1's rmse: 1.1086\n",
      "[111]\ttraining's rmse: 1.07458\tvalid_1's rmse: 1.10797\n",
      "[112]\ttraining's rmse: 1.0739\tvalid_1's rmse: 1.10736\n",
      "[113]\ttraining's rmse: 1.07315\tvalid_1's rmse: 1.10664\n",
      "[114]\ttraining's rmse: 1.07233\tvalid_1's rmse: 1.10604\n",
      "[115]\ttraining's rmse: 1.07176\tvalid_1's rmse: 1.10545\n",
      "[116]\ttraining's rmse: 1.07112\tvalid_1's rmse: 1.10481\n",
      "[117]\ttraining's rmse: 1.07035\tvalid_1's rmse: 1.10417\n",
      "[118]\ttraining's rmse: 1.06972\tvalid_1's rmse: 1.10359\n",
      "[119]\ttraining's rmse: 1.06916\tvalid_1's rmse: 1.10296\n",
      "[120]\ttraining's rmse: 1.06846\tvalid_1's rmse: 1.10234\n",
      "[121]\ttraining's rmse: 1.06785\tvalid_1's rmse: 1.10169\n",
      "[122]\ttraining's rmse: 1.06707\tvalid_1's rmse: 1.10108\n",
      "[123]\ttraining's rmse: 1.06638\tvalid_1's rmse: 1.10034\n",
      "[124]\ttraining's rmse: 1.06569\tvalid_1's rmse: 1.09968\n",
      "[125]\ttraining's rmse: 1.065\tvalid_1's rmse: 1.09908\n",
      "[126]\ttraining's rmse: 1.06443\tvalid_1's rmse: 1.09857\n",
      "[127]\ttraining's rmse: 1.06389\tvalid_1's rmse: 1.09808\n",
      "[128]\ttraining's rmse: 1.06339\tvalid_1's rmse: 1.09771\n",
      "[129]\ttraining's rmse: 1.06283\tvalid_1's rmse: 1.09706\n",
      "[130]\ttraining's rmse: 1.06212\tvalid_1's rmse: 1.09662\n",
      "[131]\ttraining's rmse: 1.0615\tvalid_1's rmse: 1.09612\n",
      "[132]\ttraining's rmse: 1.06092\tvalid_1's rmse: 1.09551\n",
      "[133]\ttraining's rmse: 1.06039\tvalid_1's rmse: 1.0951\n",
      "[134]\ttraining's rmse: 1.05977\tvalid_1's rmse: 1.09455\n",
      "[135]\ttraining's rmse: 1.0593\tvalid_1's rmse: 1.09402\n",
      "[136]\ttraining's rmse: 1.05878\tvalid_1's rmse: 1.09358\n",
      "[137]\ttraining's rmse: 1.05826\tvalid_1's rmse: 1.09304\n",
      "[138]\ttraining's rmse: 1.05768\tvalid_1's rmse: 1.09256\n",
      "[139]\ttraining's rmse: 1.05721\tvalid_1's rmse: 1.09222\n",
      "[140]\ttraining's rmse: 1.05677\tvalid_1's rmse: 1.09176\n",
      "[141]\ttraining's rmse: 1.05625\tvalid_1's rmse: 1.09132\n",
      "[142]\ttraining's rmse: 1.0558\tvalid_1's rmse: 1.09088\n",
      "[143]\ttraining's rmse: 1.055\tvalid_1's rmse: 1.09032\n",
      "[144]\ttraining's rmse: 1.05432\tvalid_1's rmse: 1.08987\n",
      "[145]\ttraining's rmse: 1.05383\tvalid_1's rmse: 1.0893\n",
      "[146]\ttraining's rmse: 1.05337\tvalid_1's rmse: 1.08887\n",
      "[147]\ttraining's rmse: 1.05276\tvalid_1's rmse: 1.08853\n",
      "[148]\ttraining's rmse: 1.05228\tvalid_1's rmse: 1.08798\n",
      "[149]\ttraining's rmse: 1.05164\tvalid_1's rmse: 1.08756\n",
      "[150]\ttraining's rmse: 1.05116\tvalid_1's rmse: 1.08714\n",
      "[151]\ttraining's rmse: 1.05053\tvalid_1's rmse: 1.08664\n",
      "[152]\ttraining's rmse: 1.0501\tvalid_1's rmse: 1.08629\n",
      "[153]\ttraining's rmse: 1.04959\tvalid_1's rmse: 1.0858\n",
      "[154]\ttraining's rmse: 1.04902\tvalid_1's rmse: 1.08553\n",
      "[155]\ttraining's rmse: 1.04858\tvalid_1's rmse: 1.08505\n",
      "[156]\ttraining's rmse: 1.04798\tvalid_1's rmse: 1.08457\n",
      "[157]\ttraining's rmse: 1.04757\tvalid_1's rmse: 1.08428\n",
      "[158]\ttraining's rmse: 1.04713\tvalid_1's rmse: 1.08381\n",
      "[159]\ttraining's rmse: 1.04678\tvalid_1's rmse: 1.08348\n",
      "[160]\ttraining's rmse: 1.04621\tvalid_1's rmse: 1.08308\n",
      "[161]\ttraining's rmse: 1.04567\tvalid_1's rmse: 1.08272\n",
      "[162]\ttraining's rmse: 1.04519\tvalid_1's rmse: 1.08243\n",
      "[163]\ttraining's rmse: 1.04477\tvalid_1's rmse: 1.08204\n",
      "[164]\ttraining's rmse: 1.04435\tvalid_1's rmse: 1.08152\n",
      "[165]\ttraining's rmse: 1.04397\tvalid_1's rmse: 1.08124\n",
      "[166]\ttraining's rmse: 1.04336\tvalid_1's rmse: 1.08079\n",
      "[167]\ttraining's rmse: 1.04282\tvalid_1's rmse: 1.08047\n",
      "[168]\ttraining's rmse: 1.04249\tvalid_1's rmse: 1.08027\n",
      "[169]\ttraining's rmse: 1.04208\tvalid_1's rmse: 1.07988\n",
      "[170]\ttraining's rmse: 1.04175\tvalid_1's rmse: 1.07964\n",
      "[171]\ttraining's rmse: 1.04142\tvalid_1's rmse: 1.07943\n",
      "[172]\ttraining's rmse: 1.04104\tvalid_1's rmse: 1.0789\n",
      "[173]\ttraining's rmse: 1.04061\tvalid_1's rmse: 1.07857\n",
      "[174]\ttraining's rmse: 1.04026\tvalid_1's rmse: 1.07831\n",
      "[175]\ttraining's rmse: 1.03984\tvalid_1's rmse: 1.07799\n",
      "[176]\ttraining's rmse: 1.03943\tvalid_1's rmse: 1.07768\n",
      "[177]\ttraining's rmse: 1.03903\tvalid_1's rmse: 1.07738\n",
      "[178]\ttraining's rmse: 1.03863\tvalid_1's rmse: 1.07708\n",
      "[179]\ttraining's rmse: 1.03827\tvalid_1's rmse: 1.0767\n",
      "[180]\ttraining's rmse: 1.03789\tvalid_1's rmse: 1.07642\n",
      "[181]\ttraining's rmse: 1.03751\tvalid_1's rmse: 1.07616\n",
      "[182]\ttraining's rmse: 1.03714\tvalid_1's rmse: 1.07583\n",
      "[183]\ttraining's rmse: 1.03678\tvalid_1's rmse: 1.07556\n",
      "[184]\ttraining's rmse: 1.03641\tvalid_1's rmse: 1.07531\n",
      "[185]\ttraining's rmse: 1.03605\tvalid_1's rmse: 1.07499\n",
      "[186]\ttraining's rmse: 1.0357\tvalid_1's rmse: 1.07462\n",
      "[187]\ttraining's rmse: 1.03535\tvalid_1's rmse: 1.07437\n",
      "[188]\ttraining's rmse: 1.03498\tvalid_1's rmse: 1.0741\n",
      "[189]\ttraining's rmse: 1.03463\tvalid_1's rmse: 1.07386\n",
      "[190]\ttraining's rmse: 1.03426\tvalid_1's rmse: 1.07356\n",
      "[191]\ttraining's rmse: 1.03393\tvalid_1's rmse: 1.07328\n",
      "[192]\ttraining's rmse: 1.03357\tvalid_1's rmse: 1.07301\n",
      "[193]\ttraining's rmse: 1.03324\tvalid_1's rmse: 1.07277\n",
      "[194]\ttraining's rmse: 1.03288\tvalid_1's rmse: 1.07246\n",
      "[195]\ttraining's rmse: 1.03256\tvalid_1's rmse: 1.07217\n",
      "[196]\ttraining's rmse: 1.03219\tvalid_1's rmse: 1.07188\n",
      "[197]\ttraining's rmse: 1.03187\tvalid_1's rmse: 1.07153\n",
      "[198]\ttraining's rmse: 1.03153\tvalid_1's rmse: 1.07124\n",
      "[199]\ttraining's rmse: 1.03121\tvalid_1's rmse: 1.07101\n",
      "[200]\ttraining's rmse: 1.03088\tvalid_1's rmse: 1.07076\n",
      "[201]\ttraining's rmse: 1.03056\tvalid_1's rmse: 1.07059\n",
      "[202]\ttraining's rmse: 1.03021\tvalid_1's rmse: 1.07031\n",
      "[203]\ttraining's rmse: 1.02989\tvalid_1's rmse: 1.07008\n",
      "[204]\ttraining's rmse: 1.02956\tvalid_1's rmse: 1.06986\n",
      "[205]\ttraining's rmse: 1.02926\tvalid_1's rmse: 1.06962\n",
      "[206]\ttraining's rmse: 1.02881\tvalid_1's rmse: 1.06945\n",
      "[207]\ttraining's rmse: 1.02849\tvalid_1's rmse: 1.06918\n",
      "[208]\ttraining's rmse: 1.02819\tvalid_1's rmse: 1.06898\n",
      "[209]\ttraining's rmse: 1.02785\tvalid_1's rmse: 1.06871\n",
      "[210]\ttraining's rmse: 1.02755\tvalid_1's rmse: 1.06845\n",
      "[211]\ttraining's rmse: 1.02703\tvalid_1's rmse: 1.06806\n",
      "[212]\ttraining's rmse: 1.02673\tvalid_1's rmse: 1.06785\n",
      "[213]\ttraining's rmse: 1.02629\tvalid_1's rmse: 1.06769\n",
      "[214]\ttraining's rmse: 1.02599\tvalid_1's rmse: 1.06751\n",
      "[215]\ttraining's rmse: 1.02557\tvalid_1's rmse: 1.0673\n",
      "[216]\ttraining's rmse: 1.02527\tvalid_1's rmse: 1.06705\n",
      "[217]\ttraining's rmse: 1.02478\tvalid_1's rmse: 1.06672\n",
      "[218]\ttraining's rmse: 1.02448\tvalid_1's rmse: 1.06644\n",
      "[219]\ttraining's rmse: 1.02401\tvalid_1's rmse: 1.06608\n",
      "[220]\ttraining's rmse: 1.02372\tvalid_1's rmse: 1.06588\n",
      "[221]\ttraining's rmse: 1.02332\tvalid_1's rmse: 1.06574\n",
      "[222]\ttraining's rmse: 1.02298\tvalid_1's rmse: 1.06559\n",
      "[223]\ttraining's rmse: 1.0227\tvalid_1's rmse: 1.06539\n",
      "[224]\ttraining's rmse: 1.02241\tvalid_1's rmse: 1.06517\n",
      "[225]\ttraining's rmse: 1.02195\tvalid_1's rmse: 1.06491\n",
      "[226]\ttraining's rmse: 1.02167\tvalid_1's rmse: 1.06472\n",
      "[227]\ttraining's rmse: 1.02137\tvalid_1's rmse: 1.06446\n",
      "[228]\ttraining's rmse: 1.02092\tvalid_1's rmse: 1.06418\n",
      "[229]\ttraining's rmse: 1.0205\tvalid_1's rmse: 1.06399\n",
      "[230]\ttraining's rmse: 1.02023\tvalid_1's rmse: 1.06379\n",
      "[231]\ttraining's rmse: 1.01998\tvalid_1's rmse: 1.06362\n",
      "[232]\ttraining's rmse: 1.0197\tvalid_1's rmse: 1.06341\n",
      "[233]\ttraining's rmse: 1.01926\tvalid_1's rmse: 1.06316\n",
      "[234]\ttraining's rmse: 1.019\tvalid_1's rmse: 1.06301\n",
      "[235]\ttraining's rmse: 1.01867\tvalid_1's rmse: 1.06279\n",
      "[236]\ttraining's rmse: 1.01825\tvalid_1's rmse: 1.06253\n",
      "[237]\ttraining's rmse: 1.01801\tvalid_1's rmse: 1.06243\n",
      "[238]\ttraining's rmse: 1.01774\tvalid_1's rmse: 1.06229\n",
      "[239]\ttraining's rmse: 1.01733\tvalid_1's rmse: 1.0621\n",
      "[240]\ttraining's rmse: 1.01701\tvalid_1's rmse: 1.06189\n",
      "[241]\ttraining's rmse: 1.01674\tvalid_1's rmse: 1.06165\n",
      "[242]\ttraining's rmse: 1.01633\tvalid_1's rmse: 1.06143\n",
      "[243]\ttraining's rmse: 1.01603\tvalid_1's rmse: 1.06122\n",
      "[244]\ttraining's rmse: 1.01577\tvalid_1's rmse: 1.06101\n",
      "[245]\ttraining's rmse: 1.01537\tvalid_1's rmse: 1.06074\n",
      "[246]\ttraining's rmse: 1.01511\tvalid_1's rmse: 1.06059\n",
      "[247]\ttraining's rmse: 1.01488\tvalid_1's rmse: 1.06046\n",
      "[248]\ttraining's rmse: 1.01463\tvalid_1's rmse: 1.06032\n",
      "[249]\ttraining's rmse: 1.01424\tvalid_1's rmse: 1.06015\n",
      "[250]\ttraining's rmse: 1.01398\tvalid_1's rmse: 1.06\n",
      "[251]\ttraining's rmse: 1.0136\tvalid_1's rmse: 1.05977\n",
      "[252]\ttraining's rmse: 1.01336\tvalid_1's rmse: 1.05965\n",
      "[253]\ttraining's rmse: 1.01312\tvalid_1's rmse: 1.05949\n",
      "[254]\ttraining's rmse: 1.01275\tvalid_1's rmse: 1.05928\n",
      "[255]\ttraining's rmse: 1.01247\tvalid_1's rmse: 1.05911\n",
      "[256]\ttraining's rmse: 1.01223\tvalid_1's rmse: 1.05894\n",
      "[257]\ttraining's rmse: 1.01187\tvalid_1's rmse: 1.05873\n",
      "[258]\ttraining's rmse: 1.01163\tvalid_1's rmse: 1.05864\n",
      "[259]\ttraining's rmse: 1.01137\tvalid_1's rmse: 1.05847\n",
      "[260]\ttraining's rmse: 1.0111\tvalid_1's rmse: 1.0583\n",
      "[261]\ttraining's rmse: 1.01088\tvalid_1's rmse: 1.05827\n",
      "[262]\ttraining's rmse: 1.01053\tvalid_1's rmse: 1.05805\n",
      "[263]\ttraining's rmse: 1.01026\tvalid_1's rmse: 1.05779\n",
      "[264]\ttraining's rmse: 1.01004\tvalid_1's rmse: 1.05768\n",
      "[265]\ttraining's rmse: 1.00982\tvalid_1's rmse: 1.0575\n",
      "[266]\ttraining's rmse: 1.00947\tvalid_1's rmse: 1.05731\n",
      "[267]\ttraining's rmse: 1.00924\tvalid_1's rmse: 1.05718\n",
      "[268]\ttraining's rmse: 1.00891\tvalid_1's rmse: 1.05699\n",
      "[269]\ttraining's rmse: 1.00865\tvalid_1's rmse: 1.05672\n",
      "[270]\ttraining's rmse: 1.0084\tvalid_1's rmse: 1.05655\n",
      "[271]\ttraining's rmse: 1.00819\tvalid_1's rmse: 1.05635\n",
      "[272]\ttraining's rmse: 1.00797\tvalid_1's rmse: 1.05627\n",
      "[273]\ttraining's rmse: 1.00765\tvalid_1's rmse: 1.0561\n",
      "[274]\ttraining's rmse: 1.0074\tvalid_1's rmse: 1.05594\n",
      "[275]\ttraining's rmse: 1.0072\tvalid_1's rmse: 1.05592\n",
      "[276]\ttraining's rmse: 1.00696\tvalid_1's rmse: 1.05579\n",
      "[277]\ttraining's rmse: 1.00665\tvalid_1's rmse: 1.05561\n",
      "[278]\ttraining's rmse: 1.00642\tvalid_1's rmse: 1.05537\n",
      "[279]\ttraining's rmse: 1.0062\tvalid_1's rmse: 1.05524\n",
      "[280]\ttraining's rmse: 1.00589\tvalid_1's rmse: 1.05509\n",
      "[281]\ttraining's rmse: 1.00566\tvalid_1's rmse: 1.05497\n",
      "[282]\ttraining's rmse: 1.00543\tvalid_1's rmse: 1.05477\n",
      "[283]\ttraining's rmse: 1.00523\tvalid_1's rmse: 1.05462\n",
      "[284]\ttraining's rmse: 1.00504\tvalid_1's rmse: 1.05447\n",
      "[285]\ttraining's rmse: 1.00472\tvalid_1's rmse: 1.05432\n",
      "[286]\ttraining's rmse: 1.0045\tvalid_1's rmse: 1.0542\n",
      "[287]\ttraining's rmse: 1.00428\tvalid_1's rmse: 1.05408\n",
      "[288]\ttraining's rmse: 1.00398\tvalid_1's rmse: 1.05394\n",
      "[289]\ttraining's rmse: 1.0038\tvalid_1's rmse: 1.05379\n",
      "[290]\ttraining's rmse: 1.00359\tvalid_1's rmse: 1.05369\n",
      "[291]\ttraining's rmse: 1.00329\tvalid_1's rmse: 1.05351\n",
      "[292]\ttraining's rmse: 1.00307\tvalid_1's rmse: 1.0534\n",
      "[293]\ttraining's rmse: 1.00288\tvalid_1's rmse: 1.05327\n",
      "[294]\ttraining's rmse: 1.00259\tvalid_1's rmse: 1.05316\n",
      "[295]\ttraining's rmse: 1.00239\tvalid_1's rmse: 1.05305\n",
      "[296]\ttraining's rmse: 1.00218\tvalid_1's rmse: 1.05283\n",
      "[297]\ttraining's rmse: 1.0019\tvalid_1's rmse: 1.05268\n",
      "[298]\ttraining's rmse: 1.00169\tvalid_1's rmse: 1.05257\n",
      "[299]\ttraining's rmse: 1.0015\tvalid_1's rmse: 1.05237\n",
      "[300]\ttraining's rmse: 1.00127\tvalid_1's rmse: 1.0522\n",
      "[301]\ttraining's rmse: 1.00104\tvalid_1's rmse: 1.05209\n",
      "[302]\ttraining's rmse: 1.00083\tvalid_1's rmse: 1.05197\n",
      "[303]\ttraining's rmse: 1.00056\tvalid_1's rmse: 1.05184\n",
      "[304]\ttraining's rmse: 1.00034\tvalid_1's rmse: 1.05166\n",
      "[305]\ttraining's rmse: 1.00014\tvalid_1's rmse: 1.05157\n",
      "[306]\ttraining's rmse: 0.999921\tvalid_1's rmse: 1.05142\n",
      "[307]\ttraining's rmse: 0.999735\tvalid_1's rmse: 1.05131\n",
      "[308]\ttraining's rmse: 0.999549\tvalid_1's rmse: 1.05123\n",
      "[309]\ttraining's rmse: 0.999366\tvalid_1's rmse: 1.05106\n",
      "[310]\ttraining's rmse: 0.999088\tvalid_1's rmse: 1.05093\n",
      "[311]\ttraining's rmse: 0.998875\tvalid_1's rmse: 1.05084\n",
      "[312]\ttraining's rmse: 0.99868\tvalid_1's rmse: 1.05072\n",
      "[313]\ttraining's rmse: 0.998421\tvalid_1's rmse: 1.05058\n",
      "[314]\ttraining's rmse: 0.998224\tvalid_1's rmse: 1.05048\n",
      "[315]\ttraining's rmse: 0.998042\tvalid_1's rmse: 1.05031\n",
      "[316]\ttraining's rmse: 0.997855\tvalid_1's rmse: 1.05024\n",
      "[317]\ttraining's rmse: 0.99765\tvalid_1's rmse: 1.05015\n",
      "[318]\ttraining's rmse: 0.997379\tvalid_1's rmse: 1.05005\n",
      "[319]\ttraining's rmse: 0.997189\tvalid_1's rmse: 1.05001\n",
      "[320]\ttraining's rmse: 0.99702\tvalid_1's rmse: 1.04988\n",
      "[321]\ttraining's rmse: 0.996752\tvalid_1's rmse: 1.04974\n",
      "[322]\ttraining's rmse: 0.996581\tvalid_1's rmse: 1.04965\n",
      "[323]\ttraining's rmse: 0.996402\tvalid_1's rmse: 1.04951\n",
      "[324]\ttraining's rmse: 0.996203\tvalid_1's rmse: 1.04933\n",
      "[325]\ttraining's rmse: 0.995935\tvalid_1's rmse: 1.04921\n",
      "[326]\ttraining's rmse: 0.995754\tvalid_1's rmse: 1.04912\n",
      "[327]\ttraining's rmse: 0.995568\tvalid_1's rmse: 1.04904\n",
      "[328]\ttraining's rmse: 0.995318\tvalid_1's rmse: 1.04891\n",
      "[329]\ttraining's rmse: 0.995147\tvalid_1's rmse: 1.04883\n",
      "[330]\ttraining's rmse: 0.994878\tvalid_1's rmse: 1.04865\n",
      "[331]\ttraining's rmse: 0.99469\tvalid_1's rmse: 1.04858\n",
      "[332]\ttraining's rmse: 0.994526\tvalid_1's rmse: 1.04848\n",
      "[333]\ttraining's rmse: 0.994289\tvalid_1's rmse: 1.04838\n",
      "[334]\ttraining's rmse: 0.994124\tvalid_1's rmse: 1.04836\n",
      "[335]\ttraining's rmse: 0.993957\tvalid_1's rmse: 1.04828\n",
      "[336]\ttraining's rmse: 0.993724\tvalid_1's rmse: 1.0482\n",
      "[337]\ttraining's rmse: 0.993567\tvalid_1's rmse: 1.04812\n",
      "[338]\ttraining's rmse: 0.993394\tvalid_1's rmse: 1.04809\n",
      "[339]\ttraining's rmse: 0.993236\tvalid_1's rmse: 1.04795\n",
      "[340]\ttraining's rmse: 0.993012\tvalid_1's rmse: 1.04784\n",
      "[341]\ttraining's rmse: 0.992843\tvalid_1's rmse: 1.04773\n",
      "[342]\ttraining's rmse: 0.992604\tvalid_1's rmse: 1.04756\n",
      "[343]\ttraining's rmse: 0.992425\tvalid_1's rmse: 1.04743\n",
      "[344]\ttraining's rmse: 0.992266\tvalid_1's rmse: 1.04738\n",
      "[345]\ttraining's rmse: 0.992025\tvalid_1's rmse: 1.04729\n",
      "[346]\ttraining's rmse: 0.991867\tvalid_1's rmse: 1.04722\n",
      "[347]\ttraining's rmse: 0.991709\tvalid_1's rmse: 1.04709\n",
      "[348]\ttraining's rmse: 0.991477\tvalid_1's rmse: 1.04695\n",
      "[349]\ttraining's rmse: 0.991311\tvalid_1's rmse: 1.04682\n",
      "[350]\ttraining's rmse: 0.991159\tvalid_1's rmse: 1.04673\n",
      "[351]\ttraining's rmse: 0.990959\tvalid_1's rmse: 1.04657\n",
      "[352]\ttraining's rmse: 0.990729\tvalid_1's rmse: 1.04644\n",
      "[353]\ttraining's rmse: 0.990534\tvalid_1's rmse: 1.04636\n",
      "[354]\ttraining's rmse: 0.990314\tvalid_1's rmse: 1.04626\n",
      "[355]\ttraining's rmse: 0.990099\tvalid_1's rmse: 1.04605\n",
      "[356]\ttraining's rmse: 0.98995\tvalid_1's rmse: 1.04598\n",
      "[357]\ttraining's rmse: 0.989799\tvalid_1's rmse: 1.04591\n",
      "[358]\ttraining's rmse: 0.989599\tvalid_1's rmse: 1.04577\n",
      "[359]\ttraining's rmse: 0.989382\tvalid_1's rmse: 1.04572\n",
      "[360]\ttraining's rmse: 0.989172\tvalid_1's rmse: 1.04553\n",
      "[361]\ttraining's rmse: 0.989011\tvalid_1's rmse: 1.0455\n",
      "[362]\ttraining's rmse: 0.988852\tvalid_1's rmse: 1.04543\n",
      "[363]\ttraining's rmse: 0.988641\tvalid_1's rmse: 1.04529\n",
      "[364]\ttraining's rmse: 0.988481\tvalid_1's rmse: 1.04521\n",
      "[365]\ttraining's rmse: 0.988278\tvalid_1's rmse: 1.0451\n",
      "[366]\ttraining's rmse: 0.988131\tvalid_1's rmse: 1.04497\n",
      "[367]\ttraining's rmse: 0.987917\tvalid_1's rmse: 1.0449\n",
      "[368]\ttraining's rmse: 0.987716\tvalid_1's rmse: 1.04471\n",
      "[369]\ttraining's rmse: 0.987576\tvalid_1's rmse: 1.04466\n",
      "[370]\ttraining's rmse: 0.987392\tvalid_1's rmse: 1.04449\n",
      "[371]\ttraining's rmse: 0.987188\tvalid_1's rmse: 1.04445\n",
      "[372]\ttraining's rmse: 0.987035\tvalid_1's rmse: 1.04442\n",
      "[373]\ttraining's rmse: 0.986855\tvalid_1's rmse: 1.04425\n",
      "[374]\ttraining's rmse: 0.986648\tvalid_1's rmse: 1.04414\n",
      "[375]\ttraining's rmse: 0.986496\tvalid_1's rmse: 1.04408\n",
      "[376]\ttraining's rmse: 0.986341\tvalid_1's rmse: 1.04401\n",
      "[377]\ttraining's rmse: 0.986195\tvalid_1's rmse: 1.04392\n",
      "[378]\ttraining's rmse: 0.986003\tvalid_1's rmse: 1.04374\n",
      "[379]\ttraining's rmse: 0.985806\tvalid_1's rmse: 1.04364\n",
      "[380]\ttraining's rmse: 0.985669\tvalid_1's rmse: 1.04358\n",
      "[381]\ttraining's rmse: 0.985484\tvalid_1's rmse: 1.04347\n",
      "[382]\ttraining's rmse: 0.98529\tvalid_1's rmse: 1.04336\n",
      "[383]\ttraining's rmse: 0.985105\tvalid_1's rmse: 1.04318\n",
      "[384]\ttraining's rmse: 0.984965\tvalid_1's rmse: 1.04307\n",
      "[385]\ttraining's rmse: 0.984775\tvalid_1's rmse: 1.043\n",
      "[386]\ttraining's rmse: 0.984639\tvalid_1's rmse: 1.04299\n",
      "[387]\ttraining's rmse: 0.984474\tvalid_1's rmse: 1.04282\n",
      "[388]\ttraining's rmse: 0.984291\tvalid_1's rmse: 1.04265\n",
      "[389]\ttraining's rmse: 0.984103\tvalid_1's rmse: 1.04259\n",
      "[390]\ttraining's rmse: 0.983914\tvalid_1's rmse: 1.04249\n",
      "[391]\ttraining's rmse: 0.983737\tvalid_1's rmse: 1.04232\n",
      "[392]\ttraining's rmse: 0.983593\tvalid_1's rmse: 1.04229\n",
      "[393]\ttraining's rmse: 0.983414\tvalid_1's rmse: 1.04223\n",
      "[394]\ttraining's rmse: 0.983237\tvalid_1's rmse: 1.04207\n",
      "[395]\ttraining's rmse: 0.983092\tvalid_1's rmse: 1.04198\n",
      "[396]\ttraining's rmse: 0.982934\tvalid_1's rmse: 1.04182\n",
      "[397]\ttraining's rmse: 0.982803\tvalid_1's rmse: 1.0418\n",
      "[398]\ttraining's rmse: 0.982628\tvalid_1's rmse: 1.0417\n",
      "[399]\ttraining's rmse: 0.982444\tvalid_1's rmse: 1.04161\n",
      "[400]\ttraining's rmse: 0.982304\tvalid_1's rmse: 1.04153\n",
      "[401]\ttraining's rmse: 0.982122\tvalid_1's rmse: 1.0415\n",
      "[402]\ttraining's rmse: 0.981972\tvalid_1's rmse: 1.04139\n",
      "[403]\ttraining's rmse: 0.981852\tvalid_1's rmse: 1.04135\n",
      "[404]\ttraining's rmse: 0.981666\tvalid_1's rmse: 1.0413\n",
      "[405]\ttraining's rmse: 0.981524\tvalid_1's rmse: 1.04115\n",
      "[406]\ttraining's rmse: 0.981387\tvalid_1's rmse: 1.04112\n",
      "[407]\ttraining's rmse: 0.981249\tvalid_1's rmse: 1.04108\n",
      "[408]\ttraining's rmse: 0.981077\tvalid_1's rmse: 1.04098\n",
      "[409]\ttraining's rmse: 0.980919\tvalid_1's rmse: 1.0408\n",
      "[410]\ttraining's rmse: 0.980772\tvalid_1's rmse: 1.04065\n",
      "[411]\ttraining's rmse: 0.980591\tvalid_1's rmse: 1.04063\n",
      "[412]\ttraining's rmse: 0.980476\tvalid_1's rmse: 1.04056\n",
      "[413]\ttraining's rmse: 0.980357\tvalid_1's rmse: 1.04048\n",
      "[414]\ttraining's rmse: 0.980209\tvalid_1's rmse: 1.04041\n",
      "[415]\ttraining's rmse: 0.980076\tvalid_1's rmse: 1.04034\n",
      "[416]\ttraining's rmse: 0.979919\tvalid_1's rmse: 1.04022\n",
      "[417]\ttraining's rmse: 0.979751\tvalid_1's rmse: 1.04017\n",
      "[418]\ttraining's rmse: 0.979607\tvalid_1's rmse: 1.04006\n",
      "[419]\ttraining's rmse: 0.97944\tvalid_1's rmse: 1.04002\n",
      "[420]\ttraining's rmse: 0.979331\tvalid_1's rmse: 1.04004\n",
      "[421]\ttraining's rmse: 0.979171\tvalid_1's rmse: 1.03993\n",
      "[422]\ttraining's rmse: 0.97905\tvalid_1's rmse: 1.03987\n",
      "[423]\ttraining's rmse: 0.978888\tvalid_1's rmse: 1.03978\n",
      "[424]\ttraining's rmse: 0.978754\tvalid_1's rmse: 1.03974\n",
      "[425]\ttraining's rmse: 0.978643\tvalid_1's rmse: 1.03967\n",
      "[426]\ttraining's rmse: 0.978483\tvalid_1's rmse: 1.03964\n",
      "[427]\ttraining's rmse: 0.978354\tvalid_1's rmse: 1.03958\n",
      "[428]\ttraining's rmse: 0.978187\tvalid_1's rmse: 1.03951\n",
      "[429]\ttraining's rmse: 0.978057\tvalid_1's rmse: 1.03947\n",
      "[430]\ttraining's rmse: 0.977896\tvalid_1's rmse: 1.03945\n",
      "[431]\ttraining's rmse: 0.977766\tvalid_1's rmse: 1.03944\n",
      "[432]\ttraining's rmse: 0.977647\tvalid_1's rmse: 1.03935\n",
      "[433]\ttraining's rmse: 0.977534\tvalid_1's rmse: 1.03928\n",
      "[434]\ttraining's rmse: 0.977409\tvalid_1's rmse: 1.03925\n",
      "[435]\ttraining's rmse: 0.977244\tvalid_1's rmse: 1.03919\n",
      "[436]\ttraining's rmse: 0.977145\tvalid_1's rmse: 1.03914\n",
      "[437]\ttraining's rmse: 0.977008\tvalid_1's rmse: 1.03906\n",
      "[438]\ttraining's rmse: 0.976884\tvalid_1's rmse: 1.03903\n",
      "[439]\ttraining's rmse: 0.976722\tvalid_1's rmse: 1.03902\n",
      "[440]\ttraining's rmse: 0.976597\tvalid_1's rmse: 1.03899\n",
      "[441]\ttraining's rmse: 0.976519\tvalid_1's rmse: 1.039\n",
      "[442]\ttraining's rmse: 0.976368\tvalid_1's rmse: 1.0389\n",
      "[443]\ttraining's rmse: 0.976258\tvalid_1's rmse: 1.03883\n",
      "[444]\ttraining's rmse: 0.976105\tvalid_1's rmse: 1.03881\n",
      "[445]\ttraining's rmse: 0.975984\tvalid_1's rmse: 1.03878\n",
      "[446]\ttraining's rmse: 0.975835\tvalid_1's rmse: 1.03872\n",
      "[447]\ttraining's rmse: 0.975717\tvalid_1's rmse: 1.03864\n",
      "[448]\ttraining's rmse: 0.97564\tvalid_1's rmse: 1.03865\n",
      "[449]\ttraining's rmse: 0.975512\tvalid_1's rmse: 1.03858\n",
      "[450]\ttraining's rmse: 0.97537\tvalid_1's rmse: 1.03854\n",
      "[451]\ttraining's rmse: 0.975264\tvalid_1's rmse: 1.03848\n",
      "[452]\ttraining's rmse: 0.975138\tvalid_1's rmse: 1.0384\n",
      "[453]\ttraining's rmse: 0.974985\tvalid_1's rmse: 1.03836\n",
      "[454]\ttraining's rmse: 0.974877\tvalid_1's rmse: 1.03827\n",
      "[455]\ttraining's rmse: 0.974743\tvalid_1's rmse: 1.03813\n",
      "[456]\ttraining's rmse: 0.974669\tvalid_1's rmse: 1.03813\n",
      "[457]\ttraining's rmse: 0.974544\tvalid_1's rmse: 1.03806\n",
      "[458]\ttraining's rmse: 0.974397\tvalid_1's rmse: 1.03799\n",
      "[459]\ttraining's rmse: 0.97426\tvalid_1's rmse: 1.03794\n",
      "[460]\ttraining's rmse: 0.974156\tvalid_1's rmse: 1.03792\n",
      "[461]\ttraining's rmse: 0.974036\tvalid_1's rmse: 1.03785\n",
      "[462]\ttraining's rmse: 0.9739\tvalid_1's rmse: 1.03774\n",
      "[463]\ttraining's rmse: 0.973749\tvalid_1's rmse: 1.03771\n",
      "[464]\ttraining's rmse: 0.973625\tvalid_1's rmse: 1.03765\n",
      "[465]\ttraining's rmse: 0.9735\tvalid_1's rmse: 1.0376\n",
      "[466]\ttraining's rmse: 0.973359\tvalid_1's rmse: 1.03749\n",
      "[467]\ttraining's rmse: 0.973223\tvalid_1's rmse: 1.03744\n",
      "[468]\ttraining's rmse: 0.973152\tvalid_1's rmse: 1.03745\n",
      "[469]\ttraining's rmse: 0.973033\tvalid_1's rmse: 1.03739\n",
      "[470]\ttraining's rmse: 0.972899\tvalid_1's rmse: 1.0373\n",
      "[471]\ttraining's rmse: 0.972798\tvalid_1's rmse: 1.03724\n",
      "[472]\ttraining's rmse: 0.97265\tvalid_1's rmse: 1.03722\n",
      "[473]\ttraining's rmse: 0.972537\tvalid_1's rmse: 1.03716\n",
      "[474]\ttraining's rmse: 0.972452\tvalid_1's rmse: 1.03717\n",
      "[475]\ttraining's rmse: 0.972319\tvalid_1's rmse: 1.03712\n",
      "[476]\ttraining's rmse: 0.972202\tvalid_1's rmse: 1.03712\n",
      "[477]\ttraining's rmse: 0.972072\tvalid_1's rmse: 1.03706\n",
      "[478]\ttraining's rmse: 0.97196\tvalid_1's rmse: 1.03699\n",
      "[479]\ttraining's rmse: 0.971823\tvalid_1's rmse: 1.03691\n",
      "[480]\ttraining's rmse: 0.971701\tvalid_1's rmse: 1.03687\n",
      "[481]\ttraining's rmse: 0.971604\tvalid_1's rmse: 1.03685\n",
      "[482]\ttraining's rmse: 0.971468\tvalid_1's rmse: 1.03681\n",
      "[483]\ttraining's rmse: 0.971394\tvalid_1's rmse: 1.03683\n",
      "[484]\ttraining's rmse: 0.97126\tvalid_1's rmse: 1.03676\n",
      "[485]\ttraining's rmse: 0.97114\tvalid_1's rmse: 1.03663\n",
      "[486]\ttraining's rmse: 0.971031\tvalid_1's rmse: 1.03658\n",
      "[487]\ttraining's rmse: 0.97091\tvalid_1's rmse: 1.03652\n",
      "[488]\ttraining's rmse: 0.970784\tvalid_1's rmse: 1.03643\n",
      "[489]\ttraining's rmse: 0.970668\tvalid_1's rmse: 1.03639\n",
      "[490]\ttraining's rmse: 0.97053\tvalid_1's rmse: 1.03639\n",
      "[491]\ttraining's rmse: 0.970423\tvalid_1's rmse: 1.03632\n",
      "[492]\ttraining's rmse: 0.970351\tvalid_1's rmse: 1.03633\n",
      "[493]\ttraining's rmse: 0.970216\tvalid_1's rmse: 1.03627\n",
      "[494]\ttraining's rmse: 0.970087\tvalid_1's rmse: 1.03621\n",
      "[495]\ttraining's rmse: 0.969992\tvalid_1's rmse: 1.03615\n",
      "[496]\ttraining's rmse: 0.969883\tvalid_1's rmse: 1.03611\n",
      "[497]\ttraining's rmse: 0.969766\tvalid_1's rmse: 1.03604\n",
      "[498]\ttraining's rmse: 0.969644\tvalid_1's rmse: 1.03598\n",
      "[499]\ttraining's rmse: 0.969533\tvalid_1's rmse: 1.03598\n",
      "[500]\ttraining's rmse: 0.969406\tvalid_1's rmse: 1.03595\n",
      "[501]\ttraining's rmse: 0.969266\tvalid_1's rmse: 1.03592\n",
      "[502]\ttraining's rmse: 0.96914\tvalid_1's rmse: 1.03591\n",
      "[503]\ttraining's rmse: 0.969021\tvalid_1's rmse: 1.03584\n",
      "[504]\ttraining's rmse: 0.968911\tvalid_1's rmse: 1.03582\n",
      "[505]\ttraining's rmse: 0.968789\tvalid_1's rmse: 1.03577\n",
      "[506]\ttraining's rmse: 0.968658\tvalid_1's rmse: 1.03572\n",
      "[507]\ttraining's rmse: 0.968587\tvalid_1's rmse: 1.03574\n",
      "[508]\ttraining's rmse: 0.968465\tvalid_1's rmse: 1.03568\n",
      "[509]\ttraining's rmse: 0.968367\tvalid_1's rmse: 1.03561\n",
      "[510]\ttraining's rmse: 0.96825\tvalid_1's rmse: 1.0355\n",
      "[511]\ttraining's rmse: 0.968125\tvalid_1's rmse: 1.03549\n",
      "[512]\ttraining's rmse: 0.968021\tvalid_1's rmse: 1.03549\n",
      "[513]\ttraining's rmse: 0.967902\tvalid_1's rmse: 1.0354\n",
      "[514]\ttraining's rmse: 0.967785\tvalid_1's rmse: 1.03536\n",
      "[515]\ttraining's rmse: 0.967663\tvalid_1's rmse: 1.03534\n",
      "[516]\ttraining's rmse: 0.967544\tvalid_1's rmse: 1.03531\n",
      "[517]\ttraining's rmse: 0.967433\tvalid_1's rmse: 1.03525\n",
      "[518]\ttraining's rmse: 0.967364\tvalid_1's rmse: 1.03527\n",
      "[519]\ttraining's rmse: 0.96725\tvalid_1's rmse: 1.0352\n",
      "[520]\ttraining's rmse: 0.967119\tvalid_1's rmse: 1.03514\n",
      "[521]\ttraining's rmse: 0.966998\tvalid_1's rmse: 1.03512\n",
      "[522]\ttraining's rmse: 0.966872\tvalid_1's rmse: 1.03509\n",
      "[523]\ttraining's rmse: 0.966757\tvalid_1's rmse: 1.03505\n",
      "[524]\ttraining's rmse: 0.966644\tvalid_1's rmse: 1.03499\n",
      "[525]\ttraining's rmse: 0.966525\tvalid_1's rmse: 1.03497\n",
      "[526]\ttraining's rmse: 0.966397\tvalid_1's rmse: 1.03494\n",
      "[527]\ttraining's rmse: 0.96633\tvalid_1's rmse: 1.03494\n",
      "[528]\ttraining's rmse: 0.966217\tvalid_1's rmse: 1.03489\n",
      "[529]\ttraining's rmse: 0.966107\tvalid_1's rmse: 1.03485\n",
      "[530]\ttraining's rmse: 0.965996\tvalid_1's rmse: 1.03479\n",
      "[531]\ttraining's rmse: 0.965884\tvalid_1's rmse: 1.03476\n",
      "[532]\ttraining's rmse: 0.965766\tvalid_1's rmse: 1.03478\n",
      "[533]\ttraining's rmse: 0.965651\tvalid_1's rmse: 1.03476\n",
      "[534]\ttraining's rmse: 0.965533\tvalid_1's rmse: 1.03472\n",
      "[535]\ttraining's rmse: 0.965432\tvalid_1's rmse: 1.03465\n",
      "[536]\ttraining's rmse: 0.965365\tvalid_1's rmse: 1.03468\n",
      "[537]\ttraining's rmse: 0.965251\tvalid_1's rmse: 1.03465\n",
      "[538]\ttraining's rmse: 0.965144\tvalid_1's rmse: 1.03458\n",
      "[539]\ttraining's rmse: 0.965036\tvalid_1's rmse: 1.03451\n",
      "[540]\ttraining's rmse: 0.964921\tvalid_1's rmse: 1.03448\n",
      "[541]\ttraining's rmse: 0.964806\tvalid_1's rmse: 1.03447\n",
      "[542]\ttraining's rmse: 0.964687\tvalid_1's rmse: 1.03445\n",
      "[543]\ttraining's rmse: 0.964579\tvalid_1's rmse: 1.03441\n",
      "[544]\ttraining's rmse: 0.964494\tvalid_1's rmse: 1.03435\n",
      "[545]\ttraining's rmse: 0.964373\tvalid_1's rmse: 1.03427\n",
      "[546]\ttraining's rmse: 0.964266\tvalid_1's rmse: 1.03418\n",
      "[547]\ttraining's rmse: 0.96417\tvalid_1's rmse: 1.03417\n",
      "[548]\ttraining's rmse: 0.964053\tvalid_1's rmse: 1.03414\n",
      "[549]\ttraining's rmse: 0.963961\tvalid_1's rmse: 1.0341\n",
      "[550]\ttraining's rmse: 0.963848\tvalid_1's rmse: 1.03408\n",
      "[551]\ttraining's rmse: 0.963739\tvalid_1's rmse: 1.03405\n",
      "[552]\ttraining's rmse: 0.963638\tvalid_1's rmse: 1.03402\n",
      "[553]\ttraining's rmse: 0.963507\tvalid_1's rmse: 1.03397\n",
      "[554]\ttraining's rmse: 0.963417\tvalid_1's rmse: 1.03393\n",
      "[555]\ttraining's rmse: 0.963299\tvalid_1's rmse: 1.0339\n",
      "[556]\ttraining's rmse: 0.963237\tvalid_1's rmse: 1.0339\n",
      "[557]\ttraining's rmse: 0.963132\tvalid_1's rmse: 1.03389\n",
      "[558]\ttraining's rmse: 0.963022\tvalid_1's rmse: 1.03387\n",
      "[559]\ttraining's rmse: 0.962912\tvalid_1's rmse: 1.03383\n",
      "[560]\ttraining's rmse: 0.962809\tvalid_1's rmse: 1.03378\n",
      "[561]\ttraining's rmse: 0.962693\tvalid_1's rmse: 1.03372\n",
      "[562]\ttraining's rmse: 0.962605\tvalid_1's rmse: 1.03368\n",
      "[563]\ttraining's rmse: 0.962497\tvalid_1's rmse: 1.03366\n",
      "[564]\ttraining's rmse: 0.962391\tvalid_1's rmse: 1.03356\n",
      "[565]\ttraining's rmse: 0.962304\tvalid_1's rmse: 1.03351\n",
      "[566]\ttraining's rmse: 0.962197\tvalid_1's rmse: 1.03347\n",
      "[567]\ttraining's rmse: 0.962162\tvalid_1's rmse: 1.03349\n",
      "[568]\ttraining's rmse: 0.962053\tvalid_1's rmse: 1.03349\n",
      "[569]\ttraining's rmse: 0.961926\tvalid_1's rmse: 1.03345\n",
      "[570]\ttraining's rmse: 0.961832\tvalid_1's rmse: 1.03346\n",
      "[571]\ttraining's rmse: 0.961741\tvalid_1's rmse: 1.03343\n",
      "[572]\ttraining's rmse: 0.961636\tvalid_1's rmse: 1.03342\n",
      "[573]\ttraining's rmse: 0.961552\tvalid_1's rmse: 1.03338\n",
      "[574]\ttraining's rmse: 0.961433\tvalid_1's rmse: 1.03335\n",
      "[575]\ttraining's rmse: 0.96131\tvalid_1's rmse: 1.0333\n",
      "[576]\ttraining's rmse: 0.961266\tvalid_1's rmse: 1.03328\n",
      "[577]\ttraining's rmse: 0.961153\tvalid_1's rmse: 1.03327\n",
      "[578]\ttraining's rmse: 0.961042\tvalid_1's rmse: 1.0332\n",
      "[579]\ttraining's rmse: 0.96094\tvalid_1's rmse: 1.03319\n",
      "[580]\ttraining's rmse: 0.960858\tvalid_1's rmse: 1.03313\n",
      "[581]\ttraining's rmse: 0.960758\tvalid_1's rmse: 1.03312\n",
      "[582]\ttraining's rmse: 0.960651\tvalid_1's rmse: 1.03305\n",
      "[583]\ttraining's rmse: 0.960569\tvalid_1's rmse: 1.03305\n",
      "[584]\ttraining's rmse: 0.96047\tvalid_1's rmse: 1.03303\n",
      "[585]\ttraining's rmse: 0.960381\tvalid_1's rmse: 1.03301\n",
      "[586]\ttraining's rmse: 0.960271\tvalid_1's rmse: 1.03299\n",
      "[587]\ttraining's rmse: 0.960164\tvalid_1's rmse: 1.03303\n",
      "[588]\ttraining's rmse: 0.960044\tvalid_1's rmse: 1.03304\n",
      "[589]\ttraining's rmse: 0.959947\tvalid_1's rmse: 1.03303\n",
      "[590]\ttraining's rmse: 0.959866\tvalid_1's rmse: 1.033\n",
      "[591]\ttraining's rmse: 0.959766\tvalid_1's rmse: 1.03299\n",
      "[592]\ttraining's rmse: 0.959686\tvalid_1's rmse: 1.033\n",
      "[593]\ttraining's rmse: 0.959592\tvalid_1's rmse: 1.03301\n",
      "[594]\ttraining's rmse: 0.959492\tvalid_1's rmse: 1.033\n",
      "[595]\ttraining's rmse: 0.959412\tvalid_1's rmse: 1.03295\n",
      "[596]\ttraining's rmse: 0.959305\tvalid_1's rmse: 1.03291\n",
      "[597]\ttraining's rmse: 0.959187\tvalid_1's rmse: 1.03287\n",
      "[598]\ttraining's rmse: 0.959097\tvalid_1's rmse: 1.03286\n",
      "[599]\ttraining's rmse: 0.959064\tvalid_1's rmse: 1.03286\n",
      "[600]\ttraining's rmse: 0.958964\tvalid_1's rmse: 1.03285\n",
      "[601]\ttraining's rmse: 0.958852\tvalid_1's rmse: 1.03279\n",
      "[602]\ttraining's rmse: 0.958749\tvalid_1's rmse: 1.03278\n",
      "[603]\ttraining's rmse: 0.95868\tvalid_1's rmse: 1.03274\n",
      "[604]\ttraining's rmse: 0.958606\tvalid_1's rmse: 1.03272\n",
      "[605]\ttraining's rmse: 0.958506\tvalid_1's rmse: 1.03271\n",
      "[606]\ttraining's rmse: 0.958409\tvalid_1's rmse: 1.03268\n",
      "[607]\ttraining's rmse: 0.958331\tvalid_1's rmse: 1.03269\n",
      "[608]\ttraining's rmse: 0.958238\tvalid_1's rmse: 1.0327\n",
      "[609]\ttraining's rmse: 0.958153\tvalid_1's rmse: 1.03268\n",
      "[610]\ttraining's rmse: 0.958039\tvalid_1's rmse: 1.03266\n",
      "[611]\ttraining's rmse: 0.957962\tvalid_1's rmse: 1.03265\n",
      "[612]\ttraining's rmse: 0.957877\tvalid_1's rmse: 1.03259\n",
      "[613]\ttraining's rmse: 0.957842\tvalid_1's rmse: 1.03256\n",
      "[614]\ttraining's rmse: 0.957737\tvalid_1's rmse: 1.03255\n",
      "[615]\ttraining's rmse: 0.957645\tvalid_1's rmse: 1.03254\n",
      "[616]\ttraining's rmse: 0.957568\tvalid_1's rmse: 1.03254\n",
      "[617]\ttraining's rmse: 0.957469\tvalid_1's rmse: 1.03252\n",
      "[618]\ttraining's rmse: 0.957404\tvalid_1's rmse: 1.03248\n",
      "[619]\ttraining's rmse: 0.957313\tvalid_1's rmse: 1.03245\n",
      "[620]\ttraining's rmse: 0.957222\tvalid_1's rmse: 1.03247\n",
      "[621]\ttraining's rmse: 0.957124\tvalid_1's rmse: 1.03244\n",
      "[622]\ttraining's rmse: 0.957049\tvalid_1's rmse: 1.0324\n",
      "[623]\ttraining's rmse: 0.956965\tvalid_1's rmse: 1.03241\n",
      "[624]\ttraining's rmse: 0.956886\tvalid_1's rmse: 1.03239\n",
      "[625]\ttraining's rmse: 0.95681\tvalid_1's rmse: 1.03236\n",
      "[626]\ttraining's rmse: 0.956697\tvalid_1's rmse: 1.03234\n",
      "[627]\ttraining's rmse: 0.956609\tvalid_1's rmse: 1.03233\n",
      "[628]\ttraining's rmse: 0.956543\tvalid_1's rmse: 1.03235\n",
      "[629]\ttraining's rmse: 0.956469\tvalid_1's rmse: 1.03233\n",
      "[630]\ttraining's rmse: 0.956363\tvalid_1's rmse: 1.03234\n",
      "[631]\ttraining's rmse: 0.956246\tvalid_1's rmse: 1.03234\n",
      "[632]\ttraining's rmse: 0.956169\tvalid_1's rmse: 1.03232\n",
      "[633]\ttraining's rmse: 0.956136\tvalid_1's rmse: 1.03229\n",
      "[634]\ttraining's rmse: 0.956047\tvalid_1's rmse: 1.03231\n",
      "[635]\ttraining's rmse: 0.955946\tvalid_1's rmse: 1.0323\n",
      "[636]\ttraining's rmse: 0.955859\tvalid_1's rmse: 1.03224\n",
      "[637]\ttraining's rmse: 0.955796\tvalid_1's rmse: 1.03223\n",
      "[638]\ttraining's rmse: 0.955718\tvalid_1's rmse: 1.03221\n",
      "[639]\ttraining's rmse: 0.955615\tvalid_1's rmse: 1.03217\n",
      "[640]\ttraining's rmse: 0.95551\tvalid_1's rmse: 1.03214\n",
      "[641]\ttraining's rmse: 0.955415\tvalid_1's rmse: 1.03211\n",
      "[642]\ttraining's rmse: 0.955341\tvalid_1's rmse: 1.03212\n",
      "[643]\ttraining's rmse: 0.955245\tvalid_1's rmse: 1.03215\n",
      "[644]\ttraining's rmse: 0.955143\tvalid_1's rmse: 1.03212\n",
      "[645]\ttraining's rmse: 0.955047\tvalid_1's rmse: 1.03205\n",
      "[646]\ttraining's rmse: 0.95496\tvalid_1's rmse: 1.03207\n",
      "[647]\ttraining's rmse: 0.954888\tvalid_1's rmse: 1.03206\n",
      "[648]\ttraining's rmse: 0.954827\tvalid_1's rmse: 1.03205\n",
      "[649]\ttraining's rmse: 0.954751\tvalid_1's rmse: 1.03204\n",
      "[650]\ttraining's rmse: 0.954719\tvalid_1's rmse: 1.03201\n",
      "[651]\ttraining's rmse: 0.954655\tvalid_1's rmse: 1.03202\n",
      "[652]\ttraining's rmse: 0.954569\tvalid_1's rmse: 1.03205\n",
      "[653]\ttraining's rmse: 0.954487\tvalid_1's rmse: 1.03203\n",
      "[654]\ttraining's rmse: 0.954393\tvalid_1's rmse: 1.03206\n",
      "[655]\ttraining's rmse: 0.954294\tvalid_1's rmse: 1.03202\n",
      "[656]\ttraining's rmse: 0.954136\tvalid_1's rmse: 1.03201\n",
      "[657]\ttraining's rmse: 0.954063\tvalid_1's rmse: 1.03198\n",
      "[658]\ttraining's rmse: 0.953979\tvalid_1's rmse: 1.032\n",
      "[659]\ttraining's rmse: 0.953905\tvalid_1's rmse: 1.03199\n",
      "[660]\ttraining's rmse: 0.953834\tvalid_1's rmse: 1.03196\n",
      "[661]\ttraining's rmse: 0.953744\tvalid_1's rmse: 1.03197\n",
      "[662]\ttraining's rmse: 0.953672\tvalid_1's rmse: 1.03198\n",
      "[663]\ttraining's rmse: 0.953642\tvalid_1's rmse: 1.03196\n",
      "[664]\ttraining's rmse: 0.953565\tvalid_1's rmse: 1.03193\n",
      "[665]\ttraining's rmse: 0.953472\tvalid_1's rmse: 1.03192\n",
      "[666]\ttraining's rmse: 0.95338\tvalid_1's rmse: 1.03193\n",
      "[667]\ttraining's rmse: 0.953297\tvalid_1's rmse: 1.03195\n",
      "[668]\ttraining's rmse: 0.95321\tvalid_1's rmse: 1.03196\n",
      "[669]\ttraining's rmse: 0.953112\tvalid_1's rmse: 1.03193\n",
      "[670]\ttraining's rmse: 0.95304\tvalid_1's rmse: 1.03192\n",
      "[671]\ttraining's rmse: 0.952941\tvalid_1's rmse: 1.03189\n",
      "[672]\ttraining's rmse: 0.95287\tvalid_1's rmse: 1.03187\n",
      "[673]\ttraining's rmse: 0.952801\tvalid_1's rmse: 1.03185\n",
      "[674]\ttraining's rmse: 0.952717\tvalid_1's rmse: 1.03188\n",
      "[675]\ttraining's rmse: 0.952656\tvalid_1's rmse: 1.0319\n",
      "[676]\ttraining's rmse: 0.952574\tvalid_1's rmse: 1.03188\n",
      "[677]\ttraining's rmse: 0.952477\tvalid_1's rmse: 1.03188\n",
      "[678]\ttraining's rmse: 0.952353\tvalid_1's rmse: 1.03189\n",
      "[679]\ttraining's rmse: 0.952326\tvalid_1's rmse: 1.0319\n",
      "[680]\ttraining's rmse: 0.952249\tvalid_1's rmse: 1.03186\n",
      "[681]\ttraining's rmse: 0.952158\tvalid_1's rmse: 1.03186\n",
      "[682]\ttraining's rmse: 0.95209\tvalid_1's rmse: 1.03185\n",
      "[683]\ttraining's rmse: 0.951998\tvalid_1's rmse: 1.03184\n",
      "[684]\ttraining's rmse: 0.951847\tvalid_1's rmse: 1.03184\n",
      "[685]\ttraining's rmse: 0.951784\tvalid_1's rmse: 1.03186\n",
      "[686]\ttraining's rmse: 0.951755\tvalid_1's rmse: 1.03183\n",
      "[687]\ttraining's rmse: 0.951654\tvalid_1's rmse: 1.03188\n",
      "[688]\ttraining's rmse: 0.95158\tvalid_1's rmse: 1.03185\n",
      "[689]\ttraining's rmse: 0.9515\tvalid_1's rmse: 1.0319\n",
      "[690]\ttraining's rmse: 0.951437\tvalid_1's rmse: 1.03192\n",
      "[691]\ttraining's rmse: 0.951328\tvalid_1's rmse: 1.03191\n",
      "[692]\ttraining's rmse: 0.951232\tvalid_1's rmse: 1.0319\n",
      "[693]\ttraining's rmse: 0.951147\tvalid_1's rmse: 1.03191\n",
      "[694]\ttraining's rmse: 0.951075\tvalid_1's rmse: 1.03191\n",
      "[695]\ttraining's rmse: 0.951049\tvalid_1's rmse: 1.03193\n",
      "[696]\ttraining's rmse: 0.950956\tvalid_1's rmse: 1.0319\n",
      "Early stopping, best iteration is:\n",
      "[686]\ttraining's rmse: 0.951755\tvalid_1's rmse: 1.03183\n",
      "fold_1 coefficients:  [0.54598203 1.59287953 2.18176521]\n",
      "[1]\ttraining's rmse: 1.25317\tvalid_1's rmse: 1.23644\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24966\tvalid_1's rmse: 1.23352\n",
      "[3]\ttraining's rmse: 1.24621\tvalid_1's rmse: 1.23066\n",
      "[4]\ttraining's rmse: 1.24282\tvalid_1's rmse: 1.22786\n",
      "[5]\ttraining's rmse: 1.23949\tvalid_1's rmse: 1.22511\n",
      "[6]\ttraining's rmse: 1.23621\tvalid_1's rmse: 1.22241\n",
      "[7]\ttraining's rmse: 1.233\tvalid_1's rmse: 1.21976\n",
      "[8]\ttraining's rmse: 1.22983\tvalid_1's rmse: 1.21717\n",
      "[9]\ttraining's rmse: 1.22673\tvalid_1's rmse: 1.21463\n",
      "[10]\ttraining's rmse: 1.22367\tvalid_1's rmse: 1.21214\n",
      "[11]\ttraining's rmse: 1.22067\tvalid_1's rmse: 1.20969\n",
      "[12]\ttraining's rmse: 1.21781\tvalid_1's rmse: 1.20739\n",
      "[13]\ttraining's rmse: 1.21499\tvalid_1's rmse: 1.20514\n",
      "[14]\ttraining's rmse: 1.21213\tvalid_1's rmse: 1.20283\n",
      "[15]\ttraining's rmse: 1.2095\tvalid_1's rmse: 1.20081\n",
      "[16]\ttraining's rmse: 1.20672\tvalid_1's rmse: 1.19858\n",
      "[17]\ttraining's rmse: 1.20418\tvalid_1's rmse: 1.19663\n",
      "[18]\ttraining's rmse: 1.20149\tvalid_1's rmse: 1.1945\n",
      "[19]\ttraining's rmse: 1.19894\tvalid_1's rmse: 1.19246\n",
      "[20]\ttraining's rmse: 1.19651\tvalid_1's rmse: 1.19063\n",
      "[21]\ttraining's rmse: 1.19392\tvalid_1's rmse: 1.18856\n",
      "[22]\ttraining's rmse: 1.19157\tvalid_1's rmse: 1.1868\n",
      "[23]\ttraining's rmse: 1.18907\tvalid_1's rmse: 1.18481\n",
      "[24]\ttraining's rmse: 1.18672\tvalid_1's rmse: 1.18297\n",
      "[25]\ttraining's rmse: 1.1843\tvalid_1's rmse: 1.18107\n",
      "[26]\ttraining's rmse: 1.1821\tvalid_1's rmse: 1.17941\n",
      "[27]\ttraining's rmse: 1.17974\tvalid_1's rmse: 1.17755\n",
      "[28]\ttraining's rmse: 1.17761\tvalid_1's rmse: 1.17596\n",
      "[29]\ttraining's rmse: 1.17534\tvalid_1's rmse: 1.17421\n",
      "[30]\ttraining's rmse: 1.17326\tvalid_1's rmse: 1.17262\n",
      "[31]\ttraining's rmse: 1.17117\tvalid_1's rmse: 1.17101\n",
      "[32]\ttraining's rmse: 1.169\tvalid_1's rmse: 1.16934\n",
      "[33]\ttraining's rmse: 1.16704\tvalid_1's rmse: 1.16791\n",
      "[34]\ttraining's rmse: 1.16492\tvalid_1's rmse: 1.16629\n",
      "[35]\ttraining's rmse: 1.16308\tvalid_1's rmse: 1.16498\n",
      "[36]\ttraining's rmse: 1.1612\tvalid_1's rmse: 1.16358\n",
      "[37]\ttraining's rmse: 1.15919\tvalid_1's rmse: 1.16206\n",
      "[38]\ttraining's rmse: 1.15743\tvalid_1's rmse: 1.1608\n",
      "[39]\ttraining's rmse: 1.15566\tvalid_1's rmse: 1.1595\n",
      "[40]\ttraining's rmse: 1.15395\tvalid_1's rmse: 1.1583\n",
      "[41]\ttraining's rmse: 1.15205\tvalid_1's rmse: 1.15684\n",
      "[42]\ttraining's rmse: 1.15037\tvalid_1's rmse: 1.15573\n",
      "[43]\ttraining's rmse: 1.14854\tvalid_1's rmse: 1.15437\n",
      "[44]\ttraining's rmse: 1.14694\tvalid_1's rmse: 1.15327\n",
      "[45]\ttraining's rmse: 1.14532\tvalid_1's rmse: 1.15208\n",
      "[46]\ttraining's rmse: 1.14372\tvalid_1's rmse: 1.15096\n",
      "[47]\ttraining's rmse: 1.14215\tvalid_1's rmse: 1.14998\n",
      "[48]\ttraining's rmse: 1.14044\tvalid_1's rmse: 1.14865\n",
      "[49]\ttraining's rmse: 1.13891\tvalid_1's rmse: 1.14758\n",
      "[50]\ttraining's rmse: 1.1374\tvalid_1's rmse: 1.14649\n",
      "[51]\ttraining's rmse: 1.13593\tvalid_1's rmse: 1.14557\n",
      "[52]\ttraining's rmse: 1.13452\tvalid_1's rmse: 1.14461\n",
      "[53]\ttraining's rmse: 1.13309\tvalid_1's rmse: 1.14359\n",
      "[54]\ttraining's rmse: 1.13151\tvalid_1's rmse: 1.14245\n",
      "[55]\ttraining's rmse: 1.13011\tvalid_1's rmse: 1.14151\n",
      "[56]\ttraining's rmse: 1.12875\tvalid_1's rmse: 1.14069\n",
      "[57]\ttraining's rmse: 1.1274\tvalid_1's rmse: 1.13974\n",
      "[58]\ttraining's rmse: 1.12607\tvalid_1's rmse: 1.13884\n",
      "[59]\ttraining's rmse: 1.12478\tvalid_1's rmse: 1.13805\n",
      "[60]\ttraining's rmse: 1.12329\tvalid_1's rmse: 1.13692\n",
      "[61]\ttraining's rmse: 1.12206\tvalid_1's rmse: 1.13615\n",
      "[62]\ttraining's rmse: 1.12064\tvalid_1's rmse: 1.13515\n",
      "[63]\ttraining's rmse: 1.11942\tvalid_1's rmse: 1.13438\n",
      "[64]\ttraining's rmse: 1.11808\tvalid_1's rmse: 1.13338\n",
      "[65]\ttraining's rmse: 1.11668\tvalid_1's rmse: 1.13236\n",
      "[66]\ttraining's rmse: 1.1153\tvalid_1's rmse: 1.13131\n",
      "[67]\ttraining's rmse: 1.11413\tvalid_1's rmse: 1.13057\n",
      "[68]\ttraining's rmse: 1.11279\tvalid_1's rmse: 1.1296\n",
      "[69]\ttraining's rmse: 1.11168\tvalid_1's rmse: 1.12886\n",
      "[70]\ttraining's rmse: 1.11034\tvalid_1's rmse: 1.12781\n",
      "[71]\ttraining's rmse: 1.10926\tvalid_1's rmse: 1.12717\n",
      "[72]\ttraining's rmse: 1.10797\tvalid_1's rmse: 1.1262\n",
      "[73]\ttraining's rmse: 1.10687\tvalid_1's rmse: 1.12551\n",
      "[74]\ttraining's rmse: 1.10586\tvalid_1's rmse: 1.12487\n",
      "[75]\ttraining's rmse: 1.10461\tvalid_1's rmse: 1.1239\n",
      "[76]\ttraining's rmse: 1.10356\tvalid_1's rmse: 1.12328\n",
      "[77]\ttraining's rmse: 1.10255\tvalid_1's rmse: 1.12267\n",
      "[78]\ttraining's rmse: 1.10136\tvalid_1's rmse: 1.12179\n",
      "[79]\ttraining's rmse: 1.10039\tvalid_1's rmse: 1.12118\n",
      "[80]\ttraining's rmse: 1.0992\tvalid_1's rmse: 1.1204\n",
      "[81]\ttraining's rmse: 1.09811\tvalid_1's rmse: 1.11959\n",
      "[82]\ttraining's rmse: 1.09698\tvalid_1's rmse: 1.11875\n",
      "[83]\ttraining's rmse: 1.09583\tvalid_1's rmse: 1.11793\n",
      "[84]\ttraining's rmse: 1.09493\tvalid_1's rmse: 1.11744\n",
      "[85]\ttraining's rmse: 1.09383\tvalid_1's rmse: 1.11656\n",
      "[86]\ttraining's rmse: 1.09297\tvalid_1's rmse: 1.11607\n",
      "[87]\ttraining's rmse: 1.09207\tvalid_1's rmse: 1.11552\n",
      "[88]\ttraining's rmse: 1.09109\tvalid_1's rmse: 1.1148\n",
      "[89]\ttraining's rmse: 1.09005\tvalid_1's rmse: 1.11397\n",
      "[90]\ttraining's rmse: 1.08901\tvalid_1's rmse: 1.11326\n",
      "[91]\ttraining's rmse: 1.08818\tvalid_1's rmse: 1.11277\n",
      "[92]\ttraining's rmse: 1.08737\tvalid_1's rmse: 1.11234\n",
      "[93]\ttraining's rmse: 1.08638\tvalid_1's rmse: 1.11157\n",
      "[94]\ttraining's rmse: 1.08552\tvalid_1's rmse: 1.11106\n",
      "[95]\ttraining's rmse: 1.08475\tvalid_1's rmse: 1.11062\n",
      "[96]\ttraining's rmse: 1.08377\tvalid_1's rmse: 1.10994\n",
      "[97]\ttraining's rmse: 1.08283\tvalid_1's rmse: 1.10923\n",
      "[98]\ttraining's rmse: 1.08197\tvalid_1's rmse: 1.10858\n",
      "[99]\ttraining's rmse: 1.08118\tvalid_1's rmse: 1.10814\n",
      "[100]\ttraining's rmse: 1.08027\tvalid_1's rmse: 1.10742\n",
      "[101]\ttraining's rmse: 1.07956\tvalid_1's rmse: 1.10694\n",
      "[102]\ttraining's rmse: 1.07874\tvalid_1's rmse: 1.10637\n",
      "[103]\ttraining's rmse: 1.07784\tvalid_1's rmse: 1.10583\n",
      "[104]\ttraining's rmse: 1.07709\tvalid_1's rmse: 1.10536\n",
      "[105]\ttraining's rmse: 1.07604\tvalid_1's rmse: 1.10453\n",
      "[106]\ttraining's rmse: 1.07537\tvalid_1's rmse: 1.10415\n",
      "[107]\ttraining's rmse: 1.07464\tvalid_1's rmse: 1.10374\n",
      "[108]\ttraining's rmse: 1.07388\tvalid_1's rmse: 1.10318\n",
      "[109]\ttraining's rmse: 1.07318\tvalid_1's rmse: 1.10287\n",
      "[110]\ttraining's rmse: 1.07219\tvalid_1's rmse: 1.10208\n",
      "[111]\ttraining's rmse: 1.07139\tvalid_1's rmse: 1.10134\n",
      "[112]\ttraining's rmse: 1.07076\tvalid_1's rmse: 1.10097\n",
      "[113]\ttraining's rmse: 1.07009\tvalid_1's rmse: 1.10047\n",
      "[114]\ttraining's rmse: 1.06943\tvalid_1's rmse: 1.10007\n",
      "[115]\ttraining's rmse: 1.06872\tvalid_1's rmse: 1.09957\n",
      "[116]\ttraining's rmse: 1.06794\tvalid_1's rmse: 1.09904\n",
      "[117]\ttraining's rmse: 1.06702\tvalid_1's rmse: 1.09836\n",
      "[118]\ttraining's rmse: 1.0662\tvalid_1's rmse: 1.09779\n",
      "[119]\ttraining's rmse: 1.06558\tvalid_1's rmse: 1.09727\n",
      "[120]\ttraining's rmse: 1.06496\tvalid_1's rmse: 1.09697\n",
      "[121]\ttraining's rmse: 1.06441\tvalid_1's rmse: 1.09674\n",
      "[122]\ttraining's rmse: 1.06375\tvalid_1's rmse: 1.09621\n",
      "[123]\ttraining's rmse: 1.06288\tvalid_1's rmse: 1.09553\n",
      "[124]\ttraining's rmse: 1.06223\tvalid_1's rmse: 1.09514\n",
      "[125]\ttraining's rmse: 1.06158\tvalid_1's rmse: 1.09473\n",
      "[126]\ttraining's rmse: 1.061\tvalid_1's rmse: 1.0944\n",
      "[127]\ttraining's rmse: 1.06017\tvalid_1's rmse: 1.09375\n",
      "[128]\ttraining's rmse: 1.05966\tvalid_1's rmse: 1.09347\n",
      "[129]\ttraining's rmse: 1.05904\tvalid_1's rmse: 1.09309\n",
      "[130]\ttraining's rmse: 1.05845\tvalid_1's rmse: 1.0927\n",
      "[131]\ttraining's rmse: 1.05786\tvalid_1's rmse: 1.09233\n",
      "[132]\ttraining's rmse: 1.05706\tvalid_1's rmse: 1.09163\n",
      "[133]\ttraining's rmse: 1.05653\tvalid_1's rmse: 1.0914\n",
      "[134]\ttraining's rmse: 1.05595\tvalid_1's rmse: 1.09106\n",
      "[135]\ttraining's rmse: 1.0554\tvalid_1's rmse: 1.09073\n",
      "[136]\ttraining's rmse: 1.05493\tvalid_1's rmse: 1.09043\n",
      "[137]\ttraining's rmse: 1.05437\tvalid_1's rmse: 1.09008\n",
      "[138]\ttraining's rmse: 1.05361\tvalid_1's rmse: 1.08945\n",
      "[139]\ttraining's rmse: 1.05307\tvalid_1's rmse: 1.08918\n",
      "[140]\ttraining's rmse: 1.05254\tvalid_1's rmse: 1.08883\n",
      "[141]\ttraining's rmse: 1.052\tvalid_1's rmse: 1.08842\n",
      "[142]\ttraining's rmse: 1.05149\tvalid_1's rmse: 1.08805\n",
      "[143]\ttraining's rmse: 1.05105\tvalid_1's rmse: 1.08777\n",
      "[144]\ttraining's rmse: 1.05033\tvalid_1's rmse: 1.0872\n",
      "[145]\ttraining's rmse: 1.04981\tvalid_1's rmse: 1.08689\n",
      "[146]\ttraining's rmse: 1.04937\tvalid_1's rmse: 1.08674\n",
      "[147]\ttraining's rmse: 1.04886\tvalid_1's rmse: 1.08644\n",
      "[148]\ttraining's rmse: 1.04845\tvalid_1's rmse: 1.0862\n",
      "[149]\ttraining's rmse: 1.04775\tvalid_1's rmse: 1.08568\n",
      "[150]\ttraining's rmse: 1.04729\tvalid_1's rmse: 1.08549\n",
      "[151]\ttraining's rmse: 1.04682\tvalid_1's rmse: 1.08511\n",
      "[152]\ttraining's rmse: 1.04637\tvalid_1's rmse: 1.08486\n",
      "[153]\ttraining's rmse: 1.0458\tvalid_1's rmse: 1.08449\n",
      "[154]\ttraining's rmse: 1.04532\tvalid_1's rmse: 1.0842\n",
      "[155]\ttraining's rmse: 1.04484\tvalid_1's rmse: 1.0839\n",
      "[156]\ttraining's rmse: 1.04419\tvalid_1's rmse: 1.08339\n",
      "[157]\ttraining's rmse: 1.04375\tvalid_1's rmse: 1.08299\n",
      "[158]\ttraining's rmse: 1.04332\tvalid_1's rmse: 1.08275\n",
      "[159]\ttraining's rmse: 1.0426\tvalid_1's rmse: 1.08219\n",
      "[160]\ttraining's rmse: 1.0419\tvalid_1's rmse: 1.08163\n",
      "[161]\ttraining's rmse: 1.0412\tvalid_1's rmse: 1.08111\n",
      "[162]\ttraining's rmse: 1.04053\tvalid_1's rmse: 1.08058\n",
      "[163]\ttraining's rmse: 1.03986\tvalid_1's rmse: 1.08007\n",
      "[164]\ttraining's rmse: 1.03921\tvalid_1's rmse: 1.07956\n",
      "[165]\ttraining's rmse: 1.03857\tvalid_1's rmse: 1.07904\n",
      "[166]\ttraining's rmse: 1.03794\tvalid_1's rmse: 1.07855\n",
      "[167]\ttraining's rmse: 1.03732\tvalid_1's rmse: 1.07805\n",
      "[168]\ttraining's rmse: 1.03672\tvalid_1's rmse: 1.07765\n",
      "[169]\ttraining's rmse: 1.03611\tvalid_1's rmse: 1.07723\n",
      "[170]\ttraining's rmse: 1.03553\tvalid_1's rmse: 1.07672\n",
      "[171]\ttraining's rmse: 1.03493\tvalid_1's rmse: 1.07625\n",
      "[172]\ttraining's rmse: 1.03437\tvalid_1's rmse: 1.07586\n",
      "[173]\ttraining's rmse: 1.03395\tvalid_1's rmse: 1.07554\n",
      "[174]\ttraining's rmse: 1.03354\tvalid_1's rmse: 1.07524\n",
      "[175]\ttraining's rmse: 1.03298\tvalid_1's rmse: 1.07486\n",
      "[176]\ttraining's rmse: 1.03258\tvalid_1's rmse: 1.07455\n",
      "[177]\ttraining's rmse: 1.03205\tvalid_1's rmse: 1.0742\n",
      "[178]\ttraining's rmse: 1.03156\tvalid_1's rmse: 1.0738\n",
      "[179]\ttraining's rmse: 1.03118\tvalid_1's rmse: 1.07358\n",
      "[180]\ttraining's rmse: 1.03066\tvalid_1's rmse: 1.07329\n",
      "[181]\ttraining's rmse: 1.0302\tvalid_1's rmse: 1.07297\n",
      "[182]\ttraining's rmse: 1.02974\tvalid_1's rmse: 1.07259\n",
      "[183]\ttraining's rmse: 1.02934\tvalid_1's rmse: 1.07249\n",
      "[184]\ttraining's rmse: 1.0289\tvalid_1's rmse: 1.07219\n",
      "[185]\ttraining's rmse: 1.0285\tvalid_1's rmse: 1.07209\n",
      "[186]\ttraining's rmse: 1.02813\tvalid_1's rmse: 1.07184\n",
      "[187]\ttraining's rmse: 1.02773\tvalid_1's rmse: 1.07172\n",
      "[188]\ttraining's rmse: 1.02728\tvalid_1's rmse: 1.07139\n",
      "[189]\ttraining's rmse: 1.02691\tvalid_1's rmse: 1.07113\n",
      "[190]\ttraining's rmse: 1.0265\tvalid_1's rmse: 1.07081\n",
      "[191]\ttraining's rmse: 1.02611\tvalid_1's rmse: 1.07067\n",
      "[192]\ttraining's rmse: 1.02568\tvalid_1's rmse: 1.07036\n",
      "[193]\ttraining's rmse: 1.02532\tvalid_1's rmse: 1.07014\n",
      "[194]\ttraining's rmse: 1.02499\tvalid_1's rmse: 1.06994\n",
      "[195]\ttraining's rmse: 1.02464\tvalid_1's rmse: 1.06971\n",
      "[196]\ttraining's rmse: 1.02423\tvalid_1's rmse: 1.06942\n",
      "[197]\ttraining's rmse: 1.02388\tvalid_1's rmse: 1.06919\n",
      "[198]\ttraining's rmse: 1.02351\tvalid_1's rmse: 1.06889\n",
      "[199]\ttraining's rmse: 1.0232\tvalid_1's rmse: 1.0686\n",
      "[200]\ttraining's rmse: 1.02283\tvalid_1's rmse: 1.06849\n",
      "[201]\ttraining's rmse: 1.02245\tvalid_1's rmse: 1.06824\n",
      "[202]\ttraining's rmse: 1.02211\tvalid_1's rmse: 1.0681\n",
      "[203]\ttraining's rmse: 1.0218\tvalid_1's rmse: 1.06793\n",
      "[204]\ttraining's rmse: 1.02143\tvalid_1's rmse: 1.06777\n",
      "[205]\ttraining's rmse: 1.02106\tvalid_1's rmse: 1.06751\n",
      "[206]\ttraining's rmse: 1.02077\tvalid_1's rmse: 1.06727\n",
      "[207]\ttraining's rmse: 1.02048\tvalid_1's rmse: 1.06712\n",
      "[208]\ttraining's rmse: 1.02018\tvalid_1's rmse: 1.06706\n",
      "[209]\ttraining's rmse: 1.01984\tvalid_1's rmse: 1.067\n",
      "[210]\ttraining's rmse: 1.01947\tvalid_1's rmse: 1.06676\n",
      "[211]\ttraining's rmse: 1.01916\tvalid_1's rmse: 1.06657\n",
      "[212]\ttraining's rmse: 1.01883\tvalid_1's rmse: 1.06654\n",
      "[213]\ttraining's rmse: 1.01853\tvalid_1's rmse: 1.06639\n",
      "[214]\ttraining's rmse: 1.01822\tvalid_1's rmse: 1.06619\n",
      "[215]\ttraining's rmse: 1.01788\tvalid_1's rmse: 1.06595\n",
      "[216]\ttraining's rmse: 1.01756\tvalid_1's rmse: 1.06591\n",
      "[217]\ttraining's rmse: 1.01729\tvalid_1's rmse: 1.06565\n",
      "[218]\ttraining's rmse: 1.01699\tvalid_1's rmse: 1.06531\n",
      "[219]\ttraining's rmse: 1.0166\tvalid_1's rmse: 1.06512\n",
      "[220]\ttraining's rmse: 1.01626\tvalid_1's rmse: 1.065\n",
      "[221]\ttraining's rmse: 1.01597\tvalid_1's rmse: 1.06483\n",
      "[222]\ttraining's rmse: 1.01565\tvalid_1's rmse: 1.06462\n",
      "[223]\ttraining's rmse: 1.01531\tvalid_1's rmse: 1.06451\n",
      "[224]\ttraining's rmse: 1.01505\tvalid_1's rmse: 1.06431\n",
      "[225]\ttraining's rmse: 1.01473\tvalid_1's rmse: 1.06411\n",
      "[226]\ttraining's rmse: 1.01446\tvalid_1's rmse: 1.06398\n",
      "[227]\ttraining's rmse: 1.01415\tvalid_1's rmse: 1.06377\n",
      "[228]\ttraining's rmse: 1.01384\tvalid_1's rmse: 1.06367\n",
      "[229]\ttraining's rmse: 1.01355\tvalid_1's rmse: 1.06362\n",
      "[230]\ttraining's rmse: 1.01319\tvalid_1's rmse: 1.06344\n",
      "[231]\ttraining's rmse: 1.01292\tvalid_1's rmse: 1.06313\n",
      "[232]\ttraining's rmse: 1.01257\tvalid_1's rmse: 1.06295\n",
      "[233]\ttraining's rmse: 1.01224\tvalid_1's rmse: 1.06279\n",
      "[234]\ttraining's rmse: 1.01198\tvalid_1's rmse: 1.06257\n",
      "[235]\ttraining's rmse: 1.01164\tvalid_1's rmse: 1.0624\n",
      "[236]\ttraining's rmse: 1.01133\tvalid_1's rmse: 1.06229\n",
      "[237]\ttraining's rmse: 1.01104\tvalid_1's rmse: 1.06215\n",
      "[238]\ttraining's rmse: 1.01077\tvalid_1's rmse: 1.06211\n",
      "[239]\ttraining's rmse: 1.01043\tvalid_1's rmse: 1.06188\n",
      "[240]\ttraining's rmse: 1.01017\tvalid_1's rmse: 1.06158\n",
      "[241]\ttraining's rmse: 1.00984\tvalid_1's rmse: 1.06142\n",
      "[242]\ttraining's rmse: 1.00955\tvalid_1's rmse: 1.06132\n",
      "[243]\ttraining's rmse: 1.00924\tvalid_1's rmse: 1.06118\n",
      "[244]\ttraining's rmse: 1.009\tvalid_1's rmse: 1.06104\n",
      "[245]\ttraining's rmse: 1.00865\tvalid_1's rmse: 1.06084\n",
      "[246]\ttraining's rmse: 1.0084\tvalid_1's rmse: 1.06055\n",
      "[247]\ttraining's rmse: 1.00806\tvalid_1's rmse: 1.06039\n",
      "[248]\ttraining's rmse: 1.00776\tvalid_1's rmse: 1.06026\n",
      "[249]\ttraining's rmse: 1.00752\tvalid_1's rmse: 1.06017\n",
      "[250]\ttraining's rmse: 1.00721\tvalid_1's rmse: 1.06001\n",
      "[251]\ttraining's rmse: 1.00692\tvalid_1's rmse: 1.05988\n",
      "[252]\ttraining's rmse: 1.0066\tvalid_1's rmse: 1.05969\n",
      "[253]\ttraining's rmse: 1.00634\tvalid_1's rmse: 1.05963\n",
      "[254]\ttraining's rmse: 1.00609\tvalid_1's rmse: 1.05961\n",
      "[255]\ttraining's rmse: 1.00583\tvalid_1's rmse: 1.05952\n",
      "[256]\ttraining's rmse: 1.00559\tvalid_1's rmse: 1.05924\n",
      "[257]\ttraining's rmse: 1.00526\tvalid_1's rmse: 1.05904\n",
      "[258]\ttraining's rmse: 1.00499\tvalid_1's rmse: 1.05895\n",
      "[259]\ttraining's rmse: 1.0047\tvalid_1's rmse: 1.05875\n",
      "[260]\ttraining's rmse: 1.00445\tvalid_1's rmse: 1.0587\n",
      "[261]\ttraining's rmse: 1.00419\tvalid_1's rmse: 1.05861\n",
      "[262]\ttraining's rmse: 1.00397\tvalid_1's rmse: 1.05835\n",
      "[263]\ttraining's rmse: 1.00365\tvalid_1's rmse: 1.05816\n",
      "[264]\ttraining's rmse: 1.00338\tvalid_1's rmse: 1.05804\n",
      "[265]\ttraining's rmse: 1.00313\tvalid_1's rmse: 1.05794\n",
      "[266]\ttraining's rmse: 1.0029\tvalid_1's rmse: 1.05764\n",
      "[267]\ttraining's rmse: 1.00261\tvalid_1's rmse: 1.0575\n",
      "[268]\ttraining's rmse: 1.00237\tvalid_1's rmse: 1.05744\n",
      "[269]\ttraining's rmse: 1.00214\tvalid_1's rmse: 1.05739\n",
      "[270]\ttraining's rmse: 1.00185\tvalid_1's rmse: 1.05717\n",
      "[271]\ttraining's rmse: 1.00162\tvalid_1's rmse: 1.05708\n",
      "[272]\ttraining's rmse: 1.00137\tvalid_1's rmse: 1.05697\n",
      "[273]\ttraining's rmse: 1.00109\tvalid_1's rmse: 1.05682\n",
      "[274]\ttraining's rmse: 1.00086\tvalid_1's rmse: 1.05676\n",
      "[275]\ttraining's rmse: 1.00062\tvalid_1's rmse: 1.05666\n",
      "[276]\ttraining's rmse: 1.00042\tvalid_1's rmse: 1.05644\n",
      "[277]\ttraining's rmse: 1.00012\tvalid_1's rmse: 1.05628\n",
      "[278]\ttraining's rmse: 0.999876\tvalid_1's rmse: 1.05617\n",
      "[279]\ttraining's rmse: 0.99964\tvalid_1's rmse: 1.05609\n",
      "[280]\ttraining's rmse: 0.999413\tvalid_1's rmse: 1.05608\n",
      "[281]\ttraining's rmse: 0.999207\tvalid_1's rmse: 1.05589\n",
      "[282]\ttraining's rmse: 0.998918\tvalid_1's rmse: 1.05573\n",
      "[283]\ttraining's rmse: 0.998683\tvalid_1's rmse: 1.05564\n",
      "[284]\ttraining's rmse: 0.998464\tvalid_1's rmse: 1.05563\n",
      "[285]\ttraining's rmse: 0.998272\tvalid_1's rmse: 1.05544\n",
      "[286]\ttraining's rmse: 0.998016\tvalid_1's rmse: 1.05532\n",
      "[287]\ttraining's rmse: 0.997788\tvalid_1's rmse: 1.05524\n",
      "[288]\ttraining's rmse: 0.997593\tvalid_1's rmse: 1.05516\n",
      "[289]\ttraining's rmse: 0.997316\tvalid_1's rmse: 1.05503\n",
      "[290]\ttraining's rmse: 0.997105\tvalid_1's rmse: 1.055\n",
      "[291]\ttraining's rmse: 0.996919\tvalid_1's rmse: 1.05481\n",
      "[292]\ttraining's rmse: 0.996648\tvalid_1's rmse: 1.05463\n",
      "[293]\ttraining's rmse: 0.996429\tvalid_1's rmse: 1.05456\n",
      "[294]\ttraining's rmse: 0.996222\tvalid_1's rmse: 1.05452\n",
      "[295]\ttraining's rmse: 0.995999\tvalid_1's rmse: 1.05445\n",
      "[296]\ttraining's rmse: 0.995742\tvalid_1's rmse: 1.05433\n",
      "[297]\ttraining's rmse: 0.995546\tvalid_1's rmse: 1.0543\n",
      "[298]\ttraining's rmse: 0.995339\tvalid_1's rmse: 1.05424\n",
      "[299]\ttraining's rmse: 0.99516\tvalid_1's rmse: 1.05405\n",
      "[300]\ttraining's rmse: 0.994957\tvalid_1's rmse: 1.05411\n",
      "[301]\ttraining's rmse: 0.99471\tvalid_1's rmse: 1.05394\n",
      "[302]\ttraining's rmse: 0.9945\tvalid_1's rmse: 1.05383\n",
      "[303]\ttraining's rmse: 0.994319\tvalid_1's rmse: 1.05378\n",
      "[304]\ttraining's rmse: 0.994107\tvalid_1's rmse: 1.05362\n",
      "[305]\ttraining's rmse: 0.993882\tvalid_1's rmse: 1.0535\n",
      "[306]\ttraining's rmse: 0.993665\tvalid_1's rmse: 1.05336\n",
      "[307]\ttraining's rmse: 0.99347\tvalid_1's rmse: 1.05321\n",
      "[308]\ttraining's rmse: 0.99328\tvalid_1's rmse: 1.05315\n",
      "[309]\ttraining's rmse: 0.993076\tvalid_1's rmse: 1.05305\n",
      "[310]\ttraining's rmse: 0.99288\tvalid_1's rmse: 1.05302\n",
      "[311]\ttraining's rmse: 0.992676\tvalid_1's rmse: 1.05289\n",
      "[312]\ttraining's rmse: 0.992485\tvalid_1's rmse: 1.05286\n",
      "[313]\ttraining's rmse: 0.992244\tvalid_1's rmse: 1.05277\n",
      "[314]\ttraining's rmse: 0.992035\tvalid_1's rmse: 1.0526\n",
      "[315]\ttraining's rmse: 0.991838\tvalid_1's rmse: 1.0525\n",
      "[316]\ttraining's rmse: 0.991618\tvalid_1's rmse: 1.05238\n",
      "[317]\ttraining's rmse: 0.991421\tvalid_1's rmse: 1.05238\n",
      "[318]\ttraining's rmse: 0.991218\tvalid_1's rmse: 1.05222\n",
      "[319]\ttraining's rmse: 0.991043\tvalid_1's rmse: 1.05214\n",
      "[320]\ttraining's rmse: 0.990824\tvalid_1's rmse: 1.0521\n",
      "[321]\ttraining's rmse: 0.990626\tvalid_1's rmse: 1.05191\n",
      "[322]\ttraining's rmse: 0.990387\tvalid_1's rmse: 1.05183\n",
      "[323]\ttraining's rmse: 0.990178\tvalid_1's rmse: 1.05178\n",
      "[324]\ttraining's rmse: 0.989994\tvalid_1's rmse: 1.0518\n",
      "[325]\ttraining's rmse: 0.989801\tvalid_1's rmse: 1.05165\n",
      "[326]\ttraining's rmse: 0.989593\tvalid_1's rmse: 1.05162\n",
      "[327]\ttraining's rmse: 0.989413\tvalid_1's rmse: 1.05152\n",
      "[328]\ttraining's rmse: 0.989228\tvalid_1's rmse: 1.05143\n",
      "[329]\ttraining's rmse: 0.989058\tvalid_1's rmse: 1.05143\n",
      "[330]\ttraining's rmse: 0.988871\tvalid_1's rmse: 1.0513\n",
      "[331]\ttraining's rmse: 0.988673\tvalid_1's rmse: 1.05125\n",
      "[332]\ttraining's rmse: 0.988475\tvalid_1's rmse: 1.05108\n",
      "[333]\ttraining's rmse: 0.98825\tvalid_1's rmse: 1.051\n",
      "[334]\ttraining's rmse: 0.988059\tvalid_1's rmse: 1.05095\n",
      "[335]\ttraining's rmse: 0.987879\tvalid_1's rmse: 1.05081\n",
      "[336]\ttraining's rmse: 0.987698\tvalid_1's rmse: 1.05081\n",
      "[337]\ttraining's rmse: 0.987459\tvalid_1's rmse: 1.05073\n",
      "[338]\ttraining's rmse: 0.987268\tvalid_1's rmse: 1.05068\n",
      "[339]\ttraining's rmse: 0.987077\tvalid_1's rmse: 1.05052\n",
      "[340]\ttraining's rmse: 0.986893\tvalid_1's rmse: 1.05047\n",
      "[341]\ttraining's rmse: 0.986659\tvalid_1's rmse: 1.05042\n",
      "[342]\ttraining's rmse: 0.986478\tvalid_1's rmse: 1.05037\n",
      "[343]\ttraining's rmse: 0.986298\tvalid_1's rmse: 1.0502\n",
      "[344]\ttraining's rmse: 0.986134\tvalid_1's rmse: 1.05017\n",
      "[345]\ttraining's rmse: 0.985946\tvalid_1's rmse: 1.04999\n",
      "[346]\ttraining's rmse: 0.985772\tvalid_1's rmse: 1.04989\n",
      "[347]\ttraining's rmse: 0.985589\tvalid_1's rmse: 1.04984\n",
      "[348]\ttraining's rmse: 0.985394\tvalid_1's rmse: 1.04972\n",
      "[349]\ttraining's rmse: 0.985215\tvalid_1's rmse: 1.04966\n",
      "[350]\ttraining's rmse: 0.985041\tvalid_1's rmse: 1.04961\n",
      "[351]\ttraining's rmse: 0.984831\tvalid_1's rmse: 1.04953\n",
      "[352]\ttraining's rmse: 0.984641\tvalid_1's rmse: 1.04942\n",
      "[353]\ttraining's rmse: 0.984471\tvalid_1's rmse: 1.04934\n",
      "[354]\ttraining's rmse: 0.984266\tvalid_1's rmse: 1.04927\n",
      "[355]\ttraining's rmse: 0.984095\tvalid_1's rmse: 1.04927\n",
      "[356]\ttraining's rmse: 0.983924\tvalid_1's rmse: 1.04919\n",
      "[357]\ttraining's rmse: 0.983761\tvalid_1's rmse: 1.0491\n",
      "[358]\ttraining's rmse: 0.98356\tvalid_1's rmse: 1.04903\n",
      "[359]\ttraining's rmse: 0.983376\tvalid_1's rmse: 1.04891\n",
      "[360]\ttraining's rmse: 0.983211\tvalid_1's rmse: 1.04878\n",
      "[361]\ttraining's rmse: 0.983033\tvalid_1's rmse: 1.0487\n",
      "[362]\ttraining's rmse: 0.982858\tvalid_1's rmse: 1.04854\n",
      "[363]\ttraining's rmse: 0.982696\tvalid_1's rmse: 1.04849\n",
      "[364]\ttraining's rmse: 0.982478\tvalid_1's rmse: 1.04839\n",
      "[365]\ttraining's rmse: 0.982315\tvalid_1's rmse: 1.04834\n",
      "[366]\ttraining's rmse: 0.982121\tvalid_1's rmse: 1.04828\n",
      "[367]\ttraining's rmse: 0.981963\tvalid_1's rmse: 1.04822\n",
      "[368]\ttraining's rmse: 0.981794\tvalid_1's rmse: 1.04806\n",
      "[369]\ttraining's rmse: 0.981641\tvalid_1's rmse: 1.04798\n",
      "[370]\ttraining's rmse: 0.981466\tvalid_1's rmse: 1.04788\n",
      "[371]\ttraining's rmse: 0.981266\tvalid_1's rmse: 1.04782\n",
      "[372]\ttraining's rmse: 0.981111\tvalid_1's rmse: 1.04786\n",
      "[373]\ttraining's rmse: 0.980944\tvalid_1's rmse: 1.04777\n",
      "[374]\ttraining's rmse: 0.980801\tvalid_1's rmse: 1.04771\n",
      "[375]\ttraining's rmse: 0.980614\tvalid_1's rmse: 1.04761\n",
      "[376]\ttraining's rmse: 0.980454\tvalid_1's rmse: 1.04747\n",
      "[377]\ttraining's rmse: 0.980288\tvalid_1's rmse: 1.04748\n",
      "[378]\ttraining's rmse: 0.980088\tvalid_1's rmse: 1.04743\n",
      "[379]\ttraining's rmse: 0.97992\tvalid_1's rmse: 1.0473\n",
      "[380]\ttraining's rmse: 0.979764\tvalid_1's rmse: 1.04714\n",
      "[381]\ttraining's rmse: 0.979613\tvalid_1's rmse: 1.0471\n",
      "[382]\ttraining's rmse: 0.979423\tvalid_1's rmse: 1.04703\n",
      "[383]\ttraining's rmse: 0.979271\tvalid_1's rmse: 1.04706\n",
      "[384]\ttraining's rmse: 0.979108\tvalid_1's rmse: 1.04691\n",
      "[385]\ttraining's rmse: 0.978914\tvalid_1's rmse: 1.04685\n",
      "[386]\ttraining's rmse: 0.978766\tvalid_1's rmse: 1.04682\n",
      "[387]\ttraining's rmse: 0.978625\tvalid_1's rmse: 1.04687\n",
      "[388]\ttraining's rmse: 0.97847\tvalid_1's rmse: 1.04678\n",
      "[389]\ttraining's rmse: 0.978279\tvalid_1's rmse: 1.04669\n",
      "[390]\ttraining's rmse: 0.978121\tvalid_1's rmse: 1.04661\n",
      "[391]\ttraining's rmse: 0.977972\tvalid_1's rmse: 1.04657\n",
      "[392]\ttraining's rmse: 0.977829\tvalid_1's rmse: 1.04655\n",
      "[393]\ttraining's rmse: 0.977656\tvalid_1's rmse: 1.04649\n",
      "[394]\ttraining's rmse: 0.9775\tvalid_1's rmse: 1.04636\n",
      "[395]\ttraining's rmse: 0.977308\tvalid_1's rmse: 1.04629\n",
      "[396]\ttraining's rmse: 0.977166\tvalid_1's rmse: 1.04632\n",
      "[397]\ttraining's rmse: 0.977027\tvalid_1's rmse: 1.04627\n",
      "[398]\ttraining's rmse: 0.976845\tvalid_1's rmse: 1.04618\n",
      "[399]\ttraining's rmse: 0.976697\tvalid_1's rmse: 1.04603\n",
      "[400]\ttraining's rmse: 0.976549\tvalid_1's rmse: 1.04599\n",
      "[401]\ttraining's rmse: 0.976373\tvalid_1's rmse: 1.04596\n",
      "[402]\ttraining's rmse: 0.976224\tvalid_1's rmse: 1.04598\n",
      "[403]\ttraining's rmse: 0.976068\tvalid_1's rmse: 1.04588\n",
      "[404]\ttraining's rmse: 0.97592\tvalid_1's rmse: 1.0458\n",
      "[405]\ttraining's rmse: 0.975791\tvalid_1's rmse: 1.04585\n",
      "[406]\ttraining's rmse: 0.975615\tvalid_1's rmse: 1.04578\n",
      "[407]\ttraining's rmse: 0.975471\tvalid_1's rmse: 1.04569\n",
      "[408]\ttraining's rmse: 0.975303\tvalid_1's rmse: 1.04563\n",
      "[409]\ttraining's rmse: 0.975166\tvalid_1's rmse: 1.04558\n",
      "[410]\ttraining's rmse: 0.975033\tvalid_1's rmse: 1.04557\n",
      "[411]\ttraining's rmse: 0.974889\tvalid_1's rmse: 1.04549\n",
      "[412]\ttraining's rmse: 0.97471\tvalid_1's rmse: 1.04548\n",
      "[413]\ttraining's rmse: 0.974561\tvalid_1's rmse: 1.0454\n",
      "[414]\ttraining's rmse: 0.974428\tvalid_1's rmse: 1.04538\n",
      "[415]\ttraining's rmse: 0.974295\tvalid_1's rmse: 1.04535\n",
      "[416]\ttraining's rmse: 0.974124\tvalid_1's rmse: 1.04531\n",
      "[417]\ttraining's rmse: 0.973996\tvalid_1's rmse: 1.04528\n",
      "[418]\ttraining's rmse: 0.973856\tvalid_1's rmse: 1.04521\n",
      "[419]\ttraining's rmse: 0.973734\tvalid_1's rmse: 1.04515\n",
      "[420]\ttraining's rmse: 0.973561\tvalid_1's rmse: 1.04514\n",
      "[421]\ttraining's rmse: 0.973425\tvalid_1's rmse: 1.04505\n",
      "[422]\ttraining's rmse: 0.973289\tvalid_1's rmse: 1.04505\n",
      "[423]\ttraining's rmse: 0.973152\tvalid_1's rmse: 1.045\n",
      "[424]\ttraining's rmse: 0.973008\tvalid_1's rmse: 1.04491\n",
      "[425]\ttraining's rmse: 0.972856\tvalid_1's rmse: 1.04486\n",
      "[426]\ttraining's rmse: 0.972731\tvalid_1's rmse: 1.04485\n",
      "[427]\ttraining's rmse: 0.972607\tvalid_1's rmse: 1.04488\n",
      "[428]\ttraining's rmse: 0.972439\tvalid_1's rmse: 1.04485\n",
      "[429]\ttraining's rmse: 0.972313\tvalid_1's rmse: 1.04487\n",
      "[430]\ttraining's rmse: 0.972179\tvalid_1's rmse: 1.04482\n",
      "[431]\ttraining's rmse: 0.972014\tvalid_1's rmse: 1.0448\n",
      "[432]\ttraining's rmse: 0.971885\tvalid_1's rmse: 1.04469\n",
      "[433]\ttraining's rmse: 0.971764\tvalid_1's rmse: 1.04468\n",
      "[434]\ttraining's rmse: 0.97164\tvalid_1's rmse: 1.04465\n",
      "[435]\ttraining's rmse: 0.971499\tvalid_1's rmse: 1.04463\n",
      "[436]\ttraining's rmse: 0.971378\tvalid_1's rmse: 1.04462\n",
      "[437]\ttraining's rmse: 0.971227\tvalid_1's rmse: 1.0446\n",
      "[438]\ttraining's rmse: 0.971109\tvalid_1's rmse: 1.04459\n",
      "[439]\ttraining's rmse: 0.97097\tvalid_1's rmse: 1.04454\n",
      "[440]\ttraining's rmse: 0.970858\tvalid_1's rmse: 1.04458\n",
      "[441]\ttraining's rmse: 0.970692\tvalid_1's rmse: 1.04457\n",
      "[442]\ttraining's rmse: 0.970566\tvalid_1's rmse: 1.04448\n",
      "[443]\ttraining's rmse: 0.970431\tvalid_1's rmse: 1.04441\n",
      "[444]\ttraining's rmse: 0.970312\tvalid_1's rmse: 1.04438\n",
      "[445]\ttraining's rmse: 0.970164\tvalid_1's rmse: 1.04437\n",
      "[446]\ttraining's rmse: 0.970038\tvalid_1's rmse: 1.04436\n",
      "[447]\ttraining's rmse: 0.969886\tvalid_1's rmse: 1.0443\n",
      "[448]\ttraining's rmse: 0.969734\tvalid_1's rmse: 1.04424\n",
      "[449]\ttraining's rmse: 0.969616\tvalid_1's rmse: 1.04421\n",
      "[450]\ttraining's rmse: 0.969501\tvalid_1's rmse: 1.04421\n",
      "[451]\ttraining's rmse: 0.969353\tvalid_1's rmse: 1.04419\n",
      "[452]\ttraining's rmse: 0.96923\tvalid_1's rmse: 1.04419\n",
      "[453]\ttraining's rmse: 0.96909\tvalid_1's rmse: 1.04418\n",
      "[454]\ttraining's rmse: 0.968959\tvalid_1's rmse: 1.04411\n",
      "[455]\ttraining's rmse: 0.968842\tvalid_1's rmse: 1.04414\n",
      "[456]\ttraining's rmse: 0.968689\tvalid_1's rmse: 1.04409\n",
      "[457]\ttraining's rmse: 0.96856\tvalid_1's rmse: 1.04399\n",
      "[458]\ttraining's rmse: 0.968449\tvalid_1's rmse: 1.04397\n",
      "[459]\ttraining's rmse: 0.968299\tvalid_1's rmse: 1.04396\n",
      "[460]\ttraining's rmse: 0.968189\tvalid_1's rmse: 1.04396\n",
      "[461]\ttraining's rmse: 0.968053\tvalid_1's rmse: 1.04394\n",
      "[462]\ttraining's rmse: 0.967934\tvalid_1's rmse: 1.04391\n",
      "[463]\ttraining's rmse: 0.967793\tvalid_1's rmse: 1.04386\n",
      "[464]\ttraining's rmse: 0.967662\tvalid_1's rmse: 1.04382\n",
      "[465]\ttraining's rmse: 0.967553\tvalid_1's rmse: 1.04379\n",
      "[466]\ttraining's rmse: 0.967431\tvalid_1's rmse: 1.04371\n",
      "[467]\ttraining's rmse: 0.967322\tvalid_1's rmse: 1.04375\n",
      "[468]\ttraining's rmse: 0.967183\tvalid_1's rmse: 1.04378\n",
      "[469]\ttraining's rmse: 0.967073\tvalid_1's rmse: 1.04373\n",
      "[470]\ttraining's rmse: 0.966989\tvalid_1's rmse: 1.04376\n",
      "[471]\ttraining's rmse: 0.966861\tvalid_1's rmse: 1.04373\n",
      "[472]\ttraining's rmse: 0.966732\tvalid_1's rmse: 1.04371\n",
      "[473]\ttraining's rmse: 0.966595\tvalid_1's rmse: 1.04369\n",
      "[474]\ttraining's rmse: 0.966459\tvalid_1's rmse: 1.04366\n",
      "[475]\ttraining's rmse: 0.966334\tvalid_1's rmse: 1.04362\n",
      "[476]\ttraining's rmse: 0.966227\tvalid_1's rmse: 1.04357\n",
      "[477]\ttraining's rmse: 0.966146\tvalid_1's rmse: 1.0436\n",
      "[478]\ttraining's rmse: 0.966023\tvalid_1's rmse: 1.0435\n",
      "[479]\ttraining's rmse: 0.965913\tvalid_1's rmse: 1.04347\n",
      "[480]\ttraining's rmse: 0.965802\tvalid_1's rmse: 1.04345\n",
      "[481]\ttraining's rmse: 0.965668\tvalid_1's rmse: 1.04346\n",
      "[482]\ttraining's rmse: 0.965535\tvalid_1's rmse: 1.04348\n",
      "[483]\ttraining's rmse: 0.96541\tvalid_1's rmse: 1.04344\n",
      "[484]\ttraining's rmse: 0.965279\tvalid_1's rmse: 1.04342\n",
      "[485]\ttraining's rmse: 0.965176\tvalid_1's rmse: 1.04342\n",
      "[486]\ttraining's rmse: 0.965052\tvalid_1's rmse: 1.04336\n",
      "[487]\ttraining's rmse: 0.964934\tvalid_1's rmse: 1.04336\n",
      "[488]\ttraining's rmse: 0.964833\tvalid_1's rmse: 1.04336\n",
      "[489]\ttraining's rmse: 0.964731\tvalid_1's rmse: 1.0433\n",
      "[490]\ttraining's rmse: 0.964612\tvalid_1's rmse: 1.04324\n",
      "[491]\ttraining's rmse: 0.96449\tvalid_1's rmse: 1.0432\n",
      "[492]\ttraining's rmse: 0.964362\tvalid_1's rmse: 1.04321\n",
      "[493]\ttraining's rmse: 0.964247\tvalid_1's rmse: 1.0432\n",
      "[494]\ttraining's rmse: 0.964123\tvalid_1's rmse: 1.04318\n",
      "[495]\ttraining's rmse: 0.964029\tvalid_1's rmse: 1.04321\n",
      "[496]\ttraining's rmse: 0.96392\tvalid_1's rmse: 1.04318\n",
      "[497]\ttraining's rmse: 0.963786\tvalid_1's rmse: 1.04319\n",
      "[498]\ttraining's rmse: 0.963635\tvalid_1's rmse: 1.04307\n",
      "[499]\ttraining's rmse: 0.963533\tvalid_1's rmse: 1.04307\n",
      "[500]\ttraining's rmse: 0.963421\tvalid_1's rmse: 1.04307\n",
      "[501]\ttraining's rmse: 0.963327\tvalid_1's rmse: 1.04309\n",
      "[502]\ttraining's rmse: 0.963235\tvalid_1's rmse: 1.04311\n",
      "[503]\ttraining's rmse: 0.963104\tvalid_1's rmse: 1.0431\n",
      "[504]\ttraining's rmse: 0.962988\tvalid_1's rmse: 1.04307\n",
      "[505]\ttraining's rmse: 0.962879\tvalid_1's rmse: 1.04307\n",
      "[506]\ttraining's rmse: 0.962775\tvalid_1's rmse: 1.04305\n",
      "[507]\ttraining's rmse: 0.962651\tvalid_1's rmse: 1.04304\n",
      "[508]\ttraining's rmse: 0.962524\tvalid_1's rmse: 1.04296\n",
      "[509]\ttraining's rmse: 0.962403\tvalid_1's rmse: 1.04299\n",
      "[510]\ttraining's rmse: 0.96229\tvalid_1's rmse: 1.04296\n",
      "[511]\ttraining's rmse: 0.962175\tvalid_1's rmse: 1.0429\n",
      "[512]\ttraining's rmse: 0.962058\tvalid_1's rmse: 1.04284\n",
      "[513]\ttraining's rmse: 0.961959\tvalid_1's rmse: 1.04282\n",
      "[514]\ttraining's rmse: 0.961852\tvalid_1's rmse: 1.04279\n",
      "[515]\ttraining's rmse: 0.961733\tvalid_1's rmse: 1.04277\n",
      "[516]\ttraining's rmse: 0.961626\tvalid_1's rmse: 1.0427\n",
      "[517]\ttraining's rmse: 0.961523\tvalid_1's rmse: 1.04269\n",
      "[518]\ttraining's rmse: 0.961417\tvalid_1's rmse: 1.04266\n",
      "[519]\ttraining's rmse: 0.961315\tvalid_1's rmse: 1.04263\n",
      "[520]\ttraining's rmse: 0.961203\tvalid_1's rmse: 1.04262\n",
      "[521]\ttraining's rmse: 0.961099\tvalid_1's rmse: 1.04262\n",
      "[522]\ttraining's rmse: 0.960983\tvalid_1's rmse: 1.04263\n",
      "[523]\ttraining's rmse: 0.960864\tvalid_1's rmse: 1.04262\n",
      "[524]\ttraining's rmse: 0.96076\tvalid_1's rmse: 1.0426\n",
      "[525]\ttraining's rmse: 0.960645\tvalid_1's rmse: 1.04257\n",
      "[526]\ttraining's rmse: 0.960528\tvalid_1's rmse: 1.04252\n",
      "[527]\ttraining's rmse: 0.960418\tvalid_1's rmse: 1.04247\n",
      "[528]\ttraining's rmse: 0.960298\tvalid_1's rmse: 1.04239\n",
      "[529]\ttraining's rmse: 0.960186\tvalid_1's rmse: 1.04238\n",
      "[530]\ttraining's rmse: 0.960064\tvalid_1's rmse: 1.04232\n",
      "[531]\ttraining's rmse: 0.959966\tvalid_1's rmse: 1.04232\n",
      "[532]\ttraining's rmse: 0.959867\tvalid_1's rmse: 1.04232\n",
      "[533]\ttraining's rmse: 0.959754\tvalid_1's rmse: 1.04229\n",
      "[534]\ttraining's rmse: 0.959628\tvalid_1's rmse: 1.04228\n",
      "[535]\ttraining's rmse: 0.959531\tvalid_1's rmse: 1.04226\n",
      "[536]\ttraining's rmse: 0.959415\tvalid_1's rmse: 1.04218\n",
      "[537]\ttraining's rmse: 0.959314\tvalid_1's rmse: 1.04214\n",
      "[538]\ttraining's rmse: 0.959204\tvalid_1's rmse: 1.04216\n",
      "[539]\ttraining's rmse: 0.959098\tvalid_1's rmse: 1.04211\n",
      "[540]\ttraining's rmse: 0.958974\tvalid_1's rmse: 1.04202\n",
      "[541]\ttraining's rmse: 0.958873\tvalid_1's rmse: 1.04198\n",
      "[542]\ttraining's rmse: 0.958778\tvalid_1's rmse: 1.04197\n",
      "[543]\ttraining's rmse: 0.95867\tvalid_1's rmse: 1.04201\n",
      "[544]\ttraining's rmse: 0.958557\tvalid_1's rmse: 1.04197\n",
      "[545]\ttraining's rmse: 0.95846\tvalid_1's rmse: 1.04195\n",
      "[546]\ttraining's rmse: 0.958344\tvalid_1's rmse: 1.04188\n",
      "[547]\ttraining's rmse: 0.958219\tvalid_1's rmse: 1.04187\n",
      "[548]\ttraining's rmse: 0.958126\tvalid_1's rmse: 1.04187\n",
      "[549]\ttraining's rmse: 0.95803\tvalid_1's rmse: 1.04188\n",
      "[550]\ttraining's rmse: 0.957924\tvalid_1's rmse: 1.0419\n",
      "[551]\ttraining's rmse: 0.957817\tvalid_1's rmse: 1.04186\n",
      "[552]\ttraining's rmse: 0.957725\tvalid_1's rmse: 1.04187\n",
      "[553]\ttraining's rmse: 0.957602\tvalid_1's rmse: 1.04186\n",
      "[554]\ttraining's rmse: 0.957507\tvalid_1's rmse: 1.04186\n",
      "[555]\ttraining's rmse: 0.957398\tvalid_1's rmse: 1.04177\n",
      "[556]\ttraining's rmse: 0.957303\tvalid_1's rmse: 1.04172\n",
      "[557]\ttraining's rmse: 0.957207\tvalid_1's rmse: 1.0417\n",
      "[558]\ttraining's rmse: 0.957117\tvalid_1's rmse: 1.04168\n",
      "[559]\ttraining's rmse: 0.957009\tvalid_1's rmse: 1.04159\n",
      "[560]\ttraining's rmse: 0.956912\tvalid_1's rmse: 1.04156\n",
      "[561]\ttraining's rmse: 0.956809\tvalid_1's rmse: 1.04156\n",
      "[562]\ttraining's rmse: 0.95671\tvalid_1's rmse: 1.04152\n",
      "[563]\ttraining's rmse: 0.956621\tvalid_1's rmse: 1.04152\n",
      "[564]\ttraining's rmse: 0.956528\tvalid_1's rmse: 1.04147\n",
      "[565]\ttraining's rmse: 0.956471\tvalid_1's rmse: 1.04149\n",
      "[566]\ttraining's rmse: 0.956352\tvalid_1's rmse: 1.04148\n",
      "[567]\ttraining's rmse: 0.95626\tvalid_1's rmse: 1.04145\n",
      "[568]\ttraining's rmse: 0.956183\tvalid_1's rmse: 1.04144\n",
      "[569]\ttraining's rmse: 0.95609\tvalid_1's rmse: 1.04142\n",
      "[570]\ttraining's rmse: 0.955986\tvalid_1's rmse: 1.04146\n",
      "[571]\ttraining's rmse: 0.955901\tvalid_1's rmse: 1.04146\n",
      "[572]\ttraining's rmse: 0.955801\tvalid_1's rmse: 1.04141\n",
      "[573]\ttraining's rmse: 0.955706\tvalid_1's rmse: 1.04133\n",
      "[574]\ttraining's rmse: 0.955618\tvalid_1's rmse: 1.04132\n",
      "[575]\ttraining's rmse: 0.955559\tvalid_1's rmse: 1.04133\n",
      "[576]\ttraining's rmse: 0.95544\tvalid_1's rmse: 1.04134\n",
      "[577]\ttraining's rmse: 0.955354\tvalid_1's rmse: 1.04139\n",
      "[578]\ttraining's rmse: 0.955268\tvalid_1's rmse: 1.0414\n",
      "[579]\ttraining's rmse: 0.955174\tvalid_1's rmse: 1.04137\n",
      "[580]\ttraining's rmse: 0.955119\tvalid_1's rmse: 1.04138\n",
      "[581]\ttraining's rmse: 0.955005\tvalid_1's rmse: 1.04127\n",
      "[582]\ttraining's rmse: 0.954908\tvalid_1's rmse: 1.04129\n",
      "[583]\ttraining's rmse: 0.954797\tvalid_1's rmse: 1.04129\n",
      "[584]\ttraining's rmse: 0.954699\tvalid_1's rmse: 1.04127\n",
      "[585]\ttraining's rmse: 0.954599\tvalid_1's rmse: 1.04124\n",
      "[586]\ttraining's rmse: 0.95451\tvalid_1's rmse: 1.04124\n",
      "[587]\ttraining's rmse: 0.954417\tvalid_1's rmse: 1.04124\n",
      "[588]\ttraining's rmse: 0.954347\tvalid_1's rmse: 1.04118\n",
      "[589]\ttraining's rmse: 0.954262\tvalid_1's rmse: 1.04118\n",
      "[590]\ttraining's rmse: 0.954179\tvalid_1's rmse: 1.04119\n",
      "[591]\ttraining's rmse: 0.95408\tvalid_1's rmse: 1.04124\n",
      "[592]\ttraining's rmse: 0.953989\tvalid_1's rmse: 1.04123\n",
      "[593]\ttraining's rmse: 0.953901\tvalid_1's rmse: 1.04121\n",
      "[594]\ttraining's rmse: 0.953846\tvalid_1's rmse: 1.04122\n",
      "[595]\ttraining's rmse: 0.953732\tvalid_1's rmse: 1.04123\n",
      "[596]\ttraining's rmse: 0.953643\tvalid_1's rmse: 1.04123\n",
      "[597]\ttraining's rmse: 0.953552\tvalid_1's rmse: 1.04124\n",
      "[598]\ttraining's rmse: 0.953457\tvalid_1's rmse: 1.04125\n",
      "[599]\ttraining's rmse: 0.953364\tvalid_1's rmse: 1.04123\n",
      "Early stopping, best iteration is:\n",
      "[589]\ttraining's rmse: 0.954262\tvalid_1's rmse: 1.04118\n",
      "fold_2 coefficients:  [0.50386565 1.63591535 2.20067206]\n",
      "[1]\ttraining's rmse: 1.25323\tvalid_1's rmse: 1.30199\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24978\tvalid_1's rmse: 1.29897\n",
      "[3]\ttraining's rmse: 1.24639\tvalid_1's rmse: 1.29597\n",
      "[4]\ttraining's rmse: 1.24305\tvalid_1's rmse: 1.29303\n",
      "[5]\ttraining's rmse: 1.23977\tvalid_1's rmse: 1.29015\n",
      "[6]\ttraining's rmse: 1.23655\tvalid_1's rmse: 1.28732\n",
      "[7]\ttraining's rmse: 1.23338\tvalid_1's rmse: 1.28458\n",
      "[8]\ttraining's rmse: 1.23027\tvalid_1's rmse: 1.28185\n",
      "[9]\ttraining's rmse: 1.22721\tvalid_1's rmse: 1.27919\n",
      "[10]\ttraining's rmse: 1.2242\tvalid_1's rmse: 1.27652\n",
      "[11]\ttraining's rmse: 1.22125\tvalid_1's rmse: 1.27395\n",
      "[12]\ttraining's rmse: 1.21835\tvalid_1's rmse: 1.27143\n",
      "[13]\ttraining's rmse: 1.21549\tvalid_1's rmse: 1.26888\n",
      "[14]\ttraining's rmse: 1.21268\tvalid_1's rmse: 1.26664\n",
      "[15]\ttraining's rmse: 1.20992\tvalid_1's rmse: 1.26442\n",
      "[16]\ttraining's rmse: 1.20721\tvalid_1's rmse: 1.26227\n",
      "[17]\ttraining's rmse: 1.20455\tvalid_1's rmse: 1.26014\n",
      "[18]\ttraining's rmse: 1.20206\tvalid_1's rmse: 1.25805\n",
      "[19]\ttraining's rmse: 1.19945\tvalid_1's rmse: 1.25578\n",
      "[20]\ttraining's rmse: 1.19704\tvalid_1's rmse: 1.25384\n",
      "[21]\ttraining's rmse: 1.19452\tvalid_1's rmse: 1.25165\n",
      "[22]\ttraining's rmse: 1.19219\tvalid_1's rmse: 1.2497\n",
      "[23]\ttraining's rmse: 1.18976\tvalid_1's rmse: 1.24764\n",
      "[24]\ttraining's rmse: 1.18751\tvalid_1's rmse: 1.24576\n",
      "[25]\ttraining's rmse: 1.18514\tvalid_1's rmse: 1.24371\n",
      "[26]\ttraining's rmse: 1.18296\tvalid_1's rmse: 1.2419\n",
      "[27]\ttraining's rmse: 1.18088\tvalid_1's rmse: 1.2402\n",
      "[28]\ttraining's rmse: 1.17853\tvalid_1's rmse: 1.23833\n",
      "[29]\ttraining's rmse: 1.17621\tvalid_1's rmse: 1.23648\n",
      "[30]\ttraining's rmse: 1.17417\tvalid_1's rmse: 1.23483\n",
      "[31]\ttraining's rmse: 1.17202\tvalid_1's rmse: 1.23302\n",
      "[32]\ttraining's rmse: 1.1701\tvalid_1's rmse: 1.23147\n",
      "[33]\ttraining's rmse: 1.16793\tvalid_1's rmse: 1.22977\n",
      "[34]\ttraining's rmse: 1.16602\tvalid_1's rmse: 1.22822\n",
      "[35]\ttraining's rmse: 1.16392\tvalid_1's rmse: 1.22659\n",
      "[36]\ttraining's rmse: 1.16213\tvalid_1's rmse: 1.22517\n",
      "[37]\ttraining's rmse: 1.16038\tvalid_1's rmse: 1.22373\n",
      "[38]\ttraining's rmse: 1.15837\tvalid_1's rmse: 1.22219\n",
      "[39]\ttraining's rmse: 1.15668\tvalid_1's rmse: 1.2208\n",
      "[40]\ttraining's rmse: 1.155\tvalid_1's rmse: 1.21949\n",
      "[41]\ttraining's rmse: 1.15337\tvalid_1's rmse: 1.21816\n",
      "[42]\ttraining's rmse: 1.15181\tvalid_1's rmse: 1.21701\n",
      "[43]\ttraining's rmse: 1.15014\tvalid_1's rmse: 1.2157\n",
      "[44]\ttraining's rmse: 1.14857\tvalid_1's rmse: 1.21437\n",
      "[45]\ttraining's rmse: 1.14708\tvalid_1's rmse: 1.21327\n",
      "[46]\ttraining's rmse: 1.14528\tvalid_1's rmse: 1.21193\n",
      "[47]\ttraining's rmse: 1.14384\tvalid_1's rmse: 1.21088\n",
      "[48]\ttraining's rmse: 1.1423\tvalid_1's rmse: 1.20967\n",
      "[49]\ttraining's rmse: 1.14059\tvalid_1's rmse: 1.20832\n",
      "[50]\ttraining's rmse: 1.13922\tvalid_1's rmse: 1.20735\n",
      "[51]\ttraining's rmse: 1.13781\tvalid_1's rmse: 1.20616\n",
      "[52]\ttraining's rmse: 1.13616\tvalid_1's rmse: 1.20497\n",
      "[53]\ttraining's rmse: 1.13487\tvalid_1's rmse: 1.20388\n",
      "[54]\ttraining's rmse: 1.13347\tvalid_1's rmse: 1.20283\n",
      "[55]\ttraining's rmse: 1.1322\tvalid_1's rmse: 1.20199\n",
      "[56]\ttraining's rmse: 1.13065\tvalid_1's rmse: 1.20082\n",
      "[57]\ttraining's rmse: 1.12946\tvalid_1's rmse: 1.19991\n",
      "[58]\ttraining's rmse: 1.12824\tvalid_1's rmse: 1.19916\n",
      "[59]\ttraining's rmse: 1.12675\tvalid_1's rmse: 1.19803\n",
      "[60]\ttraining's rmse: 1.12559\tvalid_1's rmse: 1.19709\n",
      "[61]\ttraining's rmse: 1.12407\tvalid_1's rmse: 1.19601\n",
      "[62]\ttraining's rmse: 1.12264\tvalid_1's rmse: 1.195\n",
      "[63]\ttraining's rmse: 1.1215\tvalid_1's rmse: 1.19439\n",
      "[64]\ttraining's rmse: 1.1203\tvalid_1's rmse: 1.19351\n",
      "[65]\ttraining's rmse: 1.11886\tvalid_1's rmse: 1.19241\n",
      "[66]\ttraining's rmse: 1.11769\tvalid_1's rmse: 1.1916\n",
      "[67]\ttraining's rmse: 1.11637\tvalid_1's rmse: 1.19058\n",
      "[68]\ttraining's rmse: 1.11532\tvalid_1's rmse: 1.18995\n",
      "[69]\ttraining's rmse: 1.1142\tvalid_1's rmse: 1.18929\n",
      "[70]\ttraining's rmse: 1.11288\tvalid_1's rmse: 1.18846\n",
      "[71]\ttraining's rmse: 1.11188\tvalid_1's rmse: 1.18794\n",
      "[72]\ttraining's rmse: 1.11054\tvalid_1's rmse: 1.18706\n",
      "[73]\ttraining's rmse: 1.10949\tvalid_1's rmse: 1.18636\n",
      "[74]\ttraining's rmse: 1.10849\tvalid_1's rmse: 1.18569\n",
      "[75]\ttraining's rmse: 1.10754\tvalid_1's rmse: 1.18521\n",
      "[76]\ttraining's rmse: 1.10632\tvalid_1's rmse: 1.18439\n",
      "[77]\ttraining's rmse: 1.10538\tvalid_1's rmse: 1.18376\n",
      "[78]\ttraining's rmse: 1.10445\tvalid_1's rmse: 1.18328\n",
      "[79]\ttraining's rmse: 1.1033\tvalid_1's rmse: 1.18247\n",
      "[80]\ttraining's rmse: 1.1024\tvalid_1's rmse: 1.18183\n",
      "[81]\ttraining's rmse: 1.10119\tvalid_1's rmse: 1.18102\n",
      "[82]\ttraining's rmse: 1.10028\tvalid_1's rmse: 1.18044\n",
      "[83]\ttraining's rmse: 1.09939\tvalid_1's rmse: 1.17994\n",
      "[84]\ttraining's rmse: 1.09824\tvalid_1's rmse: 1.17912\n",
      "[85]\ttraining's rmse: 1.09734\tvalid_1's rmse: 1.17834\n",
      "[86]\ttraining's rmse: 1.09629\tvalid_1's rmse: 1.17754\n",
      "[87]\ttraining's rmse: 1.09547\tvalid_1's rmse: 1.17705\n",
      "[88]\ttraining's rmse: 1.09434\tvalid_1's rmse: 1.17634\n",
      "[89]\ttraining's rmse: 1.09354\tvalid_1's rmse: 1.17594\n",
      "[90]\ttraining's rmse: 1.09242\tvalid_1's rmse: 1.1752\n",
      "[91]\ttraining's rmse: 1.09135\tvalid_1's rmse: 1.17449\n",
      "[92]\ttraining's rmse: 1.09016\tvalid_1's rmse: 1.17364\n",
      "[93]\ttraining's rmse: 1.08911\tvalid_1's rmse: 1.17302\n",
      "[94]\ttraining's rmse: 1.08813\tvalid_1's rmse: 1.17239\n",
      "[95]\ttraining's rmse: 1.08697\tvalid_1's rmse: 1.17168\n",
      "[96]\ttraining's rmse: 1.08598\tvalid_1's rmse: 1.17105\n",
      "[97]\ttraining's rmse: 1.0852\tvalid_1's rmse: 1.1706\n",
      "[98]\ttraining's rmse: 1.08444\tvalid_1's rmse: 1.16989\n",
      "[99]\ttraining's rmse: 1.0837\tvalid_1's rmse: 1.16958\n",
      "[100]\ttraining's rmse: 1.08298\tvalid_1's rmse: 1.16927\n",
      "[101]\ttraining's rmse: 1.0819\tvalid_1's rmse: 1.16859\n",
      "[102]\ttraining's rmse: 1.08096\tvalid_1's rmse: 1.16796\n",
      "[103]\ttraining's rmse: 1.08007\tvalid_1's rmse: 1.16745\n",
      "[104]\ttraining's rmse: 1.07903\tvalid_1's rmse: 1.16678\n",
      "[105]\ttraining's rmse: 1.07813\tvalid_1's rmse: 1.16623\n",
      "[106]\ttraining's rmse: 1.07744\tvalid_1's rmse: 1.16595\n",
      "[107]\ttraining's rmse: 1.07662\tvalid_1's rmse: 1.16549\n",
      "[108]\ttraining's rmse: 1.07563\tvalid_1's rmse: 1.16484\n",
      "[109]\ttraining's rmse: 1.07495\tvalid_1's rmse: 1.16445\n",
      "[110]\ttraining's rmse: 1.0741\tvalid_1's rmse: 1.16396\n",
      "[111]\ttraining's rmse: 1.07347\tvalid_1's rmse: 1.16374\n",
      "[112]\ttraining's rmse: 1.07285\tvalid_1's rmse: 1.16353\n",
      "[113]\ttraining's rmse: 1.0719\tvalid_1's rmse: 1.16293\n",
      "[114]\ttraining's rmse: 1.07109\tvalid_1's rmse: 1.16239\n",
      "[115]\ttraining's rmse: 1.07046\tvalid_1's rmse: 1.16206\n",
      "[116]\ttraining's rmse: 1.06984\tvalid_1's rmse: 1.16187\n",
      "[117]\ttraining's rmse: 1.06907\tvalid_1's rmse: 1.16141\n",
      "[118]\ttraining's rmse: 1.06816\tvalid_1's rmse: 1.16084\n",
      "[119]\ttraining's rmse: 1.06757\tvalid_1's rmse: 1.16061\n",
      "[120]\ttraining's rmse: 1.06682\tvalid_1's rmse: 1.16019\n",
      "[121]\ttraining's rmse: 1.06623\tvalid_1's rmse: 1.15987\n",
      "[122]\ttraining's rmse: 1.06536\tvalid_1's rmse: 1.15931\n",
      "[123]\ttraining's rmse: 1.06483\tvalid_1's rmse: 1.15914\n",
      "[124]\ttraining's rmse: 1.06411\tvalid_1's rmse: 1.15871\n",
      "[125]\ttraining's rmse: 1.06357\tvalid_1's rmse: 1.15841\n",
      "[126]\ttraining's rmse: 1.06274\tvalid_1's rmse: 1.15789\n",
      "[127]\ttraining's rmse: 1.06205\tvalid_1's rmse: 1.15745\n",
      "[128]\ttraining's rmse: 1.0615\tvalid_1's rmse: 1.15718\n",
      "[129]\ttraining's rmse: 1.06101\tvalid_1's rmse: 1.15695\n",
      "[130]\ttraining's rmse: 1.06031\tvalid_1's rmse: 1.15655\n",
      "[131]\ttraining's rmse: 1.05952\tvalid_1's rmse: 1.15594\n",
      "[132]\ttraining's rmse: 1.059\tvalid_1's rmse: 1.15562\n",
      "[133]\ttraining's rmse: 1.05848\tvalid_1's rmse: 1.15531\n",
      "[134]\ttraining's rmse: 1.05782\tvalid_1's rmse: 1.1549\n",
      "[135]\ttraining's rmse: 1.05706\tvalid_1's rmse: 1.15432\n",
      "[136]\ttraining's rmse: 1.05658\tvalid_1's rmse: 1.15414\n",
      "[137]\ttraining's rmse: 1.05613\tvalid_1's rmse: 1.15396\n",
      "[138]\ttraining's rmse: 1.05564\tvalid_1's rmse: 1.15383\n",
      "[139]\ttraining's rmse: 1.05501\tvalid_1's rmse: 1.15345\n",
      "[140]\ttraining's rmse: 1.05428\tvalid_1's rmse: 1.15295\n",
      "[141]\ttraining's rmse: 1.05383\tvalid_1's rmse: 1.15273\n",
      "[142]\ttraining's rmse: 1.05324\tvalid_1's rmse: 1.1524\n",
      "[143]\ttraining's rmse: 1.05276\tvalid_1's rmse: 1.15219\n",
      "[144]\ttraining's rmse: 1.05203\tvalid_1's rmse: 1.15159\n",
      "[145]\ttraining's rmse: 1.05162\tvalid_1's rmse: 1.15144\n",
      "[146]\ttraining's rmse: 1.05105\tvalid_1's rmse: 1.15111\n",
      "[147]\ttraining's rmse: 1.0506\tvalid_1's rmse: 1.1509\n",
      "[148]\ttraining's rmse: 1.04992\tvalid_1's rmse: 1.15033\n",
      "[149]\ttraining's rmse: 1.04951\tvalid_1's rmse: 1.15018\n",
      "[150]\ttraining's rmse: 1.04891\tvalid_1's rmse: 1.14976\n",
      "[151]\ttraining's rmse: 1.04844\tvalid_1's rmse: 1.14954\n",
      "[152]\ttraining's rmse: 1.04805\tvalid_1's rmse: 1.14933\n",
      "[153]\ttraining's rmse: 1.04757\tvalid_1's rmse: 1.14894\n",
      "[154]\ttraining's rmse: 1.04691\tvalid_1's rmse: 1.14844\n",
      "[155]\ttraining's rmse: 1.04649\tvalid_1's rmse: 1.14836\n",
      "[156]\ttraining's rmse: 1.04594\tvalid_1's rmse: 1.148\n",
      "[157]\ttraining's rmse: 1.04547\tvalid_1's rmse: 1.14764\n",
      "[158]\ttraining's rmse: 1.04503\tvalid_1's rmse: 1.14747\n",
      "[159]\ttraining's rmse: 1.04466\tvalid_1's rmse: 1.14725\n",
      "[160]\ttraining's rmse: 1.04418\tvalid_1's rmse: 1.1468\n",
      "[161]\ttraining's rmse: 1.04382\tvalid_1's rmse: 1.14671\n",
      "[162]\ttraining's rmse: 1.04336\tvalid_1's rmse: 1.14633\n",
      "[163]\ttraining's rmse: 1.04297\tvalid_1's rmse: 1.14626\n",
      "[164]\ttraining's rmse: 1.04244\tvalid_1's rmse: 1.14594\n",
      "[165]\ttraining's rmse: 1.04196\tvalid_1's rmse: 1.14554\n",
      "[166]\ttraining's rmse: 1.04143\tvalid_1's rmse: 1.14515\n",
      "[167]\ttraining's rmse: 1.04102\tvalid_1's rmse: 1.14487\n",
      "[168]\ttraining's rmse: 1.04067\tvalid_1's rmse: 1.14471\n",
      "[169]\ttraining's rmse: 1.04029\tvalid_1's rmse: 1.14462\n",
      "[170]\ttraining's rmse: 1.03978\tvalid_1's rmse: 1.14423\n",
      "[171]\ttraining's rmse: 1.03939\tvalid_1's rmse: 1.14392\n",
      "[172]\ttraining's rmse: 1.03893\tvalid_1's rmse: 1.14366\n",
      "[173]\ttraining's rmse: 1.03847\tvalid_1's rmse: 1.1434\n",
      "[174]\ttraining's rmse: 1.03809\tvalid_1's rmse: 1.14305\n",
      "[175]\ttraining's rmse: 1.03764\tvalid_1's rmse: 1.1428\n",
      "[176]\ttraining's rmse: 1.0372\tvalid_1's rmse: 1.14255\n",
      "[177]\ttraining's rmse: 1.03677\tvalid_1's rmse: 1.14231\n",
      "[178]\ttraining's rmse: 1.03639\tvalid_1's rmse: 1.14212\n",
      "[179]\ttraining's rmse: 1.03597\tvalid_1's rmse: 1.14188\n",
      "[180]\ttraining's rmse: 1.03556\tvalid_1's rmse: 1.14165\n",
      "[181]\ttraining's rmse: 1.03515\tvalid_1's rmse: 1.14142\n",
      "[182]\ttraining's rmse: 1.03473\tvalid_1's rmse: 1.1412\n",
      "[183]\ttraining's rmse: 1.03437\tvalid_1's rmse: 1.14088\n",
      "[184]\ttraining's rmse: 1.03398\tvalid_1's rmse: 1.14066\n",
      "[185]\ttraining's rmse: 1.03357\tvalid_1's rmse: 1.14046\n",
      "[186]\ttraining's rmse: 1.03307\tvalid_1's rmse: 1.14013\n",
      "[187]\ttraining's rmse: 1.03269\tvalid_1's rmse: 1.13992\n",
      "[188]\ttraining's rmse: 1.03229\tvalid_1's rmse: 1.13971\n",
      "[189]\ttraining's rmse: 1.03192\tvalid_1's rmse: 1.1395\n",
      "[190]\ttraining's rmse: 1.03154\tvalid_1's rmse: 1.13932\n",
      "[191]\ttraining's rmse: 1.0312\tvalid_1's rmse: 1.13902\n",
      "[192]\ttraining's rmse: 1.03083\tvalid_1's rmse: 1.13881\n",
      "[193]\ttraining's rmse: 1.03047\tvalid_1's rmse: 1.13858\n",
      "[194]\ttraining's rmse: 1.03012\tvalid_1's rmse: 1.13839\n",
      "[195]\ttraining's rmse: 1.02957\tvalid_1's rmse: 1.13807\n",
      "[196]\ttraining's rmse: 1.02922\tvalid_1's rmse: 1.13785\n",
      "[197]\ttraining's rmse: 1.02888\tvalid_1's rmse: 1.13767\n",
      "[198]\ttraining's rmse: 1.02835\tvalid_1's rmse: 1.1374\n",
      "[199]\ttraining's rmse: 1.02794\tvalid_1's rmse: 1.13711\n",
      "[200]\ttraining's rmse: 1.0276\tvalid_1's rmse: 1.13688\n",
      "[201]\ttraining's rmse: 1.02725\tvalid_1's rmse: 1.13664\n",
      "[202]\ttraining's rmse: 1.02673\tvalid_1's rmse: 1.13634\n",
      "[203]\ttraining's rmse: 1.02637\tvalid_1's rmse: 1.13608\n",
      "[204]\ttraining's rmse: 1.02606\tvalid_1's rmse: 1.13587\n",
      "[205]\ttraining's rmse: 1.02574\tvalid_1's rmse: 1.13563\n",
      "[206]\ttraining's rmse: 1.0254\tvalid_1's rmse: 1.1354\n",
      "[207]\ttraining's rmse: 1.02491\tvalid_1's rmse: 1.13512\n",
      "[208]\ttraining's rmse: 1.02453\tvalid_1's rmse: 1.13486\n",
      "[209]\ttraining's rmse: 1.02421\tvalid_1's rmse: 1.13466\n",
      "[210]\ttraining's rmse: 1.02388\tvalid_1's rmse: 1.13443\n",
      "[211]\ttraining's rmse: 1.02359\tvalid_1's rmse: 1.1342\n",
      "[212]\ttraining's rmse: 1.02329\tvalid_1's rmse: 1.13401\n",
      "[213]\ttraining's rmse: 1.02297\tvalid_1's rmse: 1.13379\n",
      "[214]\ttraining's rmse: 1.02248\tvalid_1's rmse: 1.13349\n",
      "[215]\ttraining's rmse: 1.02219\tvalid_1's rmse: 1.1333\n",
      "[216]\ttraining's rmse: 1.02189\tvalid_1's rmse: 1.13324\n",
      "[217]\ttraining's rmse: 1.0216\tvalid_1's rmse: 1.13309\n",
      "[218]\ttraining's rmse: 1.02131\tvalid_1's rmse: 1.13292\n",
      "[219]\ttraining's rmse: 1.02098\tvalid_1's rmse: 1.13278\n",
      "[220]\ttraining's rmse: 1.0207\tvalid_1's rmse: 1.1326\n",
      "[221]\ttraining's rmse: 1.0204\tvalid_1's rmse: 1.13239\n",
      "[222]\ttraining's rmse: 1.02015\tvalid_1's rmse: 1.13226\n",
      "[223]\ttraining's rmse: 1.0198\tvalid_1's rmse: 1.13205\n",
      "[224]\ttraining's rmse: 1.01935\tvalid_1's rmse: 1.13183\n",
      "[225]\ttraining's rmse: 1.0191\tvalid_1's rmse: 1.1317\n",
      "[226]\ttraining's rmse: 1.01883\tvalid_1's rmse: 1.13165\n",
      "[227]\ttraining's rmse: 1.01859\tvalid_1's rmse: 1.13155\n",
      "[228]\ttraining's rmse: 1.01832\tvalid_1's rmse: 1.13137\n",
      "[229]\ttraining's rmse: 1.01808\tvalid_1's rmse: 1.13123\n",
      "[230]\ttraining's rmse: 1.01764\tvalid_1's rmse: 1.13096\n",
      "[231]\ttraining's rmse: 1.01741\tvalid_1's rmse: 1.13086\n",
      "[232]\ttraining's rmse: 1.01708\tvalid_1's rmse: 1.13066\n",
      "[233]\ttraining's rmse: 1.0168\tvalid_1's rmse: 1.13046\n",
      "[234]\ttraining's rmse: 1.01654\tvalid_1's rmse: 1.13041\n",
      "[235]\ttraining's rmse: 1.01626\tvalid_1's rmse: 1.13017\n",
      "[236]\ttraining's rmse: 1.016\tvalid_1's rmse: 1.12999\n",
      "[237]\ttraining's rmse: 1.01558\tvalid_1's rmse: 1.12972\n",
      "[238]\ttraining's rmse: 1.01536\tvalid_1's rmse: 1.1296\n",
      "[239]\ttraining's rmse: 1.01494\tvalid_1's rmse: 1.12927\n",
      "[240]\ttraining's rmse: 1.01469\tvalid_1's rmse: 1.12917\n",
      "[241]\ttraining's rmse: 1.01448\tvalid_1's rmse: 1.12911\n",
      "[242]\ttraining's rmse: 1.01407\tvalid_1's rmse: 1.12882\n",
      "[243]\ttraining's rmse: 1.01382\tvalid_1's rmse: 1.12865\n",
      "[244]\ttraining's rmse: 1.01342\tvalid_1's rmse: 1.12836\n",
      "[245]\ttraining's rmse: 1.01301\tvalid_1's rmse: 1.12807\n",
      "[246]\ttraining's rmse: 1.01275\tvalid_1's rmse: 1.12784\n",
      "[247]\ttraining's rmse: 1.01236\tvalid_1's rmse: 1.12756\n",
      "[248]\ttraining's rmse: 1.01206\tvalid_1's rmse: 1.12738\n",
      "[249]\ttraining's rmse: 1.01186\tvalid_1's rmse: 1.12725\n",
      "[250]\ttraining's rmse: 1.01149\tvalid_1's rmse: 1.12697\n",
      "[251]\ttraining's rmse: 1.01124\tvalid_1's rmse: 1.12674\n",
      "[252]\ttraining's rmse: 1.011\tvalid_1's rmse: 1.12658\n",
      "[253]\ttraining's rmse: 1.01062\tvalid_1's rmse: 1.12626\n",
      "[254]\ttraining's rmse: 1.01038\tvalid_1's rmse: 1.12603\n",
      "[255]\ttraining's rmse: 1.01018\tvalid_1's rmse: 1.12597\n",
      "[256]\ttraining's rmse: 1.00983\tvalid_1's rmse: 1.12574\n",
      "[257]\ttraining's rmse: 1.00957\tvalid_1's rmse: 1.12551\n",
      "[258]\ttraining's rmse: 1.00937\tvalid_1's rmse: 1.12547\n",
      "[259]\ttraining's rmse: 1.0091\tvalid_1's rmse: 1.12525\n",
      "[260]\ttraining's rmse: 1.00876\tvalid_1's rmse: 1.12499\n",
      "[261]\ttraining's rmse: 1.00856\tvalid_1's rmse: 1.12487\n",
      "[262]\ttraining's rmse: 1.00831\tvalid_1's rmse: 1.12466\n",
      "[263]\ttraining's rmse: 1.00806\tvalid_1's rmse: 1.12444\n",
      "[264]\ttraining's rmse: 1.00787\tvalid_1's rmse: 1.1244\n",
      "[265]\ttraining's rmse: 1.00752\tvalid_1's rmse: 1.12412\n",
      "[266]\ttraining's rmse: 1.00727\tvalid_1's rmse: 1.1239\n",
      "[267]\ttraining's rmse: 1.00704\tvalid_1's rmse: 1.12378\n",
      "[268]\ttraining's rmse: 1.00671\tvalid_1's rmse: 1.12353\n",
      "[269]\ttraining's rmse: 1.00646\tvalid_1's rmse: 1.12334\n",
      "[270]\ttraining's rmse: 1.00623\tvalid_1's rmse: 1.12315\n",
      "[271]\ttraining's rmse: 1.00589\tvalid_1's rmse: 1.12292\n",
      "[272]\ttraining's rmse: 1.00567\tvalid_1's rmse: 1.12272\n",
      "[273]\ttraining's rmse: 1.00543\tvalid_1's rmse: 1.12254\n",
      "[274]\ttraining's rmse: 1.00511\tvalid_1's rmse: 1.1223\n",
      "[275]\ttraining's rmse: 1.00492\tvalid_1's rmse: 1.12227\n",
      "[276]\ttraining's rmse: 1.00468\tvalid_1's rmse: 1.12207\n",
      "[277]\ttraining's rmse: 1.00436\tvalid_1's rmse: 1.12192\n",
      "[278]\ttraining's rmse: 1.00415\tvalid_1's rmse: 1.12176\n",
      "[279]\ttraining's rmse: 1.00391\tvalid_1's rmse: 1.12161\n",
      "[280]\ttraining's rmse: 1.00371\tvalid_1's rmse: 1.1215\n",
      "[281]\ttraining's rmse: 1.00341\tvalid_1's rmse: 1.12125\n",
      "[282]\ttraining's rmse: 1.0032\tvalid_1's rmse: 1.12111\n",
      "[283]\ttraining's rmse: 1.00289\tvalid_1's rmse: 1.12093\n",
      "[284]\ttraining's rmse: 1.00268\tvalid_1's rmse: 1.12076\n",
      "[285]\ttraining's rmse: 1.00247\tvalid_1's rmse: 1.12061\n",
      "[286]\ttraining's rmse: 1.00227\tvalid_1's rmse: 1.12049\n",
      "[287]\ttraining's rmse: 1.00198\tvalid_1's rmse: 1.12041\n",
      "[288]\ttraining's rmse: 1.00178\tvalid_1's rmse: 1.12026\n",
      "[289]\ttraining's rmse: 1.00158\tvalid_1's rmse: 1.12023\n",
      "[290]\ttraining's rmse: 1.00138\tvalid_1's rmse: 1.12007\n",
      "[291]\ttraining's rmse: 1.00118\tvalid_1's rmse: 1.11997\n",
      "[292]\ttraining's rmse: 1.00096\tvalid_1's rmse: 1.11977\n",
      "[293]\ttraining's rmse: 1.00067\tvalid_1's rmse: 1.11956\n",
      "[294]\ttraining's rmse: 1.00051\tvalid_1's rmse: 1.11953\n",
      "[295]\ttraining's rmse: 1.00022\tvalid_1's rmse: 1.11946\n",
      "[296]\ttraining's rmse: 1.00003\tvalid_1's rmse: 1.11931\n",
      "[297]\ttraining's rmse: 0.999799\tvalid_1's rmse: 1.11918\n",
      "[298]\ttraining's rmse: 0.999504\tvalid_1's rmse: 1.119\n",
      "[299]\ttraining's rmse: 0.999235\tvalid_1's rmse: 1.11893\n",
      "[300]\ttraining's rmse: 0.999031\tvalid_1's rmse: 1.11881\n",
      "[301]\ttraining's rmse: 0.998853\tvalid_1's rmse: 1.11866\n",
      "[302]\ttraining's rmse: 0.998711\tvalid_1's rmse: 1.11863\n",
      "[303]\ttraining's rmse: 0.998504\tvalid_1's rmse: 1.11844\n",
      "[304]\ttraining's rmse: 0.99832\tvalid_1's rmse: 1.11839\n",
      "[305]\ttraining's rmse: 0.998104\tvalid_1's rmse: 1.11829\n",
      "[306]\ttraining's rmse: 0.997907\tvalid_1's rmse: 1.11814\n",
      "[307]\ttraining's rmse: 0.99771\tvalid_1's rmse: 1.11804\n",
      "[308]\ttraining's rmse: 0.997442\tvalid_1's rmse: 1.11799\n",
      "[309]\ttraining's rmse: 0.997261\tvalid_1's rmse: 1.11791\n",
      "[310]\ttraining's rmse: 0.997125\tvalid_1's rmse: 1.11789\n",
      "[311]\ttraining's rmse: 0.996928\tvalid_1's rmse: 1.1177\n",
      "[312]\ttraining's rmse: 0.996732\tvalid_1's rmse: 1.11754\n",
      "[313]\ttraining's rmse: 0.99647\tvalid_1's rmse: 1.11739\n",
      "[314]\ttraining's rmse: 0.996272\tvalid_1's rmse: 1.11727\n",
      "[315]\ttraining's rmse: 0.996099\tvalid_1's rmse: 1.11727\n",
      "[316]\ttraining's rmse: 0.995921\tvalid_1's rmse: 1.1171\n",
      "[317]\ttraining's rmse: 0.99579\tvalid_1's rmse: 1.11708\n",
      "[318]\ttraining's rmse: 0.995597\tvalid_1's rmse: 1.11691\n",
      "[319]\ttraining's rmse: 0.995405\tvalid_1's rmse: 1.1168\n",
      "[320]\ttraining's rmse: 0.995145\tvalid_1's rmse: 1.11662\n",
      "[321]\ttraining's rmse: 0.994993\tvalid_1's rmse: 1.11657\n",
      "[322]\ttraining's rmse: 0.994791\tvalid_1's rmse: 1.11646\n",
      "[323]\ttraining's rmse: 0.994537\tvalid_1's rmse: 1.11629\n",
      "[324]\ttraining's rmse: 0.994315\tvalid_1's rmse: 1.11609\n",
      "[325]\ttraining's rmse: 0.994135\tvalid_1's rmse: 1.11596\n",
      "[326]\ttraining's rmse: 0.993951\tvalid_1's rmse: 1.11584\n",
      "[327]\ttraining's rmse: 0.993708\tvalid_1's rmse: 1.1158\n",
      "[328]\ttraining's rmse: 0.993502\tvalid_1's rmse: 1.11562\n",
      "[329]\ttraining's rmse: 0.993322\tvalid_1's rmse: 1.11552\n",
      "[330]\ttraining's rmse: 0.993197\tvalid_1's rmse: 1.11549\n",
      "[331]\ttraining's rmse: 0.993016\tvalid_1's rmse: 1.11532\n",
      "[332]\ttraining's rmse: 0.992819\tvalid_1's rmse: 1.1152\n",
      "[333]\ttraining's rmse: 0.992639\tvalid_1's rmse: 1.11507\n",
      "[334]\ttraining's rmse: 0.992404\tvalid_1's rmse: 1.115\n",
      "[335]\ttraining's rmse: 0.992269\tvalid_1's rmse: 1.11495\n",
      "[336]\ttraining's rmse: 0.992071\tvalid_1's rmse: 1.1148\n",
      "[337]\ttraining's rmse: 0.991899\tvalid_1's rmse: 1.11472\n",
      "[338]\ttraining's rmse: 0.991657\tvalid_1's rmse: 1.11454\n",
      "[339]\ttraining's rmse: 0.99151\tvalid_1's rmse: 1.11445\n",
      "[340]\ttraining's rmse: 0.991389\tvalid_1's rmse: 1.11438\n",
      "[341]\ttraining's rmse: 0.991153\tvalid_1's rmse: 1.11422\n",
      "[342]\ttraining's rmse: 0.99096\tvalid_1's rmse: 1.11406\n",
      "[343]\ttraining's rmse: 0.990719\tvalid_1's rmse: 1.11389\n",
      "[344]\ttraining's rmse: 0.990525\tvalid_1's rmse: 1.1137\n",
      "[345]\ttraining's rmse: 0.990352\tvalid_1's rmse: 1.11364\n",
      "[346]\ttraining's rmse: 0.990193\tvalid_1's rmse: 1.1135\n",
      "[347]\ttraining's rmse: 0.990076\tvalid_1's rmse: 1.11343\n",
      "[348]\ttraining's rmse: 0.989836\tvalid_1's rmse: 1.11331\n",
      "[349]\ttraining's rmse: 0.989651\tvalid_1's rmse: 1.11317\n",
      "[350]\ttraining's rmse: 0.989511\tvalid_1's rmse: 1.11309\n",
      "[351]\ttraining's rmse: 0.989336\tvalid_1's rmse: 1.11298\n",
      "[352]\ttraining's rmse: 0.989177\tvalid_1's rmse: 1.11285\n",
      "[353]\ttraining's rmse: 0.988965\tvalid_1's rmse: 1.11277\n",
      "[354]\ttraining's rmse: 0.988788\tvalid_1's rmse: 1.11265\n",
      "[355]\ttraining's rmse: 0.988618\tvalid_1's rmse: 1.11255\n",
      "[356]\ttraining's rmse: 0.988458\tvalid_1's rmse: 1.1124\n",
      "[357]\ttraining's rmse: 0.988336\tvalid_1's rmse: 1.11237\n",
      "[358]\ttraining's rmse: 0.988118\tvalid_1's rmse: 1.11223\n",
      "[359]\ttraining's rmse: 0.987952\tvalid_1's rmse: 1.11212\n",
      "[360]\ttraining's rmse: 0.987781\tvalid_1's rmse: 1.11197\n",
      "[361]\ttraining's rmse: 0.987629\tvalid_1's rmse: 1.11185\n",
      "[362]\ttraining's rmse: 0.98742\tvalid_1's rmse: 1.11179\n",
      "[363]\ttraining's rmse: 0.98725\tvalid_1's rmse: 1.11168\n",
      "[364]\ttraining's rmse: 0.987146\tvalid_1's rmse: 1.11162\n",
      "[365]\ttraining's rmse: 0.986999\tvalid_1's rmse: 1.11149\n",
      "[366]\ttraining's rmse: 0.986858\tvalid_1's rmse: 1.11134\n",
      "[367]\ttraining's rmse: 0.986655\tvalid_1's rmse: 1.1112\n",
      "[368]\ttraining's rmse: 0.986497\tvalid_1's rmse: 1.1111\n",
      "[369]\ttraining's rmse: 0.986352\tvalid_1's rmse: 1.11103\n",
      "[370]\ttraining's rmse: 0.986188\tvalid_1's rmse: 1.11087\n",
      "[371]\ttraining's rmse: 0.986024\tvalid_1's rmse: 1.11078\n",
      "[372]\ttraining's rmse: 0.985813\tvalid_1's rmse: 1.11065\n",
      "[373]\ttraining's rmse: 0.985664\tvalid_1's rmse: 1.11059\n",
      "[374]\ttraining's rmse: 0.985469\tvalid_1's rmse: 1.11046\n",
      "[375]\ttraining's rmse: 0.985317\tvalid_1's rmse: 1.11043\n",
      "[376]\ttraining's rmse: 0.985165\tvalid_1's rmse: 1.11033\n",
      "[377]\ttraining's rmse: 0.985003\tvalid_1's rmse: 1.11023\n",
      "[378]\ttraining's rmse: 0.984891\tvalid_1's rmse: 1.1102\n",
      "[379]\ttraining's rmse: 0.984743\tvalid_1's rmse: 1.11014\n",
      "[380]\ttraining's rmse: 0.984594\tvalid_1's rmse: 1.11007\n",
      "[381]\ttraining's rmse: 0.984436\tvalid_1's rmse: 1.10996\n",
      "[382]\ttraining's rmse: 0.984321\tvalid_1's rmse: 1.10995\n",
      "[383]\ttraining's rmse: 0.984158\tvalid_1's rmse: 1.10993\n",
      "[384]\ttraining's rmse: 0.984002\tvalid_1's rmse: 1.10986\n",
      "[385]\ttraining's rmse: 0.983854\tvalid_1's rmse: 1.10977\n",
      "[386]\ttraining's rmse: 0.983655\tvalid_1's rmse: 1.10966\n",
      "[387]\ttraining's rmse: 0.983521\tvalid_1's rmse: 1.10964\n",
      "[388]\ttraining's rmse: 0.983376\tvalid_1's rmse: 1.10956\n",
      "[389]\ttraining's rmse: 0.983216\tvalid_1's rmse: 1.10941\n",
      "[390]\ttraining's rmse: 0.983068\tvalid_1's rmse: 1.10931\n",
      "[391]\ttraining's rmse: 0.982936\tvalid_1's rmse: 1.10923\n",
      "[392]\ttraining's rmse: 0.982809\tvalid_1's rmse: 1.10906\n",
      "[393]\ttraining's rmse: 0.982655\tvalid_1's rmse: 1.10894\n",
      "[394]\ttraining's rmse: 0.982501\tvalid_1's rmse: 1.10887\n",
      "[395]\ttraining's rmse: 0.982349\tvalid_1's rmse: 1.10885\n",
      "[396]\ttraining's rmse: 0.98219\tvalid_1's rmse: 1.10869\n",
      "[397]\ttraining's rmse: 0.982053\tvalid_1's rmse: 1.10863\n",
      "[398]\ttraining's rmse: 0.981871\tvalid_1's rmse: 1.10859\n",
      "[399]\ttraining's rmse: 0.981724\tvalid_1's rmse: 1.10849\n",
      "[400]\ttraining's rmse: 0.981576\tvalid_1's rmse: 1.10839\n",
      "[401]\ttraining's rmse: 0.981447\tvalid_1's rmse: 1.10832\n",
      "[402]\ttraining's rmse: 0.981293\tvalid_1's rmse: 1.10822\n",
      "[403]\ttraining's rmse: 0.981215\tvalid_1's rmse: 1.1082\n",
      "[404]\ttraining's rmse: 0.981083\tvalid_1's rmse: 1.10814\n",
      "[405]\ttraining's rmse: 0.980937\tvalid_1's rmse: 1.10806\n",
      "[406]\ttraining's rmse: 0.980789\tvalid_1's rmse: 1.10796\n",
      "[407]\ttraining's rmse: 0.980644\tvalid_1's rmse: 1.10787\n",
      "[408]\ttraining's rmse: 0.980457\tvalid_1's rmse: 1.10775\n",
      "[409]\ttraining's rmse: 0.980314\tvalid_1's rmse: 1.10767\n",
      "[410]\ttraining's rmse: 0.980177\tvalid_1's rmse: 1.10757\n",
      "[411]\ttraining's rmse: 0.980018\tvalid_1's rmse: 1.1075\n",
      "[412]\ttraining's rmse: 0.979868\tvalid_1's rmse: 1.10736\n",
      "[413]\ttraining's rmse: 0.979735\tvalid_1's rmse: 1.10734\n",
      "[414]\ttraining's rmse: 0.97961\tvalid_1's rmse: 1.10721\n",
      "[415]\ttraining's rmse: 0.979471\tvalid_1's rmse: 1.1071\n",
      "[416]\ttraining's rmse: 0.979287\tvalid_1's rmse: 1.107\n",
      "[417]\ttraining's rmse: 0.979142\tvalid_1's rmse: 1.1069\n",
      "[418]\ttraining's rmse: 0.978983\tvalid_1's rmse: 1.10683\n",
      "[419]\ttraining's rmse: 0.978839\tvalid_1's rmse: 1.10677\n",
      "[420]\ttraining's rmse: 0.978766\tvalid_1's rmse: 1.10675\n",
      "[421]\ttraining's rmse: 0.978625\tvalid_1's rmse: 1.10663\n",
      "[422]\ttraining's rmse: 0.978504\tvalid_1's rmse: 1.10655\n",
      "[423]\ttraining's rmse: 0.978353\tvalid_1's rmse: 1.10645\n",
      "[424]\ttraining's rmse: 0.97823\tvalid_1's rmse: 1.10643\n",
      "[425]\ttraining's rmse: 0.978074\tvalid_1's rmse: 1.10638\n",
      "[426]\ttraining's rmse: 0.977935\tvalid_1's rmse: 1.10629\n",
      "[427]\ttraining's rmse: 0.977794\tvalid_1's rmse: 1.10625\n",
      "[428]\ttraining's rmse: 0.977658\tvalid_1's rmse: 1.10613\n",
      "[429]\ttraining's rmse: 0.977526\tvalid_1's rmse: 1.10604\n",
      "[430]\ttraining's rmse: 0.977374\tvalid_1's rmse: 1.1059\n",
      "[431]\ttraining's rmse: 0.977231\tvalid_1's rmse: 1.10585\n",
      "[432]\ttraining's rmse: 0.9771\tvalid_1's rmse: 1.10572\n",
      "[433]\ttraining's rmse: 0.97695\tvalid_1's rmse: 1.10558\n",
      "[434]\ttraining's rmse: 0.976799\tvalid_1's rmse: 1.10554\n",
      "[435]\ttraining's rmse: 0.976677\tvalid_1's rmse: 1.10556\n",
      "[436]\ttraining's rmse: 0.976581\tvalid_1's rmse: 1.10553\n",
      "[437]\ttraining's rmse: 0.976456\tvalid_1's rmse: 1.10545\n",
      "[438]\ttraining's rmse: 0.976313\tvalid_1's rmse: 1.10539\n",
      "[439]\ttraining's rmse: 0.976183\tvalid_1's rmse: 1.10528\n",
      "[440]\ttraining's rmse: 0.976089\tvalid_1's rmse: 1.10526\n",
      "[441]\ttraining's rmse: 0.975951\tvalid_1's rmse: 1.10516\n",
      "[442]\ttraining's rmse: 0.975822\tvalid_1's rmse: 1.10506\n",
      "[443]\ttraining's rmse: 0.975658\tvalid_1's rmse: 1.10495\n",
      "[444]\ttraining's rmse: 0.97552\tvalid_1's rmse: 1.10492\n",
      "[445]\ttraining's rmse: 0.975395\tvalid_1's rmse: 1.1048\n",
      "[446]\ttraining's rmse: 0.97525\tvalid_1's rmse: 1.10477\n",
      "[447]\ttraining's rmse: 0.97516\tvalid_1's rmse: 1.10476\n",
      "[448]\ttraining's rmse: 0.975019\tvalid_1's rmse: 1.10462\n",
      "[449]\ttraining's rmse: 0.974884\tvalid_1's rmse: 1.10456\n",
      "[450]\ttraining's rmse: 0.974757\tvalid_1's rmse: 1.10441\n",
      "[451]\ttraining's rmse: 0.97464\tvalid_1's rmse: 1.10434\n",
      "[452]\ttraining's rmse: 0.974454\tvalid_1's rmse: 1.10421\n",
      "[453]\ttraining's rmse: 0.974321\tvalid_1's rmse: 1.10417\n",
      "[454]\ttraining's rmse: 0.974229\tvalid_1's rmse: 1.10416\n",
      "[455]\ttraining's rmse: 0.974091\tvalid_1's rmse: 1.10402\n",
      "[456]\ttraining's rmse: 0.973965\tvalid_1's rmse: 1.10397\n",
      "[457]\ttraining's rmse: 0.973826\tvalid_1's rmse: 1.10394\n",
      "[458]\ttraining's rmse: 0.973688\tvalid_1's rmse: 1.10379\n",
      "[459]\ttraining's rmse: 0.973566\tvalid_1's rmse: 1.10374\n",
      "[460]\ttraining's rmse: 0.973445\tvalid_1's rmse: 1.10363\n",
      "[461]\ttraining's rmse: 0.973253\tvalid_1's rmse: 1.1035\n",
      "[462]\ttraining's rmse: 0.973124\tvalid_1's rmse: 1.10345\n",
      "[463]\ttraining's rmse: 0.972991\tvalid_1's rmse: 1.10332\n",
      "[464]\ttraining's rmse: 0.972863\tvalid_1's rmse: 1.10331\n",
      "[465]\ttraining's rmse: 0.972783\tvalid_1's rmse: 1.10328\n",
      "[466]\ttraining's rmse: 0.972646\tvalid_1's rmse: 1.10322\n",
      "[467]\ttraining's rmse: 0.972516\tvalid_1's rmse: 1.1031\n",
      "[468]\ttraining's rmse: 0.972383\tvalid_1's rmse: 1.10307\n",
      "[469]\ttraining's rmse: 0.972274\tvalid_1's rmse: 1.10299\n",
      "[470]\ttraining's rmse: 0.97217\tvalid_1's rmse: 1.1029\n",
      "[471]\ttraining's rmse: 0.972017\tvalid_1's rmse: 1.10281\n",
      "[472]\ttraining's rmse: 0.971886\tvalid_1's rmse: 1.10278\n",
      "[473]\ttraining's rmse: 0.971779\tvalid_1's rmse: 1.10276\n",
      "[474]\ttraining's rmse: 0.971696\tvalid_1's rmse: 1.10272\n",
      "[475]\ttraining's rmse: 0.971557\tvalid_1's rmse: 1.10264\n",
      "[476]\ttraining's rmse: 0.971439\tvalid_1's rmse: 1.10253\n",
      "[477]\ttraining's rmse: 0.971311\tvalid_1's rmse: 1.1025\n",
      "[478]\ttraining's rmse: 0.97117\tvalid_1's rmse: 1.10239\n",
      "[479]\ttraining's rmse: 0.971023\tvalid_1's rmse: 1.10235\n",
      "[480]\ttraining's rmse: 0.970909\tvalid_1's rmse: 1.10227\n",
      "[481]\ttraining's rmse: 0.970785\tvalid_1's rmse: 1.10222\n",
      "[482]\ttraining's rmse: 0.970703\tvalid_1's rmse: 1.10219\n",
      "[483]\ttraining's rmse: 0.970566\tvalid_1's rmse: 1.10213\n",
      "[484]\ttraining's rmse: 0.970465\tvalid_1's rmse: 1.10204\n",
      "[485]\ttraining's rmse: 0.970333\tvalid_1's rmse: 1.10193\n",
      "[486]\ttraining's rmse: 0.970219\tvalid_1's rmse: 1.10191\n",
      "[487]\ttraining's rmse: 0.970097\tvalid_1's rmse: 1.10187\n",
      "[488]\ttraining's rmse: 0.969986\tvalid_1's rmse: 1.10178\n",
      "[489]\ttraining's rmse: 0.969909\tvalid_1's rmse: 1.10175\n",
      "[490]\ttraining's rmse: 0.969817\tvalid_1's rmse: 1.10173\n",
      "[491]\ttraining's rmse: 0.969681\tvalid_1's rmse: 1.10162\n",
      "[492]\ttraining's rmse: 0.969561\tvalid_1's rmse: 1.10159\n",
      "[493]\ttraining's rmse: 0.969421\tvalid_1's rmse: 1.10152\n",
      "[494]\ttraining's rmse: 0.969298\tvalid_1's rmse: 1.1014\n",
      "[495]\ttraining's rmse: 0.96919\tvalid_1's rmse: 1.10135\n",
      "[496]\ttraining's rmse: 0.969112\tvalid_1's rmse: 1.10133\n",
      "[497]\ttraining's rmse: 0.968993\tvalid_1's rmse: 1.10125\n",
      "[498]\ttraining's rmse: 0.968877\tvalid_1's rmse: 1.10122\n",
      "[499]\ttraining's rmse: 0.968745\tvalid_1's rmse: 1.10112\n",
      "[500]\ttraining's rmse: 0.968631\tvalid_1's rmse: 1.10107\n",
      "[501]\ttraining's rmse: 0.968534\tvalid_1's rmse: 1.10097\n",
      "[502]\ttraining's rmse: 0.96842\tvalid_1's rmse: 1.10095\n",
      "[503]\ttraining's rmse: 0.968283\tvalid_1's rmse: 1.10091\n",
      "[504]\ttraining's rmse: 0.968214\tvalid_1's rmse: 1.10086\n",
      "[505]\ttraining's rmse: 0.968098\tvalid_1's rmse: 1.1008\n",
      "[506]\ttraining's rmse: 0.967991\tvalid_1's rmse: 1.10078\n",
      "[507]\ttraining's rmse: 0.967852\tvalid_1's rmse: 1.10069\n",
      "[508]\ttraining's rmse: 0.967747\tvalid_1's rmse: 1.10062\n",
      "[509]\ttraining's rmse: 0.967634\tvalid_1's rmse: 1.10057\n",
      "[510]\ttraining's rmse: 0.967563\tvalid_1's rmse: 1.10056\n",
      "[511]\ttraining's rmse: 0.967445\tvalid_1's rmse: 1.1005\n",
      "[512]\ttraining's rmse: 0.967318\tvalid_1's rmse: 1.1004\n",
      "[513]\ttraining's rmse: 0.967207\tvalid_1's rmse: 1.10035\n",
      "[514]\ttraining's rmse: 0.967114\tvalid_1's rmse: 1.10029\n",
      "[515]\ttraining's rmse: 0.96699\tvalid_1's rmse: 1.10017\n",
      "[516]\ttraining's rmse: 0.96686\tvalid_1's rmse: 1.10015\n",
      "[517]\ttraining's rmse: 0.966802\tvalid_1's rmse: 1.10016\n",
      "[518]\ttraining's rmse: 0.966697\tvalid_1's rmse: 1.10007\n",
      "[519]\ttraining's rmse: 0.966589\tvalid_1's rmse: 1.10005\n",
      "[520]\ttraining's rmse: 0.966452\tvalid_1's rmse: 1.1\n",
      "[521]\ttraining's rmse: 0.966341\tvalid_1's rmse: 1.09993\n",
      "[522]\ttraining's rmse: 0.966247\tvalid_1's rmse: 1.09983\n",
      "[523]\ttraining's rmse: 0.966139\tvalid_1's rmse: 1.09979\n",
      "[524]\ttraining's rmse: 0.966068\tvalid_1's rmse: 1.09976\n",
      "[525]\ttraining's rmse: 0.965954\tvalid_1's rmse: 1.09969\n",
      "[526]\ttraining's rmse: 0.965848\tvalid_1's rmse: 1.09965\n",
      "[527]\ttraining's rmse: 0.965743\tvalid_1's rmse: 1.09963\n",
      "[528]\ttraining's rmse: 0.965657\tvalid_1's rmse: 1.09952\n",
      "[529]\ttraining's rmse: 0.965537\tvalid_1's rmse: 1.09944\n",
      "[530]\ttraining's rmse: 0.965414\tvalid_1's rmse: 1.09938\n",
      "[531]\ttraining's rmse: 0.96531\tvalid_1's rmse: 1.09933\n",
      "[532]\ttraining's rmse: 0.965211\tvalid_1's rmse: 1.09929\n",
      "[533]\ttraining's rmse: 0.965155\tvalid_1's rmse: 1.09929\n",
      "[534]\ttraining's rmse: 0.965043\tvalid_1's rmse: 1.0992\n",
      "[535]\ttraining's rmse: 0.964914\tvalid_1's rmse: 1.09911\n",
      "[536]\ttraining's rmse: 0.964813\tvalid_1's rmse: 1.0991\n",
      "[537]\ttraining's rmse: 0.964704\tvalid_1's rmse: 1.09906\n",
      "[538]\ttraining's rmse: 0.964647\tvalid_1's rmse: 1.09906\n",
      "[539]\ttraining's rmse: 0.964541\tvalid_1's rmse: 1.09897\n",
      "[540]\ttraining's rmse: 0.964441\tvalid_1's rmse: 1.09893\n",
      "[541]\ttraining's rmse: 0.96434\tvalid_1's rmse: 1.0989\n",
      "[542]\ttraining's rmse: 0.964226\tvalid_1's rmse: 1.09881\n",
      "[543]\ttraining's rmse: 0.964151\tvalid_1's rmse: 1.09881\n",
      "[544]\ttraining's rmse: 0.964047\tvalid_1's rmse: 1.0988\n",
      "[545]\ttraining's rmse: 0.963937\tvalid_1's rmse: 1.09878\n",
      "[546]\ttraining's rmse: 0.96381\tvalid_1's rmse: 1.09873\n",
      "[547]\ttraining's rmse: 0.963705\tvalid_1's rmse: 1.09869\n",
      "[548]\ttraining's rmse: 0.963605\tvalid_1's rmse: 1.09866\n",
      "[549]\ttraining's rmse: 0.963497\tvalid_1's rmse: 1.09864\n",
      "[550]\ttraining's rmse: 0.963425\tvalid_1's rmse: 1.09863\n",
      "[551]\ttraining's rmse: 0.963332\tvalid_1's rmse: 1.09859\n",
      "[552]\ttraining's rmse: 0.963227\tvalid_1's rmse: 1.09856\n",
      "[553]\ttraining's rmse: 0.963137\tvalid_1's rmse: 1.09853\n",
      "[554]\ttraining's rmse: 0.963013\tvalid_1's rmse: 1.09848\n",
      "[555]\ttraining's rmse: 0.962903\tvalid_1's rmse: 1.09835\n",
      "[556]\ttraining's rmse: 0.962776\tvalid_1's rmse: 1.09831\n",
      "[557]\ttraining's rmse: 0.962673\tvalid_1's rmse: 1.09831\n",
      "[558]\ttraining's rmse: 0.96257\tvalid_1's rmse: 1.09828\n",
      "[559]\ttraining's rmse: 0.96249\tvalid_1's rmse: 1.09825\n",
      "[560]\ttraining's rmse: 0.962386\tvalid_1's rmse: 1.09823\n",
      "[561]\ttraining's rmse: 0.962292\tvalid_1's rmse: 1.09822\n",
      "[562]\ttraining's rmse: 0.962198\tvalid_1's rmse: 1.09813\n",
      "[563]\ttraining's rmse: 0.962078\tvalid_1's rmse: 1.0981\n",
      "[564]\ttraining's rmse: 0.962006\tvalid_1's rmse: 1.09807\n",
      "[565]\ttraining's rmse: 0.961899\tvalid_1's rmse: 1.09793\n",
      "[566]\ttraining's rmse: 0.961805\tvalid_1's rmse: 1.09788\n",
      "[567]\ttraining's rmse: 0.961733\tvalid_1's rmse: 1.09785\n",
      "[568]\ttraining's rmse: 0.961631\tvalid_1's rmse: 1.09785\n",
      "[569]\ttraining's rmse: 0.961525\tvalid_1's rmse: 1.09773\n",
      "[570]\ttraining's rmse: 0.961406\tvalid_1's rmse: 1.09766\n",
      "[571]\ttraining's rmse: 0.961298\tvalid_1's rmse: 1.09765\n",
      "[572]\ttraining's rmse: 0.961247\tvalid_1's rmse: 1.09765\n",
      "[573]\ttraining's rmse: 0.961159\tvalid_1's rmse: 1.09753\n",
      "[574]\ttraining's rmse: 0.961063\tvalid_1's rmse: 1.09747\n",
      "[575]\ttraining's rmse: 0.960962\tvalid_1's rmse: 1.09742\n",
      "[576]\ttraining's rmse: 0.960861\tvalid_1's rmse: 1.0974\n",
      "[577]\ttraining's rmse: 0.960758\tvalid_1's rmse: 1.09728\n",
      "[578]\ttraining's rmse: 0.960708\tvalid_1's rmse: 1.09729\n",
      "[579]\ttraining's rmse: 0.960606\tvalid_1's rmse: 1.09728\n",
      "[580]\ttraining's rmse: 0.960509\tvalid_1's rmse: 1.09725\n",
      "[581]\ttraining's rmse: 0.960391\tvalid_1's rmse: 1.09722\n",
      "[582]\ttraining's rmse: 0.960296\tvalid_1's rmse: 1.09716\n",
      "[583]\ttraining's rmse: 0.960227\tvalid_1's rmse: 1.09716\n",
      "[584]\ttraining's rmse: 0.960178\tvalid_1's rmse: 1.09718\n",
      "[585]\ttraining's rmse: 0.960078\tvalid_1's rmse: 1.09716\n",
      "[586]\ttraining's rmse: 0.959982\tvalid_1's rmse: 1.09716\n",
      "[587]\ttraining's rmse: 0.959874\tvalid_1's rmse: 1.0971\n",
      "[588]\ttraining's rmse: 0.959784\tvalid_1's rmse: 1.09707\n",
      "[589]\ttraining's rmse: 0.959683\tvalid_1's rmse: 1.097\n",
      "[590]\ttraining's rmse: 0.959581\tvalid_1's rmse: 1.09697\n",
      "[591]\ttraining's rmse: 0.959491\tvalid_1's rmse: 1.09691\n",
      "[592]\ttraining's rmse: 0.959422\tvalid_1's rmse: 1.09689\n",
      "[593]\ttraining's rmse: 0.959339\tvalid_1's rmse: 1.09686\n",
      "[594]\ttraining's rmse: 0.959289\tvalid_1's rmse: 1.09685\n",
      "[595]\ttraining's rmse: 0.959192\tvalid_1's rmse: 1.09684\n",
      "[596]\ttraining's rmse: 0.959087\tvalid_1's rmse: 1.09683\n",
      "[597]\ttraining's rmse: 0.958987\tvalid_1's rmse: 1.09673\n",
      "[598]\ttraining's rmse: 0.958897\tvalid_1's rmse: 1.09663\n",
      "[599]\ttraining's rmse: 0.958782\tvalid_1's rmse: 1.0965\n",
      "[600]\ttraining's rmse: 0.95869\tvalid_1's rmse: 1.09645\n",
      "[601]\ttraining's rmse: 0.958637\tvalid_1's rmse: 1.09645\n",
      "[602]\ttraining's rmse: 0.958542\tvalid_1's rmse: 1.09638\n",
      "[603]\ttraining's rmse: 0.958446\tvalid_1's rmse: 1.0964\n",
      "[604]\ttraining's rmse: 0.958351\tvalid_1's rmse: 1.09636\n",
      "[605]\ttraining's rmse: 0.958259\tvalid_1's rmse: 1.09628\n",
      "[606]\ttraining's rmse: 0.958156\tvalid_1's rmse: 1.09621\n",
      "[607]\ttraining's rmse: 0.958056\tvalid_1's rmse: 1.09619\n",
      "[608]\ttraining's rmse: 0.957964\tvalid_1's rmse: 1.09614\n",
      "[609]\ttraining's rmse: 0.957884\tvalid_1's rmse: 1.09612\n",
      "[610]\ttraining's rmse: 0.957732\tvalid_1's rmse: 1.09603\n",
      "[611]\ttraining's rmse: 0.957609\tvalid_1's rmse: 1.09593\n",
      "[612]\ttraining's rmse: 0.957563\tvalid_1's rmse: 1.09594\n",
      "[613]\ttraining's rmse: 0.957477\tvalid_1's rmse: 1.09598\n",
      "[614]\ttraining's rmse: 0.957376\tvalid_1's rmse: 1.09595\n",
      "[615]\ttraining's rmse: 0.957275\tvalid_1's rmse: 1.09593\n",
      "[616]\ttraining's rmse: 0.957179\tvalid_1's rmse: 1.09585\n",
      "[617]\ttraining's rmse: 0.957093\tvalid_1's rmse: 1.09582\n",
      "[618]\ttraining's rmse: 0.956982\tvalid_1's rmse: 1.0957\n",
      "[619]\ttraining's rmse: 0.956888\tvalid_1's rmse: 1.0957\n",
      "[620]\ttraining's rmse: 0.956844\tvalid_1's rmse: 1.0957\n",
      "[621]\ttraining's rmse: 0.95675\tvalid_1's rmse: 1.09557\n",
      "[622]\ttraining's rmse: 0.956653\tvalid_1's rmse: 1.09554\n",
      "[623]\ttraining's rmse: 0.956574\tvalid_1's rmse: 1.09559\n",
      "[624]\ttraining's rmse: 0.956481\tvalid_1's rmse: 1.09552\n",
      "[625]\ttraining's rmse: 0.956376\tvalid_1's rmse: 1.09541\n",
      "[626]\ttraining's rmse: 0.95629\tvalid_1's rmse: 1.09536\n",
      "[627]\ttraining's rmse: 0.956201\tvalid_1's rmse: 1.09533\n",
      "[628]\ttraining's rmse: 0.956085\tvalid_1's rmse: 1.09522\n",
      "[629]\ttraining's rmse: 0.955992\tvalid_1's rmse: 1.09522\n",
      "[630]\ttraining's rmse: 0.955903\tvalid_1's rmse: 1.09519\n",
      "[631]\ttraining's rmse: 0.955816\tvalid_1's rmse: 1.09516\n",
      "[632]\ttraining's rmse: 0.955768\tvalid_1's rmse: 1.09515\n",
      "[633]\ttraining's rmse: 0.955675\tvalid_1's rmse: 1.09508\n",
      "[634]\ttraining's rmse: 0.955592\tvalid_1's rmse: 1.09508\n",
      "[635]\ttraining's rmse: 0.955511\tvalid_1's rmse: 1.09505\n",
      "[636]\ttraining's rmse: 0.95543\tvalid_1's rmse: 1.09503\n",
      "[637]\ttraining's rmse: 0.955339\tvalid_1's rmse: 1.09503\n",
      "[638]\ttraining's rmse: 0.955248\tvalid_1's rmse: 1.09497\n",
      "[639]\ttraining's rmse: 0.955106\tvalid_1's rmse: 1.09486\n",
      "[640]\ttraining's rmse: 0.954987\tvalid_1's rmse: 1.09477\n",
      "[641]\ttraining's rmse: 0.954906\tvalid_1's rmse: 1.09476\n",
      "[642]\ttraining's rmse: 0.954857\tvalid_1's rmse: 1.09475\n",
      "[643]\ttraining's rmse: 0.954763\tvalid_1's rmse: 1.09469\n",
      "[644]\ttraining's rmse: 0.954689\tvalid_1's rmse: 1.09459\n",
      "[645]\ttraining's rmse: 0.954607\tvalid_1's rmse: 1.09462\n",
      "[646]\ttraining's rmse: 0.954516\tvalid_1's rmse: 1.09456\n",
      "[647]\ttraining's rmse: 0.954433\tvalid_1's rmse: 1.09448\n",
      "[648]\ttraining's rmse: 0.954333\tvalid_1's rmse: 1.09437\n",
      "[649]\ttraining's rmse: 0.954285\tvalid_1's rmse: 1.09437\n",
      "[650]\ttraining's rmse: 0.954188\tvalid_1's rmse: 1.09435\n",
      "[651]\ttraining's rmse: 0.95411\tvalid_1's rmse: 1.09433\n",
      "[652]\ttraining's rmse: 0.954046\tvalid_1's rmse: 1.09429\n",
      "[653]\ttraining's rmse: 0.953995\tvalid_1's rmse: 1.09426\n",
      "[654]\ttraining's rmse: 0.953902\tvalid_1's rmse: 1.09424\n",
      "[655]\ttraining's rmse: 0.953814\tvalid_1's rmse: 1.0942\n",
      "[656]\ttraining's rmse: 0.953737\tvalid_1's rmse: 1.09418\n",
      "[657]\ttraining's rmse: 0.953677\tvalid_1's rmse: 1.09419\n",
      "[658]\ttraining's rmse: 0.953632\tvalid_1's rmse: 1.09419\n",
      "[659]\ttraining's rmse: 0.953545\tvalid_1's rmse: 1.09418\n",
      "[660]\ttraining's rmse: 0.953455\tvalid_1's rmse: 1.09421\n",
      "[661]\ttraining's rmse: 0.953369\tvalid_1's rmse: 1.09422\n",
      "[662]\ttraining's rmse: 0.953286\tvalid_1's rmse: 1.09415\n",
      "[663]\ttraining's rmse: 0.953223\tvalid_1's rmse: 1.09414\n",
      "[664]\ttraining's rmse: 0.953145\tvalid_1's rmse: 1.0941\n",
      "[665]\ttraining's rmse: 0.953068\tvalid_1's rmse: 1.09409\n",
      "[666]\ttraining's rmse: 0.952989\tvalid_1's rmse: 1.0941\n",
      "[667]\ttraining's rmse: 0.952888\tvalid_1's rmse: 1.09408\n",
      "[668]\ttraining's rmse: 0.952784\tvalid_1's rmse: 1.09396\n",
      "[669]\ttraining's rmse: 0.95274\tvalid_1's rmse: 1.09396\n",
      "[670]\ttraining's rmse: 0.95265\tvalid_1's rmse: 1.09393\n",
      "[671]\ttraining's rmse: 0.952565\tvalid_1's rmse: 1.09388\n",
      "[672]\ttraining's rmse: 0.952501\tvalid_1's rmse: 1.09386\n",
      "[673]\ttraining's rmse: 0.952439\tvalid_1's rmse: 1.09382\n",
      "[674]\ttraining's rmse: 0.952351\tvalid_1's rmse: 1.09384\n",
      "[675]\ttraining's rmse: 0.952216\tvalid_1's rmse: 1.09374\n",
      "[676]\ttraining's rmse: 0.952107\tvalid_1's rmse: 1.09364\n",
      "[677]\ttraining's rmse: 0.952027\tvalid_1's rmse: 1.09362\n",
      "[678]\ttraining's rmse: 0.95193\tvalid_1's rmse: 1.09359\n",
      "[679]\ttraining's rmse: 0.951846\tvalid_1's rmse: 1.09358\n",
      "[680]\ttraining's rmse: 0.951785\tvalid_1's rmse: 1.09356\n",
      "[681]\ttraining's rmse: 0.951695\tvalid_1's rmse: 1.09356\n",
      "[682]\ttraining's rmse: 0.951595\tvalid_1's rmse: 1.09347\n",
      "[683]\ttraining's rmse: 0.951529\tvalid_1's rmse: 1.09341\n",
      "[684]\ttraining's rmse: 0.951453\tvalid_1's rmse: 1.09338\n",
      "[685]\ttraining's rmse: 0.951358\tvalid_1's rmse: 1.09337\n",
      "[686]\ttraining's rmse: 0.951259\tvalid_1's rmse: 1.09331\n",
      "[687]\ttraining's rmse: 0.951163\tvalid_1's rmse: 1.09321\n",
      "[688]\ttraining's rmse: 0.951077\tvalid_1's rmse: 1.0932\n",
      "[689]\ttraining's rmse: 0.950996\tvalid_1's rmse: 1.09317\n",
      "[690]\ttraining's rmse: 0.950951\tvalid_1's rmse: 1.09318\n",
      "[691]\ttraining's rmse: 0.950867\tvalid_1's rmse: 1.09318\n",
      "[692]\ttraining's rmse: 0.950807\tvalid_1's rmse: 1.09316\n",
      "[693]\ttraining's rmse: 0.95075\tvalid_1's rmse: 1.09313\n",
      "[694]\ttraining's rmse: 0.950667\tvalid_1's rmse: 1.0931\n",
      "[695]\ttraining's rmse: 0.950569\tvalid_1's rmse: 1.09305\n",
      "[696]\ttraining's rmse: 0.950472\tvalid_1's rmse: 1.09302\n",
      "[697]\ttraining's rmse: 0.950386\tvalid_1's rmse: 1.093\n",
      "[698]\ttraining's rmse: 0.950308\tvalid_1's rmse: 1.09297\n",
      "[699]\ttraining's rmse: 0.950235\tvalid_1's rmse: 1.09297\n",
      "[700]\ttraining's rmse: 0.950142\tvalid_1's rmse: 1.09295\n",
      "[701]\ttraining's rmse: 0.950085\tvalid_1's rmse: 1.09296\n",
      "[702]\ttraining's rmse: 0.950005\tvalid_1's rmse: 1.09293\n",
      "[703]\ttraining's rmse: 0.949925\tvalid_1's rmse: 1.09294\n",
      "[704]\ttraining's rmse: 0.949835\tvalid_1's rmse: 1.09294\n",
      "[705]\ttraining's rmse: 0.949776\tvalid_1's rmse: 1.09293\n",
      "[706]\ttraining's rmse: 0.949681\tvalid_1's rmse: 1.09288\n",
      "[707]\ttraining's rmse: 0.949599\tvalid_1's rmse: 1.09286\n",
      "[708]\ttraining's rmse: 0.949536\tvalid_1's rmse: 1.0928\n",
      "[709]\ttraining's rmse: 0.949495\tvalid_1's rmse: 1.0928\n",
      "[710]\ttraining's rmse: 0.94944\tvalid_1's rmse: 1.09278\n",
      "[711]\ttraining's rmse: 0.949362\tvalid_1's rmse: 1.09275\n",
      "[712]\ttraining's rmse: 0.949282\tvalid_1's rmse: 1.09275\n",
      "[713]\ttraining's rmse: 0.9492\tvalid_1's rmse: 1.09272\n",
      "[714]\ttraining's rmse: 0.949107\tvalid_1's rmse: 1.09267\n",
      "[715]\ttraining's rmse: 0.949041\tvalid_1's rmse: 1.09266\n",
      "[716]\ttraining's rmse: 0.949\tvalid_1's rmse: 1.09266\n",
      "[717]\ttraining's rmse: 0.948942\tvalid_1's rmse: 1.09261\n",
      "[718]\ttraining's rmse: 0.948858\tvalid_1's rmse: 1.09256\n",
      "[719]\ttraining's rmse: 0.94878\tvalid_1's rmse: 1.09257\n",
      "[720]\ttraining's rmse: 0.948704\tvalid_1's rmse: 1.09253\n",
      "[721]\ttraining's rmse: 0.948626\tvalid_1's rmse: 1.09255\n",
      "[722]\ttraining's rmse: 0.948512\tvalid_1's rmse: 1.0925\n",
      "[723]\ttraining's rmse: 0.948441\tvalid_1's rmse: 1.0925\n",
      "[724]\ttraining's rmse: 0.948352\tvalid_1's rmse: 1.09248\n",
      "[725]\ttraining's rmse: 0.948273\tvalid_1's rmse: 1.09247\n",
      "[726]\ttraining's rmse: 0.948203\tvalid_1's rmse: 1.09246\n",
      "[727]\ttraining's rmse: 0.948146\tvalid_1's rmse: 1.09245\n",
      "[728]\ttraining's rmse: 0.948064\tvalid_1's rmse: 1.0924\n",
      "[729]\ttraining's rmse: 0.947985\tvalid_1's rmse: 1.09236\n",
      "[730]\ttraining's rmse: 0.94791\tvalid_1's rmse: 1.09234\n",
      "[731]\ttraining's rmse: 0.947831\tvalid_1's rmse: 1.09235\n",
      "[732]\ttraining's rmse: 0.94772\tvalid_1's rmse: 1.09231\n",
      "[733]\ttraining's rmse: 0.947633\tvalid_1's rmse: 1.09228\n",
      "[734]\ttraining's rmse: 0.947567\tvalid_1's rmse: 1.09224\n",
      "[735]\ttraining's rmse: 0.947487\tvalid_1's rmse: 1.09222\n",
      "[736]\ttraining's rmse: 0.947432\tvalid_1's rmse: 1.09221\n",
      "[737]\ttraining's rmse: 0.94734\tvalid_1's rmse: 1.09216\n",
      "[738]\ttraining's rmse: 0.947264\tvalid_1's rmse: 1.09216\n",
      "[739]\ttraining's rmse: 0.947188\tvalid_1's rmse: 1.0922\n",
      "[740]\ttraining's rmse: 0.947115\tvalid_1's rmse: 1.09218\n",
      "[741]\ttraining's rmse: 0.947013\tvalid_1's rmse: 1.09215\n",
      "[742]\ttraining's rmse: 0.946946\tvalid_1's rmse: 1.09217\n",
      "[743]\ttraining's rmse: 0.946865\tvalid_1's rmse: 1.09215\n",
      "[744]\ttraining's rmse: 0.946775\tvalid_1's rmse: 1.09214\n",
      "[745]\ttraining's rmse: 0.946667\tvalid_1's rmse: 1.0921\n",
      "[746]\ttraining's rmse: 0.946577\tvalid_1's rmse: 1.09207\n",
      "[747]\ttraining's rmse: 0.946519\tvalid_1's rmse: 1.09205\n",
      "[748]\ttraining's rmse: 0.946429\tvalid_1's rmse: 1.09197\n",
      "[749]\ttraining's rmse: 0.946324\tvalid_1's rmse: 1.09193\n",
      "[750]\ttraining's rmse: 0.946273\tvalid_1's rmse: 1.09192\n",
      "[751]\ttraining's rmse: 0.946189\tvalid_1's rmse: 1.0919\n",
      "[752]\ttraining's rmse: 0.946097\tvalid_1's rmse: 1.09188\n",
      "[753]\ttraining's rmse: 0.946023\tvalid_1's rmse: 1.09192\n",
      "[754]\ttraining's rmse: 0.945962\tvalid_1's rmse: 1.09185\n",
      "[755]\ttraining's rmse: 0.945921\tvalid_1's rmse: 1.09186\n",
      "[756]\ttraining's rmse: 0.945832\tvalid_1's rmse: 1.0918\n",
      "[757]\ttraining's rmse: 0.945759\tvalid_1's rmse: 1.0918\n",
      "[758]\ttraining's rmse: 0.945685\tvalid_1's rmse: 1.09176\n",
      "[759]\ttraining's rmse: 0.945608\tvalid_1's rmse: 1.09175\n",
      "[760]\ttraining's rmse: 0.945555\tvalid_1's rmse: 1.09175\n",
      "[761]\ttraining's rmse: 0.945475\tvalid_1's rmse: 1.09172\n",
      "[762]\ttraining's rmse: 0.945429\tvalid_1's rmse: 1.09166\n",
      "[763]\ttraining's rmse: 0.945363\tvalid_1's rmse: 1.09165\n",
      "[764]\ttraining's rmse: 0.945296\tvalid_1's rmse: 1.09165\n",
      "[765]\ttraining's rmse: 0.945223\tvalid_1's rmse: 1.09161\n",
      "[766]\ttraining's rmse: 0.945198\tvalid_1's rmse: 1.0916\n",
      "[767]\ttraining's rmse: 0.945117\tvalid_1's rmse: 1.09158\n",
      "[768]\ttraining's rmse: 0.945074\tvalid_1's rmse: 1.09158\n",
      "[769]\ttraining's rmse: 0.945015\tvalid_1's rmse: 1.09156\n",
      "[770]\ttraining's rmse: 0.94497\tvalid_1's rmse: 1.0915\n",
      "[771]\ttraining's rmse: 0.944905\tvalid_1's rmse: 1.09154\n",
      "[772]\ttraining's rmse: 0.944821\tvalid_1's rmse: 1.09151\n",
      "[773]\ttraining's rmse: 0.944739\tvalid_1's rmse: 1.09149\n",
      "[774]\ttraining's rmse: 0.94466\tvalid_1's rmse: 1.09149\n",
      "[775]\ttraining's rmse: 0.944635\tvalid_1's rmse: 1.0915\n",
      "[776]\ttraining's rmse: 0.944549\tvalid_1's rmse: 1.0915\n",
      "[777]\ttraining's rmse: 0.944468\tvalid_1's rmse: 1.09148\n",
      "[778]\ttraining's rmse: 0.944396\tvalid_1's rmse: 1.09144\n",
      "[779]\ttraining's rmse: 0.944356\tvalid_1's rmse: 1.09144\n",
      "[780]\ttraining's rmse: 0.944278\tvalid_1's rmse: 1.09142\n",
      "[781]\ttraining's rmse: 0.944223\tvalid_1's rmse: 1.09141\n",
      "[782]\ttraining's rmse: 0.944145\tvalid_1's rmse: 1.09139\n",
      "[783]\ttraining's rmse: 0.944038\tvalid_1's rmse: 1.09134\n",
      "[784]\ttraining's rmse: 0.943991\tvalid_1's rmse: 1.09134\n",
      "[785]\ttraining's rmse: 0.943904\tvalid_1's rmse: 1.09131\n",
      "[786]\ttraining's rmse: 0.943816\tvalid_1's rmse: 1.09129\n",
      "[787]\ttraining's rmse: 0.943785\tvalid_1's rmse: 1.09129\n",
      "[788]\ttraining's rmse: 0.943713\tvalid_1's rmse: 1.09133\n",
      "[789]\ttraining's rmse: 0.943648\tvalid_1's rmse: 1.09132\n",
      "[790]\ttraining's rmse: 0.94357\tvalid_1's rmse: 1.09131\n",
      "[791]\ttraining's rmse: 0.943515\tvalid_1's rmse: 1.09131\n",
      "[792]\ttraining's rmse: 0.943406\tvalid_1's rmse: 1.09127\n",
      "[793]\ttraining's rmse: 0.943337\tvalid_1's rmse: 1.09126\n",
      "[794]\ttraining's rmse: 0.943248\tvalid_1's rmse: 1.09125\n",
      "[795]\ttraining's rmse: 0.943178\tvalid_1's rmse: 1.09122\n",
      "[796]\ttraining's rmse: 0.943154\tvalid_1's rmse: 1.09121\n",
      "[797]\ttraining's rmse: 0.943041\tvalid_1's rmse: 1.09113\n",
      "[798]\ttraining's rmse: 0.942985\tvalid_1's rmse: 1.0911\n",
      "[799]\ttraining's rmse: 0.94292\tvalid_1's rmse: 1.0911\n",
      "[800]\ttraining's rmse: 0.942838\tvalid_1's rmse: 1.09109\n",
      "[801]\ttraining's rmse: 0.942738\tvalid_1's rmse: 1.091\n",
      "[802]\ttraining's rmse: 0.942674\tvalid_1's rmse: 1.091\n",
      "[803]\ttraining's rmse: 0.942595\tvalid_1's rmse: 1.09098\n",
      "[804]\ttraining's rmse: 0.942525\tvalid_1's rmse: 1.09103\n",
      "[805]\ttraining's rmse: 0.942458\tvalid_1's rmse: 1.091\n",
      "[806]\ttraining's rmse: 0.942402\tvalid_1's rmse: 1.09099\n",
      "[807]\ttraining's rmse: 0.942296\tvalid_1's rmse: 1.09095\n",
      "[808]\ttraining's rmse: 0.942222\tvalid_1's rmse: 1.09094\n",
      "[809]\ttraining's rmse: 0.942182\tvalid_1's rmse: 1.09093\n",
      "[810]\ttraining's rmse: 0.942116\tvalid_1's rmse: 1.09096\n",
      "[811]\ttraining's rmse: 0.942049\tvalid_1's rmse: 1.09093\n",
      "[812]\ttraining's rmse: 0.942005\tvalid_1's rmse: 1.09088\n",
      "[813]\ttraining's rmse: 0.941975\tvalid_1's rmse: 1.09088\n",
      "[814]\ttraining's rmse: 0.941888\tvalid_1's rmse: 1.09089\n",
      "[815]\ttraining's rmse: 0.941824\tvalid_1's rmse: 1.0909\n",
      "[816]\ttraining's rmse: 0.941744\tvalid_1's rmse: 1.09089\n",
      "[817]\ttraining's rmse: 0.941706\tvalid_1's rmse: 1.09093\n",
      "[818]\ttraining's rmse: 0.941642\tvalid_1's rmse: 1.09089\n",
      "[819]\ttraining's rmse: 0.941573\tvalid_1's rmse: 1.09093\n",
      "[820]\ttraining's rmse: 0.941508\tvalid_1's rmse: 1.0909\n",
      "[821]\ttraining's rmse: 0.941465\tvalid_1's rmse: 1.09087\n",
      "[822]\ttraining's rmse: 0.941439\tvalid_1's rmse: 1.09086\n",
      "[823]\ttraining's rmse: 0.941346\tvalid_1's rmse: 1.09088\n",
      "[824]\ttraining's rmse: 0.94128\tvalid_1's rmse: 1.09087\n",
      "[825]\ttraining's rmse: 0.941228\tvalid_1's rmse: 1.09086\n",
      "[826]\ttraining's rmse: 0.941159\tvalid_1's rmse: 1.09082\n",
      "[827]\ttraining's rmse: 0.941112\tvalid_1's rmse: 1.09079\n",
      "[828]\ttraining's rmse: 0.941042\tvalid_1's rmse: 1.09082\n",
      "[829]\ttraining's rmse: 0.941002\tvalid_1's rmse: 1.09079\n",
      "[830]\ttraining's rmse: 0.940973\tvalid_1's rmse: 1.09079\n",
      "[831]\ttraining's rmse: 0.94087\tvalid_1's rmse: 1.09073\n",
      "[832]\ttraining's rmse: 0.940798\tvalid_1's rmse: 1.09073\n",
      "[833]\ttraining's rmse: 0.940732\tvalid_1's rmse: 1.0907\n",
      "[834]\ttraining's rmse: 0.940659\tvalid_1's rmse: 1.09064\n",
      "[835]\ttraining's rmse: 0.940604\tvalid_1's rmse: 1.09063\n",
      "[836]\ttraining's rmse: 0.940524\tvalid_1's rmse: 1.09066\n",
      "[837]\ttraining's rmse: 0.940439\tvalid_1's rmse: 1.09056\n",
      "[838]\ttraining's rmse: 0.940327\tvalid_1's rmse: 1.09047\n",
      "[839]\ttraining's rmse: 0.940286\tvalid_1's rmse: 1.09043\n",
      "[840]\ttraining's rmse: 0.940221\tvalid_1's rmse: 1.09049\n",
      "[841]\ttraining's rmse: 0.940159\tvalid_1's rmse: 1.09047\n",
      "[842]\ttraining's rmse: 0.940054\tvalid_1's rmse: 1.09043\n",
      "[843]\ttraining's rmse: 0.939963\tvalid_1's rmse: 1.09037\n",
      "[844]\ttraining's rmse: 0.939871\tvalid_1's rmse: 1.09026\n",
      "[845]\ttraining's rmse: 0.93982\tvalid_1's rmse: 1.09027\n",
      "[846]\ttraining's rmse: 0.939735\tvalid_1's rmse: 1.09028\n",
      "[847]\ttraining's rmse: 0.939667\tvalid_1's rmse: 1.09027\n",
      "[848]\ttraining's rmse: 0.93958\tvalid_1's rmse: 1.09032\n",
      "[849]\ttraining's rmse: 0.939552\tvalid_1's rmse: 1.09031\n",
      "[850]\ttraining's rmse: 0.939515\tvalid_1's rmse: 1.09031\n",
      "[851]\ttraining's rmse: 0.939474\tvalid_1's rmse: 1.09031\n",
      "[852]\ttraining's rmse: 0.939414\tvalid_1's rmse: 1.09024\n",
      "[853]\ttraining's rmse: 0.939374\tvalid_1's rmse: 1.09021\n",
      "[854]\ttraining's rmse: 0.939288\tvalid_1's rmse: 1.09011\n",
      "[855]\ttraining's rmse: 0.939214\tvalid_1's rmse: 1.09008\n",
      "[856]\ttraining's rmse: 0.939151\tvalid_1's rmse: 1.09014\n",
      "[857]\ttraining's rmse: 0.939087\tvalid_1's rmse: 1.09013\n",
      "[858]\ttraining's rmse: 0.939033\tvalid_1's rmse: 1.0901\n",
      "[859]\ttraining's rmse: 0.938988\tvalid_1's rmse: 1.09007\n",
      "[860]\ttraining's rmse: 0.938922\tvalid_1's rmse: 1.09008\n",
      "[861]\ttraining's rmse: 0.938852\tvalid_1's rmse: 1.09009\n",
      "[862]\ttraining's rmse: 0.938784\tvalid_1's rmse: 1.09005\n",
      "[863]\ttraining's rmse: 0.938724\tvalid_1's rmse: 1.09003\n",
      "[864]\ttraining's rmse: 0.938648\tvalid_1's rmse: 1.09006\n",
      "[865]\ttraining's rmse: 0.938587\tvalid_1's rmse: 1.09005\n",
      "[866]\ttraining's rmse: 0.938549\tvalid_1's rmse: 1.09005\n",
      "[867]\ttraining's rmse: 0.938487\tvalid_1's rmse: 1.09\n",
      "[868]\ttraining's rmse: 0.93841\tvalid_1's rmse: 1.09002\n",
      "[869]\ttraining's rmse: 0.93835\tvalid_1's rmse: 1.09003\n",
      "[870]\ttraining's rmse: 0.938297\tvalid_1's rmse: 1.09002\n",
      "[871]\ttraining's rmse: 0.938258\tvalid_1's rmse: 1.08997\n",
      "[872]\ttraining's rmse: 0.938163\tvalid_1's rmse: 1.08994\n",
      "[873]\ttraining's rmse: 0.938102\tvalid_1's rmse: 1.08999\n",
      "[874]\ttraining's rmse: 0.938038\tvalid_1's rmse: 1.08998\n",
      "[875]\ttraining's rmse: 0.937972\tvalid_1's rmse: 1.08995\n",
      "[876]\ttraining's rmse: 0.937893\tvalid_1's rmse: 1.08986\n",
      "[877]\ttraining's rmse: 0.937866\tvalid_1's rmse: 1.08986\n",
      "[878]\ttraining's rmse: 0.937769\tvalid_1's rmse: 1.0898\n",
      "[879]\ttraining's rmse: 0.937709\tvalid_1's rmse: 1.08984\n",
      "[880]\ttraining's rmse: 0.937666\tvalid_1's rmse: 1.08981\n",
      "[881]\ttraining's rmse: 0.937622\tvalid_1's rmse: 1.08981\n",
      "[882]\ttraining's rmse: 0.937534\tvalid_1's rmse: 1.08986\n",
      "[883]\ttraining's rmse: 0.937475\tvalid_1's rmse: 1.08987\n",
      "[884]\ttraining's rmse: 0.937389\tvalid_1's rmse: 1.08982\n",
      "[885]\ttraining's rmse: 0.93733\tvalid_1's rmse: 1.0898\n",
      "[886]\ttraining's rmse: 0.937279\tvalid_1's rmse: 1.08977\n",
      "[887]\ttraining's rmse: 0.93719\tvalid_1's rmse: 1.08983\n",
      "[888]\ttraining's rmse: 0.937122\tvalid_1's rmse: 1.08986\n",
      "[889]\ttraining's rmse: 0.937086\tvalid_1's rmse: 1.08985\n",
      "[890]\ttraining's rmse: 0.936994\tvalid_1's rmse: 1.08978\n",
      "[891]\ttraining's rmse: 0.936942\tvalid_1's rmse: 1.08976\n",
      "[892]\ttraining's rmse: 0.936876\tvalid_1's rmse: 1.0897\n",
      "[893]\ttraining's rmse: 0.936835\tvalid_1's rmse: 1.08969\n",
      "[894]\ttraining's rmse: 0.936739\tvalid_1's rmse: 1.08965\n",
      "[895]\ttraining's rmse: 0.936681\tvalid_1's rmse: 1.08965\n",
      "[896]\ttraining's rmse: 0.9366\tvalid_1's rmse: 1.08956\n",
      "[897]\ttraining's rmse: 0.936538\tvalid_1's rmse: 1.08959\n",
      "[898]\ttraining's rmse: 0.936488\tvalid_1's rmse: 1.08956\n",
      "[899]\ttraining's rmse: 0.936417\tvalid_1's rmse: 1.0895\n",
      "[900]\ttraining's rmse: 0.93636\tvalid_1's rmse: 1.0895\n",
      "[901]\ttraining's rmse: 0.936323\tvalid_1's rmse: 1.08948\n",
      "[902]\ttraining's rmse: 0.93626\tvalid_1's rmse: 1.08949\n",
      "[903]\ttraining's rmse: 0.936179\tvalid_1's rmse: 1.0895\n",
      "[904]\ttraining's rmse: 0.936102\tvalid_1's rmse: 1.08941\n",
      "[905]\ttraining's rmse: 0.936051\tvalid_1's rmse: 1.0894\n",
      "[906]\ttraining's rmse: 0.935994\tvalid_1's rmse: 1.0894\n",
      "[907]\ttraining's rmse: 0.935904\tvalid_1's rmse: 1.08942\n",
      "[908]\ttraining's rmse: 0.935849\tvalid_1's rmse: 1.08943\n",
      "[909]\ttraining's rmse: 0.935809\tvalid_1's rmse: 1.08941\n",
      "[910]\ttraining's rmse: 0.935735\tvalid_1's rmse: 1.08937\n",
      "[911]\ttraining's rmse: 0.935692\tvalid_1's rmse: 1.0894\n",
      "[912]\ttraining's rmse: 0.935601\tvalid_1's rmse: 1.08941\n",
      "[913]\ttraining's rmse: 0.935519\tvalid_1's rmse: 1.08942\n",
      "[914]\ttraining's rmse: 0.935489\tvalid_1's rmse: 1.08944\n",
      "[915]\ttraining's rmse: 0.935429\tvalid_1's rmse: 1.0894\n",
      "[916]\ttraining's rmse: 0.935371\tvalid_1's rmse: 1.0894\n",
      "[917]\ttraining's rmse: 0.935306\tvalid_1's rmse: 1.08938\n",
      "[918]\ttraining's rmse: 0.935239\tvalid_1's rmse: 1.08934\n",
      "[919]\ttraining's rmse: 0.935167\tvalid_1's rmse: 1.08932\n",
      "[920]\ttraining's rmse: 0.935127\tvalid_1's rmse: 1.0893\n",
      "[921]\ttraining's rmse: 0.935044\tvalid_1's rmse: 1.08929\n",
      "[922]\ttraining's rmse: 0.934936\tvalid_1's rmse: 1.08916\n",
      "[923]\ttraining's rmse: 0.934912\tvalid_1's rmse: 1.08917\n",
      "[924]\ttraining's rmse: 0.934856\tvalid_1's rmse: 1.08919\n",
      "[925]\ttraining's rmse: 0.934782\tvalid_1's rmse: 1.08917\n",
      "[926]\ttraining's rmse: 0.934717\tvalid_1's rmse: 1.08921\n",
      "[927]\ttraining's rmse: 0.934644\tvalid_1's rmse: 1.08919\n",
      "[928]\ttraining's rmse: 0.934608\tvalid_1's rmse: 1.08918\n",
      "[929]\ttraining's rmse: 0.934541\tvalid_1's rmse: 1.08914\n",
      "[930]\ttraining's rmse: 0.934479\tvalid_1's rmse: 1.08912\n",
      "[931]\ttraining's rmse: 0.93438\tvalid_1's rmse: 1.08908\n",
      "[932]\ttraining's rmse: 0.934298\tvalid_1's rmse: 1.08905\n",
      "[933]\ttraining's rmse: 0.934232\tvalid_1's rmse: 1.08903\n",
      "[934]\ttraining's rmse: 0.934171\tvalid_1's rmse: 1.08905\n",
      "[935]\ttraining's rmse: 0.934114\tvalid_1's rmse: 1.08901\n",
      "[936]\ttraining's rmse: 0.934045\tvalid_1's rmse: 1.08902\n",
      "[937]\ttraining's rmse: 0.933969\tvalid_1's rmse: 1.08904\n",
      "[938]\ttraining's rmse: 0.933885\tvalid_1's rmse: 1.08905\n",
      "[939]\ttraining's rmse: 0.933846\tvalid_1's rmse: 1.08903\n",
      "[940]\ttraining's rmse: 0.933777\tvalid_1's rmse: 1.08902\n",
      "[941]\ttraining's rmse: 0.933753\tvalid_1's rmse: 1.08901\n",
      "[942]\ttraining's rmse: 0.933674\tvalid_1's rmse: 1.089\n",
      "[943]\ttraining's rmse: 0.933613\tvalid_1's rmse: 1.08898\n",
      "[944]\ttraining's rmse: 0.933564\tvalid_1's rmse: 1.08894\n",
      "[945]\ttraining's rmse: 0.933508\tvalid_1's rmse: 1.08895\n",
      "[946]\ttraining's rmse: 0.933461\tvalid_1's rmse: 1.08894\n",
      "[947]\ttraining's rmse: 0.933441\tvalid_1's rmse: 1.08895\n",
      "[948]\ttraining's rmse: 0.933367\tvalid_1's rmse: 1.08896\n",
      "[949]\ttraining's rmse: 0.933294\tvalid_1's rmse: 1.08894\n",
      "[950]\ttraining's rmse: 0.933186\tvalid_1's rmse: 1.08884\n",
      "[951]\ttraining's rmse: 0.933092\tvalid_1's rmse: 1.08881\n",
      "[952]\ttraining's rmse: 0.933011\tvalid_1's rmse: 1.08872\n",
      "[953]\ttraining's rmse: 0.932946\tvalid_1's rmse: 1.08872\n",
      "[954]\ttraining's rmse: 0.932864\tvalid_1's rmse: 1.08873\n",
      "[955]\ttraining's rmse: 0.932801\tvalid_1's rmse: 1.08873\n",
      "[956]\ttraining's rmse: 0.93273\tvalid_1's rmse: 1.08869\n",
      "[957]\ttraining's rmse: 0.932692\tvalid_1's rmse: 1.08867\n",
      "[958]\ttraining's rmse: 0.932613\tvalid_1's rmse: 1.08861\n",
      "[959]\ttraining's rmse: 0.932548\tvalid_1's rmse: 1.08861\n",
      "[960]\ttraining's rmse: 0.932494\tvalid_1's rmse: 1.08858\n",
      "[961]\ttraining's rmse: 0.93243\tvalid_1's rmse: 1.08853\n",
      "[962]\ttraining's rmse: 0.93234\tvalid_1's rmse: 1.08852\n",
      "[963]\ttraining's rmse: 0.932309\tvalid_1's rmse: 1.0885\n",
      "[964]\ttraining's rmse: 0.932251\tvalid_1's rmse: 1.08852\n",
      "[965]\ttraining's rmse: 0.932187\tvalid_1's rmse: 1.08852\n",
      "[966]\ttraining's rmse: 0.932077\tvalid_1's rmse: 1.08845\n",
      "[967]\ttraining's rmse: 0.932024\tvalid_1's rmse: 1.08844\n",
      "[968]\ttraining's rmse: 0.931967\tvalid_1's rmse: 1.08843\n",
      "[969]\ttraining's rmse: 0.931905\tvalid_1's rmse: 1.08846\n",
      "[970]\ttraining's rmse: 0.931848\tvalid_1's rmse: 1.08845\n",
      "[971]\ttraining's rmse: 0.931819\tvalid_1's rmse: 1.08845\n",
      "[972]\ttraining's rmse: 0.931739\tvalid_1's rmse: 1.0885\n",
      "[973]\ttraining's rmse: 0.931702\tvalid_1's rmse: 1.08849\n",
      "[974]\ttraining's rmse: 0.931622\tvalid_1's rmse: 1.08845\n",
      "[975]\ttraining's rmse: 0.931545\tvalid_1's rmse: 1.08846\n",
      "[976]\ttraining's rmse: 0.931465\tvalid_1's rmse: 1.08847\n",
      "[977]\ttraining's rmse: 0.931385\tvalid_1's rmse: 1.08838\n",
      "[978]\ttraining's rmse: 0.931313\tvalid_1's rmse: 1.08838\n",
      "[979]\ttraining's rmse: 0.931276\tvalid_1's rmse: 1.08836\n",
      "[980]\ttraining's rmse: 0.931205\tvalid_1's rmse: 1.08835\n",
      "[981]\ttraining's rmse: 0.931166\tvalid_1's rmse: 1.08832\n",
      "[982]\ttraining's rmse: 0.931104\tvalid_1's rmse: 1.08833\n",
      "[983]\ttraining's rmse: 0.931054\tvalid_1's rmse: 1.08831\n",
      "[984]\ttraining's rmse: 0.930968\tvalid_1's rmse: 1.08833\n",
      "[985]\ttraining's rmse: 0.930906\tvalid_1's rmse: 1.08829\n",
      "[986]\ttraining's rmse: 0.930819\tvalid_1's rmse: 1.0883\n",
      "[987]\ttraining's rmse: 0.930772\tvalid_1's rmse: 1.0883\n",
      "[988]\ttraining's rmse: 0.930726\tvalid_1's rmse: 1.08829\n",
      "[989]\ttraining's rmse: 0.930658\tvalid_1's rmse: 1.08824\n",
      "[990]\ttraining's rmse: 0.930616\tvalid_1's rmse: 1.08828\n",
      "[991]\ttraining's rmse: 0.930557\tvalid_1's rmse: 1.08831\n",
      "[992]\ttraining's rmse: 0.930519\tvalid_1's rmse: 1.08827\n",
      "[993]\ttraining's rmse: 0.930475\tvalid_1's rmse: 1.08826\n",
      "[994]\ttraining's rmse: 0.930422\tvalid_1's rmse: 1.08823\n",
      "[995]\ttraining's rmse: 0.930385\tvalid_1's rmse: 1.08822\n",
      "[996]\ttraining's rmse: 0.930302\tvalid_1's rmse: 1.08819\n",
      "[997]\ttraining's rmse: 0.930247\tvalid_1's rmse: 1.08819\n",
      "[998]\ttraining's rmse: 0.930188\tvalid_1's rmse: 1.08821\n",
      "[999]\ttraining's rmse: 0.930104\tvalid_1's rmse: 1.08823\n",
      "[1000]\ttraining's rmse: 0.930058\tvalid_1's rmse: 1.08823\n",
      "[1001]\ttraining's rmse: 0.929987\tvalid_1's rmse: 1.08819\n",
      "[1002]\ttraining's rmse: 0.929926\tvalid_1's rmse: 1.08817\n",
      "[1003]\ttraining's rmse: 0.929897\tvalid_1's rmse: 1.08816\n",
      "[1004]\ttraining's rmse: 0.92986\tvalid_1's rmse: 1.08817\n",
      "[1005]\ttraining's rmse: 0.929797\tvalid_1's rmse: 1.08815\n",
      "[1006]\ttraining's rmse: 0.929715\tvalid_1's rmse: 1.08814\n",
      "[1007]\ttraining's rmse: 0.929654\tvalid_1's rmse: 1.08814\n",
      "[1008]\ttraining's rmse: 0.929618\tvalid_1's rmse: 1.08813\n",
      "[1009]\ttraining's rmse: 0.929574\tvalid_1's rmse: 1.08812\n",
      "[1010]\ttraining's rmse: 0.929503\tvalid_1's rmse: 1.08807\n",
      "[1011]\ttraining's rmse: 0.929436\tvalid_1's rmse: 1.08804\n",
      "[1012]\ttraining's rmse: 0.929416\tvalid_1's rmse: 1.08805\n",
      "[1013]\ttraining's rmse: 0.929351\tvalid_1's rmse: 1.08805\n",
      "[1014]\ttraining's rmse: 0.929253\tvalid_1's rmse: 1.08797\n",
      "[1015]\ttraining's rmse: 0.929225\tvalid_1's rmse: 1.08795\n",
      "[1016]\ttraining's rmse: 0.929163\tvalid_1's rmse: 1.08796\n",
      "[1017]\ttraining's rmse: 0.929112\tvalid_1's rmse: 1.08795\n",
      "[1018]\ttraining's rmse: 0.929029\tvalid_1's rmse: 1.08794\n",
      "[1019]\ttraining's rmse: 0.92897\tvalid_1's rmse: 1.08792\n",
      "[1020]\ttraining's rmse: 0.928934\tvalid_1's rmse: 1.08791\n",
      "[1021]\ttraining's rmse: 0.928882\tvalid_1's rmse: 1.08787\n",
      "[1022]\ttraining's rmse: 0.928854\tvalid_1's rmse: 1.08786\n",
      "[1023]\ttraining's rmse: 0.928787\tvalid_1's rmse: 1.08787\n",
      "[1024]\ttraining's rmse: 0.928727\tvalid_1's rmse: 1.08784\n",
      "[1025]\ttraining's rmse: 0.928688\tvalid_1's rmse: 1.08783\n",
      "[1026]\ttraining's rmse: 0.928669\tvalid_1's rmse: 1.08784\n",
      "[1027]\ttraining's rmse: 0.928587\tvalid_1's rmse: 1.08786\n",
      "[1028]\ttraining's rmse: 0.928509\tvalid_1's rmse: 1.08785\n",
      "[1029]\ttraining's rmse: 0.928468\tvalid_1's rmse: 1.08787\n",
      "[1030]\ttraining's rmse: 0.928403\tvalid_1's rmse: 1.08786\n",
      "[1031]\ttraining's rmse: 0.92834\tvalid_1's rmse: 1.08782\n",
      "[1032]\ttraining's rmse: 0.92829\tvalid_1's rmse: 1.08777\n",
      "[1033]\ttraining's rmse: 0.928253\tvalid_1's rmse: 1.08776\n",
      "[1034]\ttraining's rmse: 0.928173\tvalid_1's rmse: 1.08776\n",
      "[1035]\ttraining's rmse: 0.928119\tvalid_1's rmse: 1.08776\n",
      "[1036]\ttraining's rmse: 0.928066\tvalid_1's rmse: 1.08776\n",
      "[1037]\ttraining's rmse: 0.928021\tvalid_1's rmse: 1.08774\n",
      "[1038]\ttraining's rmse: 0.927965\tvalid_1's rmse: 1.08769\n",
      "[1039]\ttraining's rmse: 0.92792\tvalid_1's rmse: 1.0877\n",
      "[1040]\ttraining's rmse: 0.927895\tvalid_1's rmse: 1.08767\n",
      "[1041]\ttraining's rmse: 0.927842\tvalid_1's rmse: 1.08767\n",
      "[1042]\ttraining's rmse: 0.927762\tvalid_1's rmse: 1.08769\n",
      "[1043]\ttraining's rmse: 0.927683\tvalid_1's rmse: 1.08768\n",
      "[1044]\ttraining's rmse: 0.927644\tvalid_1's rmse: 1.0877\n",
      "[1045]\ttraining's rmse: 0.927588\tvalid_1's rmse: 1.08767\n",
      "[1046]\ttraining's rmse: 0.927559\tvalid_1's rmse: 1.08766\n",
      "[1047]\ttraining's rmse: 0.927515\tvalid_1's rmse: 1.08767\n",
      "[1048]\ttraining's rmse: 0.927455\tvalid_1's rmse: 1.08765\n",
      "[1049]\ttraining's rmse: 0.927395\tvalid_1's rmse: 1.08765\n",
      "[1050]\ttraining's rmse: 0.927349\tvalid_1's rmse: 1.08765\n",
      "[1051]\ttraining's rmse: 0.927285\tvalid_1's rmse: 1.08761\n",
      "[1052]\ttraining's rmse: 0.92726\tvalid_1's rmse: 1.08757\n",
      "[1053]\ttraining's rmse: 0.927206\tvalid_1's rmse: 1.0875\n",
      "[1054]\ttraining's rmse: 0.927143\tvalid_1's rmse: 1.08753\n",
      "[1055]\ttraining's rmse: 0.927107\tvalid_1's rmse: 1.08752\n",
      "[1056]\ttraining's rmse: 0.927045\tvalid_1's rmse: 1.08748\n",
      "[1057]\ttraining's rmse: 0.926996\tvalid_1's rmse: 1.08744\n",
      "[1058]\ttraining's rmse: 0.926933\tvalid_1's rmse: 1.08743\n",
      "[1059]\ttraining's rmse: 0.926877\tvalid_1's rmse: 1.08747\n",
      "[1060]\ttraining's rmse: 0.926859\tvalid_1's rmse: 1.08748\n",
      "[1061]\ttraining's rmse: 0.926819\tvalid_1's rmse: 1.0875\n",
      "[1062]\ttraining's rmse: 0.926757\tvalid_1's rmse: 1.08747\n",
      "[1063]\ttraining's rmse: 0.926684\tvalid_1's rmse: 1.08749\n",
      "[1064]\ttraining's rmse: 0.926626\tvalid_1's rmse: 1.08747\n",
      "[1065]\ttraining's rmse: 0.926561\tvalid_1's rmse: 1.0875\n",
      "[1066]\ttraining's rmse: 0.926537\tvalid_1's rmse: 1.08748\n",
      "[1067]\ttraining's rmse: 0.926466\tvalid_1's rmse: 1.08749\n",
      "[1068]\ttraining's rmse: 0.926417\tvalid_1's rmse: 1.08749\n",
      "Early stopping, best iteration is:\n",
      "[1058]\ttraining's rmse: 0.926933\tvalid_1's rmse: 1.08743\n",
      "fold_3 coefficients:  [0.57941244 1.53277606 2.1576789 ]\n",
      "[1]\ttraining's rmse: 1.25327\tvalid_1's rmse: 1.25533\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.2498\tvalid_1's rmse: 1.25247\n",
      "[3]\ttraining's rmse: 1.24639\tvalid_1's rmse: 1.24971\n",
      "[4]\ttraining's rmse: 1.24304\tvalid_1's rmse: 1.24695\n",
      "[5]\ttraining's rmse: 1.23974\tvalid_1's rmse: 1.24429\n",
      "[6]\ttraining's rmse: 1.23651\tvalid_1's rmse: 1.24163\n",
      "[7]\ttraining's rmse: 1.23333\tvalid_1's rmse: 1.23907\n",
      "[8]\ttraining's rmse: 1.23022\tvalid_1's rmse: 1.23663\n",
      "[9]\ttraining's rmse: 1.22706\tvalid_1's rmse: 1.23419\n",
      "[10]\ttraining's rmse: 1.22407\tvalid_1's rmse: 1.23188\n",
      "[11]\ttraining's rmse: 1.22101\tvalid_1's rmse: 1.22953\n",
      "[12]\ttraining's rmse: 1.21801\tvalid_1's rmse: 1.22732\n",
      "[13]\ttraining's rmse: 1.21516\tvalid_1's rmse: 1.22513\n",
      "[14]\ttraining's rmse: 1.21226\tvalid_1's rmse: 1.22299\n",
      "[15]\ttraining's rmse: 1.20941\tvalid_1's rmse: 1.22079\n",
      "[16]\ttraining's rmse: 1.2067\tvalid_1's rmse: 1.21886\n",
      "[17]\ttraining's rmse: 1.20394\tvalid_1's rmse: 1.21684\n",
      "[18]\ttraining's rmse: 1.20132\tvalid_1's rmse: 1.21501\n",
      "[19]\ttraining's rmse: 1.19865\tvalid_1's rmse: 1.21303\n",
      "[20]\ttraining's rmse: 1.19612\tvalid_1's rmse: 1.21109\n",
      "[21]\ttraining's rmse: 1.19354\tvalid_1's rmse: 1.2091\n",
      "[22]\ttraining's rmse: 1.19126\tvalid_1's rmse: 1.20747\n",
      "[23]\ttraining's rmse: 1.18901\tvalid_1's rmse: 1.20586\n",
      "[24]\ttraining's rmse: 1.18655\tvalid_1's rmse: 1.20402\n",
      "[25]\ttraining's rmse: 1.18439\tvalid_1's rmse: 1.20247\n",
      "[26]\ttraining's rmse: 1.18199\tvalid_1's rmse: 1.20052\n",
      "[27]\ttraining's rmse: 1.17989\tvalid_1's rmse: 1.19903\n",
      "[28]\ttraining's rmse: 1.17759\tvalid_1's rmse: 1.19719\n",
      "[29]\ttraining's rmse: 1.17555\tvalid_1's rmse: 1.19575\n",
      "[30]\ttraining's rmse: 1.17331\tvalid_1's rmse: 1.1941\n",
      "[31]\ttraining's rmse: 1.17134\tvalid_1's rmse: 1.19271\n",
      "[32]\ttraining's rmse: 1.16916\tvalid_1's rmse: 1.19094\n",
      "[33]\ttraining's rmse: 1.16725\tvalid_1's rmse: 1.18959\n",
      "[34]\ttraining's rmse: 1.1652\tvalid_1's rmse: 1.18803\n",
      "[35]\ttraining's rmse: 1.16337\tvalid_1's rmse: 1.18672\n",
      "[36]\ttraining's rmse: 1.16132\tvalid_1's rmse: 1.18508\n",
      "[37]\ttraining's rmse: 1.15955\tvalid_1's rmse: 1.18382\n",
      "[38]\ttraining's rmse: 1.15758\tvalid_1's rmse: 1.18224\n",
      "[39]\ttraining's rmse: 1.15575\tvalid_1's rmse: 1.18059\n",
      "[40]\ttraining's rmse: 1.15406\tvalid_1's rmse: 1.1794\n",
      "[41]\ttraining's rmse: 1.15216\tvalid_1's rmse: 1.17787\n",
      "[42]\ttraining's rmse: 1.15048\tvalid_1's rmse: 1.17626\n",
      "[43]\ttraining's rmse: 1.14887\tvalid_1's rmse: 1.17492\n",
      "[44]\ttraining's rmse: 1.14728\tvalid_1's rmse: 1.17381\n",
      "[45]\ttraining's rmse: 1.14551\tvalid_1's rmse: 1.17244\n",
      "[46]\ttraining's rmse: 1.144\tvalid_1's rmse: 1.1712\n",
      "[47]\ttraining's rmse: 1.14232\tvalid_1's rmse: 1.16989\n",
      "[48]\ttraining's rmse: 1.14083\tvalid_1's rmse: 1.16866\n",
      "[49]\ttraining's rmse: 1.13936\tvalid_1's rmse: 1.16763\n",
      "[50]\ttraining's rmse: 1.13769\tvalid_1's rmse: 1.16629\n",
      "[51]\ttraining's rmse: 1.13626\tvalid_1's rmse: 1.16511\n",
      "[52]\ttraining's rmse: 1.13481\tvalid_1's rmse: 1.16373\n",
      "[53]\ttraining's rmse: 1.13345\tvalid_1's rmse: 1.1628\n",
      "[54]\ttraining's rmse: 1.13208\tvalid_1's rmse: 1.16184\n",
      "[55]\ttraining's rmse: 1.13074\tvalid_1's rmse: 1.16074\n",
      "[56]\ttraining's rmse: 1.12921\tvalid_1's rmse: 1.15957\n",
      "[57]\ttraining's rmse: 1.12794\tvalid_1's rmse: 1.15869\n",
      "[58]\ttraining's rmse: 1.12659\tvalid_1's rmse: 1.15783\n",
      "[59]\ttraining's rmse: 1.12512\tvalid_1's rmse: 1.15668\n",
      "[60]\ttraining's rmse: 1.12387\tvalid_1's rmse: 1.15564\n",
      "[61]\ttraining's rmse: 1.12263\tvalid_1's rmse: 1.15485\n",
      "[62]\ttraining's rmse: 1.12122\tvalid_1's rmse: 1.15376\n",
      "[63]\ttraining's rmse: 1.12001\tvalid_1's rmse: 1.15284\n",
      "[64]\ttraining's rmse: 1.11886\tvalid_1's rmse: 1.15203\n",
      "[65]\ttraining's rmse: 1.11751\tvalid_1's rmse: 1.15102\n",
      "[66]\ttraining's rmse: 1.1163\tvalid_1's rmse: 1.15015\n",
      "[67]\ttraining's rmse: 1.11517\tvalid_1's rmse: 1.14941\n",
      "[68]\ttraining's rmse: 1.11407\tvalid_1's rmse: 1.14873\n",
      "[69]\ttraining's rmse: 1.11292\tvalid_1's rmse: 1.14788\n",
      "[70]\ttraining's rmse: 1.11184\tvalid_1's rmse: 1.14714\n",
      "[71]\ttraining's rmse: 1.11077\tvalid_1's rmse: 1.1463\n",
      "[72]\ttraining's rmse: 1.10953\tvalid_1's rmse: 1.14535\n",
      "[73]\ttraining's rmse: 1.10849\tvalid_1's rmse: 1.14455\n",
      "[74]\ttraining's rmse: 1.10743\tvalid_1's rmse: 1.1439\n",
      "[75]\ttraining's rmse: 1.10614\tvalid_1's rmse: 1.14296\n",
      "[76]\ttraining's rmse: 1.1049\tvalid_1's rmse: 1.14198\n",
      "[77]\ttraining's rmse: 1.10392\tvalid_1's rmse: 1.14123\n",
      "[78]\ttraining's rmse: 1.10268\tvalid_1's rmse: 1.14035\n",
      "[79]\ttraining's rmse: 1.10178\tvalid_1's rmse: 1.1397\n",
      "[80]\ttraining's rmse: 1.10061\tvalid_1's rmse: 1.13878\n",
      "[81]\ttraining's rmse: 1.09971\tvalid_1's rmse: 1.1382\n",
      "[82]\ttraining's rmse: 1.09853\tvalid_1's rmse: 1.13741\n",
      "[83]\ttraining's rmse: 1.09727\tvalid_1's rmse: 1.13617\n",
      "[84]\ttraining's rmse: 1.09634\tvalid_1's rmse: 1.13547\n",
      "[85]\ttraining's rmse: 1.09524\tvalid_1's rmse: 1.13456\n",
      "[86]\ttraining's rmse: 1.09413\tvalid_1's rmse: 1.13378\n",
      "[87]\ttraining's rmse: 1.09328\tvalid_1's rmse: 1.13313\n",
      "[88]\ttraining's rmse: 1.09224\tvalid_1's rmse: 1.13227\n",
      "[89]\ttraining's rmse: 1.09117\tvalid_1's rmse: 1.13156\n",
      "[90]\ttraining's rmse: 1.09033\tvalid_1's rmse: 1.13085\n",
      "[91]\ttraining's rmse: 1.0895\tvalid_1's rmse: 1.13029\n",
      "[92]\ttraining's rmse: 1.08851\tvalid_1's rmse: 1.12948\n",
      "[93]\ttraining's rmse: 1.0877\tvalid_1's rmse: 1.12885\n",
      "[94]\ttraining's rmse: 1.08692\tvalid_1's rmse: 1.1283\n",
      "[95]\ttraining's rmse: 1.08595\tvalid_1's rmse: 1.12754\n",
      "[96]\ttraining's rmse: 1.08517\tvalid_1's rmse: 1.12702\n",
      "[97]\ttraining's rmse: 1.0842\tvalid_1's rmse: 1.12634\n",
      "[98]\ttraining's rmse: 1.08343\tvalid_1's rmse: 1.1257\n",
      "[99]\ttraining's rmse: 1.08265\tvalid_1's rmse: 1.12516\n",
      "[100]\ttraining's rmse: 1.08175\tvalid_1's rmse: 1.12446\n",
      "[101]\ttraining's rmse: 1.08104\tvalid_1's rmse: 1.12395\n",
      "[102]\ttraining's rmse: 1.08013\tvalid_1's rmse: 1.12324\n",
      "[103]\ttraining's rmse: 1.07926\tvalid_1's rmse: 1.12258\n",
      "[104]\ttraining's rmse: 1.07848\tvalid_1's rmse: 1.12215\n",
      "[105]\ttraining's rmse: 1.07775\tvalid_1's rmse: 1.12166\n",
      "[106]\ttraining's rmse: 1.07708\tvalid_1's rmse: 1.12124\n",
      "[107]\ttraining's rmse: 1.07625\tvalid_1's rmse: 1.12058\n",
      "[108]\ttraining's rmse: 1.07557\tvalid_1's rmse: 1.12008\n",
      "[109]\ttraining's rmse: 1.07494\tvalid_1's rmse: 1.11968\n",
      "[110]\ttraining's rmse: 1.07409\tvalid_1's rmse: 1.1191\n",
      "[111]\ttraining's rmse: 1.07327\tvalid_1's rmse: 1.11845\n",
      "[112]\ttraining's rmse: 1.07248\tvalid_1's rmse: 1.11785\n",
      "[113]\ttraining's rmse: 1.07187\tvalid_1's rmse: 1.11747\n",
      "[114]\ttraining's rmse: 1.07106\tvalid_1's rmse: 1.11695\n",
      "[115]\ttraining's rmse: 1.0703\tvalid_1's rmse: 1.11634\n",
      "[116]\ttraining's rmse: 1.06967\tvalid_1's rmse: 1.116\n",
      "[117]\ttraining's rmse: 1.06892\tvalid_1's rmse: 1.11543\n",
      "[118]\ttraining's rmse: 1.06819\tvalid_1's rmse: 1.11483\n",
      "[119]\ttraining's rmse: 1.06762\tvalid_1's rmse: 1.11444\n",
      "[120]\ttraining's rmse: 1.06687\tvalid_1's rmse: 1.11396\n",
      "[121]\ttraining's rmse: 1.0663\tvalid_1's rmse: 1.11352\n",
      "[122]\ttraining's rmse: 1.06561\tvalid_1's rmse: 1.11295\n",
      "[123]\ttraining's rmse: 1.06507\tvalid_1's rmse: 1.11268\n",
      "[124]\ttraining's rmse: 1.06452\tvalid_1's rmse: 1.11221\n",
      "[125]\ttraining's rmse: 1.06398\tvalid_1's rmse: 1.1119\n",
      "[126]\ttraining's rmse: 1.06332\tvalid_1's rmse: 1.11134\n",
      "[127]\ttraining's rmse: 1.06276\tvalid_1's rmse: 1.11107\n",
      "[128]\ttraining's rmse: 1.06225\tvalid_1's rmse: 1.11079\n",
      "[129]\ttraining's rmse: 1.0616\tvalid_1's rmse: 1.11027\n",
      "[130]\ttraining's rmse: 1.06111\tvalid_1's rmse: 1.10995\n",
      "[131]\ttraining's rmse: 1.06054\tvalid_1's rmse: 1.10947\n",
      "[132]\ttraining's rmse: 1.06001\tvalid_1's rmse: 1.10918\n",
      "[133]\ttraining's rmse: 1.05953\tvalid_1's rmse: 1.1089\n",
      "[134]\ttraining's rmse: 1.05905\tvalid_1's rmse: 1.10861\n",
      "[135]\ttraining's rmse: 1.0585\tvalid_1's rmse: 1.10825\n",
      "[136]\ttraining's rmse: 1.05764\tvalid_1's rmse: 1.10739\n",
      "[137]\ttraining's rmse: 1.05712\tvalid_1's rmse: 1.10691\n",
      "[138]\ttraining's rmse: 1.05627\tvalid_1's rmse: 1.10617\n",
      "[139]\ttraining's rmse: 1.05582\tvalid_1's rmse: 1.10587\n",
      "[140]\ttraining's rmse: 1.05522\tvalid_1's rmse: 1.10548\n",
      "[141]\ttraining's rmse: 1.05464\tvalid_1's rmse: 1.10498\n",
      "[142]\ttraining's rmse: 1.0542\tvalid_1's rmse: 1.10465\n",
      "[143]\ttraining's rmse: 1.0534\tvalid_1's rmse: 1.10387\n",
      "[144]\ttraining's rmse: 1.05291\tvalid_1's rmse: 1.10341\n",
      "[145]\ttraining's rmse: 1.05213\tvalid_1's rmse: 1.10264\n",
      "[146]\ttraining's rmse: 1.05168\tvalid_1's rmse: 1.10245\n",
      "[147]\ttraining's rmse: 1.05126\tvalid_1's rmse: 1.10227\n",
      "[148]\ttraining's rmse: 1.05049\tvalid_1's rmse: 1.1016\n",
      "[149]\ttraining's rmse: 1.05003\tvalid_1's rmse: 1.10118\n",
      "[150]\ttraining's rmse: 1.04929\tvalid_1's rmse: 1.10051\n",
      "[151]\ttraining's rmse: 1.04884\tvalid_1's rmse: 1.10008\n",
      "[152]\ttraining's rmse: 1.04844\tvalid_1's rmse: 1.09983\n",
      "[153]\ttraining's rmse: 1.04771\tvalid_1's rmse: 1.09924\n",
      "[154]\ttraining's rmse: 1.04728\tvalid_1's rmse: 1.09888\n",
      "[155]\ttraining's rmse: 1.04657\tvalid_1's rmse: 1.09829\n",
      "[156]\ttraining's rmse: 1.04621\tvalid_1's rmse: 1.09811\n",
      "[157]\ttraining's rmse: 1.04551\tvalid_1's rmse: 1.09745\n",
      "[158]\ttraining's rmse: 1.0451\tvalid_1's rmse: 1.097\n",
      "[159]\ttraining's rmse: 1.04442\tvalid_1's rmse: 1.09637\n",
      "[160]\ttraining's rmse: 1.04405\tvalid_1's rmse: 1.09625\n",
      "[161]\ttraining's rmse: 1.04338\tvalid_1's rmse: 1.09568\n",
      "[162]\ttraining's rmse: 1.04298\tvalid_1's rmse: 1.09532\n",
      "[163]\ttraining's rmse: 1.04232\tvalid_1's rmse: 1.09477\n",
      "[164]\ttraining's rmse: 1.04193\tvalid_1's rmse: 1.09444\n",
      "[165]\ttraining's rmse: 1.04129\tvalid_1's rmse: 1.09381\n",
      "[166]\ttraining's rmse: 1.04093\tvalid_1's rmse: 1.09374\n",
      "[167]\ttraining's rmse: 1.04055\tvalid_1's rmse: 1.09348\n",
      "[168]\ttraining's rmse: 1.03994\tvalid_1's rmse: 1.09291\n",
      "[169]\ttraining's rmse: 1.03957\tvalid_1's rmse: 1.09266\n",
      "[170]\ttraining's rmse: 1.03923\tvalid_1's rmse: 1.09252\n",
      "[171]\ttraining's rmse: 1.0389\tvalid_1's rmse: 1.09237\n",
      "[172]\ttraining's rmse: 1.0383\tvalid_1's rmse: 1.09184\n",
      "[173]\ttraining's rmse: 1.03793\tvalid_1's rmse: 1.09161\n",
      "[174]\ttraining's rmse: 1.0376\tvalid_1's rmse: 1.09152\n",
      "[175]\ttraining's rmse: 1.03702\tvalid_1's rmse: 1.09104\n",
      "[176]\ttraining's rmse: 1.03666\tvalid_1's rmse: 1.09065\n",
      "[177]\ttraining's rmse: 1.03608\tvalid_1's rmse: 1.09013\n",
      "[178]\ttraining's rmse: 1.03574\tvalid_1's rmse: 1.08987\n",
      "[179]\ttraining's rmse: 1.0354\tvalid_1's rmse: 1.08976\n",
      "[180]\ttraining's rmse: 1.03509\tvalid_1's rmse: 1.08963\n",
      "[181]\ttraining's rmse: 1.03453\tvalid_1's rmse: 1.08917\n",
      "[182]\ttraining's rmse: 1.0342\tvalid_1's rmse: 1.0889\n",
      "[183]\ttraining's rmse: 1.03361\tvalid_1's rmse: 1.08836\n",
      "[184]\ttraining's rmse: 1.03303\tvalid_1's rmse: 1.08783\n",
      "[185]\ttraining's rmse: 1.03246\tvalid_1's rmse: 1.08729\n",
      "[186]\ttraining's rmse: 1.0319\tvalid_1's rmse: 1.0868\n",
      "[187]\ttraining's rmse: 1.03137\tvalid_1's rmse: 1.08637\n",
      "[188]\ttraining's rmse: 1.03084\tvalid_1's rmse: 1.08585\n",
      "[189]\ttraining's rmse: 1.03029\tvalid_1's rmse: 1.08538\n",
      "[190]\ttraining's rmse: 1.02976\tvalid_1's rmse: 1.08484\n",
      "[191]\ttraining's rmse: 1.02926\tvalid_1's rmse: 1.08446\n",
      "[192]\ttraining's rmse: 1.02889\tvalid_1's rmse: 1.0842\n",
      "[193]\ttraining's rmse: 1.02857\tvalid_1's rmse: 1.08395\n",
      "[194]\ttraining's rmse: 1.02821\tvalid_1's rmse: 1.08371\n",
      "[195]\ttraining's rmse: 1.0279\tvalid_1's rmse: 1.08342\n",
      "[196]\ttraining's rmse: 1.02761\tvalid_1's rmse: 1.08322\n",
      "[197]\ttraining's rmse: 1.02731\tvalid_1's rmse: 1.08294\n",
      "[198]\ttraining's rmse: 1.02683\tvalid_1's rmse: 1.08252\n",
      "[199]\ttraining's rmse: 1.02655\tvalid_1's rmse: 1.08232\n",
      "[200]\ttraining's rmse: 1.02623\tvalid_1's rmse: 1.08202\n",
      "[201]\ttraining's rmse: 1.02574\tvalid_1's rmse: 1.08155\n",
      "[202]\ttraining's rmse: 1.02545\tvalid_1's rmse: 1.08137\n",
      "[203]\ttraining's rmse: 1.02515\tvalid_1's rmse: 1.08108\n",
      "[204]\ttraining's rmse: 1.02467\tvalid_1's rmse: 1.08065\n",
      "[205]\ttraining's rmse: 1.02438\tvalid_1's rmse: 1.08048\n",
      "[206]\ttraining's rmse: 1.0241\tvalid_1's rmse: 1.08031\n",
      "[207]\ttraining's rmse: 1.02365\tvalid_1's rmse: 1.07998\n",
      "[208]\ttraining's rmse: 1.02334\tvalid_1's rmse: 1.07968\n",
      "[209]\ttraining's rmse: 1.02305\tvalid_1's rmse: 1.07952\n",
      "[210]\ttraining's rmse: 1.02279\tvalid_1's rmse: 1.07935\n",
      "[211]\ttraining's rmse: 1.02251\tvalid_1's rmse: 1.0792\n",
      "[212]\ttraining's rmse: 1.0222\tvalid_1's rmse: 1.07891\n",
      "[213]\ttraining's rmse: 1.02194\tvalid_1's rmse: 1.07878\n",
      "[214]\ttraining's rmse: 1.02167\tvalid_1's rmse: 1.07863\n",
      "[215]\ttraining's rmse: 1.02124\tvalid_1's rmse: 1.0783\n",
      "[216]\ttraining's rmse: 1.02096\tvalid_1's rmse: 1.07801\n",
      "[217]\ttraining's rmse: 1.02071\tvalid_1's rmse: 1.07787\n",
      "[218]\ttraining's rmse: 1.02045\tvalid_1's rmse: 1.07776\n",
      "[219]\ttraining's rmse: 1.02004\tvalid_1's rmse: 1.07746\n",
      "[220]\ttraining's rmse: 1.01975\tvalid_1's rmse: 1.07719\n",
      "[221]\ttraining's rmse: 1.01949\tvalid_1's rmse: 1.07703\n",
      "[222]\ttraining's rmse: 1.01923\tvalid_1's rmse: 1.07691\n",
      "[223]\ttraining's rmse: 1.01899\tvalid_1's rmse: 1.07678\n",
      "[224]\ttraining's rmse: 1.01858\tvalid_1's rmse: 1.07641\n",
      "[225]\ttraining's rmse: 1.01829\tvalid_1's rmse: 1.07602\n",
      "[226]\ttraining's rmse: 1.01797\tvalid_1's rmse: 1.07573\n",
      "[227]\ttraining's rmse: 1.01773\tvalid_1's rmse: 1.07564\n",
      "[228]\ttraining's rmse: 1.01748\tvalid_1's rmse: 1.07551\n",
      "[229]\ttraining's rmse: 1.01709\tvalid_1's rmse: 1.07519\n",
      "[230]\ttraining's rmse: 1.01678\tvalid_1's rmse: 1.07492\n",
      "[231]\ttraining's rmse: 1.0164\tvalid_1's rmse: 1.0746\n",
      "[232]\ttraining's rmse: 1.01611\tvalid_1's rmse: 1.07427\n",
      "[233]\ttraining's rmse: 1.01587\tvalid_1's rmse: 1.07417\n",
      "[234]\ttraining's rmse: 1.01549\tvalid_1's rmse: 1.07386\n",
      "[235]\ttraining's rmse: 1.01525\tvalid_1's rmse: 1.0736\n",
      "[236]\ttraining's rmse: 1.01497\tvalid_1's rmse: 1.07326\n",
      "[237]\ttraining's rmse: 1.01475\tvalid_1's rmse: 1.07314\n",
      "[238]\ttraining's rmse: 1.01452\tvalid_1's rmse: 1.07307\n",
      "[239]\ttraining's rmse: 1.01414\tvalid_1's rmse: 1.07276\n",
      "[240]\ttraining's rmse: 1.01392\tvalid_1's rmse: 1.07258\n",
      "[241]\ttraining's rmse: 1.01356\tvalid_1's rmse: 1.07229\n",
      "[242]\ttraining's rmse: 1.01329\tvalid_1's rmse: 1.07198\n",
      "[243]\ttraining's rmse: 1.013\tvalid_1's rmse: 1.07177\n",
      "[244]\ttraining's rmse: 1.01265\tvalid_1's rmse: 1.07149\n",
      "[245]\ttraining's rmse: 1.01244\tvalid_1's rmse: 1.07136\n",
      "[246]\ttraining's rmse: 1.0122\tvalid_1's rmse: 1.07115\n",
      "[247]\ttraining's rmse: 1.01185\tvalid_1's rmse: 1.07086\n",
      "[248]\ttraining's rmse: 1.01162\tvalid_1's rmse: 1.07061\n",
      "[249]\ttraining's rmse: 1.01128\tvalid_1's rmse: 1.07033\n",
      "[250]\ttraining's rmse: 1.01107\tvalid_1's rmse: 1.07027\n",
      "[251]\ttraining's rmse: 1.01081\tvalid_1's rmse: 1.06992\n",
      "[252]\ttraining's rmse: 1.01048\tvalid_1's rmse: 1.06965\n",
      "[253]\ttraining's rmse: 1.01026\tvalid_1's rmse: 1.06947\n",
      "[254]\ttraining's rmse: 1.01004\tvalid_1's rmse: 1.06935\n",
      "[255]\ttraining's rmse: 1.0097\tvalid_1's rmse: 1.06905\n",
      "[256]\ttraining's rmse: 1.00949\tvalid_1's rmse: 1.06897\n",
      "[257]\ttraining's rmse: 1.00927\tvalid_1's rmse: 1.06875\n",
      "[258]\ttraining's rmse: 1.00894\tvalid_1's rmse: 1.06847\n",
      "[259]\ttraining's rmse: 1.00869\tvalid_1's rmse: 1.0682\n",
      "[260]\ttraining's rmse: 1.00849\tvalid_1's rmse: 1.06807\n",
      "[261]\ttraining's rmse: 1.00828\tvalid_1's rmse: 1.06789\n",
      "[262]\ttraining's rmse: 1.00796\tvalid_1's rmse: 1.06765\n",
      "[263]\ttraining's rmse: 1.00764\tvalid_1's rmse: 1.06738\n",
      "[264]\ttraining's rmse: 1.00738\tvalid_1's rmse: 1.06726\n",
      "[265]\ttraining's rmse: 1.00715\tvalid_1's rmse: 1.06704\n",
      "[266]\ttraining's rmse: 1.00689\tvalid_1's rmse: 1.06683\n",
      "[267]\ttraining's rmse: 1.00658\tvalid_1's rmse: 1.06661\n",
      "[268]\ttraining's rmse: 1.00633\tvalid_1's rmse: 1.06642\n",
      "[269]\ttraining's rmse: 1.00603\tvalid_1's rmse: 1.06618\n",
      "[270]\ttraining's rmse: 1.00579\tvalid_1's rmse: 1.06597\n",
      "[271]\ttraining's rmse: 1.00558\tvalid_1's rmse: 1.06578\n",
      "[272]\ttraining's rmse: 1.00528\tvalid_1's rmse: 1.06555\n",
      "[273]\ttraining's rmse: 1.00504\tvalid_1's rmse: 1.06543\n",
      "[274]\ttraining's rmse: 1.00486\tvalid_1's rmse: 1.06529\n",
      "[275]\ttraining's rmse: 1.00465\tvalid_1's rmse: 1.06514\n",
      "[276]\ttraining's rmse: 1.00442\tvalid_1's rmse: 1.06495\n",
      "[277]\ttraining's rmse: 1.00413\tvalid_1's rmse: 1.06473\n",
      "[278]\ttraining's rmse: 1.00389\tvalid_1's rmse: 1.06456\n",
      "[279]\ttraining's rmse: 1.00361\tvalid_1's rmse: 1.06435\n",
      "[280]\ttraining's rmse: 1.00339\tvalid_1's rmse: 1.06427\n",
      "[281]\ttraining's rmse: 1.00311\tvalid_1's rmse: 1.06404\n",
      "[282]\ttraining's rmse: 1.00289\tvalid_1's rmse: 1.06392\n",
      "[283]\ttraining's rmse: 1.00262\tvalid_1's rmse: 1.0637\n",
      "[284]\ttraining's rmse: 1.0024\tvalid_1's rmse: 1.0635\n",
      "[285]\ttraining's rmse: 1.00218\tvalid_1's rmse: 1.06328\n",
      "[286]\ttraining's rmse: 1.00196\tvalid_1's rmse: 1.06313\n",
      "[287]\ttraining's rmse: 1.00169\tvalid_1's rmse: 1.06291\n",
      "[288]\ttraining's rmse: 1.00143\tvalid_1's rmse: 1.06271\n",
      "[289]\ttraining's rmse: 1.00123\tvalid_1's rmse: 1.06255\n",
      "[290]\ttraining's rmse: 1.00103\tvalid_1's rmse: 1.06244\n",
      "[291]\ttraining's rmse: 1.00081\tvalid_1's rmse: 1.06229\n",
      "[292]\ttraining's rmse: 1.00054\tvalid_1's rmse: 1.06208\n",
      "[293]\ttraining's rmse: 1.00027\tvalid_1's rmse: 1.06189\n",
      "[294]\ttraining's rmse: 1.00006\tvalid_1's rmse: 1.06174\n",
      "[295]\ttraining's rmse: 0.999808\tvalid_1's rmse: 1.06158\n",
      "[296]\ttraining's rmse: 0.999601\tvalid_1's rmse: 1.06144\n",
      "[297]\ttraining's rmse: 0.999432\tvalid_1's rmse: 1.06135\n",
      "[298]\ttraining's rmse: 0.999233\tvalid_1's rmse: 1.06121\n",
      "[299]\ttraining's rmse: 0.999031\tvalid_1's rmse: 1.06118\n",
      "[300]\ttraining's rmse: 0.998839\tvalid_1's rmse: 1.06105\n",
      "[301]\ttraining's rmse: 0.99859\tvalid_1's rmse: 1.06086\n",
      "[302]\ttraining's rmse: 0.998337\tvalid_1's rmse: 1.06068\n",
      "[303]\ttraining's rmse: 0.998138\tvalid_1's rmse: 1.0606\n",
      "[304]\ttraining's rmse: 0.997941\tvalid_1's rmse: 1.06044\n",
      "[305]\ttraining's rmse: 0.997701\tvalid_1's rmse: 1.06029\n",
      "[306]\ttraining's rmse: 0.997507\tvalid_1's rmse: 1.06011\n",
      "[307]\ttraining's rmse: 0.997319\tvalid_1's rmse: 1.05995\n",
      "[308]\ttraining's rmse: 0.99716\tvalid_1's rmse: 1.05985\n",
      "[309]\ttraining's rmse: 0.99695\tvalid_1's rmse: 1.05971\n",
      "[310]\ttraining's rmse: 0.996712\tvalid_1's rmse: 1.05952\n",
      "[311]\ttraining's rmse: 0.996523\tvalid_1's rmse: 1.05939\n",
      "[312]\ttraining's rmse: 0.996292\tvalid_1's rmse: 1.05926\n",
      "[313]\ttraining's rmse: 0.996117\tvalid_1's rmse: 1.05911\n",
      "[314]\ttraining's rmse: 0.995936\tvalid_1's rmse: 1.05899\n",
      "[315]\ttraining's rmse: 0.995702\tvalid_1's rmse: 1.05882\n",
      "[316]\ttraining's rmse: 0.995539\tvalid_1's rmse: 1.05868\n",
      "[317]\ttraining's rmse: 0.995359\tvalid_1's rmse: 1.05853\n",
      "[318]\ttraining's rmse: 0.995169\tvalid_1's rmse: 1.05848\n",
      "[319]\ttraining's rmse: 0.995019\tvalid_1's rmse: 1.05839\n",
      "[320]\ttraining's rmse: 0.994842\tvalid_1's rmse: 1.05827\n",
      "[321]\ttraining's rmse: 0.994616\tvalid_1's rmse: 1.0581\n",
      "[322]\ttraining's rmse: 0.994399\tvalid_1's rmse: 1.05798\n",
      "[323]\ttraining's rmse: 0.994218\tvalid_1's rmse: 1.05781\n",
      "[324]\ttraining's rmse: 0.994037\tvalid_1's rmse: 1.05776\n",
      "[325]\ttraining's rmse: 0.993824\tvalid_1's rmse: 1.05765\n",
      "[326]\ttraining's rmse: 0.993653\tvalid_1's rmse: 1.05752\n",
      "[327]\ttraining's rmse: 0.993476\tvalid_1's rmse: 1.0575\n",
      "[328]\ttraining's rmse: 0.99331\tvalid_1's rmse: 1.05737\n",
      "[329]\ttraining's rmse: 0.993146\tvalid_1's rmse: 1.05725\n",
      "[330]\ttraining's rmse: 0.992953\tvalid_1's rmse: 1.05702\n",
      "[331]\ttraining's rmse: 0.992764\tvalid_1's rmse: 1.05679\n",
      "[332]\ttraining's rmse: 0.992547\tvalid_1's rmse: 1.05666\n",
      "[333]\ttraining's rmse: 0.992369\tvalid_1's rmse: 1.05664\n",
      "[334]\ttraining's rmse: 0.992195\tvalid_1's rmse: 1.05656\n",
      "[335]\ttraining's rmse: 0.992012\tvalid_1's rmse: 1.05632\n",
      "[336]\ttraining's rmse: 0.991794\tvalid_1's rmse: 1.05616\n",
      "[337]\ttraining's rmse: 0.991594\tvalid_1's rmse: 1.05604\n",
      "[338]\ttraining's rmse: 0.991419\tvalid_1's rmse: 1.05581\n",
      "[339]\ttraining's rmse: 0.991254\tvalid_1's rmse: 1.0557\n",
      "[340]\ttraining's rmse: 0.991077\tvalid_1's rmse: 1.0556\n",
      "[341]\ttraining's rmse: 0.990905\tvalid_1's rmse: 1.05558\n",
      "[342]\ttraining's rmse: 0.990736\tvalid_1's rmse: 1.05556\n",
      "[343]\ttraining's rmse: 0.99051\tvalid_1's rmse: 1.05539\n",
      "[344]\ttraining's rmse: 0.990336\tvalid_1's rmse: 1.05527\n",
      "[345]\ttraining's rmse: 0.990122\tvalid_1's rmse: 1.0551\n",
      "[346]\ttraining's rmse: 0.989963\tvalid_1's rmse: 1.05503\n",
      "[347]\ttraining's rmse: 0.989733\tvalid_1's rmse: 1.05485\n",
      "[348]\ttraining's rmse: 0.989564\tvalid_1's rmse: 1.0547\n",
      "[349]\ttraining's rmse: 0.989398\tvalid_1's rmse: 1.05473\n",
      "[350]\ttraining's rmse: 0.989162\tvalid_1's rmse: 1.0546\n",
      "[351]\ttraining's rmse: 0.988963\tvalid_1's rmse: 1.0545\n",
      "[352]\ttraining's rmse: 0.988801\tvalid_1's rmse: 1.05435\n",
      "[353]\ttraining's rmse: 0.98858\tvalid_1's rmse: 1.05419\n",
      "[354]\ttraining's rmse: 0.98842\tvalid_1's rmse: 1.0542\n",
      "[355]\ttraining's rmse: 0.988266\tvalid_1's rmse: 1.05405\n",
      "[356]\ttraining's rmse: 0.988114\tvalid_1's rmse: 1.05401\n",
      "[357]\ttraining's rmse: 0.98796\tvalid_1's rmse: 1.054\n",
      "[358]\ttraining's rmse: 0.987798\tvalid_1's rmse: 1.05388\n",
      "[359]\ttraining's rmse: 0.987582\tvalid_1's rmse: 1.05373\n",
      "[360]\ttraining's rmse: 0.987434\tvalid_1's rmse: 1.05366\n",
      "[361]\ttraining's rmse: 0.987242\tvalid_1's rmse: 1.05356\n",
      "[362]\ttraining's rmse: 0.987084\tvalid_1's rmse: 1.05348\n",
      "[363]\ttraining's rmse: 0.986873\tvalid_1's rmse: 1.05333\n",
      "[364]\ttraining's rmse: 0.986723\tvalid_1's rmse: 1.05318\n",
      "[365]\ttraining's rmse: 0.986566\tvalid_1's rmse: 1.05319\n",
      "[366]\ttraining's rmse: 0.986409\tvalid_1's rmse: 1.05316\n",
      "[367]\ttraining's rmse: 0.986254\tvalid_1's rmse: 1.05303\n",
      "[368]\ttraining's rmse: 0.9861\tvalid_1's rmse: 1.05296\n",
      "[369]\ttraining's rmse: 0.985963\tvalid_1's rmse: 1.05292\n",
      "[370]\ttraining's rmse: 0.98582\tvalid_1's rmse: 1.05291\n",
      "[371]\ttraining's rmse: 0.985669\tvalid_1's rmse: 1.05278\n",
      "[372]\ttraining's rmse: 0.985456\tvalid_1's rmse: 1.05266\n",
      "[373]\ttraining's rmse: 0.985305\tvalid_1's rmse: 1.05256\n",
      "[374]\ttraining's rmse: 0.985174\tvalid_1's rmse: 1.05255\n",
      "[375]\ttraining's rmse: 0.984973\tvalid_1's rmse: 1.05238\n",
      "[376]\ttraining's rmse: 0.984783\tvalid_1's rmse: 1.0523\n",
      "[377]\ttraining's rmse: 0.984635\tvalid_1's rmse: 1.05223\n",
      "[378]\ttraining's rmse: 0.984485\tvalid_1's rmse: 1.05224\n",
      "[379]\ttraining's rmse: 0.984338\tvalid_1's rmse: 1.05222\n",
      "[380]\ttraining's rmse: 0.984193\tvalid_1's rmse: 1.05215\n",
      "[381]\ttraining's rmse: 0.984048\tvalid_1's rmse: 1.05201\n",
      "[382]\ttraining's rmse: 0.983848\tvalid_1's rmse: 1.05188\n",
      "[383]\ttraining's rmse: 0.98369\tvalid_1's rmse: 1.05182\n",
      "[384]\ttraining's rmse: 0.983564\tvalid_1's rmse: 1.05181\n",
      "[385]\ttraining's rmse: 0.983405\tvalid_1's rmse: 1.0517\n",
      "[386]\ttraining's rmse: 0.98326\tvalid_1's rmse: 1.05155\n",
      "[387]\ttraining's rmse: 0.983078\tvalid_1's rmse: 1.05147\n",
      "[388]\ttraining's rmse: 0.982903\tvalid_1's rmse: 1.05133\n",
      "[389]\ttraining's rmse: 0.982768\tvalid_1's rmse: 1.05129\n",
      "[390]\ttraining's rmse: 0.982576\tvalid_1's rmse: 1.05117\n",
      "[391]\ttraining's rmse: 0.982432\tvalid_1's rmse: 1.05103\n",
      "[392]\ttraining's rmse: 0.982279\tvalid_1's rmse: 1.05097\n",
      "[393]\ttraining's rmse: 0.982111\tvalid_1's rmse: 1.05085\n",
      "[394]\ttraining's rmse: 0.981924\tvalid_1's rmse: 1.05067\n",
      "[395]\ttraining's rmse: 0.981781\tvalid_1's rmse: 1.05056\n",
      "[396]\ttraining's rmse: 0.981647\tvalid_1's rmse: 1.05049\n",
      "[397]\ttraining's rmse: 0.981511\tvalid_1's rmse: 1.05043\n",
      "[398]\ttraining's rmse: 0.981324\tvalid_1's rmse: 1.05033\n",
      "[399]\ttraining's rmse: 0.981182\tvalid_1's rmse: 1.05031\n",
      "[400]\ttraining's rmse: 0.981052\tvalid_1's rmse: 1.05017\n",
      "[401]\ttraining's rmse: 0.980908\tvalid_1's rmse: 1.05021\n",
      "[402]\ttraining's rmse: 0.980775\tvalid_1's rmse: 1.05011\n",
      "[403]\ttraining's rmse: 0.980599\tvalid_1's rmse: 1.05001\n",
      "[404]\ttraining's rmse: 0.980467\tvalid_1's rmse: 1.04995\n",
      "[405]\ttraining's rmse: 0.980301\tvalid_1's rmse: 1.04982\n",
      "[406]\ttraining's rmse: 0.980174\tvalid_1's rmse: 1.0498\n",
      "[407]\ttraining's rmse: 0.980022\tvalid_1's rmse: 1.04976\n",
      "[408]\ttraining's rmse: 0.979844\tvalid_1's rmse: 1.04964\n",
      "[409]\ttraining's rmse: 0.979701\tvalid_1's rmse: 1.0495\n",
      "[410]\ttraining's rmse: 0.979553\tvalid_1's rmse: 1.04942\n",
      "[411]\ttraining's rmse: 0.979384\tvalid_1's rmse: 1.04938\n",
      "[412]\ttraining's rmse: 0.979242\tvalid_1's rmse: 1.04931\n",
      "[413]\ttraining's rmse: 0.979079\tvalid_1's rmse: 1.04915\n",
      "[414]\ttraining's rmse: 0.978949\tvalid_1's rmse: 1.04906\n",
      "[415]\ttraining's rmse: 0.97881\tvalid_1's rmse: 1.04892\n",
      "[416]\ttraining's rmse: 0.978642\tvalid_1's rmse: 1.0488\n",
      "[417]\ttraining's rmse: 0.978509\tvalid_1's rmse: 1.04876\n",
      "[418]\ttraining's rmse: 0.978374\tvalid_1's rmse: 1.04865\n",
      "[419]\ttraining's rmse: 0.978226\tvalid_1's rmse: 1.04856\n",
      "[420]\ttraining's rmse: 0.978092\tvalid_1's rmse: 1.0486\n",
      "[421]\ttraining's rmse: 0.97797\tvalid_1's rmse: 1.04849\n",
      "[422]\ttraining's rmse: 0.977822\tvalid_1's rmse: 1.04838\n",
      "[423]\ttraining's rmse: 0.977648\tvalid_1's rmse: 1.04832\n",
      "[424]\ttraining's rmse: 0.977513\tvalid_1's rmse: 1.0482\n",
      "[425]\ttraining's rmse: 0.977386\tvalid_1's rmse: 1.04816\n",
      "[426]\ttraining's rmse: 0.977254\tvalid_1's rmse: 1.04807\n",
      "[427]\ttraining's rmse: 0.977091\tvalid_1's rmse: 1.04798\n",
      "[428]\ttraining's rmse: 0.976954\tvalid_1's rmse: 1.04793\n",
      "[429]\ttraining's rmse: 0.976823\tvalid_1's rmse: 1.04784\n",
      "[430]\ttraining's rmse: 0.976703\tvalid_1's rmse: 1.0478\n",
      "[431]\ttraining's rmse: 0.976537\tvalid_1's rmse: 1.04768\n",
      "[432]\ttraining's rmse: 0.976416\tvalid_1's rmse: 1.04763\n",
      "[433]\ttraining's rmse: 0.976258\tvalid_1's rmse: 1.04758\n",
      "[434]\ttraining's rmse: 0.976132\tvalid_1's rmse: 1.04747\n",
      "[435]\ttraining's rmse: 0.976004\tvalid_1's rmse: 1.04738\n",
      "[436]\ttraining's rmse: 0.975875\tvalid_1's rmse: 1.04726\n",
      "[437]\ttraining's rmse: 0.975715\tvalid_1's rmse: 1.04716\n",
      "[438]\ttraining's rmse: 0.975594\tvalid_1's rmse: 1.04712\n",
      "[439]\ttraining's rmse: 0.975474\tvalid_1's rmse: 1.04707\n",
      "[440]\ttraining's rmse: 0.975326\tvalid_1's rmse: 1.04697\n",
      "[441]\ttraining's rmse: 0.975204\tvalid_1's rmse: 1.04698\n",
      "[442]\ttraining's rmse: 0.975062\tvalid_1's rmse: 1.04687\n",
      "[443]\ttraining's rmse: 0.974933\tvalid_1's rmse: 1.04679\n",
      "[444]\ttraining's rmse: 0.974786\tvalid_1's rmse: 1.04671\n",
      "[445]\ttraining's rmse: 0.974653\tvalid_1's rmse: 1.04658\n",
      "[446]\ttraining's rmse: 0.974529\tvalid_1's rmse: 1.04654\n",
      "[447]\ttraining's rmse: 0.974404\tvalid_1's rmse: 1.04645\n",
      "[448]\ttraining's rmse: 0.974279\tvalid_1's rmse: 1.04641\n",
      "[449]\ttraining's rmse: 0.974141\tvalid_1's rmse: 1.0463\n",
      "[450]\ttraining's rmse: 0.973993\tvalid_1's rmse: 1.0462\n",
      "[451]\ttraining's rmse: 0.973876\tvalid_1's rmse: 1.04616\n",
      "[452]\ttraining's rmse: 0.973754\tvalid_1's rmse: 1.04606\n",
      "[453]\ttraining's rmse: 0.973609\tvalid_1's rmse: 1.04596\n",
      "[454]\ttraining's rmse: 0.973487\tvalid_1's rmse: 1.04594\n",
      "[455]\ttraining's rmse: 0.973366\tvalid_1's rmse: 1.04584\n",
      "[456]\ttraining's rmse: 0.973222\tvalid_1's rmse: 1.04577\n",
      "[457]\ttraining's rmse: 0.9731\tvalid_1's rmse: 1.04568\n",
      "[458]\ttraining's rmse: 0.972996\tvalid_1's rmse: 1.04565\n",
      "[459]\ttraining's rmse: 0.972877\tvalid_1's rmse: 1.04557\n",
      "[460]\ttraining's rmse: 0.972732\tvalid_1's rmse: 1.0455\n",
      "[461]\ttraining's rmse: 0.972621\tvalid_1's rmse: 1.04546\n",
      "[462]\ttraining's rmse: 0.972486\tvalid_1's rmse: 1.04538\n",
      "[463]\ttraining's rmse: 0.97236\tvalid_1's rmse: 1.04526\n",
      "[464]\ttraining's rmse: 0.972242\tvalid_1's rmse: 1.04522\n",
      "[465]\ttraining's rmse: 0.972134\tvalid_1's rmse: 1.04512\n",
      "[466]\ttraining's rmse: 0.972017\tvalid_1's rmse: 1.04512\n",
      "[467]\ttraining's rmse: 0.971877\tvalid_1's rmse: 1.04502\n",
      "[468]\ttraining's rmse: 0.971757\tvalid_1's rmse: 1.04499\n",
      "[469]\ttraining's rmse: 0.971652\tvalid_1's rmse: 1.04497\n",
      "[470]\ttraining's rmse: 0.971578\tvalid_1's rmse: 1.04495\n",
      "[471]\ttraining's rmse: 0.971446\tvalid_1's rmse: 1.04487\n",
      "[472]\ttraining's rmse: 0.971327\tvalid_1's rmse: 1.04478\n",
      "[473]\ttraining's rmse: 0.971215\tvalid_1's rmse: 1.04477\n",
      "[474]\ttraining's rmse: 0.971109\tvalid_1's rmse: 1.04473\n",
      "[475]\ttraining's rmse: 0.970964\tvalid_1's rmse: 1.04468\n",
      "[476]\ttraining's rmse: 0.970854\tvalid_1's rmse: 1.04461\n",
      "[477]\ttraining's rmse: 0.97076\tvalid_1's rmse: 1.04458\n",
      "[478]\ttraining's rmse: 0.970642\tvalid_1's rmse: 1.04444\n",
      "[479]\ttraining's rmse: 0.970514\tvalid_1's rmse: 1.04439\n",
      "[480]\ttraining's rmse: 0.970413\tvalid_1's rmse: 1.04436\n",
      "[481]\ttraining's rmse: 0.970287\tvalid_1's rmse: 1.04433\n",
      "[482]\ttraining's rmse: 0.97017\tvalid_1's rmse: 1.04425\n",
      "[483]\ttraining's rmse: 0.970041\tvalid_1's rmse: 1.04417\n",
      "[484]\ttraining's rmse: 0.969927\tvalid_1's rmse: 1.04411\n",
      "[485]\ttraining's rmse: 0.969837\tvalid_1's rmse: 1.04407\n",
      "[486]\ttraining's rmse: 0.969717\tvalid_1's rmse: 1.04399\n",
      "[487]\ttraining's rmse: 0.969603\tvalid_1's rmse: 1.04387\n",
      "[488]\ttraining's rmse: 0.969478\tvalid_1's rmse: 1.04382\n",
      "[489]\ttraining's rmse: 0.969376\tvalid_1's rmse: 1.04379\n",
      "[490]\ttraining's rmse: 0.969254\tvalid_1's rmse: 1.04372\n",
      "[491]\ttraining's rmse: 0.96916\tvalid_1's rmse: 1.04374\n",
      "[492]\ttraining's rmse: 0.969069\tvalid_1's rmse: 1.04371\n",
      "[493]\ttraining's rmse: 0.968932\tvalid_1's rmse: 1.04368\n",
      "[494]\ttraining's rmse: 0.968819\tvalid_1's rmse: 1.04358\n",
      "[495]\ttraining's rmse: 0.968699\tvalid_1's rmse: 1.04352\n",
      "[496]\ttraining's rmse: 0.968576\tvalid_1's rmse: 1.04345\n",
      "[497]\ttraining's rmse: 0.968471\tvalid_1's rmse: 1.04336\n",
      "[498]\ttraining's rmse: 0.968367\tvalid_1's rmse: 1.04326\n",
      "[499]\ttraining's rmse: 0.968251\tvalid_1's rmse: 1.04319\n",
      "[500]\ttraining's rmse: 0.968182\tvalid_1's rmse: 1.04317\n",
      "[501]\ttraining's rmse: 0.968069\tvalid_1's rmse: 1.04314\n",
      "[502]\ttraining's rmse: 0.967972\tvalid_1's rmse: 1.04305\n",
      "[503]\ttraining's rmse: 0.967846\tvalid_1's rmse: 1.043\n",
      "[504]\ttraining's rmse: 0.967736\tvalid_1's rmse: 1.04292\n",
      "[505]\ttraining's rmse: 0.96762\tvalid_1's rmse: 1.04285\n",
      "[506]\ttraining's rmse: 0.967511\tvalid_1's rmse: 1.0428\n",
      "[507]\ttraining's rmse: 0.967436\tvalid_1's rmse: 1.04275\n",
      "[508]\ttraining's rmse: 0.967336\tvalid_1's rmse: 1.04273\n",
      "[509]\ttraining's rmse: 0.967233\tvalid_1's rmse: 1.0427\n",
      "[510]\ttraining's rmse: 0.967147\tvalid_1's rmse: 1.04268\n",
      "[511]\ttraining's rmse: 0.967017\tvalid_1's rmse: 1.04264\n",
      "[512]\ttraining's rmse: 0.96691\tvalid_1's rmse: 1.0426\n",
      "[513]\ttraining's rmse: 0.966808\tvalid_1's rmse: 1.04252\n",
      "[514]\ttraining's rmse: 0.966687\tvalid_1's rmse: 1.04249\n",
      "[515]\ttraining's rmse: 0.966565\tvalid_1's rmse: 1.04241\n",
      "[516]\ttraining's rmse: 0.966474\tvalid_1's rmse: 1.04237\n",
      "[517]\ttraining's rmse: 0.966347\tvalid_1's rmse: 1.04234\n",
      "[518]\ttraining's rmse: 0.966232\tvalid_1's rmse: 1.04231\n",
      "[519]\ttraining's rmse: 0.966128\tvalid_1's rmse: 1.04223\n",
      "[520]\ttraining's rmse: 0.966009\tvalid_1's rmse: 1.04215\n",
      "[521]\ttraining's rmse: 0.965891\tvalid_1's rmse: 1.04213\n",
      "[522]\ttraining's rmse: 0.965788\tvalid_1's rmse: 1.04213\n",
      "[523]\ttraining's rmse: 0.96569\tvalid_1's rmse: 1.04214\n",
      "[524]\ttraining's rmse: 0.965619\tvalid_1's rmse: 1.04215\n",
      "[525]\ttraining's rmse: 0.965524\tvalid_1's rmse: 1.04212\n",
      "[526]\ttraining's rmse: 0.965429\tvalid_1's rmse: 1.04204\n",
      "[527]\ttraining's rmse: 0.965314\tvalid_1's rmse: 1.04202\n",
      "[528]\ttraining's rmse: 0.965207\tvalid_1's rmse: 1.04194\n",
      "[529]\ttraining's rmse: 0.965096\tvalid_1's rmse: 1.04187\n",
      "[530]\ttraining's rmse: 0.965\tvalid_1's rmse: 1.04179\n",
      "[531]\ttraining's rmse: 0.964887\tvalid_1's rmse: 1.04177\n",
      "[532]\ttraining's rmse: 0.964834\tvalid_1's rmse: 1.04175\n",
      "[533]\ttraining's rmse: 0.964705\tvalid_1's rmse: 1.0417\n",
      "[534]\ttraining's rmse: 0.964607\tvalid_1's rmse: 1.04162\n",
      "[535]\ttraining's rmse: 0.964492\tvalid_1's rmse: 1.04155\n",
      "[536]\ttraining's rmse: 0.964382\tvalid_1's rmse: 1.04152\n",
      "[537]\ttraining's rmse: 0.964318\tvalid_1's rmse: 1.0415\n",
      "[538]\ttraining's rmse: 0.964227\tvalid_1's rmse: 1.04147\n",
      "[539]\ttraining's rmse: 0.964133\tvalid_1's rmse: 1.04143\n",
      "[540]\ttraining's rmse: 0.964025\tvalid_1's rmse: 1.04136\n",
      "[541]\ttraining's rmse: 0.963919\tvalid_1's rmse: 1.04132\n",
      "[542]\ttraining's rmse: 0.963797\tvalid_1's rmse: 1.0413\n",
      "[543]\ttraining's rmse: 0.963699\tvalid_1's rmse: 1.04128\n",
      "[544]\ttraining's rmse: 0.963646\tvalid_1's rmse: 1.04124\n",
      "[545]\ttraining's rmse: 0.963542\tvalid_1's rmse: 1.04116\n",
      "[546]\ttraining's rmse: 0.96343\tvalid_1's rmse: 1.04108\n",
      "[547]\ttraining's rmse: 0.963323\tvalid_1's rmse: 1.04104\n",
      "[548]\ttraining's rmse: 0.963201\tvalid_1's rmse: 1.04102\n",
      "[549]\ttraining's rmse: 0.963121\tvalid_1's rmse: 1.04101\n",
      "[550]\ttraining's rmse: 0.963036\tvalid_1's rmse: 1.04102\n",
      "[551]\ttraining's rmse: 0.962924\tvalid_1's rmse: 1.04097\n",
      "[552]\ttraining's rmse: 0.962806\tvalid_1's rmse: 1.04096\n",
      "[553]\ttraining's rmse: 0.962714\tvalid_1's rmse: 1.04096\n",
      "[554]\ttraining's rmse: 0.962607\tvalid_1's rmse: 1.04087\n",
      "[555]\ttraining's rmse: 0.962508\tvalid_1's rmse: 1.04084\n",
      "[556]\ttraining's rmse: 0.962405\tvalid_1's rmse: 1.04082\n",
      "[557]\ttraining's rmse: 0.962321\tvalid_1's rmse: 1.04075\n",
      "[558]\ttraining's rmse: 0.962212\tvalid_1's rmse: 1.0407\n",
      "[559]\ttraining's rmse: 0.962108\tvalid_1's rmse: 1.04066\n",
      "[560]\ttraining's rmse: 0.962015\tvalid_1's rmse: 1.04064\n",
      "[561]\ttraining's rmse: 0.961911\tvalid_1's rmse: 1.04056\n",
      "[562]\ttraining's rmse: 0.961863\tvalid_1's rmse: 1.04056\n",
      "[563]\ttraining's rmse: 0.961786\tvalid_1's rmse: 1.04053\n",
      "[564]\ttraining's rmse: 0.961687\tvalid_1's rmse: 1.04053\n",
      "[565]\ttraining's rmse: 0.961573\tvalid_1's rmse: 1.0405\n",
      "[566]\ttraining's rmse: 0.961445\tvalid_1's rmse: 1.04048\n",
      "[567]\ttraining's rmse: 0.961343\tvalid_1's rmse: 1.04046\n",
      "[568]\ttraining's rmse: 0.961244\tvalid_1's rmse: 1.04031\n",
      "[569]\ttraining's rmse: 0.961155\tvalid_1's rmse: 1.04027\n",
      "[570]\ttraining's rmse: 0.961037\tvalid_1's rmse: 1.04027\n",
      "[571]\ttraining's rmse: 0.960931\tvalid_1's rmse: 1.04023\n",
      "[572]\ttraining's rmse: 0.960856\tvalid_1's rmse: 1.0402\n",
      "[573]\ttraining's rmse: 0.960769\tvalid_1's rmse: 1.04016\n",
      "[574]\ttraining's rmse: 0.960673\tvalid_1's rmse: 1.04013\n",
      "[575]\ttraining's rmse: 0.960578\tvalid_1's rmse: 1.04009\n",
      "[576]\ttraining's rmse: 0.960531\tvalid_1's rmse: 1.04009\n",
      "[577]\ttraining's rmse: 0.960419\tvalid_1's rmse: 1.04009\n",
      "[578]\ttraining's rmse: 0.960318\tvalid_1's rmse: 1.04002\n",
      "[579]\ttraining's rmse: 0.960222\tvalid_1's rmse: 1.04\n",
      "[580]\ttraining's rmse: 0.960144\tvalid_1's rmse: 1.04\n",
      "[581]\ttraining's rmse: 0.960048\tvalid_1's rmse: 1.03999\n",
      "[582]\ttraining's rmse: 0.959945\tvalid_1's rmse: 1.03994\n",
      "[583]\ttraining's rmse: 0.959847\tvalid_1's rmse: 1.03992\n",
      "[584]\ttraining's rmse: 0.95973\tvalid_1's rmse: 1.0399\n",
      "[585]\ttraining's rmse: 0.959683\tvalid_1's rmse: 1.03989\n",
      "[586]\ttraining's rmse: 0.959605\tvalid_1's rmse: 1.03985\n",
      "[587]\ttraining's rmse: 0.959515\tvalid_1's rmse: 1.03973\n",
      "[588]\ttraining's rmse: 0.959428\tvalid_1's rmse: 1.03968\n",
      "[589]\ttraining's rmse: 0.959324\tvalid_1's rmse: 1.03964\n",
      "[590]\ttraining's rmse: 0.959218\tvalid_1's rmse: 1.03963\n",
      "[591]\ttraining's rmse: 0.959162\tvalid_1's rmse: 1.03962\n",
      "[592]\ttraining's rmse: 0.95907\tvalid_1's rmse: 1.03958\n",
      "[593]\ttraining's rmse: 0.95898\tvalid_1's rmse: 1.03957\n",
      "[594]\ttraining's rmse: 0.958904\tvalid_1's rmse: 1.03957\n",
      "[595]\ttraining's rmse: 0.958802\tvalid_1's rmse: 1.03955\n",
      "[596]\ttraining's rmse: 0.958719\tvalid_1's rmse: 1.0395\n",
      "[597]\ttraining's rmse: 0.958605\tvalid_1's rmse: 1.03947\n",
      "[598]\ttraining's rmse: 0.958497\tvalid_1's rmse: 1.03944\n",
      "[599]\ttraining's rmse: 0.958385\tvalid_1's rmse: 1.03942\n",
      "[600]\ttraining's rmse: 0.9583\tvalid_1's rmse: 1.03937\n",
      "[601]\ttraining's rmse: 0.95821\tvalid_1's rmse: 1.03935\n",
      "[602]\ttraining's rmse: 0.958115\tvalid_1's rmse: 1.03932\n",
      "[603]\ttraining's rmse: 0.958029\tvalid_1's rmse: 1.03935\n",
      "[604]\ttraining's rmse: 0.95796\tvalid_1's rmse: 1.03934\n",
      "[605]\ttraining's rmse: 0.957886\tvalid_1's rmse: 1.03928\n",
      "[606]\ttraining's rmse: 0.957779\tvalid_1's rmse: 1.03927\n",
      "[607]\ttraining's rmse: 0.957682\tvalid_1's rmse: 1.03921\n",
      "[608]\ttraining's rmse: 0.957601\tvalid_1's rmse: 1.03918\n",
      "[609]\ttraining's rmse: 0.957518\tvalid_1's rmse: 1.03913\n",
      "[610]\ttraining's rmse: 0.957422\tvalid_1's rmse: 1.0391\n",
      "[611]\ttraining's rmse: 0.95733\tvalid_1's rmse: 1.03906\n",
      "[612]\ttraining's rmse: 0.957269\tvalid_1's rmse: 1.03903\n",
      "[613]\ttraining's rmse: 0.957198\tvalid_1's rmse: 1.03902\n",
      "[614]\ttraining's rmse: 0.957093\tvalid_1's rmse: 1.03899\n",
      "[615]\ttraining's rmse: 0.956985\tvalid_1's rmse: 1.03897\n",
      "[616]\ttraining's rmse: 0.956892\tvalid_1's rmse: 1.03895\n",
      "[617]\ttraining's rmse: 0.956805\tvalid_1's rmse: 1.03893\n",
      "[618]\ttraining's rmse: 0.956725\tvalid_1's rmse: 1.03889\n",
      "[619]\ttraining's rmse: 0.956655\tvalid_1's rmse: 1.03887\n",
      "[620]\ttraining's rmse: 0.956558\tvalid_1's rmse: 1.03889\n",
      "[621]\ttraining's rmse: 0.956494\tvalid_1's rmse: 1.03885\n",
      "[622]\ttraining's rmse: 0.956386\tvalid_1's rmse: 1.03883\n",
      "[623]\ttraining's rmse: 0.956305\tvalid_1's rmse: 1.0388\n",
      "[624]\ttraining's rmse: 0.956193\tvalid_1's rmse: 1.03879\n",
      "[625]\ttraining's rmse: 0.956088\tvalid_1's rmse: 1.03876\n",
      "[626]\ttraining's rmse: 0.956004\tvalid_1's rmse: 1.03874\n",
      "[627]\ttraining's rmse: 0.955898\tvalid_1's rmse: 1.03877\n",
      "[628]\ttraining's rmse: 0.955796\tvalid_1's rmse: 1.03872\n",
      "[629]\ttraining's rmse: 0.955705\tvalid_1's rmse: 1.0386\n",
      "[630]\ttraining's rmse: 0.955611\tvalid_1's rmse: 1.03859\n",
      "[631]\ttraining's rmse: 0.95554\tvalid_1's rmse: 1.03856\n",
      "[632]\ttraining's rmse: 0.955445\tvalid_1's rmse: 1.03854\n",
      "[633]\ttraining's rmse: 0.955363\tvalid_1's rmse: 1.03851\n",
      "[634]\ttraining's rmse: 0.955261\tvalid_1's rmse: 1.03836\n",
      "[635]\ttraining's rmse: 0.955183\tvalid_1's rmse: 1.03832\n",
      "[636]\ttraining's rmse: 0.955094\tvalid_1's rmse: 1.0382\n",
      "[637]\ttraining's rmse: 0.955004\tvalid_1's rmse: 1.03817\n",
      "[638]\ttraining's rmse: 0.954913\tvalid_1's rmse: 1.03817\n",
      "[639]\ttraining's rmse: 0.954823\tvalid_1's rmse: 1.03814\n",
      "[640]\ttraining's rmse: 0.954753\tvalid_1's rmse: 1.0381\n",
      "[641]\ttraining's rmse: 0.954692\tvalid_1's rmse: 1.03811\n",
      "[642]\ttraining's rmse: 0.954583\tvalid_1's rmse: 1.0381\n",
      "[643]\ttraining's rmse: 0.954507\tvalid_1's rmse: 1.03806\n",
      "[644]\ttraining's rmse: 0.954423\tvalid_1's rmse: 1.03801\n",
      "[645]\ttraining's rmse: 0.954326\tvalid_1's rmse: 1.03801\n",
      "[646]\ttraining's rmse: 0.954227\tvalid_1's rmse: 1.03787\n",
      "[647]\ttraining's rmse: 0.95414\tvalid_1's rmse: 1.03785\n",
      "[648]\ttraining's rmse: 0.954072\tvalid_1's rmse: 1.03784\n",
      "[649]\ttraining's rmse: 0.953988\tvalid_1's rmse: 1.03784\n",
      "[650]\ttraining's rmse: 0.953892\tvalid_1's rmse: 1.03777\n",
      "[651]\ttraining's rmse: 0.953825\tvalid_1's rmse: 1.03775\n",
      "[652]\ttraining's rmse: 0.953765\tvalid_1's rmse: 1.03777\n",
      "[653]\ttraining's rmse: 0.953675\tvalid_1's rmse: 1.03779\n",
      "[654]\ttraining's rmse: 0.953581\tvalid_1's rmse: 1.03779\n",
      "[655]\ttraining's rmse: 0.95349\tvalid_1's rmse: 1.03778\n",
      "[656]\ttraining's rmse: 0.953385\tvalid_1's rmse: 1.03777\n",
      "[657]\ttraining's rmse: 0.953288\tvalid_1's rmse: 1.03764\n",
      "[658]\ttraining's rmse: 0.953236\tvalid_1's rmse: 1.03761\n",
      "[659]\ttraining's rmse: 0.953177\tvalid_1's rmse: 1.03763\n",
      "[660]\ttraining's rmse: 0.953088\tvalid_1's rmse: 1.03755\n",
      "[661]\ttraining's rmse: 0.952992\tvalid_1's rmse: 1.03758\n",
      "[662]\ttraining's rmse: 0.952921\tvalid_1's rmse: 1.03757\n",
      "[663]\ttraining's rmse: 0.952842\tvalid_1's rmse: 1.03755\n",
      "[664]\ttraining's rmse: 0.952755\tvalid_1's rmse: 1.03758\n",
      "[665]\ttraining's rmse: 0.952698\tvalid_1's rmse: 1.03756\n",
      "[666]\ttraining's rmse: 0.952614\tvalid_1's rmse: 1.03755\n",
      "[667]\ttraining's rmse: 0.952563\tvalid_1's rmse: 1.03751\n",
      "[668]\ttraining's rmse: 0.952484\tvalid_1's rmse: 1.03744\n",
      "[669]\ttraining's rmse: 0.952414\tvalid_1's rmse: 1.03739\n",
      "[670]\ttraining's rmse: 0.952321\tvalid_1's rmse: 1.03733\n",
      "[671]\ttraining's rmse: 0.952228\tvalid_1's rmse: 1.03735\n",
      "[672]\ttraining's rmse: 0.952129\tvalid_1's rmse: 1.03733\n",
      "[673]\ttraining's rmse: 0.952088\tvalid_1's rmse: 1.03729\n",
      "[674]\ttraining's rmse: 0.951995\tvalid_1's rmse: 1.03716\n",
      "[675]\ttraining's rmse: 0.951938\tvalid_1's rmse: 1.03718\n",
      "[676]\ttraining's rmse: 0.951868\tvalid_1's rmse: 1.03713\n",
      "[677]\ttraining's rmse: 0.951804\tvalid_1's rmse: 1.03717\n",
      "[678]\ttraining's rmse: 0.951712\tvalid_1's rmse: 1.03718\n",
      "[679]\ttraining's rmse: 0.951647\tvalid_1's rmse: 1.03716\n",
      "[680]\ttraining's rmse: 0.95156\tvalid_1's rmse: 1.03715\n",
      "[681]\ttraining's rmse: 0.95148\tvalid_1's rmse: 1.03709\n",
      "[682]\ttraining's rmse: 0.95144\tvalid_1's rmse: 1.03705\n",
      "[683]\ttraining's rmse: 0.951349\tvalid_1's rmse: 1.03707\n",
      "[684]\ttraining's rmse: 0.951259\tvalid_1's rmse: 1.03698\n",
      "[685]\ttraining's rmse: 0.951146\tvalid_1's rmse: 1.03686\n",
      "[686]\ttraining's rmse: 0.951051\tvalid_1's rmse: 1.03688\n",
      "[687]\ttraining's rmse: 0.950976\tvalid_1's rmse: 1.03688\n",
      "[688]\ttraining's rmse: 0.950915\tvalid_1's rmse: 1.03691\n",
      "[689]\ttraining's rmse: 0.950859\tvalid_1's rmse: 1.03693\n",
      "[690]\ttraining's rmse: 0.950779\tvalid_1's rmse: 1.03694\n",
      "[691]\ttraining's rmse: 0.950669\tvalid_1's rmse: 1.03681\n",
      "[692]\ttraining's rmse: 0.95062\tvalid_1's rmse: 1.03679\n",
      "[693]\ttraining's rmse: 0.95054\tvalid_1's rmse: 1.03677\n",
      "[694]\ttraining's rmse: 0.95045\tvalid_1's rmse: 1.0367\n",
      "[695]\ttraining's rmse: 0.950387\tvalid_1's rmse: 1.03669\n",
      "[696]\ttraining's rmse: 0.950294\tvalid_1's rmse: 1.03667\n",
      "[697]\ttraining's rmse: 0.950204\tvalid_1's rmse: 1.03653\n",
      "[698]\ttraining's rmse: 0.950159\tvalid_1's rmse: 1.03646\n",
      "[699]\ttraining's rmse: 0.950076\tvalid_1's rmse: 1.0365\n",
      "[700]\ttraining's rmse: 0.949992\tvalid_1's rmse: 1.03652\n",
      "[701]\ttraining's rmse: 0.949909\tvalid_1's rmse: 1.03647\n",
      "[702]\ttraining's rmse: 0.949818\tvalid_1's rmse: 1.03641\n",
      "[703]\ttraining's rmse: 0.949735\tvalid_1's rmse: 1.03634\n",
      "[704]\ttraining's rmse: 0.949642\tvalid_1's rmse: 1.03634\n",
      "[705]\ttraining's rmse: 0.949586\tvalid_1's rmse: 1.0363\n",
      "[706]\ttraining's rmse: 0.949541\tvalid_1's rmse: 1.03627\n",
      "[707]\ttraining's rmse: 0.949447\tvalid_1's rmse: 1.03627\n",
      "[708]\ttraining's rmse: 0.949374\tvalid_1's rmse: 1.03625\n",
      "[709]\ttraining's rmse: 0.9493\tvalid_1's rmse: 1.03627\n",
      "[710]\ttraining's rmse: 0.949224\tvalid_1's rmse: 1.03622\n",
      "[711]\ttraining's rmse: 0.94913\tvalid_1's rmse: 1.03623\n",
      "[712]\ttraining's rmse: 0.949077\tvalid_1's rmse: 1.03626\n",
      "[713]\ttraining's rmse: 0.948989\tvalid_1's rmse: 1.03625\n",
      "[714]\ttraining's rmse: 0.948894\tvalid_1's rmse: 1.03622\n",
      "[715]\ttraining's rmse: 0.948825\tvalid_1's rmse: 1.03618\n",
      "[716]\ttraining's rmse: 0.948786\tvalid_1's rmse: 1.03615\n",
      "[717]\ttraining's rmse: 0.948706\tvalid_1's rmse: 1.03621\n",
      "[718]\ttraining's rmse: 0.948613\tvalid_1's rmse: 1.03622\n",
      "[719]\ttraining's rmse: 0.948532\tvalid_1's rmse: 1.03625\n",
      "[720]\ttraining's rmse: 0.948453\tvalid_1's rmse: 1.0362\n",
      "[721]\ttraining's rmse: 0.948406\tvalid_1's rmse: 1.03624\n",
      "[722]\ttraining's rmse: 0.948318\tvalid_1's rmse: 1.03617\n",
      "[723]\ttraining's rmse: 0.948236\tvalid_1's rmse: 1.03613\n",
      "[724]\ttraining's rmse: 0.948129\tvalid_1's rmse: 1.03598\n",
      "[725]\ttraining's rmse: 0.948053\tvalid_1's rmse: 1.03597\n",
      "[726]\ttraining's rmse: 0.948001\tvalid_1's rmse: 1.03598\n",
      "[727]\ttraining's rmse: 0.947954\tvalid_1's rmse: 1.03595\n",
      "[728]\ttraining's rmse: 0.947884\tvalid_1's rmse: 1.03597\n",
      "[729]\ttraining's rmse: 0.947801\tvalid_1's rmse: 1.03598\n",
      "[730]\ttraining's rmse: 0.947722\tvalid_1's rmse: 1.03594\n",
      "[731]\ttraining's rmse: 0.947632\tvalid_1's rmse: 1.03592\n",
      "[732]\ttraining's rmse: 0.947528\tvalid_1's rmse: 1.03577\n",
      "[733]\ttraining's rmse: 0.947448\tvalid_1's rmse: 1.03573\n",
      "[734]\ttraining's rmse: 0.947415\tvalid_1's rmse: 1.03573\n",
      "[735]\ttraining's rmse: 0.947329\tvalid_1's rmse: 1.03566\n",
      "[736]\ttraining's rmse: 0.947292\tvalid_1's rmse: 1.03564\n",
      "[737]\ttraining's rmse: 0.947186\tvalid_1's rmse: 1.0355\n",
      "[738]\ttraining's rmse: 0.947095\tvalid_1's rmse: 1.03554\n",
      "[739]\ttraining's rmse: 0.947062\tvalid_1's rmse: 1.03554\n",
      "[740]\ttraining's rmse: 0.947019\tvalid_1's rmse: 1.03554\n",
      "[741]\ttraining's rmse: 0.946936\tvalid_1's rmse: 1.03551\n",
      "[742]\ttraining's rmse: 0.946857\tvalid_1's rmse: 1.03551\n",
      "[743]\ttraining's rmse: 0.94678\tvalid_1's rmse: 1.03552\n",
      "[744]\ttraining's rmse: 0.946693\tvalid_1's rmse: 1.03549\n",
      "[745]\ttraining's rmse: 0.946618\tvalid_1's rmse: 1.03554\n",
      "[746]\ttraining's rmse: 0.946518\tvalid_1's rmse: 1.03539\n",
      "[747]\ttraining's rmse: 0.946458\tvalid_1's rmse: 1.03537\n",
      "[748]\ttraining's rmse: 0.946379\tvalid_1's rmse: 1.03537\n",
      "[749]\ttraining's rmse: 0.946338\tvalid_1's rmse: 1.03537\n",
      "[750]\ttraining's rmse: 0.946237\tvalid_1's rmse: 1.0353\n",
      "[751]\ttraining's rmse: 0.94617\tvalid_1's rmse: 1.03532\n",
      "[752]\ttraining's rmse: 0.946092\tvalid_1's rmse: 1.03527\n",
      "[753]\ttraining's rmse: 0.946015\tvalid_1's rmse: 1.03521\n",
      "[754]\ttraining's rmse: 0.945941\tvalid_1's rmse: 1.0352\n",
      "[755]\ttraining's rmse: 0.945899\tvalid_1's rmse: 1.03518\n",
      "[756]\ttraining's rmse: 0.945826\tvalid_1's rmse: 1.03522\n",
      "[757]\ttraining's rmse: 0.945741\tvalid_1's rmse: 1.03516\n",
      "[758]\ttraining's rmse: 0.945688\tvalid_1's rmse: 1.03514\n",
      "[759]\ttraining's rmse: 0.945601\tvalid_1's rmse: 1.0351\n",
      "[760]\ttraining's rmse: 0.945528\tvalid_1's rmse: 1.03503\n",
      "[761]\ttraining's rmse: 0.945456\tvalid_1's rmse: 1.03502\n",
      "[762]\ttraining's rmse: 0.945384\tvalid_1's rmse: 1.03501\n",
      "[763]\ttraining's rmse: 0.945344\tvalid_1's rmse: 1.03498\n",
      "[764]\ttraining's rmse: 0.945279\tvalid_1's rmse: 1.035\n",
      "[765]\ttraining's rmse: 0.945246\tvalid_1's rmse: 1.03501\n",
      "[766]\ttraining's rmse: 0.945212\tvalid_1's rmse: 1.035\n",
      "[767]\ttraining's rmse: 0.945129\tvalid_1's rmse: 1.03498\n",
      "[768]\ttraining's rmse: 0.945047\tvalid_1's rmse: 1.03493\n",
      "[769]\ttraining's rmse: 0.944977\tvalid_1's rmse: 1.03492\n",
      "[770]\ttraining's rmse: 0.944905\tvalid_1's rmse: 1.03497\n",
      "[771]\ttraining's rmse: 0.944818\tvalid_1's rmse: 1.03494\n",
      "[772]\ttraining's rmse: 0.944744\tvalid_1's rmse: 1.03493\n",
      "[773]\ttraining's rmse: 0.944702\tvalid_1's rmse: 1.03488\n",
      "[774]\ttraining's rmse: 0.944647\tvalid_1's rmse: 1.03485\n",
      "[775]\ttraining's rmse: 0.944578\tvalid_1's rmse: 1.03478\n",
      "[776]\ttraining's rmse: 0.944501\tvalid_1's rmse: 1.03478\n",
      "[777]\ttraining's rmse: 0.944412\tvalid_1's rmse: 1.03473\n",
      "[778]\ttraining's rmse: 0.944313\tvalid_1's rmse: 1.0347\n",
      "[779]\ttraining's rmse: 0.944281\tvalid_1's rmse: 1.0347\n",
      "[780]\ttraining's rmse: 0.944237\tvalid_1's rmse: 1.03473\n",
      "[781]\ttraining's rmse: 0.944156\tvalid_1's rmse: 1.03466\n",
      "[782]\ttraining's rmse: 0.944071\tvalid_1's rmse: 1.03464\n",
      "[783]\ttraining's rmse: 0.944001\tvalid_1's rmse: 1.03467\n",
      "[784]\ttraining's rmse: 0.943913\tvalid_1's rmse: 1.03467\n",
      "[785]\ttraining's rmse: 0.94384\tvalid_1's rmse: 1.03469\n",
      "[786]\ttraining's rmse: 0.943799\tvalid_1's rmse: 1.03463\n",
      "[787]\ttraining's rmse: 0.943745\tvalid_1's rmse: 1.03464\n",
      "[788]\ttraining's rmse: 0.943674\tvalid_1's rmse: 1.03461\n",
      "[789]\ttraining's rmse: 0.943574\tvalid_1's rmse: 1.03447\n",
      "[790]\ttraining's rmse: 0.943483\tvalid_1's rmse: 1.03445\n",
      "[791]\ttraining's rmse: 0.943441\tvalid_1's rmse: 1.03443\n",
      "[792]\ttraining's rmse: 0.943362\tvalid_1's rmse: 1.03442\n",
      "[793]\ttraining's rmse: 0.943329\tvalid_1's rmse: 1.0344\n",
      "[794]\ttraining's rmse: 0.94325\tvalid_1's rmse: 1.03435\n",
      "[795]\ttraining's rmse: 0.943218\tvalid_1's rmse: 1.03436\n",
      "[796]\ttraining's rmse: 0.943145\tvalid_1's rmse: 1.03436\n",
      "[797]\ttraining's rmse: 0.94306\tvalid_1's rmse: 1.03436\n",
      "[798]\ttraining's rmse: 0.943018\tvalid_1's rmse: 1.03439\n",
      "[799]\ttraining's rmse: 0.94295\tvalid_1's rmse: 1.03439\n",
      "[800]\ttraining's rmse: 0.942911\tvalid_1's rmse: 1.0344\n",
      "[801]\ttraining's rmse: 0.942817\tvalid_1's rmse: 1.03425\n",
      "[802]\ttraining's rmse: 0.942744\tvalid_1's rmse: 1.03427\n",
      "[803]\ttraining's rmse: 0.942657\tvalid_1's rmse: 1.03421\n",
      "[804]\ttraining's rmse: 0.942569\tvalid_1's rmse: 1.03426\n",
      "[805]\ttraining's rmse: 0.942503\tvalid_1's rmse: 1.03426\n",
      "[806]\ttraining's rmse: 0.942472\tvalid_1's rmse: 1.03427\n",
      "[807]\ttraining's rmse: 0.942433\tvalid_1's rmse: 1.03422\n",
      "[808]\ttraining's rmse: 0.942345\tvalid_1's rmse: 1.03417\n",
      "[809]\ttraining's rmse: 0.94226\tvalid_1's rmse: 1.03421\n",
      "[810]\ttraining's rmse: 0.942186\tvalid_1's rmse: 1.03417\n",
      "[811]\ttraining's rmse: 0.942122\tvalid_1's rmse: 1.03415\n",
      "[812]\ttraining's rmse: 0.94209\tvalid_1's rmse: 1.03413\n",
      "[813]\ttraining's rmse: 0.94201\tvalid_1's rmse: 1.03412\n",
      "[814]\ttraining's rmse: 0.941918\tvalid_1's rmse: 1.0341\n",
      "[815]\ttraining's rmse: 0.941888\tvalid_1's rmse: 1.0341\n",
      "[816]\ttraining's rmse: 0.94185\tvalid_1's rmse: 1.0341\n",
      "[817]\ttraining's rmse: 0.941768\tvalid_1's rmse: 1.03409\n",
      "[818]\ttraining's rmse: 0.941683\tvalid_1's rmse: 1.03415\n",
      "[819]\ttraining's rmse: 0.941621\tvalid_1's rmse: 1.03411\n",
      "[820]\ttraining's rmse: 0.941561\tvalid_1's rmse: 1.03412\n",
      "[821]\ttraining's rmse: 0.941479\tvalid_1's rmse: 1.03413\n",
      "[822]\ttraining's rmse: 0.941398\tvalid_1's rmse: 1.03418\n",
      "[823]\ttraining's rmse: 0.941332\tvalid_1's rmse: 1.03418\n",
      "[824]\ttraining's rmse: 0.941261\tvalid_1's rmse: 1.03416\n",
      "[825]\ttraining's rmse: 0.941228\tvalid_1's rmse: 1.03418\n",
      "[826]\ttraining's rmse: 0.94119\tvalid_1's rmse: 1.03413\n",
      "[827]\ttraining's rmse: 0.941116\tvalid_1's rmse: 1.03416\n",
      "Early stopping, best iteration is:\n",
      "[817]\ttraining's rmse: 0.941768\tvalid_1's rmse: 1.03409\n",
      "fold_4 coefficients:  [0.55247557 1.53238812 2.23270319]\n",
      "training qwk     :  [0.60799538, 0.59879844, 0.59368538, 0.62527296, 0.60912597] 0.606975626\n",
      "validation qwk   :  [0.52779521, 0.5431064, 0.49657864, 0.50276426, 0.49635583] 0.5133200680000001\n",
      "train qwk by dist:  [0.63777255, 0.6280834, 0.62674122, 0.65134776, 0.64039912] 0.63686881\n",
      "valid qwk by dist:  [0.52965395, 0.54467477, 0.51020721, 0.51748134, 0.52823091] 0.526049636\n",
      "fold_0 coefficients:  [0.5241538  1.53825865 2.31853619]\n",
      "fold_1 coefficients:  [0.56180492 1.5132103  2.19124795]\n",
      "fold_2 coefficients:  [0.5514832  1.65421811 2.02756344]\n",
      "fold_3 coefficients:  [0.56525171 1.57765175 2.16105731]\n",
      "fold_4 coefficients:  [0.63816634 1.42366827 2.10845098]\n",
      "training qwk     :  [0.53384708, 0.54940651, 0.54959165, 0.55815972, 0.55585535] 0.549372062\n",
      "validation qwk   :  [0.46375723, 0.49736797, 0.49007143, 0.42692786, 0.4718532] 0.46999553800000005\n",
      "train qwk by dist:  [0.57858279, 0.57757289, 0.58034672, 0.58262841, 0.57983194] 0.57979255\n",
      "valid qwk by dist:  [0.49740511, 0.49280765, 0.50092458, 0.46235792, 0.48611622] 0.487922296\n",
      "Train on 14151 samples, validate on 683 samples\n",
      "Epoch 1/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.5382\n",
      "Epoch 00001: val_loss improved from inf to 1.15924, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 4s 264us/sample - loss: 1.5316 - val_loss: 1.1592\n",
      "Epoch 2/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.2413\n",
      "Epoch 00002: val_loss improved from 1.15924 to 1.15206, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 174us/sample - loss: 1.2423 - val_loss: 1.1521\n",
      "Epoch 3/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.1850\n",
      "Epoch 00003: val_loss improved from 1.15206 to 1.11589, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 3s 178us/sample - loss: 1.1857 - val_loss: 1.1159\n",
      "Epoch 4/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.1211\n",
      "Epoch 00004: val_loss did not improve from 1.11589\n",
      "14151/14151 [==============================] - 3s 177us/sample - loss: 1.1215 - val_loss: 1.1259\n",
      "Epoch 5/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.1118\n",
      "Epoch 00005: val_loss did not improve from 1.11589\n",
      "14151/14151 [==============================] - 3s 177us/sample - loss: 1.1081 - val_loss: 1.1212\n",
      "Epoch 6/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.0857\n",
      "Epoch 00006: val_loss did not improve from 1.11589\n",
      "14151/14151 [==============================] - 3s 179us/sample - loss: 1.0828 - val_loss: 1.1306\n",
      "Epoch 7/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0751\n",
      "Epoch 00007: val_loss improved from 1.11589 to 1.10574, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 3s 180us/sample - loss: 1.0753 - val_loss: 1.1057\n",
      "Epoch 8/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0528\n",
      "Epoch 00008: val_loss improved from 1.10574 to 1.09221, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 3s 177us/sample - loss: 1.0526 - val_loss: 1.0922\n",
      "Epoch 9/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.0417\n",
      "Epoch 00009: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 221us/sample - loss: 1.0410 - val_loss: 1.1167\n",
      "Epoch 10/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0191\n",
      "Epoch 00010: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 215us/sample - loss: 1.0193 - val_loss: 1.1098\n",
      "Epoch 11/100\n",
      "13952/14151 [============================>.] - ETA: 0s - loss: 1.0084\n",
      "Epoch 00011: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 2s 175us/sample - loss: 1.0071 - val_loss: 1.1149\n",
      "Epoch 12/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.9898\n",
      "Epoch 00012: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 2s 175us/sample - loss: 0.9900 - val_loss: 1.1135\n",
      "Epoch 13/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.9822\n",
      "Epoch 00013: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 219us/sample - loss: 0.9829 - val_loss: 1.1223\n",
      "Epoch 14/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9626\n",
      "Epoch 00014: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 2s 176us/sample - loss: 0.9624 - val_loss: 1.1201\n",
      "Epoch 15/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.9479\n",
      "Epoch 00015: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 177us/sample - loss: 0.9483 - val_loss: 1.1232\n",
      "Epoch 16/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9366\n",
      "Epoch 00016: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 2s 175us/sample - loss: 0.9364 - val_loss: 1.1111\n",
      "Epoch 17/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9185\n",
      "Epoch 00017: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 178us/sample - loss: 0.9172 - val_loss: 1.1370\n",
      "Epoch 18/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.9155\n",
      "Epoch 00018: val_loss did not improve from 1.09221\n",
      "14151/14151 [==============================] - 3s 177us/sample - loss: 0.9160 - val_loss: 1.1253\n",
      "fold_0 coefficients:  [0.60978992 1.70918531 1.91723912]\n",
      "Train on 14152 samples, validate on 728 samples\n",
      "Epoch 1/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.7144\n",
      "Epoch 00001: val_loss improved from inf to 1.18290, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 255us/sample - loss: 1.7133 - val_loss: 1.1829\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.3335\n",
      "Epoch 00002: val_loss improved from 1.18290 to 1.17861, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 1.3328 - val_loss: 1.1786\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2275\n",
      "Epoch 00003: val_loss improved from 1.17861 to 1.13580, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 1.2260 - val_loss: 1.1358\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1687\n",
      "Epoch 00004: val_loss improved from 1.13580 to 1.10120, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.1690 - val_loss: 1.1012\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1294\n",
      "Epoch 00005: val_loss improved from 1.10120 to 1.09870, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.1309 - val_loss: 1.0987\n",
      "Epoch 6/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0939\n",
      "Epoch 00006: val_loss did not improve from 1.09870\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.0947 - val_loss: 1.1056\n",
      "Epoch 7/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0833\n",
      "Epoch 00007: val_loss did not improve from 1.09870\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0830 - val_loss: 1.1165\n",
      "Epoch 8/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0673\n",
      "Epoch 00008: val_loss did not improve from 1.09870\n",
      "14152/14152 [==============================] - 3s 180us/sample - loss: 1.0667 - val_loss: 1.1150\n",
      "Epoch 9/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0627\n",
      "Epoch 00009: val_loss did not improve from 1.09870\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.0621 - val_loss: 1.1090\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0247\n",
      "Epoch 00010: val_loss improved from 1.09870 to 1.09700, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 183us/sample - loss: 1.0231 - val_loss: 1.0970\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0155\n",
      "Epoch 00011: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.0155 - val_loss: 1.1096\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9997\n",
      "Epoch 00012: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 3s 206us/sample - loss: 0.9995 - val_loss: 1.1102\n",
      "Epoch 13/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9835\n",
      "Epoch 00013: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 3s 184us/sample - loss: 0.9810 - val_loss: 1.1285\n",
      "Epoch 14/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9785\n",
      "Epoch 00014: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9776 - val_loss: 1.1312\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9625\n",
      "Epoch 00015: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9623 - val_loss: 1.1156\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9391\n",
      "Epoch 00016: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 0.9402 - val_loss: 1.1131\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9341\n",
      "Epoch 00017: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9347 - val_loss: 1.1578\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9229\n",
      "Epoch 00018: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9226 - val_loss: 1.1672\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9129\n",
      "Epoch 00019: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9131 - val_loss: 1.1280\n",
      "Epoch 20/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8887\n",
      "Epoch 00020: val_loss did not improve from 1.09700\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 0.8865 - val_loss: 1.1460\n",
      "fold_1 coefficients:  [0.59488239 1.51426485 2.29632272]\n",
      "Train on 14152 samples, validate on 789 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.8733\n",
      "Epoch 00001: val_loss improved from inf to 1.14482, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 252us/sample - loss: 1.8643 - val_loss: 1.1448\n",
      "Epoch 2/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.3086\n",
      "Epoch 00002: val_loss did not improve from 1.14482\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.3089 - val_loss: 1.1452\n",
      "Epoch 3/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.2048\n",
      "Epoch 00003: val_loss improved from 1.14482 to 1.10952, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 1.2011 - val_loss: 1.1095\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1673\n",
      "Epoch 00004: val_loss improved from 1.10952 to 1.09762, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.1663 - val_loss: 1.0976\n",
      "Epoch 5/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1298\n",
      "Epoch 00005: val_loss improved from 1.09762 to 1.08440, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.1310 - val_loss: 1.0844\n",
      "Epoch 6/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0963\n",
      "Epoch 00006: val_loss did not improve from 1.08440\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0978 - val_loss: 1.1104\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0876\n",
      "Epoch 00007: val_loss did not improve from 1.08440\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0862 - val_loss: 1.1001\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0641\n",
      "Epoch 00008: val_loss did not improve from 1.08440\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0622 - val_loss: 1.0871\n",
      "Epoch 9/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0383\n",
      "Epoch 00009: val_loss did not improve from 1.08440\n",
      "14152/14152 [==============================] - 3s 181us/sample - loss: 1.0402 - val_loss: 1.0861\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0321\n",
      "Epoch 00010: val_loss did not improve from 1.08440\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0329 - val_loss: 1.0937\n",
      "Epoch 11/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0232\n",
      "Epoch 00011: val_loss improved from 1.08440 to 1.07844, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 180us/sample - loss: 1.0231 - val_loss: 1.0784\n",
      "Epoch 12/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00012: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 3s 180us/sample - loss: 1.0102 - val_loss: 1.1038\n",
      "Epoch 13/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9854\n",
      "Epoch 00013: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 0.9862 - val_loss: 1.0976\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9783\n",
      "Epoch 00014: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 3s 179us/sample - loss: 0.9782 - val_loss: 1.0998\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9690\n",
      "Epoch 00015: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.9688 - val_loss: 1.1018\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9607\n",
      "Epoch 00016: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9605 - val_loss: 1.1191\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9423\n",
      "Epoch 00017: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 0.9440 - val_loss: 1.0948\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9294\n",
      "Epoch 00018: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9298 - val_loss: 1.1217\n",
      "Epoch 19/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9276\n",
      "Epoch 00019: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 171us/sample - loss: 0.9286 - val_loss: 1.1412\n",
      "Epoch 20/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9003\n",
      "Epoch 00020: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.9005 - val_loss: 1.1051\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8797\n",
      "Epoch 00021: val_loss did not improve from 1.07844\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 0.8794 - val_loss: 1.1237\n",
      "fold_2 coefficients:  [0.54273646 1.56161507 2.27770946]\n",
      "Train on 14152 samples, validate on 807 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.7246\n",
      "Epoch 00001: val_loss improved from inf to 1.30589, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 4s 255us/sample - loss: 1.7238 - val_loss: 1.3059\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.2745\n",
      "Epoch 00002: val_loss improved from 1.30589 to 1.25028, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.2724 - val_loss: 1.2503\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.2118\n",
      "Epoch 00003: val_loss did not improve from 1.25028\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 1.2145 - val_loss: 1.2632\n",
      "Epoch 4/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1633\n",
      "Epoch 00004: val_loss improved from 1.25028 to 1.23425, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.1650 - val_loss: 1.2342\n",
      "Epoch 5/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1290\n",
      "Epoch 00005: val_loss did not improve from 1.23425\n",
      "14152/14152 [==============================] - 2s 177us/sample - loss: 1.1284 - val_loss: 1.2423\n",
      "Epoch 6/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1076\n",
      "Epoch 00006: val_loss did not improve from 1.23425\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 1.1078 - val_loss: 1.2508\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0828\n",
      "Epoch 00007: val_loss improved from 1.23425 to 1.20006, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 1.0827 - val_loss: 1.2001\n",
      "Epoch 8/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0808\n",
      "Epoch 00008: val_loss improved from 1.20006 to 1.19445, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0800 - val_loss: 1.1944\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0616\n",
      "Epoch 00009: val_loss improved from 1.19445 to 1.18937, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.0619 - val_loss: 1.1894\n",
      "Epoch 10/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0387\n",
      "Epoch 00010: val_loss improved from 1.18937 to 1.18778, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 1.0369 - val_loss: 1.1878\n",
      "Epoch 11/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0221\n",
      "Epoch 00011: val_loss did not improve from 1.18778\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.0206 - val_loss: 1.1908\n",
      "Epoch 12/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0060\n",
      "Epoch 00012: val_loss did not improve from 1.18778\n",
      "14152/14152 [==============================] - 3s 177us/sample - loss: 1.0056 - val_loss: 1.2043\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0046\n",
      "Epoch 00013: val_loss improved from 1.18778 to 1.18642, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 1.0048 - val_loss: 1.1864\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9835\n",
      "Epoch 00014: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.9858 - val_loss: 1.2129\n",
      "Epoch 15/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9704\n",
      "Epoch 00015: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 3s 178us/sample - loss: 0.9707 - val_loss: 1.2391\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9575\n",
      "Epoch 00016: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 0.9586 - val_loss: 1.2159\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9475\n",
      "Epoch 00017: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 177us/sample - loss: 0.9501 - val_loss: 1.1939\n",
      "Epoch 18/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9392\n",
      "Epoch 00018: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.9384 - val_loss: 1.1950\n",
      "Epoch 19/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9213\n",
      "Epoch 00019: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9222 - val_loss: 1.2214\n",
      "Epoch 20/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9148\n",
      "Epoch 00020: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 175us/sample - loss: 0.9149 - val_loss: 1.1980\n",
      "Epoch 21/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9025\n",
      "Epoch 00021: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 176us/sample - loss: 0.9021 - val_loss: 1.2925\n",
      "Epoch 22/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.8836\n",
      "Epoch 00022: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 173us/sample - loss: 0.8831 - val_loss: 1.2371\n",
      "Epoch 23/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8790\n",
      "Epoch 00023: val_loss did not improve from 1.18642\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 0.8788 - val_loss: 1.2449\n",
      "fold_3 coefficients:  [0.70114267 1.33292541 2.28467721]\n",
      "Train on 14153 samples, validate on 775 samples\n",
      "Epoch 1/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.7257\n",
      "Epoch 00001: val_loss improved from inf to 1.15621, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 4s 266us/sample - loss: 1.7199 - val_loss: 1.1562\n",
      "Epoch 2/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.3189\n",
      "Epoch 00002: val_loss improved from 1.15621 to 1.12991, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 176us/sample - loss: 1.3194 - val_loss: 1.1299\n",
      "Epoch 3/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 1.2113\n",
      "Epoch 00003: val_loss did not improve from 1.12991\n",
      "14153/14153 [==============================] - 3s 178us/sample - loss: 1.2135 - val_loss: 1.1534\n",
      "Epoch 4/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.1589\n",
      "Epoch 00004: val_loss improved from 1.12991 to 1.12403, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 177us/sample - loss: 1.1583 - val_loss: 1.1240\n",
      "Epoch 5/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.1355\n",
      "Epoch 00005: val_loss improved from 1.12403 to 1.10944, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 176us/sample - loss: 1.1357 - val_loss: 1.1094\n",
      "Epoch 6/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.1120\n",
      "Epoch 00006: val_loss did not improve from 1.10944\n",
      "14153/14153 [==============================] - 2s 175us/sample - loss: 1.1110 - val_loss: 1.1218\n",
      "Epoch 7/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 1.0870\n",
      "Epoch 00007: val_loss improved from 1.10944 to 1.09897, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 178us/sample - loss: 1.0872 - val_loss: 1.0990\n",
      "Epoch 8/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.0539\n",
      "Epoch 00008: val_loss improved from 1.09897 to 1.08502, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 179us/sample - loss: 1.0535 - val_loss: 1.0850\n",
      "Epoch 9/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.0426\n",
      "Epoch 00009: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 3s 177us/sample - loss: 1.0442 - val_loss: 1.0985\n",
      "Epoch 10/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.0307\n",
      "Epoch 00010: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 3s 177us/sample - loss: 1.0297 - val_loss: 1.1057\n",
      "Epoch 11/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.0110\n",
      "Epoch 00011: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 3s 179us/sample - loss: 1.0144 - val_loss: 1.0976\n",
      "Epoch 12/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9930\n",
      "Epoch 00012: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 2s 175us/sample - loss: 0.9927 - val_loss: 1.0860\n",
      "Epoch 13/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 0.9839\n",
      "Epoch 00013: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 3s 177us/sample - loss: 0.9851 - val_loss: 1.1145\n",
      "Epoch 14/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 0.9742\n",
      "Epoch 00014: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 2s 176us/sample - loss: 0.9746 - val_loss: 1.0907\n",
      "Epoch 15/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.9532\n",
      "Epoch 00015: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 2s 176us/sample - loss: 0.9541 - val_loss: 1.0969\n",
      "Epoch 16/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 0.9460\n",
      "Epoch 00016: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 2s 174us/sample - loss: 0.9460 - val_loss: 1.1038\n",
      "Epoch 17/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9381\n",
      "Epoch 00017: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 2s 175us/sample - loss: 0.9382 - val_loss: 1.0868\n",
      "Epoch 18/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9132\n",
      "Epoch 00018: val_loss did not improve from 1.08502\n",
      "14153/14153 [==============================] - 3s 213us/sample - loss: 0.9135 - val_loss: 1.1202\n",
      "fold_4 coefficients:  [0.5165766  1.61318654 2.24559654]\n",
      "training qwk     :  [0.57913391, 0.62474931, 0.61142609, 0.64478159, 0.61504109] 0.615026398\n",
      "validation qwk   :  [0.51818555, 0.56360087, 0.52626371, 0.52485661, 0.51690167] 0.529961682\n",
      "train qwk by dist:  [0.62744016, 0.63040983, 0.63152831, 0.64459215, 0.61986712] 0.630767514\n",
      "valid qwk by dist:  [0.53297898, 0.53702224, 0.53427763, 0.51446088, 0.53204372] 0.53015669\n"
     ]
    }
   ],
   "source": [
    "lgb_preds = models(\"lgb\", X_train_lgb, y_train_lgb, X_test_lgb)\n",
    "lr_preds = models(\"lr\", X_train_lr, y_train_lr, X_test_lr)\n",
    "nn_preds = models(\"nn\", X_train_lr, y_train_lr, X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.499\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.126\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_class = eval_qwk_lgb_regr(lgb_preds * 0.4 + nn_preds * 0.45 + lr_preds * 0.15, new_train) # threshold by distribution\n",
    "sample_submission[\"accuracy_group\"] = test_pred_class\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission[\"accuracy_group\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1608b35af66b41dea8c9f19d4c7df553": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3671b253bdce4cb89e021f7d820f28ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_41b0399a079349b3a4ba18915feaaef5",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_45360587a19e46338e04d97c8a18c3f4",
       "value": 1000
      }
     },
     "3b54cf575be14923ba20f26a08043e59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "41b0399a079349b3a4ba18915feaaef5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45360587a19e46338e04d97c8a18c3f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "555d7ee9ad1e4bfbadbab0d109c29469": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d09510b365149e4bb58550cbcdf1580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6374bdfd828943698d4836c4a92b2b6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_555d7ee9ad1e4bfbadbab0d109c29469",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1608b35af66b41dea8c9f19d4c7df553",
       "value": 3614
      }
     },
     "6937474e1b0542a091929cf76a39ef59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6b1e2b337cb44abead2e52da01c62aaa",
       "placeholder": "​",
       "style": "IPY_MODEL_3b54cf575be14923ba20f26a08043e59",
       "value": " 3614/3614 [08:42&lt;00:00,  6.92it/s]"
      }
     },
     "6b1e2b337cb44abead2e52da01c62aaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "774d61d61b1d4e79b969f75b24b1c1a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8297c61c4da741498f0e703e35b10ecd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6374bdfd828943698d4836c4a92b2b6d",
        "IPY_MODEL_6937474e1b0542a091929cf76a39ef59"
       ],
       "layout": "IPY_MODEL_a404695642184eee85205f46f9d8bdbe"
      }
     },
     "888e948ae6c546c7804eadffb5e7b7e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a404695642184eee85205f46f9d8bdbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e040f577e0324b02834b9971ad19c305": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3671b253bdce4cb89e021f7d820f28ae",
        "IPY_MODEL_ed97cdd3c1fa4eed84358d74ed85b95a"
       ],
       "layout": "IPY_MODEL_774d61d61b1d4e79b969f75b24b1c1a7"
      }
     },
     "ed97cdd3c1fa4eed84358d74ed85b95a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_888e948ae6c546c7804eadffb5e7b7e3",
       "placeholder": "​",
       "style": "IPY_MODEL_5d09510b365149e4bb58550cbcdf1580",
       "value": " 1000/1000 [01:58&lt;00:00,  8.47it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
