{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, mean_squared_error\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "np.set_printoptions(precision=8)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def qwk(a1, a2):\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "    e = e / a1.shape[0]\n",
    "    return np.round(1 - o / e, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "        return -qwk(y, X_p)\n",
    "        \n",
    "    def fit(self, X, y,random_flg=False):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        if random_flg:\n",
    "            initial_coef = [np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7), np.random.uniform(0.8,0.9)]\n",
    "        else:\n",
    "            initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        \n",
    "    def predict(self, X, coef):\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stract_hists(feature, train, test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5, label = \"train\")\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5, label = \"test\")\n",
    "        plt.show()\n",
    "    return msre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n",
    "    train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n",
    "    test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n",
    "    #specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n",
    "    sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print(\"Finish reading\")\n",
    "    return train, train_labels, test, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data(train, train_labels):\n",
    "    keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "    train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "    train = train[train.installation_id.isin(train_labels.installation_id.unique())]\n",
    "    assess_title = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
    "       'Cauldron Filler (Assessment)', 'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)']\n",
    "    additional_remove_index = []\n",
    "    for i, session in train.groupby('installation_id', sort=False):\n",
    "        last_row = session.index[-1]\n",
    "        session = session[session.title.isin(assess_title)]\n",
    "        first_row = session.index[-1] + 1\n",
    "        for j in range(first_row, last_row+1):\n",
    "            additional_remove_index.append(j)                \n",
    "    train = train[~train.index.isin(additional_remove_index)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test):\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    list_of_title_eventcode = list(set(train['title_event_code'].unique()).union(set(test['title_event_code'].unique())))\n",
    "    \n",
    "    list_of_eventid = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"level\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    test[\"level\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"round\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "    test[\"round\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)         \n",
    "        \n",
    "    return train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    last_activity = 0\n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n",
    "    title_eventcode_count = {str(ele): 0 for ele in list_of_title_eventcode}\n",
    "    eventid_count = {str(ele): 0 for ele in list_of_eventid}\n",
    "    user_world_count = {\"world_\"+str(wor) : 0 for wor in activities_world.values()}\n",
    "    \n",
    "    last_session_time_sec = 0\n",
    "    all_assessments = []\n",
    "    accuracy_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0 \n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_game_level = np.array([]); tree_game_level = np.array([]); magma_game_level = np.array([])\n",
    "    crys_game_round = np.array([]); tree_game_round = np.array([]); magma_game_round = np.array([])\n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    durations_game = []; durations_activity = []\n",
    "    \n",
    "    for i, session in user_sample.groupby('game_session', sort=False):  \n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "        \n",
    "        if session_type != 'Assessment':\n",
    "            time_spent = int(session['game_time'].iloc[-1] / 1000)\n",
    "            time_spent_each_act[activities_labels[session_title]] += time_spent   \n",
    "            \n",
    "            if session_type == \"Game\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_game_true += true\n",
    "                    crys_game_false += false\n",
    "                    crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    crys_game_level = np.concatenate([crys_game_level, session[\"level\"]], axis=0)\n",
    "                    crys_game_round = np.concatenate([crys_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_game_true += true\n",
    "                    tree_game_false += false\n",
    "                    tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    tree_game_level = np.concatenate([tree_game_level, session[\"level\"]], axis=0)\n",
    "                    tree_game_round = np.concatenate([tree_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_game_true += true\n",
    "                    magma_game_false += false\n",
    "                    magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    magma_game_level = np.concatenate([magma_game_level, session[\"level\"]], axis=0)\n",
    "                    magma_game_round = np.concatenate([magma_game_round, session[\"round\"]], axis=0)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            if session_type == \"Activity\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_act_true += true\n",
    "                    crys_act_false += false\n",
    "                    crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_act_true += true\n",
    "                    tree_act_false += false\n",
    "                    tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_act_true += true\n",
    "                    magma_act_false += false\n",
    "                    magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1): # test set or session in train_label\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum() # true in target assess\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum() # false in target assessment\n",
    "            \n",
    "            # from start of installation_id to the start of target assessment ------------------------\n",
    "            features = user_activities_count.copy() # appearance of each type without duplicates\n",
    "            features.update(time_spent_each_act.copy()) # cumulative gameplay time in each title\n",
    "            features.update(title_eventcode_count.copy()) # apperance of combi of title and event_code\n",
    "            features.update(eventid_count.copy()) # apperance of eventid\n",
    "            features.update(user_world_count.copy()) # appearance of world with duplicates\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            features[\"misses\"] = miss\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    " \n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(crys_game_level) if len(crys_game_level[~np.isnan(crys_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(crys_game_round) if len(crys_game_round[~np.isnan(crys_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(tree_game_level) if len(tree_game_level[~np.isnan(tree_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(tree_game_round) if len(tree_game_round[~np.isnan(tree_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(magma_game_level) if len(magma_game_level[~np.isnan(magma_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(magma_game_round) if len(magma_game_round[~np.isnan(magma_game_round)]) >=1 else 0\n",
    "            \n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['activity_last_duration'] = 0\n",
    "                features['activity_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['activity_last_duration'] = durations_activity[-1]\n",
    "                features['activity_max_duration'] = np.max(durations_activity)\n",
    "            \n",
    "            # unique type --------------------------------------------------------\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['session_title'] = session_title\n",
    "            \n",
    "            # nums in target assessment data ------------------------------------------\n",
    "            if durations == []: #span of timestamp in target assessment\n",
    "                features['duration_mean'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds) \n",
    "            \n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[str(features['accuracy_group'])] += 1\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            \n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "                        \n",
    "        n_of_title_eventcode = Counter(session['title_event_code']) \n",
    "        for key in n_of_title_eventcode.keys():\n",
    "            title_eventcode_count[str(key)] += n_of_title_eventcode[key]\n",
    "            \n",
    "        miss += np.sum(session[\"misses\"])\n",
    "        \n",
    "        n_of_eventid = Counter(session['event_id']) \n",
    "        for key in n_of_eventid.keys():\n",
    "            eventid_count[str(key)] += n_of_eventid[key]\n",
    "                        \n",
    "        user_world_count[\"world_\"+str(session_world)] += session.shape[0]\n",
    "\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(train, test):\n",
    "    new_train = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        new_train += get_data(user_sample)\n",
    "    new_train = pd.DataFrame(new_train)\n",
    "    print(new_train.shape)\n",
    "    del train\n",
    "    \n",
    "    new_test = []\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        a = get_data(user_sample, test_set=True)\n",
    "        new_test.append(a)   \n",
    "    new_test = pd.DataFrame(new_test)\n",
    "    print(new_test.shape)\n",
    "    del test\n",
    "    \n",
    "    return new_train, new_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(new_train, new_test, stand_flg = False):\n",
    "    X_train = new_train.copy()\n",
    "    X_test = new_test.copy()\n",
    "    y_train = new_train.accuracy_group\n",
    "    \n",
    "    if stand_flg == True:\n",
    "        features = [i for i in new_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "        categoricals = ['session_title']\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(X_train[[cat]])\n",
    "                test_cats = enc.transform(X_test[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                X_train = pd.concat([X_train, train_cats], axis=1)\n",
    "                X_test = pd.concat([X_test, test_cats], axis=1)\n",
    "            scalar = MinMaxScaler()\n",
    "            X_train[features] = scalar.fit_transform(X_train[features])\n",
    "            X_test[features] = scalar.transform(X_test[features])\n",
    "        X_train = X_train.drop([\"session_title\"], axis=1)\n",
    "        X_test = X_test.drop([\"session_title\"], axis=1)\n",
    "    \n",
    "    X_train = X_train.drop(['accuracy_group'],axis=1) \n",
    "    X_test = X_test.drop([\"installation_id\", \"accuracy_group\"], axis=1)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = []\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    #for i in high_corr_features:\n",
    "    #    if i not in remove_features:\n",
    "    #        remove_features.append(i)  \n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "    X_test = X_test.drop(remove_features, axis=1)\n",
    "    X_test = X_test[sorted(X_test.columns.tolist())]\n",
    "    print(\"train: \", X_train.shape)\n",
    "    print(\"test: \", X_test.shape)\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading\n"
     ]
    }
   ],
   "source": [
    "train, train_labels, test, sample_submission = read_data()\n",
    "train = remove_data(train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid = encode_title(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09629245b09645709228e368adfaf6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=3614, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(17690, 860)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f4a409db4149eda2c3a6c1b6a9e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=1000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1000, 860)\n"
     ]
    }
   ],
   "source": [
    "new_train, new_test = make_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = new_train.corr().abs()\n",
    "correlations = correlations.mask(np.tril(np.ones(correlations.shape)).astype(np.bool))\n",
    "correlations = correlations.stack().reset_index()\n",
    "corr_columns = [\"level_0\", \"level_1\", \"value\"]\n",
    "correlations.columns = corr_columns\n",
    "correlations = correlations.sort_values(\"value\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "high_corr = correlations[correlations[\"value\"] >= 0.995]\n",
    "\n",
    "high_corr_features = []\n",
    "for i in range(high_corr.shape[0]):\n",
    "    if high_corr.iloc[i][\"level_0\"] not in high_corr_features and high_corr.iloc[i][\"level_1\"] not in high_corr_features:\n",
    "        high_corr_features.append(high_corr.iloc[i][\"level_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajusted_test = new_test.copy()\n",
    "#for feature in ajusted_test.columns:\n",
    "#    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title'] and i not in remove_features:\n",
    "#        data = new_train[feature]\n",
    "#        train_mean = data.mean()\n",
    "#        data = ajusted_test[feature] \n",
    "#        test_mean = data.mean()\n",
    "#        try:\n",
    "#            error = stract_hists(feature, new_train, new_test, adjust=True)\n",
    "#            ajust_factor = train_mean / test_mean\n",
    "#            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "#                remove_features.append(feature)\n",
    "#                print(\"try\", feature, train_mean, test_mean, error)\n",
    "#            else:\n",
    "#                ajusted_test[feature] *= ajust_factor\n",
    "#        except:\n",
    "#            remove_features.append(feature)\n",
    "#            print(\"except\", feature, train_mean, test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (17690, 834)\n",
      "test:  (1000, 833)\n",
      "train:  (17690, 838)\n",
      "test:  (1000, 837)\n"
     ]
    }
   ],
   "source": [
    "X_train_lgb, y_train_lgb, X_test_lgb = post_process(new_train, new_test)\n",
    "X_train_lr, y_train_lr, X_test_lr = post_process(new_train, new_test, stand_flg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lgb.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_lgb.columns]\n",
    "X_train_lr.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_lr.columns]\n",
    "X_test_lgb.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_lgb.columns]\n",
    "X_test_lr.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_lr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(model_name, X_tr, y_tr, X_te):\n",
    "    n_folds=5\n",
    "    skf=StratifiedKFold(n_splits = n_folds)\n",
    "    coefficients = []\n",
    "    train_qwk_scores = []; test_qwk_scores = []; train_qwk_dist = []; test_qwk_dist = []\n",
    "    pred_value = np.zeros([X_te.shape[0]])\n",
    "\n",
    "    lgbm_params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"tree_learner\": \"serial\",\n",
    "        \"max_depth\" : 4,\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"num_leaves\" : 13,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        }\n",
    "\n",
    "    for i , (train_index, test_index) in enumerate(skf.split(X_tr, y_tr)):\n",
    "        optR = OptimizedRounder()\n",
    "        X_train2 = X_tr.iloc[train_index,:]\n",
    "        y_train2 = y_tr.iloc[train_index]\n",
    "        X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "        X_test2 = X_tr.iloc[test_index,:]\n",
    "        y_test2 = y_tr.iloc[test_index]\n",
    "        test2 = pd.concat([X_test2, y_test2], axis=1)\n",
    "        test2 = test2.groupby('installation_id').apply(lambda x: x.sample(1, random_state=1223)).reset_index(drop=True)\n",
    "        X_test2 = test2.drop([\"accuracy_group\", \"installation_id\"], axis=1)\n",
    "        y_test2 = test2[\"accuracy_group\"]\n",
    "    \n",
    "        if model_name == \"lgb\":\n",
    "            lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "            lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "    \n",
    "            clf = lgb.train(\n",
    "                lgbm_params, lgb_train,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                num_boost_round=100000,\n",
    "                early_stopping_rounds=10,\n",
    "        )\n",
    "            train_predict = clf.predict(X_train2, num_iteration = clf.best_iteration)\n",
    "            valid_predict = clf.predict(X_test2, num_iteration = clf.best_iteration)\n",
    "            pred_value += clf.predict(X_te, num_iteration = clf.best_iteration) / n_folds\n",
    "        \n",
    "        elif model_name == \"lr\":    \n",
    "            clf = LinearRegression()\n",
    "            clf.fit(X_train2, y_train2) \n",
    "            train_predict = clf.predict(X_train2)\n",
    "            valid_predict = clf.predict(X_test2)\n",
    "            pred_value += clf.predict(X_te) / n_folds\n",
    "        \n",
    "        elif model_name == \"nn\":\n",
    "            verbosity = 100\n",
    "            clf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train2.shape[1],)),\n",
    "                tf.keras.layers.Dense(200, activation='relu'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(100, activation='tanh'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                #tf.keras.layers.Dense(50, activation='relu'),\n",
    "                #tf.keras.layers.LayerNormalization(),\n",
    "                #tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(25, activation='relu'),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(1, activation='relu')\n",
    "            ])\n",
    "            clf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "            save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "            \n",
    "            clf.fit(X_train2, \n",
    "                y_train2, \n",
    "                validation_data=(X_test2, y_test2),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "            clf.load_weights('./nn_model.w8')\n",
    "            train_predict = clf.predict(X_train2)\n",
    "            valid_predict = clf.predict(X_test2)\n",
    "            test_coefficients = np.mean(coefficients, axis=0)\n",
    "            pred_value += clf.predict(X_te).reshape(X_te.shape[0],) / n_folds\n",
    "    \n",
    "        optR.fit(valid_predict.reshape(-1,), y_test2)\n",
    "        tmp_coefficients = optR.coefficients()\n",
    "        print(\"fold_\"+str(i)+\" coefficients: \", tmp_coefficients)\n",
    "        opt_train_preds = optR.predict(train_predict.reshape(-1, ), tmp_coefficients)\n",
    "        train_qwk_score = qwk(y_train2, opt_train_preds)\n",
    "        opt_test_preds = optR.predict(valid_predict.reshape(-1, ), tmp_coefficients)\n",
    "        test_qwk_score = qwk(y_test2, opt_test_preds)\n",
    "        train_qwk_scores.append(train_qwk_score)\n",
    "        test_qwk_scores.append(test_qwk_score)\n",
    "        coefficients.append(tmp_coefficients)\n",
    "    \n",
    "        train_qwk_d = qwk(y_train2, eval_qwk_lgb_regr(train_predict, new_train))\n",
    "        test_qwk_d = qwk(y_test2, eval_qwk_lgb_regr(valid_predict, new_train))\n",
    "        train_qwk_dist.append(train_qwk_d)\n",
    "        test_qwk_dist.append(test_qwk_d)\n",
    "        \n",
    "    print(\"training qwk     : \", train_qwk_scores, np.mean(train_qwk_scores))\n",
    "    print(\"validation qwk   : \", test_qwk_scores, np.mean(test_qwk_scores))\n",
    "    print(\"train qwk by dist: \", train_qwk_dist, np.mean(train_qwk_dist))\n",
    "    print(\"valid qwk by dist: \", test_qwk_dist, np.mean(test_qwk_dist))\n",
    "\n",
    "    return pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's rmse: 1.25383\tvalid_1's rmse: 1.27372\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.25095\tvalid_1's rmse: 1.27099\n",
      "[3]\ttraining's rmse: 1.24812\tvalid_1's rmse: 1.26832\n",
      "[4]\ttraining's rmse: 1.24534\tvalid_1's rmse: 1.26569\n",
      "[5]\ttraining's rmse: 1.24259\tvalid_1's rmse: 1.2631\n",
      "[6]\ttraining's rmse: 1.23987\tvalid_1's rmse: 1.26065\n",
      "[7]\ttraining's rmse: 1.23722\tvalid_1's rmse: 1.25814\n",
      "[8]\ttraining's rmse: 1.23459\tvalid_1's rmse: 1.25576\n",
      "[9]\ttraining's rmse: 1.23203\tvalid_1's rmse: 1.25339\n",
      "[10]\ttraining's rmse: 1.22937\tvalid_1's rmse: 1.25091\n",
      "[11]\ttraining's rmse: 1.22675\tvalid_1's rmse: 1.24858\n",
      "[12]\ttraining's rmse: 1.2242\tvalid_1's rmse: 1.2462\n",
      "[13]\ttraining's rmse: 1.22168\tvalid_1's rmse: 1.24395\n",
      "[14]\ttraining's rmse: 1.2192\tvalid_1's rmse: 1.24162\n",
      "[15]\ttraining's rmse: 1.21678\tvalid_1's rmse: 1.23944\n",
      "[16]\ttraining's rmse: 1.21438\tvalid_1's rmse: 1.23722\n",
      "[17]\ttraining's rmse: 1.21203\tvalid_1's rmse: 1.23507\n",
      "[18]\ttraining's rmse: 1.20973\tvalid_1's rmse: 1.23295\n",
      "[19]\ttraining's rmse: 1.20744\tvalid_1's rmse: 1.23103\n",
      "[20]\ttraining's rmse: 1.20523\tvalid_1's rmse: 1.22894\n",
      "[21]\ttraining's rmse: 1.20302\tvalid_1's rmse: 1.22707\n",
      "[22]\ttraining's rmse: 1.20085\tvalid_1's rmse: 1.22507\n",
      "[23]\ttraining's rmse: 1.19875\tvalid_1's rmse: 1.22322\n",
      "[24]\ttraining's rmse: 1.19666\tvalid_1's rmse: 1.22127\n",
      "[25]\ttraining's rmse: 1.19462\tvalid_1's rmse: 1.21952\n",
      "[26]\ttraining's rmse: 1.19259\tvalid_1's rmse: 1.21769\n",
      "[27]\ttraining's rmse: 1.19062\tvalid_1's rmse: 1.216\n",
      "[28]\ttraining's rmse: 1.18867\tvalid_1's rmse: 1.21426\n",
      "[29]\ttraining's rmse: 1.18675\tvalid_1's rmse: 1.21255\n",
      "[30]\ttraining's rmse: 1.18487\tvalid_1's rmse: 1.21086\n",
      "[31]\ttraining's rmse: 1.18302\tvalid_1's rmse: 1.20923\n",
      "[32]\ttraining's rmse: 1.1812\tvalid_1's rmse: 1.2076\n",
      "[33]\ttraining's rmse: 1.1794\tvalid_1's rmse: 1.20608\n",
      "[34]\ttraining's rmse: 1.17766\tvalid_1's rmse: 1.20445\n",
      "[35]\ttraining's rmse: 1.17592\tvalid_1's rmse: 1.20299\n",
      "[36]\ttraining's rmse: 1.17421\tvalid_1's rmse: 1.2014\n",
      "[37]\ttraining's rmse: 1.17252\tvalid_1's rmse: 1.19995\n",
      "[38]\ttraining's rmse: 1.17086\tvalid_1's rmse: 1.19845\n",
      "[39]\ttraining's rmse: 1.16925\tvalid_1's rmse: 1.19707\n",
      "[40]\ttraining's rmse: 1.16766\tvalid_1's rmse: 1.1956\n",
      "[41]\ttraining's rmse: 1.16609\tvalid_1's rmse: 1.19432\n",
      "[42]\ttraining's rmse: 1.16422\tvalid_1's rmse: 1.19282\n",
      "[43]\ttraining's rmse: 1.16249\tvalid_1's rmse: 1.19145\n",
      "[44]\ttraining's rmse: 1.16099\tvalid_1's rmse: 1.19015\n",
      "[45]\ttraining's rmse: 1.15919\tvalid_1's rmse: 1.18871\n",
      "[46]\ttraining's rmse: 1.15754\tvalid_1's rmse: 1.1874\n",
      "[47]\ttraining's rmse: 1.15579\tvalid_1's rmse: 1.18602\n",
      "[48]\ttraining's rmse: 1.1542\tvalid_1's rmse: 1.18475\n",
      "[49]\ttraining's rmse: 1.15278\tvalid_1's rmse: 1.18353\n",
      "[50]\ttraining's rmse: 1.15111\tvalid_1's rmse: 1.18221\n",
      "[51]\ttraining's rmse: 1.14958\tvalid_1's rmse: 1.18099\n",
      "[52]\ttraining's rmse: 1.14823\tvalid_1's rmse: 1.17983\n",
      "[53]\ttraining's rmse: 1.14658\tvalid_1's rmse: 1.17855\n",
      "[54]\ttraining's rmse: 1.14507\tvalid_1's rmse: 1.17739\n",
      "[55]\ttraining's rmse: 1.14347\tvalid_1's rmse: 1.17616\n",
      "[56]\ttraining's rmse: 1.14201\tvalid_1's rmse: 1.17504\n",
      "[57]\ttraining's rmse: 1.14074\tvalid_1's rmse: 1.17395\n",
      "[58]\ttraining's rmse: 1.13922\tvalid_1's rmse: 1.17279\n",
      "[59]\ttraining's rmse: 1.13781\tvalid_1's rmse: 1.17173\n",
      "[60]\ttraining's rmse: 1.13659\tvalid_1's rmse: 1.17068\n",
      "[61]\ttraining's rmse: 1.13512\tvalid_1's rmse: 1.16955\n",
      "[62]\ttraining's rmse: 1.13378\tvalid_1's rmse: 1.16853\n",
      "[63]\ttraining's rmse: 1.13236\tvalid_1's rmse: 1.16746\n",
      "[64]\ttraining's rmse: 1.13119\tvalid_1's rmse: 1.16644\n",
      "[65]\ttraining's rmse: 1.1299\tvalid_1's rmse: 1.16547\n",
      "[66]\ttraining's rmse: 1.12854\tvalid_1's rmse: 1.16439\n",
      "[67]\ttraining's rmse: 1.1273\tvalid_1's rmse: 1.16345\n",
      "[68]\ttraining's rmse: 1.12618\tvalid_1's rmse: 1.16264\n",
      "[69]\ttraining's rmse: 1.12512\tvalid_1's rmse: 1.16162\n",
      "[70]\ttraining's rmse: 1.12383\tvalid_1's rmse: 1.16064\n",
      "[71]\ttraining's rmse: 1.12279\tvalid_1's rmse: 1.15963\n",
      "[72]\ttraining's rmse: 1.12152\tvalid_1's rmse: 1.15872\n",
      "[73]\ttraining's rmse: 1.12051\tvalid_1's rmse: 1.15773\n",
      "[74]\ttraining's rmse: 1.11952\tvalid_1's rmse: 1.15688\n",
      "[75]\ttraining's rmse: 1.11853\tvalid_1's rmse: 1.15604\n",
      "[76]\ttraining's rmse: 1.1175\tvalid_1's rmse: 1.15529\n",
      "[77]\ttraining's rmse: 1.1166\tvalid_1's rmse: 1.15454\n",
      "[78]\ttraining's rmse: 1.11541\tvalid_1's rmse: 1.15365\n",
      "[79]\ttraining's rmse: 1.11453\tvalid_1's rmse: 1.15288\n",
      "[80]\ttraining's rmse: 1.11366\tvalid_1's rmse: 1.15207\n",
      "[81]\ttraining's rmse: 1.11264\tvalid_1's rmse: 1.15137\n",
      "[82]\ttraining's rmse: 1.11136\tvalid_1's rmse: 1.15039\n",
      "[83]\ttraining's rmse: 1.11053\tvalid_1's rmse: 1.14968\n",
      "[84]\ttraining's rmse: 1.10958\tvalid_1's rmse: 1.14895\n",
      "[85]\ttraining's rmse: 1.10846\tvalid_1's rmse: 1.14811\n",
      "[86]\ttraining's rmse: 1.10765\tvalid_1's rmse: 1.14736\n",
      "[87]\ttraining's rmse: 1.10642\tvalid_1's rmse: 1.1461\n",
      "[88]\ttraining's rmse: 1.10524\tvalid_1's rmse: 1.14525\n",
      "[89]\ttraining's rmse: 1.10417\tvalid_1's rmse: 1.14445\n",
      "[90]\ttraining's rmse: 1.10339\tvalid_1's rmse: 1.14379\n",
      "[91]\ttraining's rmse: 1.10235\tvalid_1's rmse: 1.14301\n",
      "[92]\ttraining's rmse: 1.1012\tvalid_1's rmse: 1.14188\n",
      "[93]\ttraining's rmse: 1.10007\tvalid_1's rmse: 1.14109\n",
      "[94]\ttraining's rmse: 1.09925\tvalid_1's rmse: 1.14057\n",
      "[95]\ttraining's rmse: 1.09812\tvalid_1's rmse: 1.13941\n",
      "[96]\ttraining's rmse: 1.09714\tvalid_1's rmse: 1.13868\n",
      "[97]\ttraining's rmse: 1.09607\tvalid_1's rmse: 1.13794\n",
      "[98]\ttraining's rmse: 1.09527\tvalid_1's rmse: 1.13745\n",
      "[99]\ttraining's rmse: 1.0942\tvalid_1's rmse: 1.13638\n",
      "[100]\ttraining's rmse: 1.09354\tvalid_1's rmse: 1.13597\n",
      "[101]\ttraining's rmse: 1.0926\tvalid_1's rmse: 1.13528\n",
      "[102]\ttraining's rmse: 1.09159\tvalid_1's rmse: 1.13457\n",
      "[103]\ttraining's rmse: 1.09055\tvalid_1's rmse: 1.13366\n",
      "[104]\ttraining's rmse: 1.08953\tvalid_1's rmse: 1.13265\n",
      "[105]\ttraining's rmse: 1.08878\tvalid_1's rmse: 1.1322\n",
      "[106]\ttraining's rmse: 1.08791\tvalid_1's rmse: 1.13144\n",
      "[107]\ttraining's rmse: 1.08694\tvalid_1's rmse: 1.13081\n",
      "[108]\ttraining's rmse: 1.08631\tvalid_1's rmse: 1.13046\n",
      "[109]\ttraining's rmse: 1.08568\tvalid_1's rmse: 1.13004\n",
      "[110]\ttraining's rmse: 1.08471\tvalid_1's rmse: 1.12904\n",
      "[111]\ttraining's rmse: 1.08412\tvalid_1's rmse: 1.12868\n",
      "[112]\ttraining's rmse: 1.0831\tvalid_1's rmse: 1.12775\n",
      "[113]\ttraining's rmse: 1.08214\tvalid_1's rmse: 1.12726\n",
      "[114]\ttraining's rmse: 1.08123\tvalid_1's rmse: 1.12628\n",
      "[115]\ttraining's rmse: 1.08063\tvalid_1's rmse: 1.12599\n",
      "[116]\ttraining's rmse: 1.08005\tvalid_1's rmse: 1.12566\n",
      "[117]\ttraining's rmse: 1.07914\tvalid_1's rmse: 1.12477\n",
      "[118]\ttraining's rmse: 1.07824\tvalid_1's rmse: 1.1243\n",
      "[119]\ttraining's rmse: 1.07729\tvalid_1's rmse: 1.12343\n",
      "[120]\ttraining's rmse: 1.07645\tvalid_1's rmse: 1.12253\n",
      "[121]\ttraining's rmse: 1.0758\tvalid_1's rmse: 1.12205\n",
      "[122]\ttraining's rmse: 1.07493\tvalid_1's rmse: 1.12162\n",
      "[123]\ttraining's rmse: 1.0743\tvalid_1's rmse: 1.12115\n",
      "[124]\ttraining's rmse: 1.07347\tvalid_1's rmse: 1.12033\n",
      "[125]\ttraining's rmse: 1.07262\tvalid_1's rmse: 1.1199\n",
      "[126]\ttraining's rmse: 1.07174\tvalid_1's rmse: 1.11911\n",
      "[127]\ttraining's rmse: 1.07115\tvalid_1's rmse: 1.11863\n",
      "[128]\ttraining's rmse: 1.07035\tvalid_1's rmse: 1.11783\n",
      "[129]\ttraining's rmse: 1.06954\tvalid_1's rmse: 1.11742\n",
      "[130]\ttraining's rmse: 1.06905\tvalid_1's rmse: 1.11704\n",
      "[131]\ttraining's rmse: 1.06821\tvalid_1's rmse: 1.11629\n",
      "[132]\ttraining's rmse: 1.06747\tvalid_1's rmse: 1.11549\n",
      "[133]\ttraining's rmse: 1.06687\tvalid_1's rmse: 1.11511\n",
      "[134]\ttraining's rmse: 1.0664\tvalid_1's rmse: 1.11474\n",
      "[135]\ttraining's rmse: 1.06563\tvalid_1's rmse: 1.114\n",
      "[136]\ttraining's rmse: 1.06517\tvalid_1's rmse: 1.11363\n",
      "[137]\ttraining's rmse: 1.06437\tvalid_1's rmse: 1.11294\n",
      "[138]\ttraining's rmse: 1.06381\tvalid_1's rmse: 1.11261\n",
      "[139]\ttraining's rmse: 1.06327\tvalid_1's rmse: 1.11221\n",
      "[140]\ttraining's rmse: 1.06282\tvalid_1's rmse: 1.11189\n",
      "[141]\ttraining's rmse: 1.06212\tvalid_1's rmse: 1.11121\n",
      "[142]\ttraining's rmse: 1.06157\tvalid_1's rmse: 1.11091\n",
      "[143]\ttraining's rmse: 1.06114\tvalid_1's rmse: 1.1106\n",
      "[144]\ttraining's rmse: 1.06044\tvalid_1's rmse: 1.10981\n",
      "[145]\ttraining's rmse: 1.0597\tvalid_1's rmse: 1.10916\n",
      "[146]\ttraining's rmse: 1.05916\tvalid_1's rmse: 1.10882\n",
      "[147]\ttraining's rmse: 1.05866\tvalid_1's rmse: 1.10845\n",
      "[148]\ttraining's rmse: 1.05799\tvalid_1's rmse: 1.10783\n",
      "[149]\ttraining's rmse: 1.05748\tvalid_1's rmse: 1.10755\n",
      "[150]\ttraining's rmse: 1.05705\tvalid_1's rmse: 1.10731\n",
      "[151]\ttraining's rmse: 1.05664\tvalid_1's rmse: 1.10707\n",
      "[152]\ttraining's rmse: 1.05598\tvalid_1's rmse: 1.10648\n",
      "[153]\ttraining's rmse: 1.05528\tvalid_1's rmse: 1.10581\n",
      "[154]\ttraining's rmse: 1.05479\tvalid_1's rmse: 1.10554\n",
      "[155]\ttraining's rmse: 1.05439\tvalid_1's rmse: 1.10531\n",
      "[156]\ttraining's rmse: 1.05398\tvalid_1's rmse: 1.10513\n",
      "[157]\ttraining's rmse: 1.05334\tvalid_1's rmse: 1.1045\n",
      "[158]\ttraining's rmse: 1.05296\tvalid_1's rmse: 1.10425\n",
      "[159]\ttraining's rmse: 1.05231\tvalid_1's rmse: 1.10387\n",
      "[160]\ttraining's rmse: 1.05169\tvalid_1's rmse: 1.10316\n",
      "[161]\ttraining's rmse: 1.05103\tvalid_1's rmse: 1.10253\n",
      "[162]\ttraining's rmse: 1.05056\tvalid_1's rmse: 1.10224\n",
      "[163]\ttraining's rmse: 1.05016\tvalid_1's rmse: 1.102\n",
      "[164]\ttraining's rmse: 1.04969\tvalid_1's rmse: 1.10178\n",
      "[165]\ttraining's rmse: 1.04924\tvalid_1's rmse: 1.10157\n",
      "[166]\ttraining's rmse: 1.04865\tvalid_1's rmse: 1.10105\n",
      "[167]\ttraining's rmse: 1.04801\tvalid_1's rmse: 1.10044\n",
      "[168]\ttraining's rmse: 1.04757\tvalid_1's rmse: 1.1002\n",
      "[169]\ttraining's rmse: 1.04714\tvalid_1's rmse: 1.09993\n",
      "[170]\ttraining's rmse: 1.04679\tvalid_1's rmse: 1.09969\n",
      "[171]\ttraining's rmse: 1.04623\tvalid_1's rmse: 1.09915\n",
      "[172]\ttraining's rmse: 1.04565\tvalid_1's rmse: 1.09889\n",
      "[173]\ttraining's rmse: 1.0452\tvalid_1's rmse: 1.0987\n",
      "[174]\ttraining's rmse: 1.04486\tvalid_1's rmse: 1.09854\n",
      "[175]\ttraining's rmse: 1.0443\tvalid_1's rmse: 1.09802\n",
      "[176]\ttraining's rmse: 1.04369\tvalid_1's rmse: 1.09745\n",
      "[177]\ttraining's rmse: 1.04311\tvalid_1's rmse: 1.09712\n",
      "[178]\ttraining's rmse: 1.04271\tvalid_1's rmse: 1.09697\n",
      "[179]\ttraining's rmse: 1.04218\tvalid_1's rmse: 1.09644\n",
      "[180]\ttraining's rmse: 1.04161\tvalid_1's rmse: 1.09588\n",
      "[181]\ttraining's rmse: 1.0412\tvalid_1's rmse: 1.09569\n",
      "[182]\ttraining's rmse: 1.04081\tvalid_1's rmse: 1.09552\n",
      "[183]\ttraining's rmse: 1.04028\tvalid_1's rmse: 1.09524\n",
      "[184]\ttraining's rmse: 1.03977\tvalid_1's rmse: 1.09464\n",
      "[185]\ttraining's rmse: 1.03938\tvalid_1's rmse: 1.09445\n",
      "[186]\ttraining's rmse: 1.03904\tvalid_1's rmse: 1.0943\n",
      "[187]\ttraining's rmse: 1.0385\tvalid_1's rmse: 1.09377\n",
      "[188]\ttraining's rmse: 1.03814\tvalid_1's rmse: 1.09359\n",
      "[189]\ttraining's rmse: 1.03765\tvalid_1's rmse: 1.09309\n",
      "[190]\ttraining's rmse: 1.03726\tvalid_1's rmse: 1.09287\n",
      "[191]\ttraining's rmse: 1.03692\tvalid_1's rmse: 1.09271\n",
      "[192]\ttraining's rmse: 1.03662\tvalid_1's rmse: 1.09255\n",
      "[193]\ttraining's rmse: 1.03623\tvalid_1's rmse: 1.09238\n",
      "[194]\ttraining's rmse: 1.03576\tvalid_1's rmse: 1.09181\n",
      "[195]\ttraining's rmse: 1.03524\tvalid_1's rmse: 1.09155\n",
      "[196]\ttraining's rmse: 1.03473\tvalid_1's rmse: 1.09104\n",
      "[197]\ttraining's rmse: 1.03438\tvalid_1's rmse: 1.09092\n",
      "[198]\ttraining's rmse: 1.03403\tvalid_1's rmse: 1.09075\n",
      "[199]\ttraining's rmse: 1.03356\tvalid_1's rmse: 1.09031\n",
      "[200]\ttraining's rmse: 1.03319\tvalid_1's rmse: 1.0901\n",
      "[201]\ttraining's rmse: 1.03271\tvalid_1's rmse: 1.08983\n",
      "[202]\ttraining's rmse: 1.03238\tvalid_1's rmse: 1.08963\n",
      "[203]\ttraining's rmse: 1.03192\tvalid_1's rmse: 1.08924\n",
      "[204]\ttraining's rmse: 1.03144\tvalid_1's rmse: 1.08877\n",
      "[205]\ttraining's rmse: 1.03111\tvalid_1's rmse: 1.08864\n",
      "[206]\ttraining's rmse: 1.03078\tvalid_1's rmse: 1.08847\n",
      "[207]\ttraining's rmse: 1.03032\tvalid_1's rmse: 1.08822\n",
      "[208]\ttraining's rmse: 1.03004\tvalid_1's rmse: 1.08809\n",
      "[209]\ttraining's rmse: 1.02961\tvalid_1's rmse: 1.08757\n",
      "[210]\ttraining's rmse: 1.02929\tvalid_1's rmse: 1.08742\n",
      "[211]\ttraining's rmse: 1.0289\tvalid_1's rmse: 1.08711\n",
      "[212]\ttraining's rmse: 1.02843\tvalid_1's rmse: 1.08664\n",
      "[213]\ttraining's rmse: 1.02812\tvalid_1's rmse: 1.08653\n",
      "[214]\ttraining's rmse: 1.02768\tvalid_1's rmse: 1.0863\n",
      "[215]\ttraining's rmse: 1.02737\tvalid_1's rmse: 1.08614\n",
      "[216]\ttraining's rmse: 1.02697\tvalid_1's rmse: 1.08571\n",
      "[217]\ttraining's rmse: 1.02647\tvalid_1's rmse: 1.08543\n",
      "[218]\ttraining's rmse: 1.02617\tvalid_1's rmse: 1.08531\n",
      "[219]\ttraining's rmse: 1.02586\tvalid_1's rmse: 1.08523\n",
      "[220]\ttraining's rmse: 1.02538\tvalid_1's rmse: 1.08495\n",
      "[221]\ttraining's rmse: 1.02498\tvalid_1's rmse: 1.08453\n",
      "[222]\ttraining's rmse: 1.02456\tvalid_1's rmse: 1.0841\n",
      "[223]\ttraining's rmse: 1.02426\tvalid_1's rmse: 1.0839\n",
      "[224]\ttraining's rmse: 1.02379\tvalid_1's rmse: 1.08362\n",
      "[225]\ttraining's rmse: 1.0235\tvalid_1's rmse: 1.08349\n",
      "[226]\ttraining's rmse: 1.02312\tvalid_1's rmse: 1.08314\n",
      "[227]\ttraining's rmse: 1.02284\tvalid_1's rmse: 1.08295\n",
      "[228]\ttraining's rmse: 1.02239\tvalid_1's rmse: 1.08269\n",
      "[229]\ttraining's rmse: 1.02211\tvalid_1's rmse: 1.08257\n",
      "[230]\ttraining's rmse: 1.02173\tvalid_1's rmse: 1.08225\n",
      "[231]\ttraining's rmse: 1.02132\tvalid_1's rmse: 1.08182\n",
      "[232]\ttraining's rmse: 1.02089\tvalid_1's rmse: 1.08152\n",
      "[233]\ttraining's rmse: 1.02061\tvalid_1's rmse: 1.0814\n",
      "[234]\ttraining's rmse: 1.02024\tvalid_1's rmse: 1.08109\n",
      "[235]\ttraining's rmse: 1.01982\tvalid_1's rmse: 1.08085\n",
      "[236]\ttraining's rmse: 1.01954\tvalid_1's rmse: 1.08068\n",
      "[237]\ttraining's rmse: 1.0193\tvalid_1's rmse: 1.08051\n",
      "[238]\ttraining's rmse: 1.01896\tvalid_1's rmse: 1.08013\n",
      "[239]\ttraining's rmse: 1.01865\tvalid_1's rmse: 1.07982\n",
      "[240]\ttraining's rmse: 1.01824\tvalid_1's rmse: 1.07957\n",
      "[241]\ttraining's rmse: 1.01796\tvalid_1's rmse: 1.07943\n",
      "[242]\ttraining's rmse: 1.01769\tvalid_1's rmse: 1.07937\n",
      "[243]\ttraining's rmse: 1.01738\tvalid_1's rmse: 1.07913\n",
      "[244]\ttraining's rmse: 1.0171\tvalid_1's rmse: 1.07901\n",
      "[245]\ttraining's rmse: 1.01676\tvalid_1's rmse: 1.07872\n",
      "[246]\ttraining's rmse: 1.01653\tvalid_1's rmse: 1.07856\n",
      "[247]\ttraining's rmse: 1.01623\tvalid_1's rmse: 1.07832\n",
      "[248]\ttraining's rmse: 1.01595\tvalid_1's rmse: 1.07815\n",
      "[249]\ttraining's rmse: 1.0157\tvalid_1's rmse: 1.07804\n",
      "[250]\ttraining's rmse: 1.01537\tvalid_1's rmse: 1.07776\n",
      "[251]\ttraining's rmse: 1.01499\tvalid_1's rmse: 1.07751\n",
      "[252]\ttraining's rmse: 1.01473\tvalid_1's rmse: 1.07737\n",
      "[253]\ttraining's rmse: 1.01438\tvalid_1's rmse: 1.07702\n",
      "[254]\ttraining's rmse: 1.01401\tvalid_1's rmse: 1.07679\n",
      "[255]\ttraining's rmse: 1.0137\tvalid_1's rmse: 1.07651\n",
      "[256]\ttraining's rmse: 1.01343\tvalid_1's rmse: 1.07629\n",
      "[257]\ttraining's rmse: 1.0132\tvalid_1's rmse: 1.07621\n",
      "[258]\ttraining's rmse: 1.01284\tvalid_1's rmse: 1.07601\n",
      "[259]\ttraining's rmse: 1.01254\tvalid_1's rmse: 1.07575\n",
      "[260]\ttraining's rmse: 1.01232\tvalid_1's rmse: 1.07563\n",
      "[261]\ttraining's rmse: 1.01206\tvalid_1's rmse: 1.07545\n",
      "[262]\ttraining's rmse: 1.01181\tvalid_1's rmse: 1.07529\n",
      "[263]\ttraining's rmse: 1.01154\tvalid_1's rmse: 1.07503\n",
      "[264]\ttraining's rmse: 1.0112\tvalid_1's rmse: 1.07479\n",
      "[265]\ttraining's rmse: 1.01096\tvalid_1's rmse: 1.07462\n",
      "[266]\ttraining's rmse: 1.01072\tvalid_1's rmse: 1.07465\n",
      "[267]\ttraining's rmse: 1.01045\tvalid_1's rmse: 1.07436\n",
      "[268]\ttraining's rmse: 1.01022\tvalid_1's rmse: 1.07424\n",
      "[269]\ttraining's rmse: 1.00999\tvalid_1's rmse: 1.07409\n",
      "[270]\ttraining's rmse: 1.00975\tvalid_1's rmse: 1.07391\n",
      "[271]\ttraining's rmse: 1.00954\tvalid_1's rmse: 1.0738\n",
      "[272]\ttraining's rmse: 1.0093\tvalid_1's rmse: 1.07363\n",
      "[273]\ttraining's rmse: 1.00897\tvalid_1's rmse: 1.07343\n",
      "[274]\ttraining's rmse: 1.00875\tvalid_1's rmse: 1.07325\n",
      "[275]\ttraining's rmse: 1.00854\tvalid_1's rmse: 1.07311\n",
      "[276]\ttraining's rmse: 1.00827\tvalid_1's rmse: 1.07288\n",
      "[277]\ttraining's rmse: 1.00803\tvalid_1's rmse: 1.0727\n",
      "[278]\ttraining's rmse: 1.00778\tvalid_1's rmse: 1.07264\n",
      "[279]\ttraining's rmse: 1.00746\tvalid_1's rmse: 1.07241\n",
      "[280]\ttraining's rmse: 1.00725\tvalid_1's rmse: 1.07229\n",
      "[281]\ttraining's rmse: 1.00703\tvalid_1's rmse: 1.07213\n",
      "[282]\ttraining's rmse: 1.00681\tvalid_1's rmse: 1.07214\n",
      "[283]\ttraining's rmse: 1.00655\tvalid_1's rmse: 1.07193\n",
      "[284]\ttraining's rmse: 1.00631\tvalid_1's rmse: 1.07187\n",
      "[285]\ttraining's rmse: 1.0061\tvalid_1's rmse: 1.07171\n",
      "[286]\ttraining's rmse: 1.0059\tvalid_1's rmse: 1.07161\n",
      "[287]\ttraining's rmse: 1.0057\tvalid_1's rmse: 1.07155\n",
      "[288]\ttraining's rmse: 1.00548\tvalid_1's rmse: 1.07155\n",
      "[289]\ttraining's rmse: 1.00528\tvalid_1's rmse: 1.07145\n",
      "[290]\ttraining's rmse: 1.00507\tvalid_1's rmse: 1.0713\n",
      "[291]\ttraining's rmse: 1.00487\tvalid_1's rmse: 1.07118\n",
      "[292]\ttraining's rmse: 1.00467\tvalid_1's rmse: 1.07108\n",
      "[293]\ttraining's rmse: 1.00442\tvalid_1's rmse: 1.07091\n",
      "[294]\ttraining's rmse: 1.0041\tvalid_1's rmse: 1.07065\n",
      "[295]\ttraining's rmse: 1.0039\tvalid_1's rmse: 1.07049\n",
      "[296]\ttraining's rmse: 1.00368\tvalid_1's rmse: 1.07043\n",
      "[297]\ttraining's rmse: 1.00343\tvalid_1's rmse: 1.07024\n",
      "[298]\ttraining's rmse: 1.00322\tvalid_1's rmse: 1.07024\n",
      "[299]\ttraining's rmse: 1.00294\tvalid_1's rmse: 1.07008\n",
      "[300]\ttraining's rmse: 1.00274\tvalid_1's rmse: 1.07001\n",
      "[301]\ttraining's rmse: 1.00254\tvalid_1's rmse: 1.06987\n",
      "[302]\ttraining's rmse: 1.00235\tvalid_1's rmse: 1.06984\n",
      "[303]\ttraining's rmse: 1.00205\tvalid_1's rmse: 1.06959\n",
      "[304]\ttraining's rmse: 1.00177\tvalid_1's rmse: 1.0695\n",
      "[305]\ttraining's rmse: 1.00157\tvalid_1's rmse: 1.06937\n",
      "[306]\ttraining's rmse: 1.00137\tvalid_1's rmse: 1.0694\n",
      "[307]\ttraining's rmse: 1.00116\tvalid_1's rmse: 1.06935\n",
      "[308]\ttraining's rmse: 1.00098\tvalid_1's rmse: 1.06931\n",
      "[309]\ttraining's rmse: 1.0008\tvalid_1's rmse: 1.0692\n",
      "[310]\ttraining's rmse: 1.0006\tvalid_1's rmse: 1.06912\n",
      "[311]\ttraining's rmse: 1.00042\tvalid_1's rmse: 1.06897\n",
      "[312]\ttraining's rmse: 1.00013\tvalid_1's rmse: 1.06873\n",
      "[313]\ttraining's rmse: 0.999964\tvalid_1's rmse: 1.06862\n",
      "[314]\ttraining's rmse: 0.999782\tvalid_1's rmse: 1.06859\n",
      "[315]\ttraining's rmse: 0.999548\tvalid_1's rmse: 1.06843\n",
      "[316]\ttraining's rmse: 0.999375\tvalid_1's rmse: 1.0684\n",
      "[317]\ttraining's rmse: 0.99919\tvalid_1's rmse: 1.06826\n",
      "[318]\ttraining's rmse: 0.99899\tvalid_1's rmse: 1.06817\n",
      "[319]\ttraining's rmse: 0.99871\tvalid_1's rmse: 1.06789\n",
      "[320]\ttraining's rmse: 0.998525\tvalid_1's rmse: 1.06779\n",
      "[321]\ttraining's rmse: 0.998329\tvalid_1's rmse: 1.06772\n",
      "[322]\ttraining's rmse: 0.99806\tvalid_1's rmse: 1.06748\n",
      "[323]\ttraining's rmse: 0.997877\tvalid_1's rmse: 1.06749\n",
      "[324]\ttraining's rmse: 0.997716\tvalid_1's rmse: 1.06737\n",
      "[325]\ttraining's rmse: 0.997548\tvalid_1's rmse: 1.06733\n",
      "[326]\ttraining's rmse: 0.997318\tvalid_1's rmse: 1.06721\n",
      "[327]\ttraining's rmse: 0.99713\tvalid_1's rmse: 1.06713\n",
      "[328]\ttraining's rmse: 0.996953\tvalid_1's rmse: 1.06705\n",
      "[329]\ttraining's rmse: 0.996741\tvalid_1's rmse: 1.06681\n",
      "[330]\ttraining's rmse: 0.996478\tvalid_1's rmse: 1.06654\n",
      "[331]\ttraining's rmse: 0.996283\tvalid_1's rmse: 1.06651\n",
      "[332]\ttraining's rmse: 0.996128\tvalid_1's rmse: 1.06637\n",
      "[333]\ttraining's rmse: 0.99594\tvalid_1's rmse: 1.06632\n",
      "[334]\ttraining's rmse: 0.995687\tvalid_1's rmse: 1.0661\n",
      "[335]\ttraining's rmse: 0.995516\tvalid_1's rmse: 1.06602\n",
      "[336]\ttraining's rmse: 0.995333\tvalid_1's rmse: 1.06597\n",
      "[337]\ttraining's rmse: 0.995144\tvalid_1's rmse: 1.06594\n",
      "[338]\ttraining's rmse: 0.994928\tvalid_1's rmse: 1.06582\n",
      "[339]\ttraining's rmse: 0.99475\tvalid_1's rmse: 1.06576\n",
      "[340]\ttraining's rmse: 0.994501\tvalid_1's rmse: 1.06552\n",
      "[341]\ttraining's rmse: 0.994342\tvalid_1's rmse: 1.06547\n",
      "[342]\ttraining's rmse: 0.994188\tvalid_1's rmse: 1.06535\n",
      "[343]\ttraining's rmse: 0.994011\tvalid_1's rmse: 1.06531\n",
      "[344]\ttraining's rmse: 0.993849\tvalid_1's rmse: 1.06518\n",
      "[345]\ttraining's rmse: 0.993666\tvalid_1's rmse: 1.06515\n",
      "[346]\ttraining's rmse: 0.993424\tvalid_1's rmse: 1.06491\n",
      "[347]\ttraining's rmse: 0.993255\tvalid_1's rmse: 1.06485\n",
      "[348]\ttraining's rmse: 0.993083\tvalid_1's rmse: 1.06481\n",
      "[349]\ttraining's rmse: 0.992932\tvalid_1's rmse: 1.06476\n",
      "[350]\ttraining's rmse: 0.992762\tvalid_1's rmse: 1.06474\n",
      "[351]\ttraining's rmse: 0.99248\tvalid_1's rmse: 1.06456\n",
      "[352]\ttraining's rmse: 0.992332\tvalid_1's rmse: 1.06451\n",
      "[353]\ttraining's rmse: 0.992166\tvalid_1's rmse: 1.06447\n",
      "[354]\ttraining's rmse: 0.991889\tvalid_1's rmse: 1.06424\n",
      "[355]\ttraining's rmse: 0.991727\tvalid_1's rmse: 1.06414\n",
      "[356]\ttraining's rmse: 0.991568\tvalid_1's rmse: 1.06416\n",
      "[357]\ttraining's rmse: 0.991398\tvalid_1's rmse: 1.06413\n",
      "[358]\ttraining's rmse: 0.991128\tvalid_1's rmse: 1.0639\n",
      "[359]\ttraining's rmse: 0.990976\tvalid_1's rmse: 1.06379\n",
      "[360]\ttraining's rmse: 0.990804\tvalid_1's rmse: 1.06377\n",
      "[361]\ttraining's rmse: 0.990634\tvalid_1's rmse: 1.06375\n",
      "[362]\ttraining's rmse: 0.990429\tvalid_1's rmse: 1.06374\n",
      "[363]\ttraining's rmse: 0.990164\tvalid_1's rmse: 1.06349\n",
      "[364]\ttraining's rmse: 0.990021\tvalid_1's rmse: 1.06348\n",
      "[365]\ttraining's rmse: 0.989855\tvalid_1's rmse: 1.06343\n",
      "[366]\ttraining's rmse: 0.989705\tvalid_1's rmse: 1.06335\n",
      "[367]\ttraining's rmse: 0.989448\tvalid_1's rmse: 1.06311\n",
      "[368]\ttraining's rmse: 0.989285\tvalid_1's rmse: 1.06302\n",
      "[369]\ttraining's rmse: 0.989089\tvalid_1's rmse: 1.06297\n",
      "[370]\ttraining's rmse: 0.988931\tvalid_1's rmse: 1.06294\n",
      "[371]\ttraining's rmse: 0.988676\tvalid_1's rmse: 1.06272\n",
      "[372]\ttraining's rmse: 0.988512\tvalid_1's rmse: 1.06271\n",
      "[373]\ttraining's rmse: 0.98837\tvalid_1's rmse: 1.0627\n",
      "[374]\ttraining's rmse: 0.988216\tvalid_1's rmse: 1.06272\n",
      "[375]\ttraining's rmse: 0.987969\tvalid_1's rmse: 1.06249\n",
      "[376]\ttraining's rmse: 0.987778\tvalid_1's rmse: 1.06248\n",
      "[377]\ttraining's rmse: 0.987625\tvalid_1's rmse: 1.06245\n",
      "[378]\ttraining's rmse: 0.987382\tvalid_1's rmse: 1.06225\n",
      "[379]\ttraining's rmse: 0.987227\tvalid_1's rmse: 1.06217\n",
      "[380]\ttraining's rmse: 0.986989\tvalid_1's rmse: 1.06197\n",
      "[381]\ttraining's rmse: 0.986806\tvalid_1's rmse: 1.06193\n",
      "[382]\ttraining's rmse: 0.98667\tvalid_1's rmse: 1.06192\n",
      "[383]\ttraining's rmse: 0.986522\tvalid_1's rmse: 1.0619\n",
      "[384]\ttraining's rmse: 0.986285\tvalid_1's rmse: 1.06167\n",
      "[385]\ttraining's rmse: 0.986129\tvalid_1's rmse: 1.06168\n",
      "[386]\ttraining's rmse: 0.98599\tvalid_1's rmse: 1.0616\n",
      "[387]\ttraining's rmse: 0.985845\tvalid_1's rmse: 1.06154\n",
      "[388]\ttraining's rmse: 0.985666\tvalid_1's rmse: 1.06153\n",
      "[389]\ttraining's rmse: 0.985539\tvalid_1's rmse: 1.06153\n",
      "[390]\ttraining's rmse: 0.985384\tvalid_1's rmse: 1.0615\n",
      "[391]\ttraining's rmse: 0.985225\tvalid_1's rmse: 1.06147\n",
      "[392]\ttraining's rmse: 0.984997\tvalid_1's rmse: 1.0613\n",
      "[393]\ttraining's rmse: 0.984862\tvalid_1's rmse: 1.0613\n",
      "[394]\ttraining's rmse: 0.98469\tvalid_1's rmse: 1.06126\n",
      "[395]\ttraining's rmse: 0.984541\tvalid_1's rmse: 1.06121\n",
      "[396]\ttraining's rmse: 0.984397\tvalid_1's rmse: 1.06114\n",
      "[397]\ttraining's rmse: 0.984175\tvalid_1's rmse: 1.06092\n",
      "[398]\ttraining's rmse: 0.984038\tvalid_1's rmse: 1.06091\n",
      "[399]\ttraining's rmse: 0.983867\tvalid_1's rmse: 1.06087\n",
      "[400]\ttraining's rmse: 0.983728\tvalid_1's rmse: 1.06083\n",
      "[401]\ttraining's rmse: 0.983519\tvalid_1's rmse: 1.06067\n",
      "[402]\ttraining's rmse: 0.983373\tvalid_1's rmse: 1.06069\n",
      "[403]\ttraining's rmse: 0.983227\tvalid_1's rmse: 1.06067\n",
      "[404]\ttraining's rmse: 0.983008\tvalid_1's rmse: 1.06051\n",
      "[405]\ttraining's rmse: 0.982845\tvalid_1's rmse: 1.06049\n",
      "[406]\ttraining's rmse: 0.982715\tvalid_1's rmse: 1.06042\n",
      "[407]\ttraining's rmse: 0.982577\tvalid_1's rmse: 1.06035\n",
      "[408]\ttraining's rmse: 0.982375\tvalid_1's rmse: 1.06021\n",
      "[409]\ttraining's rmse: 0.982225\tvalid_1's rmse: 1.06016\n",
      "[410]\ttraining's rmse: 0.982064\tvalid_1's rmse: 1.06017\n",
      "[411]\ttraining's rmse: 0.981934\tvalid_1's rmse: 1.06012\n",
      "[412]\ttraining's rmse: 0.981802\tvalid_1's rmse: 1.06011\n",
      "[413]\ttraining's rmse: 0.981591\tvalid_1's rmse: 1.05998\n",
      "[414]\ttraining's rmse: 0.981465\tvalid_1's rmse: 1.05993\n",
      "[415]\ttraining's rmse: 0.981339\tvalid_1's rmse: 1.05995\n",
      "[416]\ttraining's rmse: 0.981129\tvalid_1's rmse: 1.05978\n",
      "[417]\ttraining's rmse: 0.980976\tvalid_1's rmse: 1.05979\n",
      "[418]\ttraining's rmse: 0.980783\tvalid_1's rmse: 1.05965\n",
      "[419]\ttraining's rmse: 0.980657\tvalid_1's rmse: 1.05956\n",
      "[420]\ttraining's rmse: 0.980529\tvalid_1's rmse: 1.05947\n",
      "[421]\ttraining's rmse: 0.980373\tvalid_1's rmse: 1.05942\n",
      "[422]\ttraining's rmse: 0.980171\tvalid_1's rmse: 1.05929\n",
      "[423]\ttraining's rmse: 0.980048\tvalid_1's rmse: 1.05929\n",
      "[424]\ttraining's rmse: 0.979919\tvalid_1's rmse: 1.05928\n",
      "[425]\ttraining's rmse: 0.979724\tvalid_1's rmse: 1.05918\n",
      "[426]\ttraining's rmse: 0.979576\tvalid_1's rmse: 1.05922\n",
      "[427]\ttraining's rmse: 0.979448\tvalid_1's rmse: 1.05915\n",
      "[428]\ttraining's rmse: 0.979304\tvalid_1's rmse: 1.05917\n",
      "[429]\ttraining's rmse: 0.97918\tvalid_1's rmse: 1.05913\n",
      "[430]\ttraining's rmse: 0.978982\tvalid_1's rmse: 1.05902\n",
      "[431]\ttraining's rmse: 0.978845\tvalid_1's rmse: 1.05899\n",
      "[432]\ttraining's rmse: 0.978712\tvalid_1's rmse: 1.05901\n",
      "[433]\ttraining's rmse: 0.978596\tvalid_1's rmse: 1.05902\n",
      "[434]\ttraining's rmse: 0.978467\tvalid_1's rmse: 1.05902\n",
      "[435]\ttraining's rmse: 0.978333\tvalid_1's rmse: 1.059\n",
      "[436]\ttraining's rmse: 0.978138\tvalid_1's rmse: 1.05883\n",
      "[437]\ttraining's rmse: 0.977997\tvalid_1's rmse: 1.05879\n",
      "[438]\ttraining's rmse: 0.977811\tvalid_1's rmse: 1.05871\n",
      "[439]\ttraining's rmse: 0.977685\tvalid_1's rmse: 1.05866\n",
      "[440]\ttraining's rmse: 0.977544\tvalid_1's rmse: 1.05871\n",
      "[441]\ttraining's rmse: 0.977412\tvalid_1's rmse: 1.0587\n",
      "[442]\ttraining's rmse: 0.97723\tvalid_1's rmse: 1.05861\n",
      "[443]\ttraining's rmse: 0.977114\tvalid_1's rmse: 1.05855\n",
      "[444]\ttraining's rmse: 0.976977\tvalid_1's rmse: 1.0585\n",
      "[445]\ttraining's rmse: 0.976862\tvalid_1's rmse: 1.05852\n",
      "[446]\ttraining's rmse: 0.976688\tvalid_1's rmse: 1.05842\n",
      "[447]\ttraining's rmse: 0.976552\tvalid_1's rmse: 1.05838\n",
      "[448]\ttraining's rmse: 0.976409\tvalid_1's rmse: 1.05833\n",
      "[449]\ttraining's rmse: 0.976281\tvalid_1's rmse: 1.05833\n",
      "[450]\ttraining's rmse: 0.976156\tvalid_1's rmse: 1.05829\n",
      "[451]\ttraining's rmse: 0.976042\tvalid_1's rmse: 1.05832\n",
      "[452]\ttraining's rmse: 0.975871\tvalid_1's rmse: 1.05823\n",
      "[453]\ttraining's rmse: 0.975731\tvalid_1's rmse: 1.0582\n",
      "[454]\ttraining's rmse: 0.97562\tvalid_1's rmse: 1.05813\n",
      "[455]\ttraining's rmse: 0.975494\tvalid_1's rmse: 1.0581\n",
      "[456]\ttraining's rmse: 0.975379\tvalid_1's rmse: 1.05811\n",
      "[457]\ttraining's rmse: 0.975242\tvalid_1's rmse: 1.0581\n",
      "[458]\ttraining's rmse: 0.975075\tvalid_1's rmse: 1.058\n",
      "[459]\ttraining's rmse: 0.974967\tvalid_1's rmse: 1.05802\n",
      "[460]\ttraining's rmse: 0.974815\tvalid_1's rmse: 1.05789\n",
      "[461]\ttraining's rmse: 0.974704\tvalid_1's rmse: 1.05793\n",
      "[462]\ttraining's rmse: 0.974555\tvalid_1's rmse: 1.05781\n",
      "[463]\ttraining's rmse: 0.974391\tvalid_1's rmse: 1.05766\n",
      "[464]\ttraining's rmse: 0.974255\tvalid_1's rmse: 1.05761\n",
      "[465]\ttraining's rmse: 0.974133\tvalid_1's rmse: 1.05761\n",
      "[466]\ttraining's rmse: 0.974016\tvalid_1's rmse: 1.0576\n",
      "[467]\ttraining's rmse: 0.973891\tvalid_1's rmse: 1.05755\n",
      "[468]\ttraining's rmse: 0.973714\tvalid_1's rmse: 1.05738\n",
      "[469]\ttraining's rmse: 0.973594\tvalid_1's rmse: 1.05741\n",
      "[470]\ttraining's rmse: 0.973464\tvalid_1's rmse: 1.05741\n",
      "[471]\ttraining's rmse: 0.973347\tvalid_1's rmse: 1.05731\n",
      "[472]\ttraining's rmse: 0.973225\tvalid_1's rmse: 1.05729\n",
      "[473]\ttraining's rmse: 0.973087\tvalid_1's rmse: 1.05729\n",
      "[474]\ttraining's rmse: 0.972953\tvalid_1's rmse: 1.05728\n",
      "[475]\ttraining's rmse: 0.97284\tvalid_1's rmse: 1.05727\n",
      "[476]\ttraining's rmse: 0.972724\tvalid_1's rmse: 1.05729\n",
      "[477]\ttraining's rmse: 0.972624\tvalid_1's rmse: 1.05734\n",
      "[478]\ttraining's rmse: 0.972498\tvalid_1's rmse: 1.0573\n",
      "[479]\ttraining's rmse: 0.972368\tvalid_1's rmse: 1.05728\n",
      "[480]\ttraining's rmse: 0.972245\tvalid_1's rmse: 1.0572\n",
      "[481]\ttraining's rmse: 0.97212\tvalid_1's rmse: 1.05717\n",
      "[482]\ttraining's rmse: 0.972005\tvalid_1's rmse: 1.05717\n",
      "[483]\ttraining's rmse: 0.971888\tvalid_1's rmse: 1.05715\n",
      "[484]\ttraining's rmse: 0.97178\tvalid_1's rmse: 1.05712\n",
      "[485]\ttraining's rmse: 0.971675\tvalid_1's rmse: 1.05711\n",
      "[486]\ttraining's rmse: 0.971554\tvalid_1's rmse: 1.05708\n",
      "[487]\ttraining's rmse: 0.971419\tvalid_1's rmse: 1.05699\n",
      "[488]\ttraining's rmse: 0.971299\tvalid_1's rmse: 1.05693\n",
      "[489]\ttraining's rmse: 0.97118\tvalid_1's rmse: 1.05683\n",
      "[490]\ttraining's rmse: 0.971008\tvalid_1's rmse: 1.05673\n",
      "[491]\ttraining's rmse: 0.970876\tvalid_1's rmse: 1.05675\n",
      "[492]\ttraining's rmse: 0.970745\tvalid_1's rmse: 1.05668\n",
      "[493]\ttraining's rmse: 0.970622\tvalid_1's rmse: 1.05666\n",
      "[494]\ttraining's rmse: 0.970456\tvalid_1's rmse: 1.05655\n",
      "[495]\ttraining's rmse: 0.970336\tvalid_1's rmse: 1.0566\n",
      "[496]\ttraining's rmse: 0.970232\tvalid_1's rmse: 1.05652\n",
      "[497]\ttraining's rmse: 0.970103\tvalid_1's rmse: 1.05651\n",
      "[498]\ttraining's rmse: 0.969975\tvalid_1's rmse: 1.05642\n",
      "[499]\ttraining's rmse: 0.969851\tvalid_1's rmse: 1.05639\n",
      "[500]\ttraining's rmse: 0.969748\tvalid_1's rmse: 1.0564\n",
      "[501]\ttraining's rmse: 0.969643\tvalid_1's rmse: 1.05635\n",
      "[502]\ttraining's rmse: 0.969524\tvalid_1's rmse: 1.05634\n",
      "[503]\ttraining's rmse: 0.969361\tvalid_1's rmse: 1.05621\n",
      "[504]\ttraining's rmse: 0.969263\tvalid_1's rmse: 1.05613\n",
      "[505]\ttraining's rmse: 0.969137\tvalid_1's rmse: 1.05611\n",
      "[506]\ttraining's rmse: 0.968982\tvalid_1's rmse: 1.05601\n",
      "[507]\ttraining's rmse: 0.968857\tvalid_1's rmse: 1.05595\n",
      "[508]\ttraining's rmse: 0.968743\tvalid_1's rmse: 1.05594\n",
      "[509]\ttraining's rmse: 0.968614\tvalid_1's rmse: 1.05592\n",
      "[510]\ttraining's rmse: 0.968515\tvalid_1's rmse: 1.05594\n",
      "[511]\ttraining's rmse: 0.968402\tvalid_1's rmse: 1.05591\n",
      "[512]\ttraining's rmse: 0.968247\tvalid_1's rmse: 1.05576\n",
      "[513]\ttraining's rmse: 0.968131\tvalid_1's rmse: 1.05582\n",
      "[514]\ttraining's rmse: 0.968032\tvalid_1's rmse: 1.05571\n",
      "[515]\ttraining's rmse: 0.967908\tvalid_1's rmse: 1.05571\n",
      "[516]\ttraining's rmse: 0.967756\tvalid_1's rmse: 1.05562\n",
      "[517]\ttraining's rmse: 0.967646\tvalid_1's rmse: 1.05561\n",
      "[518]\ttraining's rmse: 0.967539\tvalid_1's rmse: 1.05553\n",
      "[519]\ttraining's rmse: 0.96742\tvalid_1's rmse: 1.05548\n",
      "[520]\ttraining's rmse: 0.967305\tvalid_1's rmse: 1.05546\n",
      "[521]\ttraining's rmse: 0.967147\tvalid_1's rmse: 1.05534\n",
      "[522]\ttraining's rmse: 0.967039\tvalid_1's rmse: 1.05527\n",
      "[523]\ttraining's rmse: 0.966923\tvalid_1's rmse: 1.05532\n",
      "[524]\ttraining's rmse: 0.966819\tvalid_1's rmse: 1.05531\n",
      "[525]\ttraining's rmse: 0.966661\tvalid_1's rmse: 1.0552\n",
      "[526]\ttraining's rmse: 0.966558\tvalid_1's rmse: 1.05515\n",
      "[527]\ttraining's rmse: 0.966442\tvalid_1's rmse: 1.05509\n",
      "[528]\ttraining's rmse: 0.966334\tvalid_1's rmse: 1.05502\n",
      "[529]\ttraining's rmse: 0.96622\tvalid_1's rmse: 1.05508\n",
      "[530]\ttraining's rmse: 0.966098\tvalid_1's rmse: 1.05506\n",
      "[531]\ttraining's rmse: 0.966\tvalid_1's rmse: 1.05506\n",
      "[532]\ttraining's rmse: 0.965844\tvalid_1's rmse: 1.05496\n",
      "[533]\ttraining's rmse: 0.965739\tvalid_1's rmse: 1.05497\n",
      "[534]\ttraining's rmse: 0.965658\tvalid_1's rmse: 1.0549\n",
      "[535]\ttraining's rmse: 0.965542\tvalid_1's rmse: 1.05488\n",
      "[536]\ttraining's rmse: 0.965428\tvalid_1's rmse: 1.05491\n",
      "[537]\ttraining's rmse: 0.965268\tvalid_1's rmse: 1.05483\n",
      "[538]\ttraining's rmse: 0.965163\tvalid_1's rmse: 1.05481\n",
      "[539]\ttraining's rmse: 0.965049\tvalid_1's rmse: 1.05477\n",
      "[540]\ttraining's rmse: 0.964947\tvalid_1's rmse: 1.0547\n",
      "[541]\ttraining's rmse: 0.964838\tvalid_1's rmse: 1.05474\n",
      "[542]\ttraining's rmse: 0.964715\tvalid_1's rmse: 1.05474\n",
      "[543]\ttraining's rmse: 0.964614\tvalid_1's rmse: 1.05468\n",
      "[544]\ttraining's rmse: 0.964502\tvalid_1's rmse: 1.05468\n",
      "[545]\ttraining's rmse: 0.964355\tvalid_1's rmse: 1.05458\n",
      "[546]\ttraining's rmse: 0.964236\tvalid_1's rmse: 1.05456\n",
      "[547]\ttraining's rmse: 0.964136\tvalid_1's rmse: 1.05455\n",
      "[548]\ttraining's rmse: 0.964036\tvalid_1's rmse: 1.05449\n",
      "[549]\ttraining's rmse: 0.963922\tvalid_1's rmse: 1.0545\n",
      "[550]\ttraining's rmse: 0.963844\tvalid_1's rmse: 1.05442\n",
      "[551]\ttraining's rmse: 0.963734\tvalid_1's rmse: 1.05446\n",
      "[552]\ttraining's rmse: 0.963641\tvalid_1's rmse: 1.05446\n",
      "[553]\ttraining's rmse: 0.963535\tvalid_1's rmse: 1.05452\n",
      "[554]\ttraining's rmse: 0.963425\tvalid_1's rmse: 1.0545\n",
      "[555]\ttraining's rmse: 0.963317\tvalid_1's rmse: 1.05446\n",
      "[556]\ttraining's rmse: 0.963219\tvalid_1's rmse: 1.05445\n",
      "[557]\ttraining's rmse: 0.963074\tvalid_1's rmse: 1.0544\n",
      "[558]\ttraining's rmse: 0.962965\tvalid_1's rmse: 1.05429\n",
      "[559]\ttraining's rmse: 0.962853\tvalid_1's rmse: 1.05423\n",
      "[560]\ttraining's rmse: 0.962755\tvalid_1's rmse: 1.05425\n",
      "[561]\ttraining's rmse: 0.962655\tvalid_1's rmse: 1.05419\n",
      "[562]\ttraining's rmse: 0.962559\tvalid_1's rmse: 1.05413\n",
      "[563]\ttraining's rmse: 0.962451\tvalid_1's rmse: 1.05402\n",
      "[564]\ttraining's rmse: 0.962341\tvalid_1's rmse: 1.05402\n",
      "[565]\ttraining's rmse: 0.962233\tvalid_1's rmse: 1.05404\n",
      "[566]\ttraining's rmse: 0.962116\tvalid_1's rmse: 1.05398\n",
      "[567]\ttraining's rmse: 0.961993\tvalid_1's rmse: 1.05395\n",
      "[568]\ttraining's rmse: 0.961899\tvalid_1's rmse: 1.05387\n",
      "[569]\ttraining's rmse: 0.961777\tvalid_1's rmse: 1.05383\n",
      "[570]\ttraining's rmse: 0.961688\tvalid_1's rmse: 1.05383\n",
      "[571]\ttraining's rmse: 0.961588\tvalid_1's rmse: 1.0539\n",
      "[572]\ttraining's rmse: 0.961457\tvalid_1's rmse: 1.0538\n",
      "[573]\ttraining's rmse: 0.961382\tvalid_1's rmse: 1.05374\n",
      "[574]\ttraining's rmse: 0.961283\tvalid_1's rmse: 1.05372\n",
      "[575]\ttraining's rmse: 0.961184\tvalid_1's rmse: 1.05371\n",
      "[576]\ttraining's rmse: 0.961071\tvalid_1's rmse: 1.05372\n",
      "[577]\ttraining's rmse: 0.960957\tvalid_1's rmse: 1.05366\n",
      "[578]\ttraining's rmse: 0.960848\tvalid_1's rmse: 1.0536\n",
      "[579]\ttraining's rmse: 0.960746\tvalid_1's rmse: 1.05356\n",
      "[580]\ttraining's rmse: 0.960667\tvalid_1's rmse: 1.05354\n",
      "[581]\ttraining's rmse: 0.960564\tvalid_1's rmse: 1.05356\n",
      "[582]\ttraining's rmse: 0.960429\tvalid_1's rmse: 1.05347\n",
      "[583]\ttraining's rmse: 0.96032\tvalid_1's rmse: 1.05341\n",
      "[584]\ttraining's rmse: 0.960229\tvalid_1's rmse: 1.05334\n",
      "[585]\ttraining's rmse: 0.96014\tvalid_1's rmse: 1.05328\n",
      "[586]\ttraining's rmse: 0.960074\tvalid_1's rmse: 1.05327\n",
      "[587]\ttraining's rmse: 0.959975\tvalid_1's rmse: 1.05334\n",
      "[588]\ttraining's rmse: 0.959875\tvalid_1's rmse: 1.05333\n",
      "[589]\ttraining's rmse: 0.959766\tvalid_1's rmse: 1.05332\n",
      "[590]\ttraining's rmse: 0.959635\tvalid_1's rmse: 1.05321\n",
      "[591]\ttraining's rmse: 0.95957\tvalid_1's rmse: 1.05322\n",
      "[592]\ttraining's rmse: 0.959461\tvalid_1's rmse: 1.05318\n",
      "[593]\ttraining's rmse: 0.95937\tvalid_1's rmse: 1.05317\n",
      "[594]\ttraining's rmse: 0.959276\tvalid_1's rmse: 1.05316\n",
      "[595]\ttraining's rmse: 0.959143\tvalid_1's rmse: 1.05318\n",
      "[596]\ttraining's rmse: 0.959041\tvalid_1's rmse: 1.05314\n",
      "[597]\ttraining's rmse: 0.958917\tvalid_1's rmse: 1.05305\n",
      "[598]\ttraining's rmse: 0.958854\tvalid_1's rmse: 1.05304\n",
      "[599]\ttraining's rmse: 0.958747\tvalid_1's rmse: 1.05298\n",
      "[600]\ttraining's rmse: 0.958649\tvalid_1's rmse: 1.05299\n",
      "[601]\ttraining's rmse: 0.958511\tvalid_1's rmse: 1.05293\n",
      "[602]\ttraining's rmse: 0.958405\tvalid_1's rmse: 1.05286\n",
      "[603]\ttraining's rmse: 0.958307\tvalid_1's rmse: 1.05284\n",
      "[604]\ttraining's rmse: 0.958217\tvalid_1's rmse: 1.05282\n",
      "[605]\ttraining's rmse: 0.958116\tvalid_1's rmse: 1.05272\n",
      "[606]\ttraining's rmse: 0.958031\tvalid_1's rmse: 1.05265\n",
      "[607]\ttraining's rmse: 0.957933\tvalid_1's rmse: 1.05271\n",
      "[608]\ttraining's rmse: 0.957827\tvalid_1's rmse: 1.05267\n",
      "[609]\ttraining's rmse: 0.957766\tvalid_1's rmse: 1.05267\n",
      "[610]\ttraining's rmse: 0.957675\tvalid_1's rmse: 1.05267\n",
      "[611]\ttraining's rmse: 0.95758\tvalid_1's rmse: 1.05269\n",
      "[612]\ttraining's rmse: 0.957484\tvalid_1's rmse: 1.05265\n",
      "[613]\ttraining's rmse: 0.957382\tvalid_1's rmse: 1.05258\n",
      "[614]\ttraining's rmse: 0.957277\tvalid_1's rmse: 1.05256\n",
      "[615]\ttraining's rmse: 0.957173\tvalid_1's rmse: 1.05256\n",
      "[616]\ttraining's rmse: 0.957086\tvalid_1's rmse: 1.05253\n",
      "[617]\ttraining's rmse: 0.95696\tvalid_1's rmse: 1.05248\n",
      "[618]\ttraining's rmse: 0.956878\tvalid_1's rmse: 1.05246\n",
      "[619]\ttraining's rmse: 0.956774\tvalid_1's rmse: 1.05243\n",
      "[620]\ttraining's rmse: 0.956707\tvalid_1's rmse: 1.05241\n",
      "[621]\ttraining's rmse: 0.95659\tvalid_1's rmse: 1.05233\n",
      "[622]\ttraining's rmse: 0.95652\tvalid_1's rmse: 1.0523\n",
      "[623]\ttraining's rmse: 0.956422\tvalid_1's rmse: 1.0522\n",
      "[624]\ttraining's rmse: 0.95638\tvalid_1's rmse: 1.0522\n",
      "[625]\ttraining's rmse: 0.956321\tvalid_1's rmse: 1.05219\n",
      "[626]\ttraining's rmse: 0.956234\tvalid_1's rmse: 1.05215\n",
      "[627]\ttraining's rmse: 0.956135\tvalid_1's rmse: 1.05211\n",
      "[628]\ttraining's rmse: 0.956056\tvalid_1's rmse: 1.0521\n",
      "[629]\ttraining's rmse: 0.95596\tvalid_1's rmse: 1.052\n",
      "[630]\ttraining's rmse: 0.955835\tvalid_1's rmse: 1.05189\n",
      "[631]\ttraining's rmse: 0.955737\tvalid_1's rmse: 1.05182\n",
      "[632]\ttraining's rmse: 0.95567\tvalid_1's rmse: 1.05178\n",
      "[633]\ttraining's rmse: 0.955577\tvalid_1's rmse: 1.05176\n",
      "[634]\ttraining's rmse: 0.955484\tvalid_1's rmse: 1.05169\n",
      "[635]\ttraining's rmse: 0.955356\tvalid_1's rmse: 1.05163\n",
      "[636]\ttraining's rmse: 0.955264\tvalid_1's rmse: 1.05159\n",
      "[637]\ttraining's rmse: 0.955183\tvalid_1's rmse: 1.05157\n",
      "[638]\ttraining's rmse: 0.955126\tvalid_1's rmse: 1.05155\n",
      "[639]\ttraining's rmse: 0.955003\tvalid_1's rmse: 1.05151\n",
      "[640]\ttraining's rmse: 0.95492\tvalid_1's rmse: 1.05151\n",
      "[641]\ttraining's rmse: 0.954822\tvalid_1's rmse: 1.05149\n",
      "[642]\ttraining's rmse: 0.954729\tvalid_1's rmse: 1.0514\n",
      "[643]\ttraining's rmse: 0.954648\tvalid_1's rmse: 1.05135\n",
      "[644]\ttraining's rmse: 0.954568\tvalid_1's rmse: 1.05135\n",
      "[645]\ttraining's rmse: 0.954489\tvalid_1's rmse: 1.05135\n",
      "[646]\ttraining's rmse: 0.954391\tvalid_1's rmse: 1.05131\n",
      "[647]\ttraining's rmse: 0.954313\tvalid_1's rmse: 1.05125\n",
      "[648]\ttraining's rmse: 0.954209\tvalid_1's rmse: 1.05123\n",
      "[649]\ttraining's rmse: 0.954124\tvalid_1's rmse: 1.05122\n",
      "[650]\ttraining's rmse: 0.954034\tvalid_1's rmse: 1.05119\n",
      "[651]\ttraining's rmse: 0.954002\tvalid_1's rmse: 1.05118\n",
      "[652]\ttraining's rmse: 0.953946\tvalid_1's rmse: 1.05121\n",
      "[653]\ttraining's rmse: 0.95388\tvalid_1's rmse: 1.05121\n",
      "[654]\ttraining's rmse: 0.953768\tvalid_1's rmse: 1.05115\n",
      "[655]\ttraining's rmse: 0.953699\tvalid_1's rmse: 1.05109\n",
      "[656]\ttraining's rmse: 0.953609\tvalid_1's rmse: 1.05111\n",
      "[657]\ttraining's rmse: 0.953508\tvalid_1's rmse: 1.05109\n",
      "[658]\ttraining's rmse: 0.953478\tvalid_1's rmse: 1.05108\n",
      "[659]\ttraining's rmse: 0.953389\tvalid_1's rmse: 1.05101\n",
      "[660]\ttraining's rmse: 0.953287\tvalid_1's rmse: 1.05098\n",
      "[661]\ttraining's rmse: 0.95322\tvalid_1's rmse: 1.05094\n",
      "[662]\ttraining's rmse: 0.953112\tvalid_1's rmse: 1.05086\n",
      "[663]\ttraining's rmse: 0.953022\tvalid_1's rmse: 1.05078\n",
      "[664]\ttraining's rmse: 0.952939\tvalid_1's rmse: 1.05075\n",
      "[665]\ttraining's rmse: 0.952881\tvalid_1's rmse: 1.05076\n",
      "[666]\ttraining's rmse: 0.952804\tvalid_1's rmse: 1.05077\n",
      "[667]\ttraining's rmse: 0.952765\tvalid_1's rmse: 1.05077\n",
      "[668]\ttraining's rmse: 0.952678\tvalid_1's rmse: 1.05072\n",
      "[669]\ttraining's rmse: 0.952621\tvalid_1's rmse: 1.05073\n",
      "[670]\ttraining's rmse: 0.952501\tvalid_1's rmse: 1.05069\n",
      "[671]\ttraining's rmse: 0.952414\tvalid_1's rmse: 1.05059\n",
      "[672]\ttraining's rmse: 0.952335\tvalid_1's rmse: 1.05059\n",
      "[673]\ttraining's rmse: 0.95225\tvalid_1's rmse: 1.05063\n",
      "[674]\ttraining's rmse: 0.952185\tvalid_1's rmse: 1.05059\n",
      "[675]\ttraining's rmse: 0.952118\tvalid_1's rmse: 1.05059\n",
      "[676]\ttraining's rmse: 0.952025\tvalid_1's rmse: 1.05062\n",
      "[677]\ttraining's rmse: 0.951939\tvalid_1's rmse: 1.05064\n",
      "[678]\ttraining's rmse: 0.951886\tvalid_1's rmse: 1.05064\n",
      "[679]\ttraining's rmse: 0.951803\tvalid_1's rmse: 1.05067\n",
      "[680]\ttraining's rmse: 0.951712\tvalid_1's rmse: 1.05064\n",
      "[681]\ttraining's rmse: 0.951625\tvalid_1's rmse: 1.05056\n",
      "[682]\ttraining's rmse: 0.951564\tvalid_1's rmse: 1.05056\n",
      "[683]\ttraining's rmse: 0.951459\tvalid_1's rmse: 1.0505\n",
      "[684]\ttraining's rmse: 0.951379\tvalid_1's rmse: 1.05047\n",
      "[685]\ttraining's rmse: 0.951288\tvalid_1's rmse: 1.05043\n",
      "[686]\ttraining's rmse: 0.951197\tvalid_1's rmse: 1.05036\n",
      "[687]\ttraining's rmse: 0.951169\tvalid_1's rmse: 1.05033\n",
      "[688]\ttraining's rmse: 0.951094\tvalid_1's rmse: 1.0503\n",
      "[689]\ttraining's rmse: 0.951046\tvalid_1's rmse: 1.05029\n",
      "[690]\ttraining's rmse: 0.950936\tvalid_1's rmse: 1.0503\n",
      "[691]\ttraining's rmse: 0.950867\tvalid_1's rmse: 1.0503\n",
      "[692]\ttraining's rmse: 0.950735\tvalid_1's rmse: 1.05023\n",
      "[693]\ttraining's rmse: 0.950646\tvalid_1's rmse: 1.05026\n",
      "[694]\ttraining's rmse: 0.950562\tvalid_1's rmse: 1.05021\n",
      "[695]\ttraining's rmse: 0.950504\tvalid_1's rmse: 1.05021\n",
      "[696]\ttraining's rmse: 0.950455\tvalid_1's rmse: 1.05022\n",
      "[697]\ttraining's rmse: 0.950362\tvalid_1's rmse: 1.05015\n",
      "[698]\ttraining's rmse: 0.950282\tvalid_1's rmse: 1.05016\n",
      "[699]\ttraining's rmse: 0.950217\tvalid_1's rmse: 1.05016\n",
      "[700]\ttraining's rmse: 0.950122\tvalid_1's rmse: 1.05011\n",
      "[701]\ttraining's rmse: 0.950045\tvalid_1's rmse: 1.05013\n",
      "[702]\ttraining's rmse: 0.949965\tvalid_1's rmse: 1.0501\n",
      "[703]\ttraining's rmse: 0.949876\tvalid_1's rmse: 1.05003\n",
      "[704]\ttraining's rmse: 0.949784\tvalid_1's rmse: 1.05005\n",
      "[705]\ttraining's rmse: 0.94973\tvalid_1's rmse: 1.05001\n",
      "[706]\ttraining's rmse: 0.949696\tvalid_1's rmse: 1.05001\n",
      "[707]\ttraining's rmse: 0.949636\tvalid_1's rmse: 1.05002\n",
      "[708]\ttraining's rmse: 0.94959\tvalid_1's rmse: 1.05002\n",
      "[709]\ttraining's rmse: 0.949502\tvalid_1's rmse: 1.04997\n",
      "[710]\ttraining's rmse: 0.949414\tvalid_1's rmse: 1.04994\n",
      "[711]\ttraining's rmse: 0.949359\tvalid_1's rmse: 1.0499\n",
      "[712]\ttraining's rmse: 0.949269\tvalid_1's rmse: 1.04985\n",
      "[713]\ttraining's rmse: 0.949179\tvalid_1's rmse: 1.04978\n",
      "[714]\ttraining's rmse: 0.949146\tvalid_1's rmse: 1.04977\n",
      "[715]\ttraining's rmse: 0.949068\tvalid_1's rmse: 1.04975\n",
      "[716]\ttraining's rmse: 0.949022\tvalid_1's rmse: 1.04973\n",
      "[717]\ttraining's rmse: 0.948921\tvalid_1's rmse: 1.04971\n",
      "[718]\ttraining's rmse: 0.948868\tvalid_1's rmse: 1.04972\n",
      "[719]\ttraining's rmse: 0.948777\tvalid_1's rmse: 1.04963\n",
      "[720]\ttraining's rmse: 0.948691\tvalid_1's rmse: 1.04956\n",
      "[721]\ttraining's rmse: 0.948604\tvalid_1's rmse: 1.04951\n",
      "[722]\ttraining's rmse: 0.948529\tvalid_1's rmse: 1.04953\n",
      "[723]\ttraining's rmse: 0.948458\tvalid_1's rmse: 1.04951\n",
      "[724]\ttraining's rmse: 0.948375\tvalid_1's rmse: 1.04944\n",
      "[725]\ttraining's rmse: 0.948322\tvalid_1's rmse: 1.0494\n",
      "[726]\ttraining's rmse: 0.948233\tvalid_1's rmse: 1.04939\n",
      "[727]\ttraining's rmse: 0.94812\tvalid_1's rmse: 1.0493\n",
      "[728]\ttraining's rmse: 0.948057\tvalid_1's rmse: 1.04931\n",
      "[729]\ttraining's rmse: 0.947964\tvalid_1's rmse: 1.04929\n",
      "[730]\ttraining's rmse: 0.94788\tvalid_1's rmse: 1.04928\n",
      "[731]\ttraining's rmse: 0.94783\tvalid_1's rmse: 1.04922\n",
      "[732]\ttraining's rmse: 0.947773\tvalid_1's rmse: 1.04923\n",
      "[733]\ttraining's rmse: 0.94774\tvalid_1's rmse: 1.04923\n",
      "[734]\ttraining's rmse: 0.947643\tvalid_1's rmse: 1.04918\n",
      "[735]\ttraining's rmse: 0.947594\tvalid_1's rmse: 1.04918\n",
      "[736]\ttraining's rmse: 0.94751\tvalid_1's rmse: 1.04914\n",
      "[737]\ttraining's rmse: 0.947432\tvalid_1's rmse: 1.04915\n",
      "[738]\ttraining's rmse: 0.947379\tvalid_1's rmse: 1.04915\n",
      "[739]\ttraining's rmse: 0.947344\tvalid_1's rmse: 1.04914\n",
      "[740]\ttraining's rmse: 0.947255\tvalid_1's rmse: 1.0491\n",
      "[741]\ttraining's rmse: 0.947175\tvalid_1's rmse: 1.04912\n",
      "[742]\ttraining's rmse: 0.947123\tvalid_1's rmse: 1.04908\n",
      "[743]\ttraining's rmse: 0.947049\tvalid_1's rmse: 1.04904\n",
      "[744]\ttraining's rmse: 0.946964\tvalid_1's rmse: 1.04904\n",
      "[745]\ttraining's rmse: 0.946901\tvalid_1's rmse: 1.04905\n",
      "[746]\ttraining's rmse: 0.946805\tvalid_1's rmse: 1.04903\n",
      "[747]\ttraining's rmse: 0.946742\tvalid_1's rmse: 1.04904\n",
      "[748]\ttraining's rmse: 0.946696\tvalid_1's rmse: 1.04904\n",
      "[749]\ttraining's rmse: 0.946614\tvalid_1's rmse: 1.04899\n",
      "[750]\ttraining's rmse: 0.946525\tvalid_1's rmse: 1.04895\n",
      "[751]\ttraining's rmse: 0.946424\tvalid_1's rmse: 1.0489\n",
      "[752]\ttraining's rmse: 0.94639\tvalid_1's rmse: 1.0489\n",
      "[753]\ttraining's rmse: 0.946288\tvalid_1's rmse: 1.04891\n",
      "[754]\ttraining's rmse: 0.9462\tvalid_1's rmse: 1.04886\n",
      "[755]\ttraining's rmse: 0.946148\tvalid_1's rmse: 1.04882\n",
      "[756]\ttraining's rmse: 0.94611\tvalid_1's rmse: 1.0488\n",
      "[757]\ttraining's rmse: 0.946015\tvalid_1's rmse: 1.04879\n",
      "[758]\ttraining's rmse: 0.945929\tvalid_1's rmse: 1.04873\n",
      "[759]\ttraining's rmse: 0.94583\tvalid_1's rmse: 1.04869\n",
      "[760]\ttraining's rmse: 0.945813\tvalid_1's rmse: 1.04867\n",
      "[761]\ttraining's rmse: 0.945755\tvalid_1's rmse: 1.04863\n",
      "[762]\ttraining's rmse: 0.94566\tvalid_1's rmse: 1.04866\n",
      "[763]\ttraining's rmse: 0.945583\tvalid_1's rmse: 1.04864\n",
      "[764]\ttraining's rmse: 0.945506\tvalid_1's rmse: 1.04862\n",
      "[765]\ttraining's rmse: 0.945444\tvalid_1's rmse: 1.04864\n",
      "[766]\ttraining's rmse: 0.945393\tvalid_1's rmse: 1.04865\n",
      "[767]\ttraining's rmse: 0.945354\tvalid_1's rmse: 1.04862\n",
      "[768]\ttraining's rmse: 0.94524\tvalid_1's rmse: 1.04856\n",
      "[769]\ttraining's rmse: 0.945186\tvalid_1's rmse: 1.04858\n",
      "[770]\ttraining's rmse: 0.945108\tvalid_1's rmse: 1.04858\n",
      "[771]\ttraining's rmse: 0.945025\tvalid_1's rmse: 1.04851\n",
      "[772]\ttraining's rmse: 0.944946\tvalid_1's rmse: 1.04852\n",
      "[773]\ttraining's rmse: 0.94489\tvalid_1's rmse: 1.04849\n",
      "[774]\ttraining's rmse: 0.944838\tvalid_1's rmse: 1.04852\n",
      "[775]\ttraining's rmse: 0.944756\tvalid_1's rmse: 1.04845\n",
      "[776]\ttraining's rmse: 0.944657\tvalid_1's rmse: 1.04841\n",
      "[777]\ttraining's rmse: 0.944641\tvalid_1's rmse: 1.04841\n",
      "[778]\ttraining's rmse: 0.944553\tvalid_1's rmse: 1.04838\n",
      "[779]\ttraining's rmse: 0.944429\tvalid_1's rmse: 1.04834\n",
      "[780]\ttraining's rmse: 0.944363\tvalid_1's rmse: 1.04833\n",
      "[781]\ttraining's rmse: 0.944303\tvalid_1's rmse: 1.04834\n",
      "[782]\ttraining's rmse: 0.944227\tvalid_1's rmse: 1.04833\n",
      "[783]\ttraining's rmse: 0.944178\tvalid_1's rmse: 1.0483\n",
      "[784]\ttraining's rmse: 0.944099\tvalid_1's rmse: 1.04825\n",
      "[785]\ttraining's rmse: 0.943998\tvalid_1's rmse: 1.04827\n",
      "[786]\ttraining's rmse: 0.943912\tvalid_1's rmse: 1.0482\n",
      "[787]\ttraining's rmse: 0.943852\tvalid_1's rmse: 1.04821\n",
      "[788]\ttraining's rmse: 0.943819\tvalid_1's rmse: 1.04821\n",
      "[789]\ttraining's rmse: 0.943766\tvalid_1's rmse: 1.04819\n",
      "[790]\ttraining's rmse: 0.943688\tvalid_1's rmse: 1.04824\n",
      "[791]\ttraining's rmse: 0.94362\tvalid_1's rmse: 1.04825\n",
      "[792]\ttraining's rmse: 0.943526\tvalid_1's rmse: 1.04822\n",
      "[793]\ttraining's rmse: 0.943446\tvalid_1's rmse: 1.04824\n",
      "[794]\ttraining's rmse: 0.943377\tvalid_1's rmse: 1.04821\n",
      "[795]\ttraining's rmse: 0.943284\tvalid_1's rmse: 1.04819\n",
      "[796]\ttraining's rmse: 0.943205\tvalid_1's rmse: 1.04821\n",
      "[797]\ttraining's rmse: 0.943124\tvalid_1's rmse: 1.04818\n",
      "[798]\ttraining's rmse: 0.943026\tvalid_1's rmse: 1.04822\n",
      "[799]\ttraining's rmse: 0.942947\tvalid_1's rmse: 1.04822\n",
      "[800]\ttraining's rmse: 0.942852\tvalid_1's rmse: 1.04822\n",
      "[801]\ttraining's rmse: 0.942779\tvalid_1's rmse: 1.04824\n",
      "[802]\ttraining's rmse: 0.942704\tvalid_1's rmse: 1.04823\n",
      "[803]\ttraining's rmse: 0.942624\tvalid_1's rmse: 1.0482\n",
      "[804]\ttraining's rmse: 0.942535\tvalid_1's rmse: 1.04814\n",
      "[805]\ttraining's rmse: 0.942472\tvalid_1's rmse: 1.04814\n",
      "[806]\ttraining's rmse: 0.942384\tvalid_1's rmse: 1.04807\n",
      "[807]\ttraining's rmse: 0.942304\tvalid_1's rmse: 1.04799\n",
      "[808]\ttraining's rmse: 0.942234\tvalid_1's rmse: 1.04799\n",
      "[809]\ttraining's rmse: 0.942175\tvalid_1's rmse: 1.04797\n",
      "[810]\ttraining's rmse: 0.942086\tvalid_1's rmse: 1.04792\n",
      "[811]\ttraining's rmse: 0.941995\tvalid_1's rmse: 1.04789\n",
      "[812]\ttraining's rmse: 0.941917\tvalid_1's rmse: 1.04788\n",
      "[813]\ttraining's rmse: 0.941832\tvalid_1's rmse: 1.04783\n",
      "[814]\ttraining's rmse: 0.94176\tvalid_1's rmse: 1.04786\n",
      "[815]\ttraining's rmse: 0.941686\tvalid_1's rmse: 1.04783\n",
      "[816]\ttraining's rmse: 0.941635\tvalid_1's rmse: 1.04781\n",
      "[817]\ttraining's rmse: 0.94156\tvalid_1's rmse: 1.04779\n",
      "[818]\ttraining's rmse: 0.941473\tvalid_1's rmse: 1.04777\n",
      "[819]\ttraining's rmse: 0.941366\tvalid_1's rmse: 1.04773\n",
      "[820]\ttraining's rmse: 0.941316\tvalid_1's rmse: 1.04769\n",
      "[821]\ttraining's rmse: 0.94125\tvalid_1's rmse: 1.0477\n",
      "[822]\ttraining's rmse: 0.941167\tvalid_1's rmse: 1.0477\n",
      "[823]\ttraining's rmse: 0.941136\tvalid_1's rmse: 1.0477\n",
      "[824]\ttraining's rmse: 0.941031\tvalid_1's rmse: 1.0477\n",
      "[825]\ttraining's rmse: 0.94095\tvalid_1's rmse: 1.04762\n",
      "[826]\ttraining's rmse: 0.940881\tvalid_1's rmse: 1.04761\n",
      "[827]\ttraining's rmse: 0.940793\tvalid_1's rmse: 1.04759\n",
      "[828]\ttraining's rmse: 0.940688\tvalid_1's rmse: 1.04756\n",
      "[829]\ttraining's rmse: 0.94061\tvalid_1's rmse: 1.04753\n",
      "[830]\ttraining's rmse: 0.940562\tvalid_1's rmse: 1.04754\n",
      "[831]\ttraining's rmse: 0.940515\tvalid_1's rmse: 1.04757\n",
      "[832]\ttraining's rmse: 0.940467\tvalid_1's rmse: 1.04753\n",
      "[833]\ttraining's rmse: 0.940426\tvalid_1's rmse: 1.04752\n",
      "[834]\ttraining's rmse: 0.940351\tvalid_1's rmse: 1.04754\n",
      "[835]\ttraining's rmse: 0.940309\tvalid_1's rmse: 1.04753\n",
      "[836]\ttraining's rmse: 0.940222\tvalid_1's rmse: 1.04753\n",
      "[837]\ttraining's rmse: 0.940152\tvalid_1's rmse: 1.04754\n",
      "[838]\ttraining's rmse: 0.940062\tvalid_1's rmse: 1.04755\n",
      "[839]\ttraining's rmse: 0.939982\tvalid_1's rmse: 1.0475\n",
      "[840]\ttraining's rmse: 0.939905\tvalid_1's rmse: 1.04749\n",
      "[841]\ttraining's rmse: 0.93983\tvalid_1's rmse: 1.04745\n",
      "[842]\ttraining's rmse: 0.939743\tvalid_1's rmse: 1.04747\n",
      "[843]\ttraining's rmse: 0.939641\tvalid_1's rmse: 1.04744\n",
      "[844]\ttraining's rmse: 0.939585\tvalid_1's rmse: 1.04747\n",
      "[845]\ttraining's rmse: 0.939504\tvalid_1's rmse: 1.04746\n",
      "[846]\ttraining's rmse: 0.939461\tvalid_1's rmse: 1.04743\n",
      "[847]\ttraining's rmse: 0.939407\tvalid_1's rmse: 1.04744\n",
      "[848]\ttraining's rmse: 0.939327\tvalid_1's rmse: 1.04745\n",
      "[849]\ttraining's rmse: 0.939257\tvalid_1's rmse: 1.04742\n",
      "[850]\ttraining's rmse: 0.939199\tvalid_1's rmse: 1.04738\n",
      "[851]\ttraining's rmse: 0.939152\tvalid_1's rmse: 1.04739\n",
      "[852]\ttraining's rmse: 0.939065\tvalid_1's rmse: 1.04738\n",
      "[853]\ttraining's rmse: 0.938989\tvalid_1's rmse: 1.04735\n",
      "[854]\ttraining's rmse: 0.938918\tvalid_1's rmse: 1.04732\n",
      "[855]\ttraining's rmse: 0.938849\tvalid_1's rmse: 1.04735\n",
      "[856]\ttraining's rmse: 0.938776\tvalid_1's rmse: 1.04737\n",
      "[857]\ttraining's rmse: 0.938708\tvalid_1's rmse: 1.04735\n",
      "[858]\ttraining's rmse: 0.938646\tvalid_1's rmse: 1.04736\n",
      "[859]\ttraining's rmse: 0.938573\tvalid_1's rmse: 1.04736\n",
      "[860]\ttraining's rmse: 0.938468\tvalid_1's rmse: 1.04731\n",
      "[861]\ttraining's rmse: 0.938368\tvalid_1's rmse: 1.04729\n",
      "[862]\ttraining's rmse: 0.938338\tvalid_1's rmse: 1.04729\n",
      "[863]\ttraining's rmse: 0.93827\tvalid_1's rmse: 1.04733\n",
      "[864]\ttraining's rmse: 0.938203\tvalid_1's rmse: 1.04734\n",
      "[865]\ttraining's rmse: 0.938156\tvalid_1's rmse: 1.04731\n",
      "[866]\ttraining's rmse: 0.938101\tvalid_1's rmse: 1.04734\n",
      "[867]\ttraining's rmse: 0.938018\tvalid_1's rmse: 1.04732\n",
      "[868]\ttraining's rmse: 0.937943\tvalid_1's rmse: 1.04729\n",
      "[869]\ttraining's rmse: 0.937872\tvalid_1's rmse: 1.0473\n",
      "[870]\ttraining's rmse: 0.937818\tvalid_1's rmse: 1.04732\n",
      "[871]\ttraining's rmse: 0.937752\tvalid_1's rmse: 1.04733\n",
      "Early stopping, best iteration is:\n",
      "[861]\ttraining's rmse: 0.938368\tvalid_1's rmse: 1.04729\n",
      "fold_0 coefficients:  [0.52783252 1.6864297  2.04559241]\n",
      "[1]\ttraining's rmse: 1.25299\tvalid_1's rmse: 1.28171\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24929\tvalid_1's rmse: 1.27818\n",
      "[3]\ttraining's rmse: 1.24566\tvalid_1's rmse: 1.27475\n",
      "[4]\ttraining's rmse: 1.24209\tvalid_1's rmse: 1.27139\n",
      "[5]\ttraining's rmse: 1.23858\tvalid_1's rmse: 1.26805\n",
      "[6]\ttraining's rmse: 1.23513\tvalid_1's rmse: 1.26479\n",
      "[7]\ttraining's rmse: 1.23173\tvalid_1's rmse: 1.26157\n",
      "[8]\ttraining's rmse: 1.22839\tvalid_1's rmse: 1.2584\n",
      "[9]\ttraining's rmse: 1.2251\tvalid_1's rmse: 1.25532\n",
      "[10]\ttraining's rmse: 1.22187\tvalid_1's rmse: 1.25226\n",
      "[11]\ttraining's rmse: 1.2187\tvalid_1's rmse: 1.24931\n",
      "[12]\ttraining's rmse: 1.21557\tvalid_1's rmse: 1.24635\n",
      "[13]\ttraining's rmse: 1.2125\tvalid_1's rmse: 1.24346\n",
      "[14]\ttraining's rmse: 1.20949\tvalid_1's rmse: 1.24063\n",
      "[15]\ttraining's rmse: 1.20652\tvalid_1's rmse: 1.23785\n",
      "[16]\ttraining's rmse: 1.2036\tvalid_1's rmse: 1.23513\n",
      "[17]\ttraining's rmse: 1.20074\tvalid_1's rmse: 1.23244\n",
      "[18]\ttraining's rmse: 1.19792\tvalid_1's rmse: 1.22987\n",
      "[19]\ttraining's rmse: 1.19578\tvalid_1's rmse: 1.22763\n",
      "[20]\ttraining's rmse: 1.19368\tvalid_1's rmse: 1.22543\n",
      "[21]\ttraining's rmse: 1.19162\tvalid_1's rmse: 1.22327\n",
      "[22]\ttraining's rmse: 1.18959\tvalid_1's rmse: 1.22115\n",
      "[23]\ttraining's rmse: 1.1876\tvalid_1's rmse: 1.21906\n",
      "[24]\ttraining's rmse: 1.18565\tvalid_1's rmse: 1.21702\n",
      "[25]\ttraining's rmse: 1.18373\tvalid_1's rmse: 1.21501\n",
      "[26]\ttraining's rmse: 1.18184\tvalid_1's rmse: 1.21308\n",
      "[27]\ttraining's rmse: 1.17999\tvalid_1's rmse: 1.21119\n",
      "[28]\ttraining's rmse: 1.17817\tvalid_1's rmse: 1.2093\n",
      "[29]\ttraining's rmse: 1.17638\tvalid_1's rmse: 1.20751\n",
      "[30]\ttraining's rmse: 1.17463\tvalid_1's rmse: 1.20572\n",
      "[31]\ttraining's rmse: 1.17289\tvalid_1's rmse: 1.20405\n",
      "[32]\ttraining's rmse: 1.17119\tvalid_1's rmse: 1.20232\n",
      "[33]\ttraining's rmse: 1.16952\tvalid_1's rmse: 1.20071\n",
      "[34]\ttraining's rmse: 1.16788\tvalid_1's rmse: 1.19899\n",
      "[35]\ttraining's rmse: 1.16627\tvalid_1's rmse: 1.19744\n",
      "[36]\ttraining's rmse: 1.16431\tvalid_1's rmse: 1.19557\n",
      "[37]\ttraining's rmse: 1.16275\tvalid_1's rmse: 1.19396\n",
      "[38]\ttraining's rmse: 1.16122\tvalid_1's rmse: 1.19248\n",
      "[39]\ttraining's rmse: 1.15963\tvalid_1's rmse: 1.19093\n",
      "[40]\ttraining's rmse: 1.15787\tvalid_1's rmse: 1.18939\n",
      "[41]\ttraining's rmse: 1.15632\tvalid_1's rmse: 1.18791\n",
      "[42]\ttraining's rmse: 1.15462\tvalid_1's rmse: 1.18641\n",
      "[43]\ttraining's rmse: 1.15294\tvalid_1's rmse: 1.18494\n",
      "[44]\ttraining's rmse: 1.1513\tvalid_1's rmse: 1.18349\n",
      "[45]\ttraining's rmse: 1.14968\tvalid_1's rmse: 1.18203\n",
      "[46]\ttraining's rmse: 1.14809\tvalid_1's rmse: 1.18059\n",
      "[47]\ttraining's rmse: 1.14654\tvalid_1's rmse: 1.17922\n",
      "[48]\ttraining's rmse: 1.14513\tvalid_1's rmse: 1.17784\n",
      "[49]\ttraining's rmse: 1.14361\tvalid_1's rmse: 1.17645\n",
      "[50]\ttraining's rmse: 1.14213\tvalid_1's rmse: 1.17523\n",
      "[51]\ttraining's rmse: 1.14066\tvalid_1's rmse: 1.17388\n",
      "[52]\ttraining's rmse: 1.13923\tvalid_1's rmse: 1.17262\n",
      "[53]\ttraining's rmse: 1.1379\tvalid_1's rmse: 1.17133\n",
      "[54]\ttraining's rmse: 1.13615\tvalid_1's rmse: 1.16974\n",
      "[55]\ttraining's rmse: 1.13478\tvalid_1's rmse: 1.16862\n",
      "[56]\ttraining's rmse: 1.13367\tvalid_1's rmse: 1.16756\n",
      "[57]\ttraining's rmse: 1.13199\tvalid_1's rmse: 1.16603\n",
      "[58]\ttraining's rmse: 1.13067\tvalid_1's rmse: 1.1649\n",
      "[59]\ttraining's rmse: 1.12961\tvalid_1's rmse: 1.16389\n",
      "[60]\ttraining's rmse: 1.12821\tvalid_1's rmse: 1.16267\n",
      "[61]\ttraining's rmse: 1.12694\tvalid_1's rmse: 1.16152\n",
      "[62]\ttraining's rmse: 1.12593\tvalid_1's rmse: 1.16057\n",
      "[63]\ttraining's rmse: 1.12458\tvalid_1's rmse: 1.1594\n",
      "[64]\ttraining's rmse: 1.12326\tvalid_1's rmse: 1.15826\n",
      "[65]\ttraining's rmse: 1.12228\tvalid_1's rmse: 1.15734\n",
      "[66]\ttraining's rmse: 1.12129\tvalid_1's rmse: 1.1566\n",
      "[67]\ttraining's rmse: 1.12001\tvalid_1's rmse: 1.15549\n",
      "[68]\ttraining's rmse: 1.11908\tvalid_1's rmse: 1.15459\n",
      "[69]\ttraining's rmse: 1.11812\tvalid_1's rmse: 1.1539\n",
      "[70]\ttraining's rmse: 1.11688\tvalid_1's rmse: 1.15289\n",
      "[71]\ttraining's rmse: 1.11599\tvalid_1's rmse: 1.15205\n",
      "[72]\ttraining's rmse: 1.11512\tvalid_1's rmse: 1.15143\n",
      "[73]\ttraining's rmse: 1.11387\tvalid_1's rmse: 1.15029\n",
      "[74]\ttraining's rmse: 1.11261\tvalid_1's rmse: 1.1492\n",
      "[75]\ttraining's rmse: 1.11166\tvalid_1's rmse: 1.14853\n",
      "[76]\ttraining's rmse: 1.1104\tvalid_1's rmse: 1.14735\n",
      "[77]\ttraining's rmse: 1.10947\tvalid_1's rmse: 1.14664\n",
      "[78]\ttraining's rmse: 1.10823\tvalid_1's rmse: 1.14534\n",
      "[79]\ttraining's rmse: 1.10733\tvalid_1's rmse: 1.1447\n",
      "[80]\ttraining's rmse: 1.10612\tvalid_1's rmse: 1.14348\n",
      "[81]\ttraining's rmse: 1.10523\tvalid_1's rmse: 1.14281\n",
      "[82]\ttraining's rmse: 1.10404\tvalid_1's rmse: 1.14159\n",
      "[83]\ttraining's rmse: 1.10306\tvalid_1's rmse: 1.14082\n",
      "[84]\ttraining's rmse: 1.1019\tvalid_1's rmse: 1.13967\n",
      "[85]\ttraining's rmse: 1.10114\tvalid_1's rmse: 1.13911\n",
      "[86]\ttraining's rmse: 1.10001\tvalid_1's rmse: 1.13795\n",
      "[87]\ttraining's rmse: 1.09907\tvalid_1's rmse: 1.13721\n",
      "[88]\ttraining's rmse: 1.09797\tvalid_1's rmse: 1.1363\n",
      "[89]\ttraining's rmse: 1.09686\tvalid_1's rmse: 1.13518\n",
      "[90]\ttraining's rmse: 1.09606\tvalid_1's rmse: 1.13457\n",
      "[91]\ttraining's rmse: 1.09513\tvalid_1's rmse: 1.13393\n",
      "[92]\ttraining's rmse: 1.09403\tvalid_1's rmse: 1.13274\n",
      "[93]\ttraining's rmse: 1.09332\tvalid_1's rmse: 1.13221\n",
      "[94]\ttraining's rmse: 1.09222\tvalid_1's rmse: 1.13135\n",
      "[95]\ttraining's rmse: 1.09118\tvalid_1's rmse: 1.1303\n",
      "[96]\ttraining's rmse: 1.0905\tvalid_1's rmse: 1.12978\n",
      "[97]\ttraining's rmse: 1.08994\tvalid_1's rmse: 1.12944\n",
      "[98]\ttraining's rmse: 1.0889\tvalid_1's rmse: 1.1283\n",
      "[99]\ttraining's rmse: 1.08818\tvalid_1's rmse: 1.12766\n",
      "[100]\ttraining's rmse: 1.08762\tvalid_1's rmse: 1.12732\n",
      "[101]\ttraining's rmse: 1.08663\tvalid_1's rmse: 1.12632\n",
      "[102]\ttraining's rmse: 1.08579\tvalid_1's rmse: 1.12577\n",
      "[103]\ttraining's rmse: 1.08515\tvalid_1's rmse: 1.12529\n",
      "[104]\ttraining's rmse: 1.08416\tvalid_1's rmse: 1.12421\n",
      "[105]\ttraining's rmse: 1.08317\tvalid_1's rmse: 1.12342\n",
      "[106]\ttraining's rmse: 1.0825\tvalid_1's rmse: 1.12281\n",
      "[107]\ttraining's rmse: 1.08154\tvalid_1's rmse: 1.12177\n",
      "[108]\ttraining's rmse: 1.08059\tvalid_1's rmse: 1.12101\n",
      "[109]\ttraining's rmse: 1.0798\tvalid_1's rmse: 1.12034\n",
      "[110]\ttraining's rmse: 1.07897\tvalid_1's rmse: 1.11956\n",
      "[111]\ttraining's rmse: 1.07838\tvalid_1's rmse: 1.11918\n",
      "[112]\ttraining's rmse: 1.07749\tvalid_1's rmse: 1.11832\n",
      "[113]\ttraining's rmse: 1.07658\tvalid_1's rmse: 1.11763\n",
      "[114]\ttraining's rmse: 1.07583\tvalid_1's rmse: 1.11698\n",
      "[115]\ttraining's rmse: 1.07505\tvalid_1's rmse: 1.11627\n",
      "[116]\ttraining's rmse: 1.07447\tvalid_1's rmse: 1.11593\n",
      "[117]\ttraining's rmse: 1.07363\tvalid_1's rmse: 1.11508\n",
      "[118]\ttraining's rmse: 1.07276\tvalid_1's rmse: 1.1144\n",
      "[119]\ttraining's rmse: 1.07189\tvalid_1's rmse: 1.11371\n",
      "[120]\ttraining's rmse: 1.07129\tvalid_1's rmse: 1.11315\n",
      "[121]\ttraining's rmse: 1.07056\tvalid_1's rmse: 1.11247\n",
      "[122]\ttraining's rmse: 1.06991\tvalid_1's rmse: 1.11192\n",
      "[123]\ttraining's rmse: 1.0691\tvalid_1's rmse: 1.11104\n",
      "[124]\ttraining's rmse: 1.06827\tvalid_1's rmse: 1.11034\n",
      "[125]\ttraining's rmse: 1.06746\tvalid_1's rmse: 1.1097\n",
      "[126]\ttraining's rmse: 1.06684\tvalid_1's rmse: 1.10916\n",
      "[127]\ttraining's rmse: 1.06609\tvalid_1's rmse: 1.10843\n",
      "[128]\ttraining's rmse: 1.0653\tvalid_1's rmse: 1.1078\n",
      "[129]\ttraining's rmse: 1.06475\tvalid_1's rmse: 1.10736\n",
      "[130]\ttraining's rmse: 1.06398\tvalid_1's rmse: 1.10674\n",
      "[131]\ttraining's rmse: 1.06339\tvalid_1's rmse: 1.10624\n",
      "[132]\ttraining's rmse: 1.06266\tvalid_1's rmse: 1.10541\n",
      "[133]\ttraining's rmse: 1.06193\tvalid_1's rmse: 1.10491\n",
      "[134]\ttraining's rmse: 1.06136\tvalid_1's rmse: 1.10442\n",
      "[135]\ttraining's rmse: 1.06065\tvalid_1's rmse: 1.10362\n",
      "[136]\ttraining's rmse: 1.06\tvalid_1's rmse: 1.10306\n",
      "[137]\ttraining's rmse: 1.05928\tvalid_1's rmse: 1.10255\n",
      "[138]\ttraining's rmse: 1.05878\tvalid_1's rmse: 1.10228\n",
      "[139]\ttraining's rmse: 1.05809\tvalid_1's rmse: 1.10181\n",
      "[140]\ttraining's rmse: 1.05743\tvalid_1's rmse: 1.10117\n",
      "[141]\ttraining's rmse: 1.05674\tvalid_1's rmse: 1.10062\n",
      "[142]\ttraining's rmse: 1.05621\tvalid_1's rmse: 1.10016\n",
      "[143]\ttraining's rmse: 1.05578\tvalid_1's rmse: 1.09979\n",
      "[144]\ttraining's rmse: 1.05513\tvalid_1's rmse: 1.09909\n",
      "[145]\ttraining's rmse: 1.05454\tvalid_1's rmse: 1.09859\n",
      "[146]\ttraining's rmse: 1.0539\tvalid_1's rmse: 1.09811\n",
      "[147]\ttraining's rmse: 1.05324\tvalid_1's rmse: 1.09767\n",
      "[148]\ttraining's rmse: 1.05273\tvalid_1's rmse: 1.09723\n",
      "[149]\ttraining's rmse: 1.05233\tvalid_1's rmse: 1.09688\n",
      "[150]\ttraining's rmse: 1.05173\tvalid_1's rmse: 1.09633\n",
      "[151]\ttraining's rmse: 1.05122\tvalid_1's rmse: 1.09581\n",
      "[152]\ttraining's rmse: 1.05067\tvalid_1's rmse: 1.09536\n",
      "[153]\ttraining's rmse: 1.05007\tvalid_1's rmse: 1.09486\n",
      "[154]\ttraining's rmse: 1.04944\tvalid_1's rmse: 1.09447\n",
      "[155]\ttraining's rmse: 1.04895\tvalid_1's rmse: 1.09398\n",
      "[156]\ttraining's rmse: 1.04848\tvalid_1's rmse: 1.09364\n",
      "[157]\ttraining's rmse: 1.04811\tvalid_1's rmse: 1.09346\n",
      "[158]\ttraining's rmse: 1.04749\tvalid_1's rmse: 1.09292\n",
      "[159]\ttraining's rmse: 1.04689\tvalid_1's rmse: 1.09251\n",
      "[160]\ttraining's rmse: 1.04632\tvalid_1's rmse: 1.09206\n",
      "[161]\ttraining's rmse: 1.04586\tvalid_1's rmse: 1.09159\n",
      "[162]\ttraining's rmse: 1.0453\tvalid_1's rmse: 1.09097\n",
      "[163]\ttraining's rmse: 1.04488\tvalid_1's rmse: 1.09075\n",
      "[164]\ttraining's rmse: 1.04443\tvalid_1's rmse: 1.09027\n",
      "[165]\ttraining's rmse: 1.04388\tvalid_1's rmse: 1.08963\n",
      "[166]\ttraining's rmse: 1.04355\tvalid_1's rmse: 1.08947\n",
      "[167]\ttraining's rmse: 1.04299\tvalid_1's rmse: 1.08912\n",
      "[168]\ttraining's rmse: 1.04249\tvalid_1's rmse: 1.08873\n",
      "[169]\ttraining's rmse: 1.04204\tvalid_1's rmse: 1.08844\n",
      "[170]\ttraining's rmse: 1.04151\tvalid_1's rmse: 1.08806\n",
      "[171]\ttraining's rmse: 1.04095\tvalid_1's rmse: 1.08756\n",
      "[172]\ttraining's rmse: 1.04052\tvalid_1's rmse: 1.08731\n",
      "[173]\ttraining's rmse: 1.0401\tvalid_1's rmse: 1.08688\n",
      "[174]\ttraining's rmse: 1.03949\tvalid_1's rmse: 1.08643\n",
      "[175]\ttraining's rmse: 1.03896\tvalid_1's rmse: 1.08607\n",
      "[176]\ttraining's rmse: 1.03856\tvalid_1's rmse: 1.08586\n",
      "[177]\ttraining's rmse: 1.03808\tvalid_1's rmse: 1.08541\n",
      "[178]\ttraining's rmse: 1.03766\tvalid_1's rmse: 1.08515\n",
      "[179]\ttraining's rmse: 1.03728\tvalid_1's rmse: 1.0849\n",
      "[180]\ttraining's rmse: 1.03675\tvalid_1's rmse: 1.08446\n",
      "[181]\ttraining's rmse: 1.03618\tvalid_1's rmse: 1.08404\n",
      "[182]\ttraining's rmse: 1.03568\tvalid_1's rmse: 1.08371\n",
      "[183]\ttraining's rmse: 1.03531\tvalid_1's rmse: 1.08347\n",
      "[184]\ttraining's rmse: 1.03501\tvalid_1's rmse: 1.0833\n",
      "[185]\ttraining's rmse: 1.03454\tvalid_1's rmse: 1.08278\n",
      "[186]\ttraining's rmse: 1.03415\tvalid_1's rmse: 1.08245\n",
      "[187]\ttraining's rmse: 1.03377\tvalid_1's rmse: 1.08211\n",
      "[188]\ttraining's rmse: 1.03328\tvalid_1's rmse: 1.08168\n",
      "[189]\ttraining's rmse: 1.03274\tvalid_1's rmse: 1.08126\n",
      "[190]\ttraining's rmse: 1.03235\tvalid_1's rmse: 1.08096\n",
      "[191]\ttraining's rmse: 1.03198\tvalid_1's rmse: 1.08063\n",
      "[192]\ttraining's rmse: 1.03162\tvalid_1's rmse: 1.08043\n",
      "[193]\ttraining's rmse: 1.03118\tvalid_1's rmse: 1.08003\n",
      "[194]\ttraining's rmse: 1.03082\tvalid_1's rmse: 1.07969\n",
      "[195]\ttraining's rmse: 1.03054\tvalid_1's rmse: 1.07956\n",
      "[196]\ttraining's rmse: 1.03019\tvalid_1's rmse: 1.07926\n",
      "[197]\ttraining's rmse: 1.02973\tvalid_1's rmse: 1.07885\n",
      "[198]\ttraining's rmse: 1.02922\tvalid_1's rmse: 1.07846\n",
      "[199]\ttraining's rmse: 1.02876\tvalid_1's rmse: 1.07812\n",
      "[200]\ttraining's rmse: 1.02844\tvalid_1's rmse: 1.07791\n",
      "[201]\ttraining's rmse: 1.02802\tvalid_1's rmse: 1.07751\n",
      "[202]\ttraining's rmse: 1.02776\tvalid_1's rmse: 1.07735\n",
      "[203]\ttraining's rmse: 1.02728\tvalid_1's rmse: 1.07696\n",
      "[204]\ttraining's rmse: 1.02695\tvalid_1's rmse: 1.07675\n",
      "[205]\ttraining's rmse: 1.02651\tvalid_1's rmse: 1.07637\n",
      "[206]\ttraining's rmse: 1.02616\tvalid_1's rmse: 1.07611\n",
      "[207]\ttraining's rmse: 1.02578\tvalid_1's rmse: 1.07592\n",
      "[208]\ttraining's rmse: 1.02545\tvalid_1's rmse: 1.07561\n",
      "[209]\ttraining's rmse: 1.02513\tvalid_1's rmse: 1.0754\n",
      "[210]\ttraining's rmse: 1.02483\tvalid_1's rmse: 1.07523\n",
      "[211]\ttraining's rmse: 1.0244\tvalid_1's rmse: 1.07491\n",
      "[212]\ttraining's rmse: 1.02416\tvalid_1's rmse: 1.07475\n",
      "[213]\ttraining's rmse: 1.02369\tvalid_1's rmse: 1.07438\n",
      "[214]\ttraining's rmse: 1.02328\tvalid_1's rmse: 1.07407\n",
      "[215]\ttraining's rmse: 1.0229\tvalid_1's rmse: 1.07372\n",
      "[216]\ttraining's rmse: 1.02257\tvalid_1's rmse: 1.07345\n",
      "[217]\ttraining's rmse: 1.02228\tvalid_1's rmse: 1.07328\n",
      "[218]\ttraining's rmse: 1.02189\tvalid_1's rmse: 1.07295\n",
      "[219]\ttraining's rmse: 1.0216\tvalid_1's rmse: 1.07278\n",
      "[220]\ttraining's rmse: 1.02121\tvalid_1's rmse: 1.07251\n",
      "[221]\ttraining's rmse: 1.02077\tvalid_1's rmse: 1.07215\n",
      "[222]\ttraining's rmse: 1.02048\tvalid_1's rmse: 1.07201\n",
      "[223]\ttraining's rmse: 1.02013\tvalid_1's rmse: 1.07173\n",
      "[224]\ttraining's rmse: 1.01982\tvalid_1's rmse: 1.07152\n",
      "[225]\ttraining's rmse: 1.01953\tvalid_1's rmse: 1.07137\n",
      "[226]\ttraining's rmse: 1.01915\tvalid_1's rmse: 1.07109\n",
      "[227]\ttraining's rmse: 1.01884\tvalid_1's rmse: 1.07083\n",
      "[228]\ttraining's rmse: 1.01847\tvalid_1's rmse: 1.07052\n",
      "[229]\ttraining's rmse: 1.01806\tvalid_1's rmse: 1.07023\n",
      "[230]\ttraining's rmse: 1.01779\tvalid_1's rmse: 1.06995\n",
      "[231]\ttraining's rmse: 1.01752\tvalid_1's rmse: 1.06974\n",
      "[232]\ttraining's rmse: 1.01715\tvalid_1's rmse: 1.06954\n",
      "[233]\ttraining's rmse: 1.01693\tvalid_1's rmse: 1.06943\n",
      "[234]\ttraining's rmse: 1.01661\tvalid_1's rmse: 1.06917\n",
      "[235]\ttraining's rmse: 1.01632\tvalid_1's rmse: 1.06899\n",
      "[236]\ttraining's rmse: 1.01605\tvalid_1's rmse: 1.06888\n",
      "[237]\ttraining's rmse: 1.01571\tvalid_1's rmse: 1.06863\n",
      "[238]\ttraining's rmse: 1.01532\tvalid_1's rmse: 1.06829\n",
      "[239]\ttraining's rmse: 1.01496\tvalid_1's rmse: 1.06811\n",
      "[240]\ttraining's rmse: 1.01469\tvalid_1's rmse: 1.06799\n",
      "[241]\ttraining's rmse: 1.01444\tvalid_1's rmse: 1.06778\n",
      "[242]\ttraining's rmse: 1.01412\tvalid_1's rmse: 1.06757\n",
      "[243]\ttraining's rmse: 1.01377\tvalid_1's rmse: 1.06736\n",
      "[244]\ttraining's rmse: 1.01351\tvalid_1's rmse: 1.0671\n",
      "[245]\ttraining's rmse: 1.01324\tvalid_1's rmse: 1.06695\n",
      "[246]\ttraining's rmse: 1.01292\tvalid_1's rmse: 1.06673\n",
      "[247]\ttraining's rmse: 1.01266\tvalid_1's rmse: 1.06662\n",
      "[248]\ttraining's rmse: 1.01237\tvalid_1's rmse: 1.06646\n",
      "[249]\ttraining's rmse: 1.01212\tvalid_1's rmse: 1.06624\n",
      "[250]\ttraining's rmse: 1.01189\tvalid_1's rmse: 1.06611\n",
      "[251]\ttraining's rmse: 1.0115\tvalid_1's rmse: 1.06591\n",
      "[252]\ttraining's rmse: 1.01117\tvalid_1's rmse: 1.06573\n",
      "[253]\ttraining's rmse: 1.01094\tvalid_1's rmse: 1.06553\n",
      "[254]\ttraining's rmse: 1.01067\tvalid_1's rmse: 1.0654\n",
      "[255]\ttraining's rmse: 1.01034\tvalid_1's rmse: 1.06521\n",
      "[256]\ttraining's rmse: 1.01006\tvalid_1's rmse: 1.065\n",
      "[257]\ttraining's rmse: 1.00982\tvalid_1's rmse: 1.06483\n",
      "[258]\ttraining's rmse: 1.00954\tvalid_1's rmse: 1.0646\n",
      "[259]\ttraining's rmse: 1.0093\tvalid_1's rmse: 1.06448\n",
      "[260]\ttraining's rmse: 1.00905\tvalid_1's rmse: 1.06442\n",
      "[261]\ttraining's rmse: 1.00869\tvalid_1's rmse: 1.06421\n",
      "[262]\ttraining's rmse: 1.00839\tvalid_1's rmse: 1.06404\n",
      "[263]\ttraining's rmse: 1.00816\tvalid_1's rmse: 1.06393\n",
      "[264]\ttraining's rmse: 1.00789\tvalid_1's rmse: 1.06376\n",
      "[265]\ttraining's rmse: 1.00766\tvalid_1's rmse: 1.06366\n",
      "[266]\ttraining's rmse: 1.00736\tvalid_1's rmse: 1.06351\n",
      "[267]\ttraining's rmse: 1.00713\tvalid_1's rmse: 1.06332\n",
      "[268]\ttraining's rmse: 1.00691\tvalid_1's rmse: 1.06321\n",
      "[269]\ttraining's rmse: 1.00657\tvalid_1's rmse: 1.06305\n",
      "[270]\ttraining's rmse: 1.00635\tvalid_1's rmse: 1.06294\n",
      "[271]\ttraining's rmse: 1.0061\tvalid_1's rmse: 1.06275\n",
      "[272]\ttraining's rmse: 1.00589\tvalid_1's rmse: 1.06268\n",
      "[273]\ttraining's rmse: 1.0056\tvalid_1's rmse: 1.06254\n",
      "[274]\ttraining's rmse: 1.00539\tvalid_1's rmse: 1.06245\n",
      "[275]\ttraining's rmse: 1.00507\tvalid_1's rmse: 1.06228\n",
      "[276]\ttraining's rmse: 1.00483\tvalid_1's rmse: 1.06212\n",
      "[277]\ttraining's rmse: 1.00463\tvalid_1's rmse: 1.06202\n",
      "[278]\ttraining's rmse: 1.00432\tvalid_1's rmse: 1.06183\n",
      "[279]\ttraining's rmse: 1.00408\tvalid_1's rmse: 1.06168\n",
      "[280]\ttraining's rmse: 1.00377\tvalid_1's rmse: 1.06151\n",
      "[281]\ttraining's rmse: 1.0035\tvalid_1's rmse: 1.06132\n",
      "[282]\ttraining's rmse: 1.0033\tvalid_1's rmse: 1.06125\n",
      "[283]\ttraining's rmse: 1.00307\tvalid_1's rmse: 1.06112\n",
      "[284]\ttraining's rmse: 1.00287\tvalid_1's rmse: 1.06099\n",
      "[285]\ttraining's rmse: 1.00257\tvalid_1's rmse: 1.06081\n",
      "[286]\ttraining's rmse: 1.00235\tvalid_1's rmse: 1.06064\n",
      "[287]\ttraining's rmse: 1.00206\tvalid_1's rmse: 1.06048\n",
      "[288]\ttraining's rmse: 1.00185\tvalid_1's rmse: 1.06038\n",
      "[289]\ttraining's rmse: 1.00164\tvalid_1's rmse: 1.06029\n",
      "[290]\ttraining's rmse: 1.00141\tvalid_1's rmse: 1.06014\n",
      "[291]\ttraining's rmse: 1.00121\tvalid_1's rmse: 1.06008\n",
      "[292]\ttraining's rmse: 1.001\tvalid_1's rmse: 1.06007\n",
      "[293]\ttraining's rmse: 1.00072\tvalid_1's rmse: 1.05995\n",
      "[294]\ttraining's rmse: 1.0005\tvalid_1's rmse: 1.05981\n",
      "[295]\ttraining's rmse: 1.00022\tvalid_1's rmse: 1.05965\n",
      "[296]\ttraining's rmse: 0.999977\tvalid_1's rmse: 1.05948\n",
      "[297]\ttraining's rmse: 0.99978\tvalid_1's rmse: 1.05944\n",
      "[298]\ttraining's rmse: 0.999583\tvalid_1's rmse: 1.05935\n",
      "[299]\ttraining's rmse: 0.999398\tvalid_1's rmse: 1.05929\n",
      "[300]\ttraining's rmse: 0.999134\tvalid_1's rmse: 1.05917\n",
      "[301]\ttraining's rmse: 0.99892\tvalid_1's rmse: 1.05909\n",
      "[302]\ttraining's rmse: 0.998712\tvalid_1's rmse: 1.05897\n",
      "[303]\ttraining's rmse: 0.998528\tvalid_1's rmse: 1.05891\n",
      "[304]\ttraining's rmse: 0.998264\tvalid_1's rmse: 1.05876\n",
      "[305]\ttraining's rmse: 0.998074\tvalid_1's rmse: 1.05861\n",
      "[306]\ttraining's rmse: 0.997886\tvalid_1's rmse: 1.05849\n",
      "[307]\ttraining's rmse: 0.997659\tvalid_1's rmse: 1.05833\n",
      "[308]\ttraining's rmse: 0.997487\tvalid_1's rmse: 1.0583\n",
      "[309]\ttraining's rmse: 0.997231\tvalid_1's rmse: 1.05814\n",
      "[310]\ttraining's rmse: 0.997005\tvalid_1's rmse: 1.058\n",
      "[311]\ttraining's rmse: 0.996816\tvalid_1's rmse: 1.05799\n",
      "[312]\ttraining's rmse: 0.996619\tvalid_1's rmse: 1.05786\n",
      "[313]\ttraining's rmse: 0.996402\tvalid_1's rmse: 1.0578\n",
      "[314]\ttraining's rmse: 0.99622\tvalid_1's rmse: 1.05772\n",
      "[315]\ttraining's rmse: 0.996024\tvalid_1's rmse: 1.05761\n",
      "[316]\ttraining's rmse: 0.995838\tvalid_1's rmse: 1.05749\n",
      "[317]\ttraining's rmse: 0.995637\tvalid_1's rmse: 1.05738\n",
      "[318]\ttraining's rmse: 0.995445\tvalid_1's rmse: 1.05724\n",
      "[319]\ttraining's rmse: 0.995276\tvalid_1's rmse: 1.05717\n",
      "[320]\ttraining's rmse: 0.995086\tvalid_1's rmse: 1.05709\n",
      "[321]\ttraining's rmse: 0.99488\tvalid_1's rmse: 1.057\n",
      "[322]\ttraining's rmse: 0.994685\tvalid_1's rmse: 1.05691\n",
      "[323]\ttraining's rmse: 0.994518\tvalid_1's rmse: 1.05687\n",
      "[324]\ttraining's rmse: 0.994317\tvalid_1's rmse: 1.05678\n",
      "[325]\ttraining's rmse: 0.994109\tvalid_1's rmse: 1.05659\n",
      "[326]\ttraining's rmse: 0.993928\tvalid_1's rmse: 1.0565\n",
      "[327]\ttraining's rmse: 0.993739\tvalid_1's rmse: 1.05641\n",
      "[328]\ttraining's rmse: 0.993578\tvalid_1's rmse: 1.05635\n",
      "[329]\ttraining's rmse: 0.993392\tvalid_1's rmse: 1.0562\n",
      "[330]\ttraining's rmse: 0.993188\tvalid_1's rmse: 1.05612\n",
      "[331]\ttraining's rmse: 0.993004\tvalid_1's rmse: 1.05603\n",
      "[332]\ttraining's rmse: 0.992792\tvalid_1's rmse: 1.0558\n",
      "[333]\ttraining's rmse: 0.992553\tvalid_1's rmse: 1.05568\n",
      "[334]\ttraining's rmse: 0.992356\tvalid_1's rmse: 1.0556\n",
      "[335]\ttraining's rmse: 0.992146\tvalid_1's rmse: 1.05541\n",
      "[336]\ttraining's rmse: 0.991968\tvalid_1's rmse: 1.05534\n",
      "[337]\ttraining's rmse: 0.991745\tvalid_1's rmse: 1.05525\n",
      "[338]\ttraining's rmse: 0.991566\tvalid_1's rmse: 1.05512\n",
      "[339]\ttraining's rmse: 0.991396\tvalid_1's rmse: 1.05504\n",
      "[340]\ttraining's rmse: 0.991222\tvalid_1's rmse: 1.05496\n",
      "[341]\ttraining's rmse: 0.991036\tvalid_1's rmse: 1.05485\n",
      "[342]\ttraining's rmse: 0.990863\tvalid_1's rmse: 1.05472\n",
      "[343]\ttraining's rmse: 0.990662\tvalid_1's rmse: 1.05451\n",
      "[344]\ttraining's rmse: 0.990439\tvalid_1's rmse: 1.05435\n",
      "[345]\ttraining's rmse: 0.990275\tvalid_1's rmse: 1.05426\n",
      "[346]\ttraining's rmse: 0.990106\tvalid_1's rmse: 1.05418\n",
      "[347]\ttraining's rmse: 0.989905\tvalid_1's rmse: 1.05404\n",
      "[348]\ttraining's rmse: 0.989684\tvalid_1's rmse: 1.05396\n",
      "[349]\ttraining's rmse: 0.989518\tvalid_1's rmse: 1.05383\n",
      "[350]\ttraining's rmse: 0.989336\tvalid_1's rmse: 1.05379\n",
      "[351]\ttraining's rmse: 0.989135\tvalid_1's rmse: 1.05365\n",
      "[352]\ttraining's rmse: 0.988972\tvalid_1's rmse: 1.05358\n",
      "[353]\ttraining's rmse: 0.988809\tvalid_1's rmse: 1.05352\n",
      "[354]\ttraining's rmse: 0.988655\tvalid_1's rmse: 1.05342\n",
      "[355]\ttraining's rmse: 0.988448\tvalid_1's rmse: 1.05327\n",
      "[356]\ttraining's rmse: 0.988276\tvalid_1's rmse: 1.05318\n",
      "[357]\ttraining's rmse: 0.988114\tvalid_1's rmse: 1.05315\n",
      "[358]\ttraining's rmse: 0.987922\tvalid_1's rmse: 1.05301\n",
      "[359]\ttraining's rmse: 0.987769\tvalid_1's rmse: 1.05289\n",
      "[360]\ttraining's rmse: 0.987598\tvalid_1's rmse: 1.05283\n",
      "[361]\ttraining's rmse: 0.987436\tvalid_1's rmse: 1.05274\n",
      "[362]\ttraining's rmse: 0.987283\tvalid_1's rmse: 1.0526\n",
      "[363]\ttraining's rmse: 0.987126\tvalid_1's rmse: 1.05254\n",
      "[364]\ttraining's rmse: 0.986913\tvalid_1's rmse: 1.05241\n",
      "[365]\ttraining's rmse: 0.98673\tvalid_1's rmse: 1.05222\n",
      "[366]\ttraining's rmse: 0.986545\tvalid_1's rmse: 1.05203\n",
      "[367]\ttraining's rmse: 0.9864\tvalid_1's rmse: 1.05194\n",
      "[368]\ttraining's rmse: 0.986198\tvalid_1's rmse: 1.05185\n",
      "[369]\ttraining's rmse: 0.986017\tvalid_1's rmse: 1.05172\n",
      "[370]\ttraining's rmse: 0.985852\tvalid_1's rmse: 1.05162\n",
      "[371]\ttraining's rmse: 0.985712\tvalid_1's rmse: 1.05148\n",
      "[372]\ttraining's rmse: 0.985568\tvalid_1's rmse: 1.05137\n",
      "[373]\ttraining's rmse: 0.985403\tvalid_1's rmse: 1.05135\n",
      "[374]\ttraining's rmse: 0.985246\tvalid_1's rmse: 1.05132\n",
      "[375]\ttraining's rmse: 0.985098\tvalid_1's rmse: 1.05127\n",
      "[376]\ttraining's rmse: 0.98491\tvalid_1's rmse: 1.05114\n",
      "[377]\ttraining's rmse: 0.984767\tvalid_1's rmse: 1.05105\n",
      "[378]\ttraining's rmse: 0.984593\tvalid_1's rmse: 1.05095\n",
      "[379]\ttraining's rmse: 0.984436\tvalid_1's rmse: 1.05088\n",
      "[380]\ttraining's rmse: 0.984294\tvalid_1's rmse: 1.05082\n",
      "[381]\ttraining's rmse: 0.984126\tvalid_1's rmse: 1.05073\n",
      "[382]\ttraining's rmse: 0.983953\tvalid_1's rmse: 1.05058\n",
      "[383]\ttraining's rmse: 0.983798\tvalid_1's rmse: 1.05053\n",
      "[384]\ttraining's rmse: 0.983661\tvalid_1's rmse: 1.05045\n",
      "[385]\ttraining's rmse: 0.983503\tvalid_1's rmse: 1.05039\n",
      "[386]\ttraining's rmse: 0.983333\tvalid_1's rmse: 1.05029\n",
      "[387]\ttraining's rmse: 0.9832\tvalid_1's rmse: 1.0502\n",
      "[388]\ttraining's rmse: 0.983031\tvalid_1's rmse: 1.05014\n",
      "[389]\ttraining's rmse: 0.982883\tvalid_1's rmse: 1.05012\n",
      "[390]\ttraining's rmse: 0.982716\tvalid_1's rmse: 1.04998\n",
      "[391]\ttraining's rmse: 0.982558\tvalid_1's rmse: 1.04992\n",
      "[392]\ttraining's rmse: 0.982421\tvalid_1's rmse: 1.04988\n",
      "[393]\ttraining's rmse: 0.982283\tvalid_1's rmse: 1.04984\n",
      "[394]\ttraining's rmse: 0.982092\tvalid_1's rmse: 1.04974\n",
      "[395]\ttraining's rmse: 0.981931\tvalid_1's rmse: 1.04966\n",
      "[396]\ttraining's rmse: 0.98177\tvalid_1's rmse: 1.0495\n",
      "[397]\ttraining's rmse: 0.981642\tvalid_1's rmse: 1.04942\n",
      "[398]\ttraining's rmse: 0.981506\tvalid_1's rmse: 1.04938\n",
      "[399]\ttraining's rmse: 0.981356\tvalid_1's rmse: 1.04933\n",
      "[400]\ttraining's rmse: 0.981201\tvalid_1's rmse: 1.04924\n",
      "[401]\ttraining's rmse: 0.981077\tvalid_1's rmse: 1.04911\n",
      "[402]\ttraining's rmse: 0.9809\tvalid_1's rmse: 1.04906\n",
      "[403]\ttraining's rmse: 0.980759\tvalid_1's rmse: 1.04899\n",
      "[404]\ttraining's rmse: 0.980605\tvalid_1's rmse: 1.04888\n",
      "[405]\ttraining's rmse: 0.980444\tvalid_1's rmse: 1.04884\n",
      "[406]\ttraining's rmse: 0.9803\tvalid_1's rmse: 1.04875\n",
      "[407]\ttraining's rmse: 0.980155\tvalid_1's rmse: 1.04864\n",
      "[408]\ttraining's rmse: 0.98002\tvalid_1's rmse: 1.04861\n",
      "[409]\ttraining's rmse: 0.979866\tvalid_1's rmse: 1.04854\n",
      "[410]\ttraining's rmse: 0.979717\tvalid_1's rmse: 1.04836\n",
      "[411]\ttraining's rmse: 0.979584\tvalid_1's rmse: 1.04831\n",
      "[412]\ttraining's rmse: 0.979464\tvalid_1's rmse: 1.04819\n",
      "[413]\ttraining's rmse: 0.979317\tvalid_1's rmse: 1.04814\n",
      "[414]\ttraining's rmse: 0.979168\tvalid_1's rmse: 1.04804\n",
      "[415]\ttraining's rmse: 0.978997\tvalid_1's rmse: 1.04796\n",
      "[416]\ttraining's rmse: 0.978852\tvalid_1's rmse: 1.04784\n",
      "[417]\ttraining's rmse: 0.978731\tvalid_1's rmse: 1.04779\n",
      "[418]\ttraining's rmse: 0.978593\tvalid_1's rmse: 1.0477\n",
      "[419]\ttraining's rmse: 0.97845\tvalid_1's rmse: 1.04765\n",
      "[420]\ttraining's rmse: 0.978321\tvalid_1's rmse: 1.04763\n",
      "[421]\ttraining's rmse: 0.978181\tvalid_1's rmse: 1.04749\n",
      "[422]\ttraining's rmse: 0.977998\tvalid_1's rmse: 1.04737\n",
      "[423]\ttraining's rmse: 0.977881\tvalid_1's rmse: 1.04729\n",
      "[424]\ttraining's rmse: 0.977738\tvalid_1's rmse: 1.0472\n",
      "[425]\ttraining's rmse: 0.977579\tvalid_1's rmse: 1.04714\n",
      "[426]\ttraining's rmse: 0.977438\tvalid_1's rmse: 1.04699\n",
      "[427]\ttraining's rmse: 0.977303\tvalid_1's rmse: 1.04694\n",
      "[428]\ttraining's rmse: 0.977177\tvalid_1's rmse: 1.04689\n",
      "[429]\ttraining's rmse: 0.97704\tvalid_1's rmse: 1.04685\n",
      "[430]\ttraining's rmse: 0.976901\tvalid_1's rmse: 1.0468\n",
      "[431]\ttraining's rmse: 0.976767\tvalid_1's rmse: 1.04675\n",
      "[432]\ttraining's rmse: 0.976611\tvalid_1's rmse: 1.04668\n",
      "[433]\ttraining's rmse: 0.976472\tvalid_1's rmse: 1.04663\n",
      "[434]\ttraining's rmse: 0.976348\tvalid_1's rmse: 1.0466\n",
      "[435]\ttraining's rmse: 0.976184\tvalid_1's rmse: 1.04651\n",
      "[436]\ttraining's rmse: 0.976055\tvalid_1's rmse: 1.04646\n",
      "[437]\ttraining's rmse: 0.975919\tvalid_1's rmse: 1.04643\n",
      "[438]\ttraining's rmse: 0.975787\tvalid_1's rmse: 1.04639\n",
      "[439]\ttraining's rmse: 0.975681\tvalid_1's rmse: 1.04636\n",
      "[440]\ttraining's rmse: 0.975562\tvalid_1's rmse: 1.04622\n",
      "[441]\ttraining's rmse: 0.975393\tvalid_1's rmse: 1.04612\n",
      "[442]\ttraining's rmse: 0.975271\tvalid_1's rmse: 1.04609\n",
      "[443]\ttraining's rmse: 0.975162\tvalid_1's rmse: 1.04604\n",
      "[444]\ttraining's rmse: 0.975029\tvalid_1's rmse: 1.04599\n",
      "[445]\ttraining's rmse: 0.974899\tvalid_1's rmse: 1.04596\n",
      "[446]\ttraining's rmse: 0.974783\tvalid_1's rmse: 1.04588\n",
      "[447]\ttraining's rmse: 0.974663\tvalid_1's rmse: 1.04587\n",
      "[448]\ttraining's rmse: 0.974506\tvalid_1's rmse: 1.0458\n",
      "[449]\ttraining's rmse: 0.974369\tvalid_1's rmse: 1.04575\n",
      "[450]\ttraining's rmse: 0.974256\tvalid_1's rmse: 1.04575\n",
      "[451]\ttraining's rmse: 0.974169\tvalid_1's rmse: 1.04574\n",
      "[452]\ttraining's rmse: 0.974036\tvalid_1's rmse: 1.04572\n",
      "[453]\ttraining's rmse: 0.973919\tvalid_1's rmse: 1.0457\n",
      "[454]\ttraining's rmse: 0.973807\tvalid_1's rmse: 1.04561\n",
      "[455]\ttraining's rmse: 0.97366\tvalid_1's rmse: 1.04554\n",
      "[456]\ttraining's rmse: 0.973513\tvalid_1's rmse: 1.04546\n",
      "[457]\ttraining's rmse: 0.973397\tvalid_1's rmse: 1.04545\n",
      "[458]\ttraining's rmse: 0.973262\tvalid_1's rmse: 1.04541\n",
      "[459]\ttraining's rmse: 0.973135\tvalid_1's rmse: 1.04535\n",
      "[460]\ttraining's rmse: 0.973043\tvalid_1's rmse: 1.04532\n",
      "[461]\ttraining's rmse: 0.972935\tvalid_1's rmse: 1.04531\n",
      "[462]\ttraining's rmse: 0.972781\tvalid_1's rmse: 1.04522\n",
      "[463]\ttraining's rmse: 0.972654\tvalid_1's rmse: 1.04518\n",
      "[464]\ttraining's rmse: 0.972554\tvalid_1's rmse: 1.04511\n",
      "[465]\ttraining's rmse: 0.972434\tvalid_1's rmse: 1.04508\n",
      "[466]\ttraining's rmse: 0.972326\tvalid_1's rmse: 1.04507\n",
      "[467]\ttraining's rmse: 0.972183\tvalid_1's rmse: 1.04499\n",
      "[468]\ttraining's rmse: 0.972052\tvalid_1's rmse: 1.04498\n",
      "[469]\ttraining's rmse: 0.971929\tvalid_1's rmse: 1.04496\n",
      "[470]\ttraining's rmse: 0.97184\tvalid_1's rmse: 1.04494\n",
      "[471]\ttraining's rmse: 0.971732\tvalid_1's rmse: 1.04491\n",
      "[472]\ttraining's rmse: 0.971581\tvalid_1's rmse: 1.04483\n",
      "[473]\ttraining's rmse: 0.971452\tvalid_1's rmse: 1.04484\n",
      "[474]\ttraining's rmse: 0.971324\tvalid_1's rmse: 1.0447\n",
      "[475]\ttraining's rmse: 0.971238\tvalid_1's rmse: 1.0447\n",
      "[476]\ttraining's rmse: 0.971105\tvalid_1's rmse: 1.04466\n",
      "[477]\ttraining's rmse: 0.971003\tvalid_1's rmse: 1.04459\n",
      "[478]\ttraining's rmse: 0.97089\tvalid_1's rmse: 1.04458\n",
      "[479]\ttraining's rmse: 0.970782\tvalid_1's rmse: 1.04459\n",
      "[480]\ttraining's rmse: 0.970696\tvalid_1's rmse: 1.04456\n",
      "[481]\ttraining's rmse: 0.970574\tvalid_1's rmse: 1.04454\n",
      "[482]\ttraining's rmse: 0.970463\tvalid_1's rmse: 1.04451\n",
      "[483]\ttraining's rmse: 0.970317\tvalid_1's rmse: 1.04444\n",
      "[484]\ttraining's rmse: 0.970224\tvalid_1's rmse: 1.0444\n",
      "[485]\ttraining's rmse: 0.970121\tvalid_1's rmse: 1.04441\n",
      "[486]\ttraining's rmse: 0.969997\tvalid_1's rmse: 1.04427\n",
      "[487]\ttraining's rmse: 0.969879\tvalid_1's rmse: 1.0442\n",
      "[488]\ttraining's rmse: 0.969746\tvalid_1's rmse: 1.04414\n",
      "[489]\ttraining's rmse: 0.969667\tvalid_1's rmse: 1.04413\n",
      "[490]\ttraining's rmse: 0.969575\tvalid_1's rmse: 1.04405\n",
      "[491]\ttraining's rmse: 0.969468\tvalid_1's rmse: 1.04405\n",
      "[492]\ttraining's rmse: 0.969363\tvalid_1's rmse: 1.04404\n",
      "[493]\ttraining's rmse: 0.969229\tvalid_1's rmse: 1.04397\n",
      "[494]\ttraining's rmse: 0.969107\tvalid_1's rmse: 1.04389\n",
      "[495]\ttraining's rmse: 0.968967\tvalid_1's rmse: 1.04381\n",
      "[496]\ttraining's rmse: 0.968878\tvalid_1's rmse: 1.04375\n",
      "[497]\ttraining's rmse: 0.968774\tvalid_1's rmse: 1.04376\n",
      "[498]\ttraining's rmse: 0.968677\tvalid_1's rmse: 1.04377\n",
      "[499]\ttraining's rmse: 0.968561\tvalid_1's rmse: 1.04374\n",
      "[500]\ttraining's rmse: 0.968482\tvalid_1's rmse: 1.0437\n",
      "[501]\ttraining's rmse: 0.968346\tvalid_1's rmse: 1.04362\n",
      "[502]\ttraining's rmse: 0.968271\tvalid_1's rmse: 1.04355\n",
      "[503]\ttraining's rmse: 0.968178\tvalid_1's rmse: 1.04345\n",
      "[504]\ttraining's rmse: 0.968055\tvalid_1's rmse: 1.04343\n",
      "[505]\ttraining's rmse: 0.967954\tvalid_1's rmse: 1.04342\n",
      "[506]\ttraining's rmse: 0.967831\tvalid_1's rmse: 1.04337\n",
      "[507]\ttraining's rmse: 0.96775\tvalid_1's rmse: 1.04335\n",
      "[508]\ttraining's rmse: 0.967628\tvalid_1's rmse: 1.04335\n",
      "[509]\ttraining's rmse: 0.967524\tvalid_1's rmse: 1.04336\n",
      "[510]\ttraining's rmse: 0.967413\tvalid_1's rmse: 1.04333\n",
      "[511]\ttraining's rmse: 0.967341\tvalid_1's rmse: 1.0433\n",
      "[512]\ttraining's rmse: 0.967247\tvalid_1's rmse: 1.04331\n",
      "[513]\ttraining's rmse: 0.967116\tvalid_1's rmse: 1.04322\n",
      "[514]\ttraining's rmse: 0.967037\tvalid_1's rmse: 1.04324\n",
      "[515]\ttraining's rmse: 0.966936\tvalid_1's rmse: 1.04324\n",
      "[516]\ttraining's rmse: 0.966839\tvalid_1's rmse: 1.04324\n",
      "[517]\ttraining's rmse: 0.966764\tvalid_1's rmse: 1.04322\n",
      "[518]\ttraining's rmse: 0.966646\tvalid_1's rmse: 1.04319\n",
      "[519]\ttraining's rmse: 0.966523\tvalid_1's rmse: 1.04315\n",
      "[520]\ttraining's rmse: 0.966421\tvalid_1's rmse: 1.0431\n",
      "[521]\ttraining's rmse: 0.966328\tvalid_1's rmse: 1.04311\n",
      "[522]\ttraining's rmse: 0.966214\tvalid_1's rmse: 1.04305\n",
      "[523]\ttraining's rmse: 0.966102\tvalid_1's rmse: 1.04302\n",
      "[524]\ttraining's rmse: 0.966032\tvalid_1's rmse: 1.04296\n",
      "[525]\ttraining's rmse: 0.965932\tvalid_1's rmse: 1.04291\n",
      "[526]\ttraining's rmse: 0.965805\tvalid_1's rmse: 1.04282\n",
      "[527]\ttraining's rmse: 0.965687\tvalid_1's rmse: 1.04278\n",
      "[528]\ttraining's rmse: 0.965616\tvalid_1's rmse: 1.04273\n",
      "[529]\ttraining's rmse: 0.965508\tvalid_1's rmse: 1.04273\n",
      "[530]\ttraining's rmse: 0.965413\tvalid_1's rmse: 1.04274\n",
      "[531]\ttraining's rmse: 0.965301\tvalid_1's rmse: 1.04273\n",
      "[532]\ttraining's rmse: 0.965173\tvalid_1's rmse: 1.04271\n",
      "[533]\ttraining's rmse: 0.965082\tvalid_1's rmse: 1.04264\n",
      "[534]\ttraining's rmse: 0.965014\tvalid_1's rmse: 1.04258\n",
      "[535]\ttraining's rmse: 0.964897\tvalid_1's rmse: 1.04255\n",
      "[536]\ttraining's rmse: 0.964826\tvalid_1's rmse: 1.04253\n",
      "[537]\ttraining's rmse: 0.964709\tvalid_1's rmse: 1.04248\n",
      "[538]\ttraining's rmse: 0.964617\tvalid_1's rmse: 1.04247\n",
      "[539]\ttraining's rmse: 0.964522\tvalid_1's rmse: 1.04242\n",
      "[540]\ttraining's rmse: 0.964419\tvalid_1's rmse: 1.0424\n",
      "[541]\ttraining's rmse: 0.964315\tvalid_1's rmse: 1.0424\n",
      "[542]\ttraining's rmse: 0.964194\tvalid_1's rmse: 1.04232\n",
      "[543]\ttraining's rmse: 0.96408\tvalid_1's rmse: 1.04233\n",
      "[544]\ttraining's rmse: 0.963956\tvalid_1's rmse: 1.04231\n",
      "[545]\ttraining's rmse: 0.963891\tvalid_1's rmse: 1.04225\n",
      "[546]\ttraining's rmse: 0.963804\tvalid_1's rmse: 1.04217\n",
      "[547]\ttraining's rmse: 0.963693\tvalid_1's rmse: 1.04217\n",
      "[548]\ttraining's rmse: 0.963627\tvalid_1's rmse: 1.04215\n",
      "[549]\ttraining's rmse: 0.963524\tvalid_1's rmse: 1.04208\n",
      "[550]\ttraining's rmse: 0.963463\tvalid_1's rmse: 1.04206\n",
      "[551]\ttraining's rmse: 0.963358\tvalid_1's rmse: 1.04203\n",
      "[552]\ttraining's rmse: 0.963245\tvalid_1's rmse: 1.04202\n",
      "[553]\ttraining's rmse: 0.963164\tvalid_1's rmse: 1.04196\n",
      "[554]\ttraining's rmse: 0.963053\tvalid_1's rmse: 1.04195\n",
      "[555]\ttraining's rmse: 0.962954\tvalid_1's rmse: 1.04193\n",
      "[556]\ttraining's rmse: 0.962844\tvalid_1's rmse: 1.04188\n",
      "[557]\ttraining's rmse: 0.962738\tvalid_1's rmse: 1.04188\n",
      "[558]\ttraining's rmse: 0.962654\tvalid_1's rmse: 1.0418\n",
      "[559]\ttraining's rmse: 0.962531\tvalid_1's rmse: 1.04181\n",
      "[560]\ttraining's rmse: 0.96245\tvalid_1's rmse: 1.04181\n",
      "[561]\ttraining's rmse: 0.962375\tvalid_1's rmse: 1.04179\n",
      "[562]\ttraining's rmse: 0.962276\tvalid_1's rmse: 1.04177\n",
      "[563]\ttraining's rmse: 0.962171\tvalid_1's rmse: 1.04174\n",
      "[564]\ttraining's rmse: 0.962069\tvalid_1's rmse: 1.04171\n",
      "[565]\ttraining's rmse: 0.96196\tvalid_1's rmse: 1.04171\n",
      "[566]\ttraining's rmse: 0.961898\tvalid_1's rmse: 1.04165\n",
      "[567]\ttraining's rmse: 0.961811\tvalid_1's rmse: 1.04161\n",
      "[568]\ttraining's rmse: 0.961715\tvalid_1's rmse: 1.04158\n",
      "[569]\ttraining's rmse: 0.96161\tvalid_1's rmse: 1.04154\n",
      "[570]\ttraining's rmse: 0.961514\tvalid_1's rmse: 1.04155\n",
      "[571]\ttraining's rmse: 0.961417\tvalid_1's rmse: 1.0415\n",
      "[572]\ttraining's rmse: 0.961332\tvalid_1's rmse: 1.04143\n",
      "[573]\ttraining's rmse: 0.961225\tvalid_1's rmse: 1.04136\n",
      "[574]\ttraining's rmse: 0.961191\tvalid_1's rmse: 1.04136\n",
      "[575]\ttraining's rmse: 0.96109\tvalid_1's rmse: 1.04137\n",
      "[576]\ttraining's rmse: 0.961004\tvalid_1's rmse: 1.04133\n",
      "[577]\ttraining's rmse: 0.960909\tvalid_1's rmse: 1.04133\n",
      "[578]\ttraining's rmse: 0.960803\tvalid_1's rmse: 1.04132\n",
      "[579]\ttraining's rmse: 0.960713\tvalid_1's rmse: 1.0413\n",
      "[580]\ttraining's rmse: 0.9606\tvalid_1's rmse: 1.04129\n",
      "[581]\ttraining's rmse: 0.960508\tvalid_1's rmse: 1.04127\n",
      "[582]\ttraining's rmse: 0.960429\tvalid_1's rmse: 1.0412\n",
      "[583]\ttraining's rmse: 0.960327\tvalid_1's rmse: 1.04116\n",
      "[584]\ttraining's rmse: 0.960243\tvalid_1's rmse: 1.04114\n",
      "[585]\ttraining's rmse: 0.96021\tvalid_1's rmse: 1.04115\n",
      "[586]\ttraining's rmse: 0.960098\tvalid_1's rmse: 1.04115\n",
      "[587]\ttraining's rmse: 0.960018\tvalid_1's rmse: 1.04104\n",
      "[588]\ttraining's rmse: 0.959928\tvalid_1's rmse: 1.04104\n",
      "[589]\ttraining's rmse: 0.959846\tvalid_1's rmse: 1.04107\n",
      "[590]\ttraining's rmse: 0.959763\tvalid_1's rmse: 1.04103\n",
      "[591]\ttraining's rmse: 0.95966\tvalid_1's rmse: 1.04104\n",
      "[592]\ttraining's rmse: 0.959577\tvalid_1's rmse: 1.04106\n",
      "[593]\ttraining's rmse: 0.959488\tvalid_1's rmse: 1.041\n",
      "[594]\ttraining's rmse: 0.959413\tvalid_1's rmse: 1.041\n",
      "[595]\ttraining's rmse: 0.959316\tvalid_1's rmse: 1.04103\n",
      "[596]\ttraining's rmse: 0.959236\tvalid_1's rmse: 1.041\n",
      "[597]\ttraining's rmse: 0.959145\tvalid_1's rmse: 1.041\n",
      "[598]\ttraining's rmse: 0.959038\tvalid_1's rmse: 1.04094\n",
      "[599]\ttraining's rmse: 0.958955\tvalid_1's rmse: 1.04093\n",
      "[600]\ttraining's rmse: 0.958873\tvalid_1's rmse: 1.04085\n",
      "[601]\ttraining's rmse: 0.958775\tvalid_1's rmse: 1.04082\n",
      "[602]\ttraining's rmse: 0.95867\tvalid_1's rmse: 1.04085\n",
      "[603]\ttraining's rmse: 0.95857\tvalid_1's rmse: 1.04085\n",
      "[604]\ttraining's rmse: 0.95848\tvalid_1's rmse: 1.04083\n",
      "[605]\ttraining's rmse: 0.9584\tvalid_1's rmse: 1.04086\n",
      "[606]\ttraining's rmse: 0.958316\tvalid_1's rmse: 1.04078\n",
      "[607]\ttraining's rmse: 0.95823\tvalid_1's rmse: 1.0408\n",
      "[608]\ttraining's rmse: 0.958151\tvalid_1's rmse: 1.04077\n",
      "[609]\ttraining's rmse: 0.95807\tvalid_1's rmse: 1.04079\n",
      "[610]\ttraining's rmse: 0.958012\tvalid_1's rmse: 1.04078\n",
      "[611]\ttraining's rmse: 0.957926\tvalid_1's rmse: 1.04076\n",
      "[612]\ttraining's rmse: 0.957819\tvalid_1's rmse: 1.04076\n",
      "[613]\ttraining's rmse: 0.957738\tvalid_1's rmse: 1.0408\n",
      "[614]\ttraining's rmse: 0.957655\tvalid_1's rmse: 1.0408\n",
      "[615]\ttraining's rmse: 0.957579\tvalid_1's rmse: 1.04074\n",
      "[616]\ttraining's rmse: 0.957483\tvalid_1's rmse: 1.04075\n",
      "[617]\ttraining's rmse: 0.9574\tvalid_1's rmse: 1.04068\n",
      "[618]\ttraining's rmse: 0.957297\tvalid_1's rmse: 1.04062\n",
      "[619]\ttraining's rmse: 0.957201\tvalid_1's rmse: 1.04063\n",
      "[620]\ttraining's rmse: 0.95717\tvalid_1's rmse: 1.04063\n",
      "[621]\ttraining's rmse: 0.957078\tvalid_1's rmse: 1.04065\n",
      "[622]\ttraining's rmse: 0.957003\tvalid_1's rmse: 1.04059\n",
      "[623]\ttraining's rmse: 0.956949\tvalid_1's rmse: 1.04056\n",
      "[624]\ttraining's rmse: 0.956869\tvalid_1's rmse: 1.0406\n",
      "[625]\ttraining's rmse: 0.956779\tvalid_1's rmse: 1.04054\n",
      "[626]\ttraining's rmse: 0.956716\tvalid_1's rmse: 1.04045\n",
      "[627]\ttraining's rmse: 0.956642\tvalid_1's rmse: 1.04044\n",
      "[628]\ttraining's rmse: 0.956562\tvalid_1's rmse: 1.04048\n",
      "[629]\ttraining's rmse: 0.956483\tvalid_1's rmse: 1.04039\n",
      "[630]\ttraining's rmse: 0.956381\tvalid_1's rmse: 1.04035\n",
      "[631]\ttraining's rmse: 0.956302\tvalid_1's rmse: 1.04033\n",
      "[632]\ttraining's rmse: 0.956223\tvalid_1's rmse: 1.04029\n",
      "[633]\ttraining's rmse: 0.956134\tvalid_1's rmse: 1.04033\n",
      "[634]\ttraining's rmse: 0.956104\tvalid_1's rmse: 1.04033\n",
      "[635]\ttraining's rmse: 0.956002\tvalid_1's rmse: 1.04033\n",
      "[636]\ttraining's rmse: 0.955886\tvalid_1's rmse: 1.0403\n",
      "[637]\ttraining's rmse: 0.955831\tvalid_1's rmse: 1.04026\n",
      "[638]\ttraining's rmse: 0.955758\tvalid_1's rmse: 1.04021\n",
      "[639]\ttraining's rmse: 0.955681\tvalid_1's rmse: 1.04022\n",
      "[640]\ttraining's rmse: 0.955599\tvalid_1's rmse: 1.04012\n",
      "[641]\ttraining's rmse: 0.955524\tvalid_1's rmse: 1.04012\n",
      "[642]\ttraining's rmse: 0.955436\tvalid_1's rmse: 1.04015\n",
      "[643]\ttraining's rmse: 0.955349\tvalid_1's rmse: 1.04016\n",
      "[644]\ttraining's rmse: 0.955271\tvalid_1's rmse: 1.04017\n",
      "[645]\ttraining's rmse: 0.955204\tvalid_1's rmse: 1.04016\n",
      "[646]\ttraining's rmse: 0.955143\tvalid_1's rmse: 1.04015\n",
      "[647]\ttraining's rmse: 0.95509\tvalid_1's rmse: 1.04009\n",
      "[648]\ttraining's rmse: 0.955004\tvalid_1's rmse: 1.04012\n",
      "[649]\ttraining's rmse: 0.954897\tvalid_1's rmse: 1.0401\n",
      "[650]\ttraining's rmse: 0.95487\tvalid_1's rmse: 1.0401\n",
      "[651]\ttraining's rmse: 0.954775\tvalid_1's rmse: 1.04\n",
      "[652]\ttraining's rmse: 0.954675\tvalid_1's rmse: 1.03994\n",
      "[653]\ttraining's rmse: 0.9546\tvalid_1's rmse: 1.03998\n",
      "[654]\ttraining's rmse: 0.954503\tvalid_1's rmse: 1.03998\n",
      "[655]\ttraining's rmse: 0.954427\tvalid_1's rmse: 1.0399\n",
      "[656]\ttraining's rmse: 0.954356\tvalid_1's rmse: 1.03988\n",
      "[657]\ttraining's rmse: 0.954277\tvalid_1's rmse: 1.03986\n",
      "[658]\ttraining's rmse: 0.954192\tvalid_1's rmse: 1.0399\n",
      "[659]\ttraining's rmse: 0.954087\tvalid_1's rmse: 1.03985\n",
      "[660]\ttraining's rmse: 0.953998\tvalid_1's rmse: 1.03988\n",
      "[661]\ttraining's rmse: 0.953957\tvalid_1's rmse: 1.0399\n",
      "[662]\ttraining's rmse: 0.953893\tvalid_1's rmse: 1.03991\n",
      "[663]\ttraining's rmse: 0.953787\tvalid_1's rmse: 1.03988\n",
      "[664]\ttraining's rmse: 0.95372\tvalid_1's rmse: 1.03989\n",
      "[665]\ttraining's rmse: 0.953651\tvalid_1's rmse: 1.03988\n",
      "[666]\ttraining's rmse: 0.953555\tvalid_1's rmse: 1.03982\n",
      "[667]\ttraining's rmse: 0.953483\tvalid_1's rmse: 1.03982\n",
      "[668]\ttraining's rmse: 0.953455\tvalid_1's rmse: 1.03982\n",
      "[669]\ttraining's rmse: 0.953392\tvalid_1's rmse: 1.03982\n",
      "[670]\ttraining's rmse: 0.953318\tvalid_1's rmse: 1.03983\n",
      "[671]\ttraining's rmse: 0.953249\tvalid_1's rmse: 1.03975\n",
      "[672]\ttraining's rmse: 0.953142\tvalid_1's rmse: 1.03971\n",
      "[673]\ttraining's rmse: 0.953045\tvalid_1's rmse: 1.03965\n",
      "[674]\ttraining's rmse: 0.952976\tvalid_1's rmse: 1.03961\n",
      "[675]\ttraining's rmse: 0.952913\tvalid_1's rmse: 1.03962\n",
      "[676]\ttraining's rmse: 0.952825\tvalid_1's rmse: 1.03959\n",
      "[677]\ttraining's rmse: 0.952799\tvalid_1's rmse: 1.03959\n",
      "[678]\ttraining's rmse: 0.952705\tvalid_1's rmse: 1.03961\n",
      "[679]\ttraining's rmse: 0.95263\tvalid_1's rmse: 1.03962\n",
      "[680]\ttraining's rmse: 0.952537\tvalid_1's rmse: 1.03959\n",
      "[681]\ttraining's rmse: 0.952488\tvalid_1's rmse: 1.03956\n",
      "[682]\ttraining's rmse: 0.952379\tvalid_1's rmse: 1.03952\n",
      "[683]\ttraining's rmse: 0.95234\tvalid_1's rmse: 1.03949\n",
      "[684]\ttraining's rmse: 0.952274\tvalid_1's rmse: 1.03951\n",
      "[685]\ttraining's rmse: 0.952192\tvalid_1's rmse: 1.0395\n",
      "[686]\ttraining's rmse: 0.952127\tvalid_1's rmse: 1.03948\n",
      "[687]\ttraining's rmse: 0.952061\tvalid_1's rmse: 1.03947\n",
      "[688]\ttraining's rmse: 0.951962\tvalid_1's rmse: 1.03943\n",
      "[689]\ttraining's rmse: 0.951892\tvalid_1's rmse: 1.03944\n",
      "[690]\ttraining's rmse: 0.951849\tvalid_1's rmse: 1.03942\n",
      "[691]\ttraining's rmse: 0.951823\tvalid_1's rmse: 1.03942\n",
      "[692]\ttraining's rmse: 0.951774\tvalid_1's rmse: 1.03936\n",
      "[693]\ttraining's rmse: 0.951702\tvalid_1's rmse: 1.03937\n",
      "[694]\ttraining's rmse: 0.951642\tvalid_1's rmse: 1.03938\n",
      "[695]\ttraining's rmse: 0.951576\tvalid_1's rmse: 1.03938\n",
      "[696]\ttraining's rmse: 0.951491\tvalid_1's rmse: 1.03934\n",
      "[697]\ttraining's rmse: 0.95139\tvalid_1's rmse: 1.03931\n",
      "[698]\ttraining's rmse: 0.951316\tvalid_1's rmse: 1.03931\n",
      "[699]\ttraining's rmse: 0.951269\tvalid_1's rmse: 1.03929\n",
      "[700]\ttraining's rmse: 0.951205\tvalid_1's rmse: 1.0393\n",
      "[701]\ttraining's rmse: 0.951137\tvalid_1's rmse: 1.0393\n",
      "[702]\ttraining's rmse: 0.951113\tvalid_1's rmse: 1.03931\n",
      "[703]\ttraining's rmse: 0.951023\tvalid_1's rmse: 1.0393\n",
      "[704]\ttraining's rmse: 0.950943\tvalid_1's rmse: 1.03929\n",
      "[705]\ttraining's rmse: 0.950851\tvalid_1's rmse: 1.03931\n",
      "[706]\ttraining's rmse: 0.950764\tvalid_1's rmse: 1.03926\n",
      "[707]\ttraining's rmse: 0.950665\tvalid_1's rmse: 1.03922\n",
      "[708]\ttraining's rmse: 0.950583\tvalid_1's rmse: 1.0392\n",
      "[709]\ttraining's rmse: 0.950491\tvalid_1's rmse: 1.03915\n",
      "[710]\ttraining's rmse: 0.950426\tvalid_1's rmse: 1.03918\n",
      "[711]\ttraining's rmse: 0.950368\tvalid_1's rmse: 1.03915\n",
      "[712]\ttraining's rmse: 0.950327\tvalid_1's rmse: 1.03912\n",
      "[713]\ttraining's rmse: 0.950259\tvalid_1's rmse: 1.03913\n",
      "[714]\ttraining's rmse: 0.950195\tvalid_1's rmse: 1.03912\n",
      "[715]\ttraining's rmse: 0.950102\tvalid_1's rmse: 1.03908\n",
      "[716]\ttraining's rmse: 0.950036\tvalid_1's rmse: 1.03905\n",
      "[717]\ttraining's rmse: 0.949979\tvalid_1's rmse: 1.03906\n",
      "[718]\ttraining's rmse: 0.949954\tvalid_1's rmse: 1.03906\n",
      "[719]\ttraining's rmse: 0.949909\tvalid_1's rmse: 1.03904\n",
      "[720]\ttraining's rmse: 0.949831\tvalid_1's rmse: 1.039\n",
      "[721]\ttraining's rmse: 0.949768\tvalid_1's rmse: 1.03903\n",
      "[722]\ttraining's rmse: 0.94968\tvalid_1's rmse: 1.039\n",
      "[723]\ttraining's rmse: 0.949566\tvalid_1's rmse: 1.03892\n",
      "[724]\ttraining's rmse: 0.949495\tvalid_1's rmse: 1.03892\n",
      "[725]\ttraining's rmse: 0.949429\tvalid_1's rmse: 1.03894\n",
      "[726]\ttraining's rmse: 0.949349\tvalid_1's rmse: 1.03895\n",
      "[727]\ttraining's rmse: 0.949293\tvalid_1's rmse: 1.03891\n",
      "[728]\ttraining's rmse: 0.949216\tvalid_1's rmse: 1.03888\n",
      "[729]\ttraining's rmse: 0.949128\tvalid_1's rmse: 1.03885\n",
      "[730]\ttraining's rmse: 0.949073\tvalid_1's rmse: 1.03886\n",
      "[731]\ttraining's rmse: 0.949011\tvalid_1's rmse: 1.03885\n",
      "[732]\ttraining's rmse: 0.948921\tvalid_1's rmse: 1.03883\n",
      "[733]\ttraining's rmse: 0.948899\tvalid_1's rmse: 1.03883\n",
      "[734]\ttraining's rmse: 0.948811\tvalid_1's rmse: 1.03872\n",
      "[735]\ttraining's rmse: 0.948755\tvalid_1's rmse: 1.03873\n",
      "[736]\ttraining's rmse: 0.948715\tvalid_1's rmse: 1.0387\n",
      "[737]\ttraining's rmse: 0.948631\tvalid_1's rmse: 1.03867\n",
      "[738]\ttraining's rmse: 0.948567\tvalid_1's rmse: 1.03868\n",
      "[739]\ttraining's rmse: 0.948491\tvalid_1's rmse: 1.03865\n",
      "[740]\ttraining's rmse: 0.948415\tvalid_1's rmse: 1.03865\n",
      "[741]\ttraining's rmse: 0.948357\tvalid_1's rmse: 1.03861\n",
      "[742]\ttraining's rmse: 0.948295\tvalid_1's rmse: 1.0386\n",
      "[743]\ttraining's rmse: 0.948242\tvalid_1's rmse: 1.03861\n",
      "[744]\ttraining's rmse: 0.948156\tvalid_1's rmse: 1.03858\n",
      "[745]\ttraining's rmse: 0.948081\tvalid_1's rmse: 1.03854\n",
      "[746]\ttraining's rmse: 0.948007\tvalid_1's rmse: 1.03855\n",
      "[747]\ttraining's rmse: 0.947983\tvalid_1's rmse: 1.03857\n",
      "[748]\ttraining's rmse: 0.947939\tvalid_1's rmse: 1.03855\n",
      "[749]\ttraining's rmse: 0.94786\tvalid_1's rmse: 1.03855\n",
      "[750]\ttraining's rmse: 0.947796\tvalid_1's rmse: 1.0385\n",
      "[751]\ttraining's rmse: 0.947711\tvalid_1's rmse: 1.03847\n",
      "[752]\ttraining's rmse: 0.947613\tvalid_1's rmse: 1.0384\n",
      "[753]\ttraining's rmse: 0.947554\tvalid_1's rmse: 1.03841\n",
      "[754]\ttraining's rmse: 0.94753\tvalid_1's rmse: 1.03843\n",
      "[755]\ttraining's rmse: 0.947506\tvalid_1's rmse: 1.03844\n",
      "[756]\ttraining's rmse: 0.947395\tvalid_1's rmse: 1.03838\n",
      "[757]\ttraining's rmse: 0.947316\tvalid_1's rmse: 1.03842\n",
      "[758]\ttraining's rmse: 0.947262\tvalid_1's rmse: 1.0384\n",
      "[759]\ttraining's rmse: 0.947224\tvalid_1's rmse: 1.03832\n",
      "[760]\ttraining's rmse: 0.947133\tvalid_1's rmse: 1.03832\n",
      "[761]\ttraining's rmse: 0.94706\tvalid_1's rmse: 1.03832\n",
      "[762]\ttraining's rmse: 0.946967\tvalid_1's rmse: 1.03824\n",
      "[763]\ttraining's rmse: 0.946879\tvalid_1's rmse: 1.03818\n",
      "[764]\ttraining's rmse: 0.94682\tvalid_1's rmse: 1.0382\n",
      "[765]\ttraining's rmse: 0.946727\tvalid_1's rmse: 1.03822\n",
      "[766]\ttraining's rmse: 0.946628\tvalid_1's rmse: 1.03817\n",
      "[767]\ttraining's rmse: 0.946568\tvalid_1's rmse: 1.03815\n",
      "[768]\ttraining's rmse: 0.946477\tvalid_1's rmse: 1.03813\n",
      "[769]\ttraining's rmse: 0.946403\tvalid_1's rmse: 1.03812\n",
      "[770]\ttraining's rmse: 0.94638\tvalid_1's rmse: 1.03814\n",
      "[771]\ttraining's rmse: 0.946304\tvalid_1's rmse: 1.03809\n",
      "[772]\ttraining's rmse: 0.946251\tvalid_1's rmse: 1.03808\n",
      "[773]\ttraining's rmse: 0.946172\tvalid_1's rmse: 1.03807\n",
      "[774]\ttraining's rmse: 0.946148\tvalid_1's rmse: 1.03807\n",
      "[775]\ttraining's rmse: 0.946113\tvalid_1's rmse: 1.03804\n",
      "[776]\ttraining's rmse: 0.946008\tvalid_1's rmse: 1.03804\n",
      "[777]\ttraining's rmse: 0.945946\tvalid_1's rmse: 1.03804\n",
      "[778]\ttraining's rmse: 0.945894\tvalid_1's rmse: 1.03799\n",
      "[779]\ttraining's rmse: 0.94581\tvalid_1's rmse: 1.03796\n",
      "[780]\ttraining's rmse: 0.945739\tvalid_1's rmse: 1.03796\n",
      "[781]\ttraining's rmse: 0.94566\tvalid_1's rmse: 1.03793\n",
      "[782]\ttraining's rmse: 0.945603\tvalid_1's rmse: 1.03793\n",
      "[783]\ttraining's rmse: 0.945507\tvalid_1's rmse: 1.03788\n",
      "[784]\ttraining's rmse: 0.945414\tvalid_1's rmse: 1.03781\n",
      "[785]\ttraining's rmse: 0.945358\tvalid_1's rmse: 1.03782\n",
      "[786]\ttraining's rmse: 0.945282\tvalid_1's rmse: 1.03786\n",
      "[787]\ttraining's rmse: 0.945198\tvalid_1's rmse: 1.03783\n",
      "[788]\ttraining's rmse: 0.945163\tvalid_1's rmse: 1.0378\n",
      "[789]\ttraining's rmse: 0.945073\tvalid_1's rmse: 1.03782\n",
      "[790]\ttraining's rmse: 0.945\tvalid_1's rmse: 1.0378\n",
      "[791]\ttraining's rmse: 0.944949\tvalid_1's rmse: 1.03776\n",
      "[792]\ttraining's rmse: 0.944843\tvalid_1's rmse: 1.03771\n",
      "[793]\ttraining's rmse: 0.94477\tvalid_1's rmse: 1.03773\n",
      "[794]\ttraining's rmse: 0.944683\tvalid_1's rmse: 1.03772\n",
      "[795]\ttraining's rmse: 0.944628\tvalid_1's rmse: 1.0377\n",
      "[796]\ttraining's rmse: 0.944605\tvalid_1's rmse: 1.03772\n",
      "[797]\ttraining's rmse: 0.944554\tvalid_1's rmse: 1.03772\n",
      "[798]\ttraining's rmse: 0.944497\tvalid_1's rmse: 1.03772\n",
      "[799]\ttraining's rmse: 0.944414\tvalid_1's rmse: 1.03767\n",
      "[800]\ttraining's rmse: 0.944393\tvalid_1's rmse: 1.03767\n",
      "[801]\ttraining's rmse: 0.944316\tvalid_1's rmse: 1.03768\n",
      "[802]\ttraining's rmse: 0.944211\tvalid_1's rmse: 1.03765\n",
      "[803]\ttraining's rmse: 0.944117\tvalid_1's rmse: 1.03759\n",
      "[804]\ttraining's rmse: 0.944032\tvalid_1's rmse: 1.03759\n",
      "[805]\ttraining's rmse: 0.944001\tvalid_1's rmse: 1.03756\n",
      "[806]\ttraining's rmse: 0.943932\tvalid_1's rmse: 1.03756\n",
      "[807]\ttraining's rmse: 0.943845\tvalid_1's rmse: 1.03749\n",
      "[808]\ttraining's rmse: 0.943786\tvalid_1's rmse: 1.03752\n",
      "[809]\ttraining's rmse: 0.943702\tvalid_1's rmse: 1.03747\n",
      "[810]\ttraining's rmse: 0.943647\tvalid_1's rmse: 1.03747\n",
      "[811]\ttraining's rmse: 0.943577\tvalid_1's rmse: 1.03749\n",
      "[812]\ttraining's rmse: 0.943474\tvalid_1's rmse: 1.03746\n",
      "[813]\ttraining's rmse: 0.943403\tvalid_1's rmse: 1.03744\n",
      "[814]\ttraining's rmse: 0.943372\tvalid_1's rmse: 1.03739\n",
      "[815]\ttraining's rmse: 0.943351\tvalid_1's rmse: 1.0374\n",
      "[816]\ttraining's rmse: 0.943294\tvalid_1's rmse: 1.03738\n",
      "[817]\ttraining's rmse: 0.943209\tvalid_1's rmse: 1.03738\n",
      "[818]\ttraining's rmse: 0.943165\tvalid_1's rmse: 1.03739\n",
      "[819]\ttraining's rmse: 0.943073\tvalid_1's rmse: 1.03734\n",
      "[820]\ttraining's rmse: 0.942971\tvalid_1's rmse: 1.03728\n",
      "[821]\ttraining's rmse: 0.942923\tvalid_1's rmse: 1.03731\n",
      "[822]\ttraining's rmse: 0.94284\tvalid_1's rmse: 1.0373\n",
      "[823]\ttraining's rmse: 0.942776\tvalid_1's rmse: 1.03725\n",
      "[824]\ttraining's rmse: 0.942719\tvalid_1's rmse: 1.03727\n",
      "[825]\ttraining's rmse: 0.942644\tvalid_1's rmse: 1.03725\n",
      "[826]\ttraining's rmse: 0.942547\tvalid_1's rmse: 1.03726\n",
      "[827]\ttraining's rmse: 0.942493\tvalid_1's rmse: 1.03724\n",
      "[828]\ttraining's rmse: 0.942412\tvalid_1's rmse: 1.03722\n",
      "[829]\ttraining's rmse: 0.942391\tvalid_1's rmse: 1.03722\n",
      "[830]\ttraining's rmse: 0.942328\tvalid_1's rmse: 1.03717\n",
      "[831]\ttraining's rmse: 0.942271\tvalid_1's rmse: 1.0372\n",
      "[832]\ttraining's rmse: 0.942217\tvalid_1's rmse: 1.03721\n",
      "[833]\ttraining's rmse: 0.94216\tvalid_1's rmse: 1.03719\n",
      "[834]\ttraining's rmse: 0.942092\tvalid_1's rmse: 1.03721\n",
      "[835]\ttraining's rmse: 0.942059\tvalid_1's rmse: 1.03717\n",
      "[836]\ttraining's rmse: 0.942011\tvalid_1's rmse: 1.03715\n",
      "[837]\ttraining's rmse: 0.941941\tvalid_1's rmse: 1.03711\n",
      "[838]\ttraining's rmse: 0.941876\tvalid_1's rmse: 1.0371\n",
      "[839]\ttraining's rmse: 0.941806\tvalid_1's rmse: 1.03708\n",
      "[840]\ttraining's rmse: 0.941773\tvalid_1's rmse: 1.03707\n",
      "[841]\ttraining's rmse: 0.941753\tvalid_1's rmse: 1.03708\n",
      "[842]\ttraining's rmse: 0.941703\tvalid_1's rmse: 1.03708\n",
      "[843]\ttraining's rmse: 0.941652\tvalid_1's rmse: 1.0371\n",
      "[844]\ttraining's rmse: 0.941576\tvalid_1's rmse: 1.03708\n",
      "[845]\ttraining's rmse: 0.941555\tvalid_1's rmse: 1.03708\n",
      "[846]\ttraining's rmse: 0.941487\tvalid_1's rmse: 1.03707\n",
      "[847]\ttraining's rmse: 0.941401\tvalid_1's rmse: 1.03702\n",
      "[848]\ttraining's rmse: 0.941372\tvalid_1's rmse: 1.03699\n",
      "[849]\ttraining's rmse: 0.941325\tvalid_1's rmse: 1.03697\n",
      "[850]\ttraining's rmse: 0.94127\tvalid_1's rmse: 1.037\n",
      "[851]\ttraining's rmse: 0.941203\tvalid_1's rmse: 1.03696\n",
      "[852]\ttraining's rmse: 0.941152\tvalid_1's rmse: 1.03698\n",
      "[853]\ttraining's rmse: 0.941084\tvalid_1's rmse: 1.037\n",
      "[854]\ttraining's rmse: 0.941064\tvalid_1's rmse: 1.03701\n",
      "[855]\ttraining's rmse: 0.941025\tvalid_1's rmse: 1.03702\n",
      "[856]\ttraining's rmse: 0.940959\tvalid_1's rmse: 1.03703\n",
      "[857]\ttraining's rmse: 0.940924\tvalid_1's rmse: 1.03702\n",
      "[858]\ttraining's rmse: 0.940841\tvalid_1's rmse: 1.03703\n",
      "[859]\ttraining's rmse: 0.94078\tvalid_1's rmse: 1.03701\n",
      "[860]\ttraining's rmse: 0.940761\tvalid_1's rmse: 1.03702\n",
      "[861]\ttraining's rmse: 0.940683\tvalid_1's rmse: 1.03699\n",
      "Early stopping, best iteration is:\n",
      "[851]\ttraining's rmse: 0.941203\tvalid_1's rmse: 1.03696\n",
      "fold_1 coefficients:  [0.54262123 1.69591814 2.18291589]\n",
      "[1]\ttraining's rmse: 1.25378\tvalid_1's rmse: 1.23679\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.25087\tvalid_1's rmse: 1.23422\n",
      "[3]\ttraining's rmse: 1.248\tvalid_1's rmse: 1.23169\n",
      "[4]\ttraining's rmse: 1.24517\tvalid_1's rmse: 1.22917\n",
      "[5]\ttraining's rmse: 1.24239\tvalid_1's rmse: 1.22673\n",
      "[6]\ttraining's rmse: 1.23966\tvalid_1's rmse: 1.22433\n",
      "[7]\ttraining's rmse: 1.23697\tvalid_1's rmse: 1.22195\n",
      "[8]\ttraining's rmse: 1.23435\tvalid_1's rmse: 1.21971\n",
      "[9]\ttraining's rmse: 1.23176\tvalid_1's rmse: 1.21742\n",
      "[10]\ttraining's rmse: 1.22921\tvalid_1's rmse: 1.21525\n",
      "[11]\ttraining's rmse: 1.22671\tvalid_1's rmse: 1.21304\n",
      "[12]\ttraining's rmse: 1.22425\tvalid_1's rmse: 1.21087\n",
      "[13]\ttraining's rmse: 1.22184\tvalid_1's rmse: 1.20874\n",
      "[14]\ttraining's rmse: 1.21946\tvalid_1's rmse: 1.20668\n",
      "[15]\ttraining's rmse: 1.21713\tvalid_1's rmse: 1.20463\n",
      "[16]\ttraining's rmse: 1.21484\tvalid_1's rmse: 1.20267\n",
      "[17]\ttraining's rmse: 1.21258\tvalid_1's rmse: 1.2007\n",
      "[18]\ttraining's rmse: 1.21038\tvalid_1's rmse: 1.1988\n",
      "[19]\ttraining's rmse: 1.20819\tvalid_1's rmse: 1.19688\n",
      "[20]\ttraining's rmse: 1.20607\tvalid_1's rmse: 1.19513\n",
      "[21]\ttraining's rmse: 1.20396\tvalid_1's rmse: 1.1933\n",
      "[22]\ttraining's rmse: 1.20189\tvalid_1's rmse: 1.19157\n",
      "[23]\ttraining's rmse: 1.19986\tvalid_1's rmse: 1.1899\n",
      "[24]\ttraining's rmse: 1.19707\tvalid_1's rmse: 1.18763\n",
      "[25]\ttraining's rmse: 1.19434\tvalid_1's rmse: 1.18541\n",
      "[26]\ttraining's rmse: 1.19166\tvalid_1's rmse: 1.18323\n",
      "[27]\ttraining's rmse: 1.18902\tvalid_1's rmse: 1.1811\n",
      "[28]\ttraining's rmse: 1.18702\tvalid_1's rmse: 1.17943\n",
      "[29]\ttraining's rmse: 1.18446\tvalid_1's rmse: 1.17737\n",
      "[30]\ttraining's rmse: 1.18195\tvalid_1's rmse: 1.17535\n",
      "[31]\ttraining's rmse: 1.17949\tvalid_1's rmse: 1.17337\n",
      "[32]\ttraining's rmse: 1.17706\tvalid_1's rmse: 1.17143\n",
      "[33]\ttraining's rmse: 1.17521\tvalid_1's rmse: 1.16986\n",
      "[34]\ttraining's rmse: 1.17286\tvalid_1's rmse: 1.16799\n",
      "[35]\ttraining's rmse: 1.17055\tvalid_1's rmse: 1.16616\n",
      "[36]\ttraining's rmse: 1.16829\tvalid_1's rmse: 1.16433\n",
      "[37]\ttraining's rmse: 1.16606\tvalid_1's rmse: 1.16257\n",
      "[38]\ttraining's rmse: 1.16435\tvalid_1's rmse: 1.1612\n",
      "[39]\ttraining's rmse: 1.1622\tvalid_1's rmse: 1.15947\n",
      "[40]\ttraining's rmse: 1.16008\tvalid_1's rmse: 1.15781\n",
      "[41]\ttraining's rmse: 1.15799\tvalid_1's rmse: 1.15614\n",
      "[42]\ttraining's rmse: 1.15595\tvalid_1's rmse: 1.15455\n",
      "[43]\ttraining's rmse: 1.15437\tvalid_1's rmse: 1.15327\n",
      "[44]\ttraining's rmse: 1.15238\tvalid_1's rmse: 1.15173\n",
      "[45]\ttraining's rmse: 1.15043\tvalid_1's rmse: 1.15022\n",
      "[46]\ttraining's rmse: 1.14852\tvalid_1's rmse: 1.14871\n",
      "[47]\ttraining's rmse: 1.14703\tvalid_1's rmse: 1.14756\n",
      "[48]\ttraining's rmse: 1.14517\tvalid_1's rmse: 1.14616\n",
      "[49]\ttraining's rmse: 1.14373\tvalid_1's rmse: 1.14503\n",
      "[50]\ttraining's rmse: 1.142\tvalid_1's rmse: 1.14357\n",
      "[51]\ttraining's rmse: 1.14073\tvalid_1's rmse: 1.14252\n",
      "[52]\ttraining's rmse: 1.13894\tvalid_1's rmse: 1.14119\n",
      "[53]\ttraining's rmse: 1.13759\tvalid_1's rmse: 1.14013\n",
      "[54]\ttraining's rmse: 1.13594\tvalid_1's rmse: 1.13875\n",
      "[55]\ttraining's rmse: 1.13474\tvalid_1's rmse: 1.13776\n",
      "[56]\ttraining's rmse: 1.13304\tvalid_1's rmse: 1.13649\n",
      "[57]\ttraining's rmse: 1.13187\tvalid_1's rmse: 1.13548\n",
      "[58]\ttraining's rmse: 1.1303\tvalid_1's rmse: 1.13418\n",
      "[59]\ttraining's rmse: 1.12917\tvalid_1's rmse: 1.13327\n",
      "[60]\ttraining's rmse: 1.12789\tvalid_1's rmse: 1.13229\n",
      "[61]\ttraining's rmse: 1.12627\tvalid_1's rmse: 1.13113\n",
      "[62]\ttraining's rmse: 1.12517\tvalid_1's rmse: 1.13022\n",
      "[63]\ttraining's rmse: 1.1237\tvalid_1's rmse: 1.129\n",
      "[64]\ttraining's rmse: 1.12263\tvalid_1's rmse: 1.12815\n",
      "[65]\ttraining's rmse: 1.12108\tvalid_1's rmse: 1.12703\n",
      "[66]\ttraining's rmse: 1.12003\tvalid_1's rmse: 1.12626\n",
      "[67]\ttraining's rmse: 1.11884\tvalid_1's rmse: 1.12537\n",
      "[68]\ttraining's rmse: 1.11745\tvalid_1's rmse: 1.12422\n",
      "[69]\ttraining's rmse: 1.11643\tvalid_1's rmse: 1.12343\n",
      "[70]\ttraining's rmse: 1.11496\tvalid_1's rmse: 1.12237\n",
      "[71]\ttraining's rmse: 1.11398\tvalid_1's rmse: 1.12155\n",
      "[72]\ttraining's rmse: 1.11266\tvalid_1's rmse: 1.12048\n",
      "[73]\ttraining's rmse: 1.11168\tvalid_1's rmse: 1.11977\n",
      "[74]\ttraining's rmse: 1.11069\tvalid_1's rmse: 1.11906\n",
      "[75]\ttraining's rmse: 1.1093\tvalid_1's rmse: 1.11807\n",
      "[76]\ttraining's rmse: 1.10837\tvalid_1's rmse: 1.11748\n",
      "[77]\ttraining's rmse: 1.10738\tvalid_1's rmse: 1.11679\n",
      "[78]\ttraining's rmse: 1.10644\tvalid_1's rmse: 1.11613\n",
      "[79]\ttraining's rmse: 1.10511\tvalid_1's rmse: 1.11515\n",
      "[80]\ttraining's rmse: 1.10423\tvalid_1's rmse: 1.11449\n",
      "[81]\ttraining's rmse: 1.10303\tvalid_1's rmse: 1.11352\n",
      "[82]\ttraining's rmse: 1.10175\tvalid_1's rmse: 1.11254\n",
      "[83]\ttraining's rmse: 1.10089\tvalid_1's rmse: 1.11182\n",
      "[84]\ttraining's rmse: 1.1\tvalid_1's rmse: 1.11124\n",
      "[85]\ttraining's rmse: 1.09914\tvalid_1's rmse: 1.11063\n",
      "[86]\ttraining's rmse: 1.09791\tvalid_1's rmse: 1.10979\n",
      "[87]\ttraining's rmse: 1.09676\tvalid_1's rmse: 1.10887\n",
      "[88]\ttraining's rmse: 1.09588\tvalid_1's rmse: 1.10831\n",
      "[89]\ttraining's rmse: 1.09508\tvalid_1's rmse: 1.10767\n",
      "[90]\ttraining's rmse: 1.09389\tvalid_1's rmse: 1.10678\n",
      "[91]\ttraining's rmse: 1.09302\tvalid_1's rmse: 1.10614\n",
      "[92]\ttraining's rmse: 1.0922\tvalid_1's rmse: 1.10562\n",
      "[93]\ttraining's rmse: 1.09132\tvalid_1's rmse: 1.10493\n",
      "[94]\ttraining's rmse: 1.09026\tvalid_1's rmse: 1.10405\n",
      "[95]\ttraining's rmse: 1.08912\tvalid_1's rmse: 1.10329\n",
      "[96]\ttraining's rmse: 1.08838\tvalid_1's rmse: 1.10268\n",
      "[97]\ttraining's rmse: 1.08761\tvalid_1's rmse: 1.10221\n",
      "[98]\ttraining's rmse: 1.08652\tvalid_1's rmse: 1.10147\n",
      "[99]\ttraining's rmse: 1.0857\tvalid_1's rmse: 1.10081\n",
      "[100]\ttraining's rmse: 1.0847\tvalid_1's rmse: 1.1\n",
      "[101]\ttraining's rmse: 1.08392\tvalid_1's rmse: 1.09956\n",
      "[102]\ttraining's rmse: 1.08292\tvalid_1's rmse: 1.09883\n",
      "[103]\ttraining's rmse: 1.08199\tvalid_1's rmse: 1.0981\n",
      "[104]\ttraining's rmse: 1.08104\tvalid_1's rmse: 1.09732\n",
      "[105]\ttraining's rmse: 1.08036\tvalid_1's rmse: 1.0969\n",
      "[106]\ttraining's rmse: 1.0794\tvalid_1's rmse: 1.09624\n",
      "[107]\ttraining's rmse: 1.07864\tvalid_1's rmse: 1.09576\n",
      "[108]\ttraining's rmse: 1.07768\tvalid_1's rmse: 1.09489\n",
      "[109]\ttraining's rmse: 1.0771\tvalid_1's rmse: 1.09457\n",
      "[110]\ttraining's rmse: 1.07637\tvalid_1's rmse: 1.09398\n",
      "[111]\ttraining's rmse: 1.07538\tvalid_1's rmse: 1.09325\n",
      "[112]\ttraining's rmse: 1.0745\tvalid_1's rmse: 1.09254\n",
      "[113]\ttraining's rmse: 1.07385\tvalid_1's rmse: 1.09209\n",
      "[114]\ttraining's rmse: 1.07303\tvalid_1's rmse: 1.09143\n",
      "[115]\ttraining's rmse: 1.07214\tvalid_1's rmse: 1.09084\n",
      "[116]\ttraining's rmse: 1.07148\tvalid_1's rmse: 1.09041\n",
      "[117]\ttraining's rmse: 1.0706\tvalid_1's rmse: 1.08961\n",
      "[118]\ttraining's rmse: 1.06977\tvalid_1's rmse: 1.08895\n",
      "[119]\ttraining's rmse: 1.06899\tvalid_1's rmse: 1.08832\n",
      "[120]\ttraining's rmse: 1.06816\tvalid_1's rmse: 1.08774\n",
      "[121]\ttraining's rmse: 1.06757\tvalid_1's rmse: 1.08733\n",
      "[122]\ttraining's rmse: 1.06668\tvalid_1's rmse: 1.08666\n",
      "[123]\ttraining's rmse: 1.06605\tvalid_1's rmse: 1.08626\n",
      "[124]\ttraining's rmse: 1.06524\tvalid_1's rmse: 1.08572\n",
      "[125]\ttraining's rmse: 1.06453\tvalid_1's rmse: 1.08524\n",
      "[126]\ttraining's rmse: 1.06377\tvalid_1's rmse: 1.0846\n",
      "[127]\ttraining's rmse: 1.0632\tvalid_1's rmse: 1.08408\n",
      "[128]\ttraining's rmse: 1.06249\tvalid_1's rmse: 1.08352\n",
      "[129]\ttraining's rmse: 1.06172\tvalid_1's rmse: 1.083\n",
      "[130]\ttraining's rmse: 1.06118\tvalid_1's rmse: 1.08266\n",
      "[131]\ttraining's rmse: 1.06042\tvalid_1's rmse: 1.08197\n",
      "[132]\ttraining's rmse: 1.0597\tvalid_1's rmse: 1.08137\n",
      "[133]\ttraining's rmse: 1.05903\tvalid_1's rmse: 1.08084\n",
      "[134]\ttraining's rmse: 1.05851\tvalid_1's rmse: 1.08049\n",
      "[135]\ttraining's rmse: 1.05773\tvalid_1's rmse: 1.07992\n",
      "[136]\ttraining's rmse: 1.05701\tvalid_1's rmse: 1.07948\n",
      "[137]\ttraining's rmse: 1.05651\tvalid_1's rmse: 1.07914\n",
      "[138]\ttraining's rmse: 1.05589\tvalid_1's rmse: 1.07875\n",
      "[139]\ttraining's rmse: 1.05522\tvalid_1's rmse: 1.07819\n",
      "[140]\ttraining's rmse: 1.05453\tvalid_1's rmse: 1.07771\n",
      "[141]\ttraining's rmse: 1.05405\tvalid_1's rmse: 1.07737\n",
      "[142]\ttraining's rmse: 1.05342\tvalid_1's rmse: 1.07689\n",
      "[143]\ttraining's rmse: 1.05274\tvalid_1's rmse: 1.0763\n",
      "[144]\ttraining's rmse: 1.05229\tvalid_1's rmse: 1.07607\n",
      "[145]\ttraining's rmse: 1.05163\tvalid_1's rmse: 1.07569\n",
      "[146]\ttraining's rmse: 1.05114\tvalid_1's rmse: 1.07544\n",
      "[147]\ttraining's rmse: 1.05052\tvalid_1's rmse: 1.07493\n",
      "[148]\ttraining's rmse: 1.04994\tvalid_1's rmse: 1.07451\n",
      "[149]\ttraining's rmse: 1.04948\tvalid_1's rmse: 1.07426\n",
      "[150]\ttraining's rmse: 1.04885\tvalid_1's rmse: 1.07384\n",
      "[151]\ttraining's rmse: 1.04841\tvalid_1's rmse: 1.07355\n",
      "[152]\ttraining's rmse: 1.04773\tvalid_1's rmse: 1.07309\n",
      "[153]\ttraining's rmse: 1.04719\tvalid_1's rmse: 1.07275\n",
      "[154]\ttraining's rmse: 1.04665\tvalid_1's rmse: 1.07237\n",
      "[155]\ttraining's rmse: 1.04622\tvalid_1's rmse: 1.07207\n",
      "[156]\ttraining's rmse: 1.04562\tvalid_1's rmse: 1.07172\n",
      "[157]\ttraining's rmse: 1.04508\tvalid_1's rmse: 1.07133\n",
      "[158]\ttraining's rmse: 1.04456\tvalid_1's rmse: 1.07097\n",
      "[159]\ttraining's rmse: 1.04415\tvalid_1's rmse: 1.07064\n",
      "[160]\ttraining's rmse: 1.04376\tvalid_1's rmse: 1.07049\n",
      "[161]\ttraining's rmse: 1.04311\tvalid_1's rmse: 1.07009\n",
      "[162]\ttraining's rmse: 1.04261\tvalid_1's rmse: 1.06973\n",
      "[163]\ttraining's rmse: 1.04205\tvalid_1's rmse: 1.06938\n",
      "[164]\ttraining's rmse: 1.04146\tvalid_1's rmse: 1.06891\n",
      "[165]\ttraining's rmse: 1.04108\tvalid_1's rmse: 1.06879\n",
      "[166]\ttraining's rmse: 1.0406\tvalid_1's rmse: 1.0684\n",
      "[167]\ttraining's rmse: 1.03999\tvalid_1's rmse: 1.06803\n",
      "[168]\ttraining's rmse: 1.03962\tvalid_1's rmse: 1.06791\n",
      "[169]\ttraining's rmse: 1.03908\tvalid_1's rmse: 1.0676\n",
      "[170]\ttraining's rmse: 1.03861\tvalid_1's rmse: 1.06728\n",
      "[171]\ttraining's rmse: 1.03814\tvalid_1's rmse: 1.06696\n",
      "[172]\ttraining's rmse: 1.03762\tvalid_1's rmse: 1.06664\n",
      "[173]\ttraining's rmse: 1.03724\tvalid_1's rmse: 1.06628\n",
      "[174]\ttraining's rmse: 1.03667\tvalid_1's rmse: 1.06594\n",
      "[175]\ttraining's rmse: 1.03621\tvalid_1's rmse: 1.06565\n",
      "[176]\ttraining's rmse: 1.03587\tvalid_1's rmse: 1.06557\n",
      "[177]\ttraining's rmse: 1.03534\tvalid_1's rmse: 1.06516\n",
      "[178]\ttraining's rmse: 1.03493\tvalid_1's rmse: 1.06489\n",
      "[179]\ttraining's rmse: 1.03457\tvalid_1's rmse: 1.06474\n",
      "[180]\ttraining's rmse: 1.03402\tvalid_1's rmse: 1.06442\n",
      "[181]\ttraining's rmse: 1.03353\tvalid_1's rmse: 1.06415\n",
      "[182]\ttraining's rmse: 1.03312\tvalid_1's rmse: 1.06382\n",
      "[183]\ttraining's rmse: 1.03277\tvalid_1's rmse: 1.06357\n",
      "[184]\ttraining's rmse: 1.03227\tvalid_1's rmse: 1.06319\n",
      "[185]\ttraining's rmse: 1.03186\tvalid_1's rmse: 1.06289\n",
      "[186]\ttraining's rmse: 1.03144\tvalid_1's rmse: 1.06263\n",
      "[187]\ttraining's rmse: 1.03109\tvalid_1's rmse: 1.06235\n",
      "[188]\ttraining's rmse: 1.03063\tvalid_1's rmse: 1.0621\n",
      "[189]\ttraining's rmse: 1.03025\tvalid_1's rmse: 1.06187\n",
      "[190]\ttraining's rmse: 1.02974\tvalid_1's rmse: 1.06158\n",
      "[191]\ttraining's rmse: 1.02943\tvalid_1's rmse: 1.06133\n",
      "[192]\ttraining's rmse: 1.02906\tvalid_1's rmse: 1.0611\n",
      "[193]\ttraining's rmse: 1.02857\tvalid_1's rmse: 1.06083\n",
      "[194]\ttraining's rmse: 1.02814\tvalid_1's rmse: 1.06059\n",
      "[195]\ttraining's rmse: 1.0278\tvalid_1's rmse: 1.06031\n",
      "[196]\ttraining's rmse: 1.02742\tvalid_1's rmse: 1.06002\n",
      "[197]\ttraining's rmse: 1.02703\tvalid_1's rmse: 1.05975\n",
      "[198]\ttraining's rmse: 1.0267\tvalid_1's rmse: 1.05962\n",
      "[199]\ttraining's rmse: 1.02628\tvalid_1's rmse: 1.05929\n",
      "[200]\ttraining's rmse: 1.02586\tvalid_1's rmse: 1.05905\n",
      "[201]\ttraining's rmse: 1.02557\tvalid_1's rmse: 1.0588\n",
      "[202]\ttraining's rmse: 1.02521\tvalid_1's rmse: 1.05858\n",
      "[203]\ttraining's rmse: 1.02475\tvalid_1's rmse: 1.05835\n",
      "[204]\ttraining's rmse: 1.02443\tvalid_1's rmse: 1.05816\n",
      "[205]\ttraining's rmse: 1.02402\tvalid_1's rmse: 1.0578\n",
      "[206]\ttraining's rmse: 1.02359\tvalid_1's rmse: 1.05757\n",
      "[207]\ttraining's rmse: 1.0232\tvalid_1's rmse: 1.05735\n",
      "[208]\ttraining's rmse: 1.02292\tvalid_1's rmse: 1.05717\n",
      "[209]\ttraining's rmse: 1.02257\tvalid_1's rmse: 1.05698\n",
      "[210]\ttraining's rmse: 1.02217\tvalid_1's rmse: 1.05667\n",
      "[211]\ttraining's rmse: 1.02188\tvalid_1's rmse: 1.05638\n",
      "[212]\ttraining's rmse: 1.0215\tvalid_1's rmse: 1.05617\n",
      "[213]\ttraining's rmse: 1.02123\tvalid_1's rmse: 1.056\n",
      "[214]\ttraining's rmse: 1.02089\tvalid_1's rmse: 1.05579\n",
      "[215]\ttraining's rmse: 1.02049\tvalid_1's rmse: 1.05556\n",
      "[216]\ttraining's rmse: 1.02006\tvalid_1's rmse: 1.05526\n",
      "[217]\ttraining's rmse: 1.01979\tvalid_1's rmse: 1.05502\n",
      "[218]\ttraining's rmse: 1.01939\tvalid_1's rmse: 1.05481\n",
      "[219]\ttraining's rmse: 1.0191\tvalid_1's rmse: 1.05471\n",
      "[220]\ttraining's rmse: 1.01869\tvalid_1's rmse: 1.05441\n",
      "[221]\ttraining's rmse: 1.01842\tvalid_1's rmse: 1.05425\n",
      "[222]\ttraining's rmse: 1.01813\tvalid_1's rmse: 1.05422\n",
      "[223]\ttraining's rmse: 1.01788\tvalid_1's rmse: 1.05419\n",
      "[224]\ttraining's rmse: 1.0175\tvalid_1's rmse: 1.05398\n",
      "[225]\ttraining's rmse: 1.01719\tvalid_1's rmse: 1.05382\n",
      "[226]\ttraining's rmse: 1.0168\tvalid_1's rmse: 1.05354\n",
      "[227]\ttraining's rmse: 1.01644\tvalid_1's rmse: 1.05325\n",
      "[228]\ttraining's rmse: 1.01614\tvalid_1's rmse: 1.053\n",
      "[229]\ttraining's rmse: 1.01582\tvalid_1's rmse: 1.0528\n",
      "[230]\ttraining's rmse: 1.01557\tvalid_1's rmse: 1.05266\n",
      "[231]\ttraining's rmse: 1.01532\tvalid_1's rmse: 1.05252\n",
      "[232]\ttraining's rmse: 1.01505\tvalid_1's rmse: 1.05247\n",
      "[233]\ttraining's rmse: 1.01476\tvalid_1's rmse: 1.0523\n",
      "[234]\ttraining's rmse: 1.01445\tvalid_1's rmse: 1.0521\n",
      "[235]\ttraining's rmse: 1.01416\tvalid_1's rmse: 1.05187\n",
      "[236]\ttraining's rmse: 1.01388\tvalid_1's rmse: 1.05176\n",
      "[237]\ttraining's rmse: 1.01357\tvalid_1's rmse: 1.05158\n",
      "[238]\ttraining's rmse: 1.01333\tvalid_1's rmse: 1.05146\n",
      "[239]\ttraining's rmse: 1.01309\tvalid_1's rmse: 1.05131\n",
      "[240]\ttraining's rmse: 1.01273\tvalid_1's rmse: 1.05094\n",
      "[241]\ttraining's rmse: 1.01247\tvalid_1's rmse: 1.05095\n",
      "[242]\ttraining's rmse: 1.01219\tvalid_1's rmse: 1.05082\n",
      "[243]\ttraining's rmse: 1.01192\tvalid_1's rmse: 1.05071\n",
      "[244]\ttraining's rmse: 1.01158\tvalid_1's rmse: 1.05053\n",
      "[245]\ttraining's rmse: 1.01134\tvalid_1's rmse: 1.05048\n",
      "[246]\ttraining's rmse: 1.01106\tvalid_1's rmse: 1.05033\n",
      "[247]\ttraining's rmse: 1.01072\tvalid_1's rmse: 1.04997\n",
      "[248]\ttraining's rmse: 1.0104\tvalid_1's rmse: 1.04967\n",
      "[249]\ttraining's rmse: 1.01015\tvalid_1's rmse: 1.04962\n",
      "[250]\ttraining's rmse: 1.00989\tvalid_1's rmse: 1.04951\n",
      "[251]\ttraining's rmse: 1.00957\tvalid_1's rmse: 1.04926\n",
      "[252]\ttraining's rmse: 1.00935\tvalid_1's rmse: 1.04914\n",
      "[253]\ttraining's rmse: 1.00913\tvalid_1's rmse: 1.04903\n",
      "[254]\ttraining's rmse: 1.00881\tvalid_1's rmse: 1.04869\n",
      "[255]\ttraining's rmse: 1.00856\tvalid_1's rmse: 1.04863\n",
      "[256]\ttraining's rmse: 1.00831\tvalid_1's rmse: 1.0485\n",
      "[257]\ttraining's rmse: 1.00807\tvalid_1's rmse: 1.04846\n",
      "[258]\ttraining's rmse: 1.00781\tvalid_1's rmse: 1.04832\n",
      "[259]\ttraining's rmse: 1.00751\tvalid_1's rmse: 1.04806\n",
      "[260]\ttraining's rmse: 1.0073\tvalid_1's rmse: 1.04795\n",
      "[261]\ttraining's rmse: 1.00707\tvalid_1's rmse: 1.04789\n",
      "[262]\ttraining's rmse: 1.00683\tvalid_1's rmse: 1.04781\n",
      "[263]\ttraining's rmse: 1.0066\tvalid_1's rmse: 1.04778\n",
      "[264]\ttraining's rmse: 1.0063\tvalid_1's rmse: 1.04762\n",
      "[265]\ttraining's rmse: 1.006\tvalid_1's rmse: 1.04736\n",
      "[266]\ttraining's rmse: 1.00577\tvalid_1's rmse: 1.04727\n",
      "[267]\ttraining's rmse: 1.00557\tvalid_1's rmse: 1.04717\n",
      "[268]\ttraining's rmse: 1.00534\tvalid_1's rmse: 1.04711\n",
      "[269]\ttraining's rmse: 1.00505\tvalid_1's rmse: 1.0469\n",
      "[270]\ttraining's rmse: 1.00483\tvalid_1's rmse: 1.04687\n",
      "[271]\ttraining's rmse: 1.00453\tvalid_1's rmse: 1.04671\n",
      "[272]\ttraining's rmse: 1.00425\tvalid_1's rmse: 1.04646\n",
      "[273]\ttraining's rmse: 1.00403\tvalid_1's rmse: 1.04636\n",
      "[274]\ttraining's rmse: 1.0038\tvalid_1's rmse: 1.04631\n",
      "[275]\ttraining's rmse: 1.00358\tvalid_1's rmse: 1.04626\n",
      "[276]\ttraining's rmse: 1.00336\tvalid_1's rmse: 1.04612\n",
      "[277]\ttraining's rmse: 1.00314\tvalid_1's rmse: 1.04606\n",
      "[278]\ttraining's rmse: 1.00292\tvalid_1's rmse: 1.04598\n",
      "[279]\ttraining's rmse: 1.00265\tvalid_1's rmse: 1.04576\n",
      "[280]\ttraining's rmse: 1.00239\tvalid_1's rmse: 1.04552\n",
      "[281]\ttraining's rmse: 1.00212\tvalid_1's rmse: 1.04535\n",
      "[282]\ttraining's rmse: 1.00192\tvalid_1's rmse: 1.04532\n",
      "[283]\ttraining's rmse: 1.00171\tvalid_1's rmse: 1.04526\n",
      "[284]\ttraining's rmse: 1.0015\tvalid_1's rmse: 1.04506\n",
      "[285]\ttraining's rmse: 1.0013\tvalid_1's rmse: 1.04504\n",
      "[286]\ttraining's rmse: 1.00112\tvalid_1's rmse: 1.04499\n",
      "[287]\ttraining's rmse: 1.00093\tvalid_1's rmse: 1.04488\n",
      "[288]\ttraining's rmse: 1.00072\tvalid_1's rmse: 1.04483\n",
      "[289]\ttraining's rmse: 1.00047\tvalid_1's rmse: 1.04464\n",
      "[290]\ttraining's rmse: 1.00022\tvalid_1's rmse: 1.04458\n",
      "[291]\ttraining's rmse: 0.999949\tvalid_1's rmse: 1.04445\n",
      "[292]\ttraining's rmse: 0.999704\tvalid_1's rmse: 1.04439\n",
      "[293]\ttraining's rmse: 0.999504\tvalid_1's rmse: 1.04435\n",
      "[294]\ttraining's rmse: 0.999302\tvalid_1's rmse: 1.04426\n",
      "[295]\ttraining's rmse: 0.999064\tvalid_1's rmse: 1.0442\n",
      "[296]\ttraining's rmse: 0.998817\tvalid_1's rmse: 1.04405\n",
      "[297]\ttraining's rmse: 0.998583\tvalid_1's rmse: 1.04399\n",
      "[298]\ttraining's rmse: 0.998379\tvalid_1's rmse: 1.04396\n",
      "[299]\ttraining's rmse: 0.998138\tvalid_1's rmse: 1.04376\n",
      "[300]\ttraining's rmse: 0.997936\tvalid_1's rmse: 1.04373\n",
      "[301]\ttraining's rmse: 0.997707\tvalid_1's rmse: 1.04364\n",
      "[302]\ttraining's rmse: 0.997481\tvalid_1's rmse: 1.04359\n",
      "[303]\ttraining's rmse: 0.997243\tvalid_1's rmse: 1.04344\n",
      "[304]\ttraining's rmse: 0.997011\tvalid_1's rmse: 1.04333\n",
      "[305]\ttraining's rmse: 0.996791\tvalid_1's rmse: 1.04328\n",
      "[306]\ttraining's rmse: 0.996605\tvalid_1's rmse: 1.04324\n",
      "[307]\ttraining's rmse: 0.996355\tvalid_1's rmse: 1.04313\n",
      "[308]\ttraining's rmse: 0.996127\tvalid_1's rmse: 1.04299\n",
      "[309]\ttraining's rmse: 0.99591\tvalid_1's rmse: 1.04294\n",
      "[310]\ttraining's rmse: 0.995696\tvalid_1's rmse: 1.04289\n",
      "[311]\ttraining's rmse: 0.995502\tvalid_1's rmse: 1.0429\n",
      "[312]\ttraining's rmse: 0.995282\tvalid_1's rmse: 1.04282\n",
      "[313]\ttraining's rmse: 0.995074\tvalid_1's rmse: 1.04278\n",
      "[314]\ttraining's rmse: 0.994852\tvalid_1's rmse: 1.04261\n",
      "[315]\ttraining's rmse: 0.994665\tvalid_1's rmse: 1.04251\n",
      "[316]\ttraining's rmse: 0.994489\tvalid_1's rmse: 1.04247\n",
      "[317]\ttraining's rmse: 0.994283\tvalid_1's rmse: 1.04244\n",
      "[318]\ttraining's rmse: 0.994064\tvalid_1's rmse: 1.04231\n",
      "[319]\ttraining's rmse: 0.993862\tvalid_1's rmse: 1.04228\n",
      "[320]\ttraining's rmse: 0.993659\tvalid_1's rmse: 1.04221\n",
      "[321]\ttraining's rmse: 0.993441\tvalid_1's rmse: 1.04205\n",
      "[322]\ttraining's rmse: 0.993256\tvalid_1's rmse: 1.04206\n",
      "[323]\ttraining's rmse: 0.993061\tvalid_1's rmse: 1.04201\n",
      "[324]\ttraining's rmse: 0.992883\tvalid_1's rmse: 1.04192\n",
      "[325]\ttraining's rmse: 0.992652\tvalid_1's rmse: 1.04182\n",
      "[326]\ttraining's rmse: 0.992488\tvalid_1's rmse: 1.04175\n",
      "[327]\ttraining's rmse: 0.992297\tvalid_1's rmse: 1.04171\n",
      "[328]\ttraining's rmse: 0.992103\tvalid_1's rmse: 1.04165\n",
      "[329]\ttraining's rmse: 0.991937\tvalid_1's rmse: 1.04163\n",
      "[330]\ttraining's rmse: 0.99176\tvalid_1's rmse: 1.04163\n",
      "[331]\ttraining's rmse: 0.991571\tvalid_1's rmse: 1.0416\n",
      "[332]\ttraining's rmse: 0.991363\tvalid_1's rmse: 1.04146\n",
      "[333]\ttraining's rmse: 0.991197\tvalid_1's rmse: 1.04139\n",
      "[334]\ttraining's rmse: 0.99102\tvalid_1's rmse: 1.04128\n",
      "[335]\ttraining's rmse: 0.990838\tvalid_1's rmse: 1.04123\n",
      "[336]\ttraining's rmse: 0.990597\tvalid_1's rmse: 1.0411\n",
      "[337]\ttraining's rmse: 0.990395\tvalid_1's rmse: 1.04096\n",
      "[338]\ttraining's rmse: 0.990207\tvalid_1's rmse: 1.04095\n",
      "[339]\ttraining's rmse: 0.989972\tvalid_1's rmse: 1.04082\n",
      "[340]\ttraining's rmse: 0.989756\tvalid_1's rmse: 1.04073\n",
      "[341]\ttraining's rmse: 0.989586\tvalid_1's rmse: 1.04074\n",
      "[342]\ttraining's rmse: 0.989409\tvalid_1's rmse: 1.04065\n",
      "[343]\ttraining's rmse: 0.989229\tvalid_1's rmse: 1.04059\n",
      "[344]\ttraining's rmse: 0.989078\tvalid_1's rmse: 1.04052\n",
      "[345]\ttraining's rmse: 0.988926\tvalid_1's rmse: 1.04053\n",
      "[346]\ttraining's rmse: 0.988758\tvalid_1's rmse: 1.04044\n",
      "[347]\ttraining's rmse: 0.988531\tvalid_1's rmse: 1.04032\n",
      "[348]\ttraining's rmse: 0.98836\tvalid_1's rmse: 1.04029\n",
      "[349]\ttraining's rmse: 0.988213\tvalid_1's rmse: 1.04024\n",
      "[350]\ttraining's rmse: 0.988054\tvalid_1's rmse: 1.0402\n",
      "[351]\ttraining's rmse: 0.987901\tvalid_1's rmse: 1.04017\n",
      "[352]\ttraining's rmse: 0.98768\tvalid_1's rmse: 1.04006\n",
      "[353]\ttraining's rmse: 0.987516\tvalid_1's rmse: 1.03999\n",
      "[354]\ttraining's rmse: 0.98733\tvalid_1's rmse: 1.03982\n",
      "[355]\ttraining's rmse: 0.987169\tvalid_1's rmse: 1.03984\n",
      "[356]\ttraining's rmse: 0.987014\tvalid_1's rmse: 1.03978\n",
      "[357]\ttraining's rmse: 0.9868\tvalid_1's rmse: 1.03967\n",
      "[358]\ttraining's rmse: 0.986628\tvalid_1's rmse: 1.03967\n",
      "[359]\ttraining's rmse: 0.986382\tvalid_1's rmse: 1.03951\n",
      "[360]\ttraining's rmse: 0.986216\tvalid_1's rmse: 1.03945\n",
      "[361]\ttraining's rmse: 0.98606\tvalid_1's rmse: 1.03946\n",
      "[362]\ttraining's rmse: 0.985892\tvalid_1's rmse: 1.0394\n",
      "[363]\ttraining's rmse: 0.985684\tvalid_1's rmse: 1.03929\n",
      "[364]\ttraining's rmse: 0.985519\tvalid_1's rmse: 1.03924\n",
      "[365]\ttraining's rmse: 0.985355\tvalid_1's rmse: 1.03918\n",
      "[366]\ttraining's rmse: 0.985112\tvalid_1's rmse: 1.03902\n",
      "[367]\ttraining's rmse: 0.984962\tvalid_1's rmse: 1.03893\n",
      "[368]\ttraining's rmse: 0.984802\tvalid_1's rmse: 1.03888\n",
      "[369]\ttraining's rmse: 0.984567\tvalid_1's rmse: 1.03874\n",
      "[370]\ttraining's rmse: 0.984417\tvalid_1's rmse: 1.03876\n",
      "[371]\ttraining's rmse: 0.984216\tvalid_1's rmse: 1.03866\n",
      "[372]\ttraining's rmse: 0.984055\tvalid_1's rmse: 1.03864\n",
      "[373]\ttraining's rmse: 0.983889\tvalid_1's rmse: 1.03858\n",
      "[374]\ttraining's rmse: 0.983662\tvalid_1's rmse: 1.03846\n",
      "[375]\ttraining's rmse: 0.98352\tvalid_1's rmse: 1.03842\n",
      "[376]\ttraining's rmse: 0.983362\tvalid_1's rmse: 1.03839\n",
      "[377]\ttraining's rmse: 0.983208\tvalid_1's rmse: 1.03834\n",
      "[378]\ttraining's rmse: 0.982984\tvalid_1's rmse: 1.03825\n",
      "[379]\ttraining's rmse: 0.98284\tvalid_1's rmse: 1.03827\n",
      "[380]\ttraining's rmse: 0.982701\tvalid_1's rmse: 1.03825\n",
      "[381]\ttraining's rmse: 0.982478\tvalid_1's rmse: 1.03815\n",
      "[382]\ttraining's rmse: 0.982326\tvalid_1's rmse: 1.03808\n",
      "[383]\ttraining's rmse: 0.982133\tvalid_1's rmse: 1.03801\n",
      "[384]\ttraining's rmse: 0.981981\tvalid_1's rmse: 1.038\n",
      "[385]\ttraining's rmse: 0.981771\tvalid_1's rmse: 1.03788\n",
      "[386]\ttraining's rmse: 0.98162\tvalid_1's rmse: 1.03785\n",
      "[387]\ttraining's rmse: 0.981474\tvalid_1's rmse: 1.03783\n",
      "[388]\ttraining's rmse: 0.98134\tvalid_1's rmse: 1.03782\n",
      "[389]\ttraining's rmse: 0.981188\tvalid_1's rmse: 1.03776\n",
      "[390]\ttraining's rmse: 0.98105\tvalid_1's rmse: 1.03778\n",
      "[391]\ttraining's rmse: 0.980841\tvalid_1's rmse: 1.03769\n",
      "[392]\ttraining's rmse: 0.980689\tvalid_1's rmse: 1.03764\n",
      "[393]\ttraining's rmse: 0.980503\tvalid_1's rmse: 1.03758\n",
      "[394]\ttraining's rmse: 0.980315\tvalid_1's rmse: 1.0375\n",
      "[395]\ttraining's rmse: 0.98017\tvalid_1's rmse: 1.03747\n",
      "[396]\ttraining's rmse: 0.980032\tvalid_1's rmse: 1.03741\n",
      "[397]\ttraining's rmse: 0.979898\tvalid_1's rmse: 1.03742\n",
      "[398]\ttraining's rmse: 0.979771\tvalid_1's rmse: 1.03739\n",
      "[399]\ttraining's rmse: 0.979564\tvalid_1's rmse: 1.03731\n",
      "[400]\ttraining's rmse: 0.979367\tvalid_1's rmse: 1.03723\n",
      "[401]\ttraining's rmse: 0.97921\tvalid_1's rmse: 1.03718\n",
      "[402]\ttraining's rmse: 0.979065\tvalid_1's rmse: 1.03715\n",
      "[403]\ttraining's rmse: 0.978887\tvalid_1's rmse: 1.03709\n",
      "[404]\ttraining's rmse: 0.978738\tvalid_1's rmse: 1.03707\n",
      "[405]\ttraining's rmse: 0.978587\tvalid_1's rmse: 1.03705\n",
      "[406]\ttraining's rmse: 0.978392\tvalid_1's rmse: 1.03695\n",
      "[407]\ttraining's rmse: 0.978244\tvalid_1's rmse: 1.03693\n",
      "[408]\ttraining's rmse: 0.978107\tvalid_1's rmse: 1.03691\n",
      "[409]\ttraining's rmse: 0.977958\tvalid_1's rmse: 1.03683\n",
      "[410]\ttraining's rmse: 0.977762\tvalid_1's rmse: 1.03676\n",
      "[411]\ttraining's rmse: 0.97761\tvalid_1's rmse: 1.03664\n",
      "[412]\ttraining's rmse: 0.97746\tvalid_1's rmse: 1.03653\n",
      "[413]\ttraining's rmse: 0.977316\tvalid_1's rmse: 1.03651\n",
      "[414]\ttraining's rmse: 0.977187\tvalid_1's rmse: 1.03652\n",
      "[415]\ttraining's rmse: 0.977053\tvalid_1's rmse: 1.03648\n",
      "[416]\ttraining's rmse: 0.97686\tvalid_1's rmse: 1.03639\n",
      "[417]\ttraining's rmse: 0.976714\tvalid_1's rmse: 1.03629\n",
      "[418]\ttraining's rmse: 0.976574\tvalid_1's rmse: 1.03627\n",
      "[419]\ttraining's rmse: 0.976445\tvalid_1's rmse: 1.03625\n",
      "[420]\ttraining's rmse: 0.976305\tvalid_1's rmse: 1.03624\n",
      "[421]\ttraining's rmse: 0.97612\tvalid_1's rmse: 1.03615\n",
      "[422]\ttraining's rmse: 0.976002\tvalid_1's rmse: 1.03612\n",
      "[423]\ttraining's rmse: 0.975867\tvalid_1's rmse: 1.03612\n",
      "[424]\ttraining's rmse: 0.975676\tvalid_1's rmse: 1.03605\n",
      "[425]\ttraining's rmse: 0.975538\tvalid_1's rmse: 1.03599\n",
      "[426]\ttraining's rmse: 0.975397\tvalid_1's rmse: 1.03588\n",
      "[427]\ttraining's rmse: 0.975276\tvalid_1's rmse: 1.0359\n",
      "[428]\ttraining's rmse: 0.975098\tvalid_1's rmse: 1.03579\n",
      "[429]\ttraining's rmse: 0.974913\tvalid_1's rmse: 1.03581\n",
      "[430]\ttraining's rmse: 0.974788\tvalid_1's rmse: 1.03578\n",
      "[431]\ttraining's rmse: 0.974651\tvalid_1's rmse: 1.03568\n",
      "[432]\ttraining's rmse: 0.97453\tvalid_1's rmse: 1.03567\n",
      "[433]\ttraining's rmse: 0.974388\tvalid_1's rmse: 1.03568\n",
      "[434]\ttraining's rmse: 0.974273\tvalid_1's rmse: 1.03563\n",
      "[435]\ttraining's rmse: 0.974091\tvalid_1's rmse: 1.03553\n",
      "[436]\ttraining's rmse: 0.973957\tvalid_1's rmse: 1.03551\n",
      "[437]\ttraining's rmse: 0.973834\tvalid_1's rmse: 1.0355\n",
      "[438]\ttraining's rmse: 0.973655\tvalid_1's rmse: 1.03552\n",
      "[439]\ttraining's rmse: 0.973536\tvalid_1's rmse: 1.03549\n",
      "[440]\ttraining's rmse: 0.973403\tvalid_1's rmse: 1.03538\n",
      "[441]\ttraining's rmse: 0.973276\tvalid_1's rmse: 1.03538\n",
      "[442]\ttraining's rmse: 0.973154\tvalid_1's rmse: 1.03534\n",
      "[443]\ttraining's rmse: 0.973038\tvalid_1's rmse: 1.03534\n",
      "[444]\ttraining's rmse: 0.972911\tvalid_1's rmse: 1.03526\n",
      "[445]\ttraining's rmse: 0.972741\tvalid_1's rmse: 1.0352\n",
      "[446]\ttraining's rmse: 0.972614\tvalid_1's rmse: 1.0351\n",
      "[447]\ttraining's rmse: 0.972492\tvalid_1's rmse: 1.0351\n",
      "[448]\ttraining's rmse: 0.972375\tvalid_1's rmse: 1.03513\n",
      "[449]\ttraining's rmse: 0.972201\tvalid_1's rmse: 1.03506\n",
      "[450]\ttraining's rmse: 0.972084\tvalid_1's rmse: 1.03503\n",
      "[451]\ttraining's rmse: 0.971915\tvalid_1's rmse: 1.03505\n",
      "[452]\ttraining's rmse: 0.971788\tvalid_1's rmse: 1.03495\n",
      "[453]\ttraining's rmse: 0.971671\tvalid_1's rmse: 1.03492\n",
      "[454]\ttraining's rmse: 0.971548\tvalid_1's rmse: 1.03493\n",
      "[455]\ttraining's rmse: 0.971428\tvalid_1's rmse: 1.03493\n",
      "[456]\ttraining's rmse: 0.971295\tvalid_1's rmse: 1.03493\n",
      "[457]\ttraining's rmse: 0.971129\tvalid_1's rmse: 1.03494\n",
      "[458]\ttraining's rmse: 0.970977\tvalid_1's rmse: 1.03489\n",
      "[459]\ttraining's rmse: 0.970863\tvalid_1's rmse: 1.03482\n",
      "[460]\ttraining's rmse: 0.970747\tvalid_1's rmse: 1.03477\n",
      "[461]\ttraining's rmse: 0.97061\tvalid_1's rmse: 1.03458\n",
      "[462]\ttraining's rmse: 0.97048\tvalid_1's rmse: 1.03458\n",
      "[463]\ttraining's rmse: 0.970367\tvalid_1's rmse: 1.03457\n",
      "[464]\ttraining's rmse: 0.970248\tvalid_1's rmse: 1.03448\n",
      "[465]\ttraining's rmse: 0.970139\tvalid_1's rmse: 1.03448\n",
      "[466]\ttraining's rmse: 0.969973\tvalid_1's rmse: 1.03439\n",
      "[467]\ttraining's rmse: 0.969855\tvalid_1's rmse: 1.0343\n",
      "[468]\ttraining's rmse: 0.969744\tvalid_1's rmse: 1.03427\n",
      "[469]\ttraining's rmse: 0.969586\tvalid_1's rmse: 1.03429\n",
      "[470]\ttraining's rmse: 0.969473\tvalid_1's rmse: 1.03422\n",
      "[471]\ttraining's rmse: 0.969358\tvalid_1's rmse: 1.03424\n",
      "[472]\ttraining's rmse: 0.969232\tvalid_1's rmse: 1.03425\n",
      "[473]\ttraining's rmse: 0.969123\tvalid_1's rmse: 1.03425\n",
      "[474]\ttraining's rmse: 0.968968\tvalid_1's rmse: 1.03427\n",
      "[475]\ttraining's rmse: 0.968852\tvalid_1's rmse: 1.03428\n",
      "[476]\ttraining's rmse: 0.968741\tvalid_1's rmse: 1.03429\n",
      "[477]\ttraining's rmse: 0.968629\tvalid_1's rmse: 1.03428\n",
      "[478]\ttraining's rmse: 0.968523\tvalid_1's rmse: 1.03426\n",
      "[479]\ttraining's rmse: 0.968408\tvalid_1's rmse: 1.03427\n",
      "[480]\ttraining's rmse: 0.968297\tvalid_1's rmse: 1.03419\n",
      "[481]\ttraining's rmse: 0.968145\tvalid_1's rmse: 1.03414\n",
      "[482]\ttraining's rmse: 0.968014\tvalid_1's rmse: 1.03396\n",
      "[483]\ttraining's rmse: 0.967908\tvalid_1's rmse: 1.03395\n",
      "[484]\ttraining's rmse: 0.967801\tvalid_1's rmse: 1.03389\n",
      "[485]\ttraining's rmse: 0.967698\tvalid_1's rmse: 1.03387\n",
      "[486]\ttraining's rmse: 0.967573\tvalid_1's rmse: 1.03383\n",
      "[487]\ttraining's rmse: 0.967435\tvalid_1's rmse: 1.03382\n",
      "[488]\ttraining's rmse: 0.967325\tvalid_1's rmse: 1.03384\n",
      "[489]\ttraining's rmse: 0.967212\tvalid_1's rmse: 1.03375\n",
      "[490]\ttraining's rmse: 0.967072\tvalid_1's rmse: 1.03371\n",
      "[491]\ttraining's rmse: 0.966942\tvalid_1's rmse: 1.03373\n",
      "[492]\ttraining's rmse: 0.966819\tvalid_1's rmse: 1.03359\n",
      "[493]\ttraining's rmse: 0.9667\tvalid_1's rmse: 1.03359\n",
      "[494]\ttraining's rmse: 0.966612\tvalid_1's rmse: 1.0336\n",
      "[495]\ttraining's rmse: 0.966501\tvalid_1's rmse: 1.03357\n",
      "[496]\ttraining's rmse: 0.966391\tvalid_1's rmse: 1.03358\n",
      "[497]\ttraining's rmse: 0.966245\tvalid_1's rmse: 1.0336\n",
      "[498]\ttraining's rmse: 0.966117\tvalid_1's rmse: 1.03359\n",
      "[499]\ttraining's rmse: 0.966004\tvalid_1's rmse: 1.03353\n",
      "[500]\ttraining's rmse: 0.965868\tvalid_1's rmse: 1.0335\n",
      "[501]\ttraining's rmse: 0.965743\tvalid_1's rmse: 1.03352\n",
      "[502]\ttraining's rmse: 0.965639\tvalid_1's rmse: 1.03349\n",
      "[503]\ttraining's rmse: 0.965514\tvalid_1's rmse: 1.03352\n",
      "[504]\ttraining's rmse: 0.965411\tvalid_1's rmse: 1.03348\n",
      "[505]\ttraining's rmse: 0.965278\tvalid_1's rmse: 1.03345\n",
      "[506]\ttraining's rmse: 0.965156\tvalid_1's rmse: 1.03346\n",
      "[507]\ttraining's rmse: 0.965015\tvalid_1's rmse: 1.03347\n",
      "[508]\ttraining's rmse: 0.964893\tvalid_1's rmse: 1.0334\n",
      "[509]\ttraining's rmse: 0.964772\tvalid_1's rmse: 1.03342\n",
      "[510]\ttraining's rmse: 0.964653\tvalid_1's rmse: 1.03336\n",
      "[511]\ttraining's rmse: 0.964549\tvalid_1's rmse: 1.03325\n",
      "[512]\ttraining's rmse: 0.964428\tvalid_1's rmse: 1.03328\n",
      "[513]\ttraining's rmse: 0.964325\tvalid_1's rmse: 1.03328\n",
      "[514]\ttraining's rmse: 0.964197\tvalid_1's rmse: 1.03325\n",
      "[515]\ttraining's rmse: 0.964085\tvalid_1's rmse: 1.0333\n",
      "[516]\ttraining's rmse: 0.963962\tvalid_1's rmse: 1.03314\n",
      "[517]\ttraining's rmse: 0.963828\tvalid_1's rmse: 1.03315\n",
      "[518]\ttraining's rmse: 0.963712\tvalid_1's rmse: 1.03309\n",
      "[519]\ttraining's rmse: 0.9636\tvalid_1's rmse: 1.03313\n",
      "[520]\ttraining's rmse: 0.963475\tvalid_1's rmse: 1.03312\n",
      "[521]\ttraining's rmse: 0.963356\tvalid_1's rmse: 1.03296\n",
      "[522]\ttraining's rmse: 0.963243\tvalid_1's rmse: 1.0329\n",
      "[523]\ttraining's rmse: 0.963107\tvalid_1's rmse: 1.03286\n",
      "[524]\ttraining's rmse: 0.963005\tvalid_1's rmse: 1.03278\n",
      "[525]\ttraining's rmse: 0.962908\tvalid_1's rmse: 1.03277\n",
      "[526]\ttraining's rmse: 0.962815\tvalid_1's rmse: 1.03274\n",
      "[527]\ttraining's rmse: 0.962707\tvalid_1's rmse: 1.03279\n",
      "[528]\ttraining's rmse: 0.962569\tvalid_1's rmse: 1.03274\n",
      "[529]\ttraining's rmse: 0.96246\tvalid_1's rmse: 1.03268\n",
      "[530]\ttraining's rmse: 0.962339\tvalid_1's rmse: 1.03265\n",
      "[531]\ttraining's rmse: 0.962242\tvalid_1's rmse: 1.03265\n",
      "[532]\ttraining's rmse: 0.962158\tvalid_1's rmse: 1.03264\n",
      "[533]\ttraining's rmse: 0.962023\tvalid_1's rmse: 1.03258\n",
      "[534]\ttraining's rmse: 0.961914\tvalid_1's rmse: 1.03252\n",
      "[535]\ttraining's rmse: 0.961812\tvalid_1's rmse: 1.03253\n",
      "[536]\ttraining's rmse: 0.961711\tvalid_1's rmse: 1.03252\n",
      "[537]\ttraining's rmse: 0.961578\tvalid_1's rmse: 1.03248\n",
      "[538]\ttraining's rmse: 0.961472\tvalid_1's rmse: 1.03241\n",
      "[539]\ttraining's rmse: 0.961361\tvalid_1's rmse: 1.03242\n",
      "[540]\ttraining's rmse: 0.961262\tvalid_1's rmse: 1.03242\n",
      "[541]\ttraining's rmse: 0.961147\tvalid_1's rmse: 1.03226\n",
      "[542]\ttraining's rmse: 0.961056\tvalid_1's rmse: 1.03226\n",
      "[543]\ttraining's rmse: 0.960943\tvalid_1's rmse: 1.03221\n",
      "[544]\ttraining's rmse: 0.960826\tvalid_1's rmse: 1.0322\n",
      "[545]\ttraining's rmse: 0.960732\tvalid_1's rmse: 1.03216\n",
      "[546]\ttraining's rmse: 0.96063\tvalid_1's rmse: 1.03222\n",
      "[547]\ttraining's rmse: 0.960497\tvalid_1's rmse: 1.03218\n",
      "[548]\ttraining's rmse: 0.960388\tvalid_1's rmse: 1.0321\n",
      "[549]\ttraining's rmse: 0.960279\tvalid_1's rmse: 1.03212\n",
      "[550]\ttraining's rmse: 0.960177\tvalid_1's rmse: 1.03209\n",
      "[551]\ttraining's rmse: 0.960068\tvalid_1's rmse: 1.03211\n",
      "[552]\ttraining's rmse: 0.959965\tvalid_1's rmse: 1.03204\n",
      "[553]\ttraining's rmse: 0.959853\tvalid_1's rmse: 1.03189\n",
      "[554]\ttraining's rmse: 0.959734\tvalid_1's rmse: 1.03189\n",
      "[555]\ttraining's rmse: 0.959619\tvalid_1's rmse: 1.03187\n",
      "[556]\ttraining's rmse: 0.959522\tvalid_1's rmse: 1.03192\n",
      "[557]\ttraining's rmse: 0.959421\tvalid_1's rmse: 1.03186\n",
      "[558]\ttraining's rmse: 0.959312\tvalid_1's rmse: 1.03171\n",
      "[559]\ttraining's rmse: 0.959218\tvalid_1's rmse: 1.03174\n",
      "[560]\ttraining's rmse: 0.959093\tvalid_1's rmse: 1.03168\n",
      "[561]\ttraining's rmse: 0.959001\tvalid_1's rmse: 1.03167\n",
      "[562]\ttraining's rmse: 0.958914\tvalid_1's rmse: 1.03159\n",
      "[563]\ttraining's rmse: 0.95882\tvalid_1's rmse: 1.03152\n",
      "[564]\ttraining's rmse: 0.958723\tvalid_1's rmse: 1.03152\n",
      "[565]\ttraining's rmse: 0.958604\tvalid_1's rmse: 1.03149\n",
      "[566]\ttraining's rmse: 0.958504\tvalid_1's rmse: 1.03152\n",
      "[567]\ttraining's rmse: 0.958392\tvalid_1's rmse: 1.03152\n",
      "[568]\ttraining's rmse: 0.958294\tvalid_1's rmse: 1.0315\n",
      "[569]\ttraining's rmse: 0.958184\tvalid_1's rmse: 1.0315\n",
      "[570]\ttraining's rmse: 0.958079\tvalid_1's rmse: 1.03151\n",
      "[571]\ttraining's rmse: 0.957985\tvalid_1's rmse: 1.03153\n",
      "[572]\ttraining's rmse: 0.957901\tvalid_1's rmse: 1.03146\n",
      "[573]\ttraining's rmse: 0.957807\tvalid_1's rmse: 1.03144\n",
      "[574]\ttraining's rmse: 0.957698\tvalid_1's rmse: 1.03144\n",
      "[575]\ttraining's rmse: 0.957608\tvalid_1's rmse: 1.03144\n",
      "[576]\ttraining's rmse: 0.957515\tvalid_1's rmse: 1.03145\n",
      "[577]\ttraining's rmse: 0.957391\tvalid_1's rmse: 1.03141\n",
      "[578]\ttraining's rmse: 0.957314\tvalid_1's rmse: 1.03135\n",
      "[579]\ttraining's rmse: 0.957213\tvalid_1's rmse: 1.03134\n",
      "[580]\ttraining's rmse: 0.957116\tvalid_1's rmse: 1.03135\n",
      "[581]\ttraining's rmse: 0.957008\tvalid_1's rmse: 1.03123\n",
      "[582]\ttraining's rmse: 0.95692\tvalid_1's rmse: 1.0312\n",
      "[583]\ttraining's rmse: 0.956826\tvalid_1's rmse: 1.03119\n",
      "[584]\ttraining's rmse: 0.956716\tvalid_1's rmse: 1.03118\n",
      "[585]\ttraining's rmse: 0.956629\tvalid_1's rmse: 1.03119\n",
      "[586]\ttraining's rmse: 0.956532\tvalid_1's rmse: 1.03123\n",
      "[587]\ttraining's rmse: 0.956445\tvalid_1's rmse: 1.03122\n",
      "[588]\ttraining's rmse: 0.956357\tvalid_1's rmse: 1.03115\n",
      "[589]\ttraining's rmse: 0.956246\tvalid_1's rmse: 1.0311\n",
      "[590]\ttraining's rmse: 0.956153\tvalid_1's rmse: 1.03116\n",
      "[591]\ttraining's rmse: 0.956047\tvalid_1's rmse: 1.03114\n",
      "[592]\ttraining's rmse: 0.955957\tvalid_1's rmse: 1.03114\n",
      "[593]\ttraining's rmse: 0.955856\tvalid_1's rmse: 1.03108\n",
      "[594]\ttraining's rmse: 0.955749\tvalid_1's rmse: 1.03106\n",
      "[595]\ttraining's rmse: 0.955695\tvalid_1's rmse: 1.03104\n",
      "[596]\ttraining's rmse: 0.955593\tvalid_1's rmse: 1.031\n",
      "[597]\ttraining's rmse: 0.955513\tvalid_1's rmse: 1.031\n",
      "[598]\ttraining's rmse: 0.955425\tvalid_1's rmse: 1.031\n",
      "[599]\ttraining's rmse: 0.955308\tvalid_1's rmse: 1.03095\n",
      "[600]\ttraining's rmse: 0.955206\tvalid_1's rmse: 1.03093\n",
      "[601]\ttraining's rmse: 0.955154\tvalid_1's rmse: 1.03091\n",
      "[602]\ttraining's rmse: 0.955066\tvalid_1's rmse: 1.03088\n",
      "[603]\ttraining's rmse: 0.95496\tvalid_1's rmse: 1.03084\n",
      "[604]\ttraining's rmse: 0.954884\tvalid_1's rmse: 1.03085\n",
      "[605]\ttraining's rmse: 0.95479\tvalid_1's rmse: 1.03088\n",
      "[606]\ttraining's rmse: 0.954722\tvalid_1's rmse: 1.03085\n",
      "[607]\ttraining's rmse: 0.954631\tvalid_1's rmse: 1.03088\n",
      "[608]\ttraining's rmse: 0.95455\tvalid_1's rmse: 1.03084\n",
      "[609]\ttraining's rmse: 0.954441\tvalid_1's rmse: 1.03085\n",
      "[610]\ttraining's rmse: 0.954357\tvalid_1's rmse: 1.03082\n",
      "[611]\ttraining's rmse: 0.954295\tvalid_1's rmse: 1.03084\n",
      "[612]\ttraining's rmse: 0.954208\tvalid_1's rmse: 1.03084\n",
      "[613]\ttraining's rmse: 0.954111\tvalid_1's rmse: 1.03078\n",
      "[614]\ttraining's rmse: 0.954031\tvalid_1's rmse: 1.03071\n",
      "[615]\ttraining's rmse: 0.953978\tvalid_1's rmse: 1.03066\n",
      "[616]\ttraining's rmse: 0.953881\tvalid_1's rmse: 1.03063\n",
      "[617]\ttraining's rmse: 0.953791\tvalid_1's rmse: 1.03067\n",
      "[618]\ttraining's rmse: 0.953662\tvalid_1's rmse: 1.03068\n",
      "[619]\ttraining's rmse: 0.953564\tvalid_1's rmse: 1.03066\n",
      "[620]\ttraining's rmse: 0.953475\tvalid_1's rmse: 1.03064\n",
      "[621]\ttraining's rmse: 0.953375\tvalid_1's rmse: 1.03061\n",
      "[622]\ttraining's rmse: 0.953272\tvalid_1's rmse: 1.03061\n",
      "[623]\ttraining's rmse: 0.953187\tvalid_1's rmse: 1.03067\n",
      "[624]\ttraining's rmse: 0.953109\tvalid_1's rmse: 1.03061\n",
      "[625]\ttraining's rmse: 0.953057\tvalid_1's rmse: 1.03055\n",
      "[626]\ttraining's rmse: 0.952977\tvalid_1's rmse: 1.03056\n",
      "[627]\ttraining's rmse: 0.952907\tvalid_1's rmse: 1.03055\n",
      "[628]\ttraining's rmse: 0.952819\tvalid_1's rmse: 1.03058\n",
      "[629]\ttraining's rmse: 0.952733\tvalid_1's rmse: 1.03053\n",
      "[630]\ttraining's rmse: 0.952626\tvalid_1's rmse: 1.03048\n",
      "[631]\ttraining's rmse: 0.952527\tvalid_1's rmse: 1.03044\n",
      "[632]\ttraining's rmse: 0.952456\tvalid_1's rmse: 1.03046\n",
      "[633]\ttraining's rmse: 0.952364\tvalid_1's rmse: 1.03044\n",
      "[634]\ttraining's rmse: 0.952263\tvalid_1's rmse: 1.03044\n",
      "[635]\ttraining's rmse: 0.952178\tvalid_1's rmse: 1.03048\n",
      "[636]\ttraining's rmse: 0.952042\tvalid_1's rmse: 1.03046\n",
      "[637]\ttraining's rmse: 0.951989\tvalid_1's rmse: 1.03043\n",
      "[638]\ttraining's rmse: 0.951905\tvalid_1's rmse: 1.03045\n",
      "[639]\ttraining's rmse: 0.951828\tvalid_1's rmse: 1.03041\n",
      "[640]\ttraining's rmse: 0.95172\tvalid_1's rmse: 1.03036\n",
      "[641]\ttraining's rmse: 0.951645\tvalid_1's rmse: 1.03038\n",
      "[642]\ttraining's rmse: 0.951565\tvalid_1's rmse: 1.03041\n",
      "[643]\ttraining's rmse: 0.951497\tvalid_1's rmse: 1.03037\n",
      "[644]\ttraining's rmse: 0.951434\tvalid_1's rmse: 1.03031\n",
      "[645]\ttraining's rmse: 0.951365\tvalid_1's rmse: 1.03033\n",
      "[646]\ttraining's rmse: 0.951263\tvalid_1's rmse: 1.03032\n",
      "[647]\ttraining's rmse: 0.951165\tvalid_1's rmse: 1.03032\n",
      "[648]\ttraining's rmse: 0.951082\tvalid_1's rmse: 1.03034\n",
      "[649]\ttraining's rmse: 0.950987\tvalid_1's rmse: 1.03033\n",
      "[650]\ttraining's rmse: 0.950887\tvalid_1's rmse: 1.03027\n",
      "[651]\ttraining's rmse: 0.950825\tvalid_1's rmse: 1.03021\n",
      "[652]\ttraining's rmse: 0.95074\tvalid_1's rmse: 1.03021\n",
      "[653]\ttraining's rmse: 0.950658\tvalid_1's rmse: 1.03023\n",
      "[654]\ttraining's rmse: 0.950593\tvalid_1's rmse: 1.03016\n",
      "[655]\ttraining's rmse: 0.950517\tvalid_1's rmse: 1.03017\n",
      "[656]\ttraining's rmse: 0.950426\tvalid_1's rmse: 1.03016\n",
      "[657]\ttraining's rmse: 0.950381\tvalid_1's rmse: 1.03015\n",
      "[658]\ttraining's rmse: 0.950304\tvalid_1's rmse: 1.03011\n",
      "[659]\ttraining's rmse: 0.950207\tvalid_1's rmse: 1.03005\n",
      "[660]\ttraining's rmse: 0.950109\tvalid_1's rmse: 1.03005\n",
      "[661]\ttraining's rmse: 0.950048\tvalid_1's rmse: 1.03\n",
      "[662]\ttraining's rmse: 0.949956\tvalid_1's rmse: 1.02995\n",
      "[663]\ttraining's rmse: 0.949877\tvalid_1's rmse: 1.0299\n",
      "[664]\ttraining's rmse: 0.949779\tvalid_1's rmse: 1.02989\n",
      "[665]\ttraining's rmse: 0.949706\tvalid_1's rmse: 1.02988\n",
      "[666]\ttraining's rmse: 0.949602\tvalid_1's rmse: 1.02986\n",
      "[667]\ttraining's rmse: 0.949524\tvalid_1's rmse: 1.02981\n",
      "[668]\ttraining's rmse: 0.949437\tvalid_1's rmse: 1.02978\n",
      "[669]\ttraining's rmse: 0.949345\tvalid_1's rmse: 1.02972\n",
      "[670]\ttraining's rmse: 0.949268\tvalid_1's rmse: 1.02975\n",
      "[671]\ttraining's rmse: 0.949173\tvalid_1's rmse: 1.02975\n",
      "[672]\ttraining's rmse: 0.94911\tvalid_1's rmse: 1.02972\n",
      "[673]\ttraining's rmse: 0.949046\tvalid_1's rmse: 1.02968\n",
      "[674]\ttraining's rmse: 0.948976\tvalid_1's rmse: 1.0297\n",
      "[675]\ttraining's rmse: 0.948901\tvalid_1's rmse: 1.02972\n",
      "[676]\ttraining's rmse: 0.948808\tvalid_1's rmse: 1.02972\n",
      "[677]\ttraining's rmse: 0.948718\tvalid_1's rmse: 1.02971\n",
      "[678]\ttraining's rmse: 0.948612\tvalid_1's rmse: 1.02972\n",
      "[679]\ttraining's rmse: 0.94854\tvalid_1's rmse: 1.02971\n",
      "[680]\ttraining's rmse: 0.948441\tvalid_1's rmse: 1.02966\n",
      "[681]\ttraining's rmse: 0.948354\tvalid_1's rmse: 1.02965\n",
      "[682]\ttraining's rmse: 0.948307\tvalid_1's rmse: 1.02962\n",
      "[683]\ttraining's rmse: 0.948199\tvalid_1's rmse: 1.02961\n",
      "[684]\ttraining's rmse: 0.948107\tvalid_1's rmse: 1.02955\n",
      "[685]\ttraining's rmse: 0.948012\tvalid_1's rmse: 1.02951\n",
      "[686]\ttraining's rmse: 0.947909\tvalid_1's rmse: 1.02944\n",
      "[687]\ttraining's rmse: 0.947825\tvalid_1's rmse: 1.0294\n",
      "[688]\ttraining's rmse: 0.947763\tvalid_1's rmse: 1.02936\n",
      "[689]\ttraining's rmse: 0.947717\tvalid_1's rmse: 1.02939\n",
      "[690]\ttraining's rmse: 0.947673\tvalid_1's rmse: 1.02937\n",
      "[691]\ttraining's rmse: 0.947584\tvalid_1's rmse: 1.02936\n",
      "[692]\ttraining's rmse: 0.947487\tvalid_1's rmse: 1.02934\n",
      "[693]\ttraining's rmse: 0.947395\tvalid_1's rmse: 1.02931\n",
      "[694]\ttraining's rmse: 0.947304\tvalid_1's rmse: 1.02931\n",
      "[695]\ttraining's rmse: 0.947219\tvalid_1's rmse: 1.02929\n",
      "[696]\ttraining's rmse: 0.947134\tvalid_1's rmse: 1.02925\n",
      "[697]\ttraining's rmse: 0.947084\tvalid_1's rmse: 1.02919\n",
      "[698]\ttraining's rmse: 0.947022\tvalid_1's rmse: 1.02915\n",
      "[699]\ttraining's rmse: 0.946968\tvalid_1's rmse: 1.02912\n",
      "[700]\ttraining's rmse: 0.946886\tvalid_1's rmse: 1.02914\n",
      "[701]\ttraining's rmse: 0.946806\tvalid_1's rmse: 1.02914\n",
      "[702]\ttraining's rmse: 0.946713\tvalid_1's rmse: 1.02913\n",
      "[703]\ttraining's rmse: 0.94667\tvalid_1's rmse: 1.02911\n",
      "[704]\ttraining's rmse: 0.946587\tvalid_1's rmse: 1.02921\n",
      "[705]\ttraining's rmse: 0.94652\tvalid_1's rmse: 1.02923\n",
      "[706]\ttraining's rmse: 0.946432\tvalid_1's rmse: 1.02917\n",
      "[707]\ttraining's rmse: 0.946335\tvalid_1's rmse: 1.02915\n",
      "[708]\ttraining's rmse: 0.946266\tvalid_1's rmse: 1.0291\n",
      "[709]\ttraining's rmse: 0.946186\tvalid_1's rmse: 1.02909\n",
      "[710]\ttraining's rmse: 0.946101\tvalid_1's rmse: 1.0291\n",
      "[711]\ttraining's rmse: 0.946046\tvalid_1's rmse: 1.02906\n",
      "[712]\ttraining's rmse: 0.945978\tvalid_1's rmse: 1.02904\n",
      "[713]\ttraining's rmse: 0.945925\tvalid_1's rmse: 1.02905\n",
      "[714]\ttraining's rmse: 0.945834\tvalid_1's rmse: 1.02904\n",
      "[715]\ttraining's rmse: 0.945753\tvalid_1's rmse: 1.02914\n",
      "[716]\ttraining's rmse: 0.94571\tvalid_1's rmse: 1.02913\n",
      "[717]\ttraining's rmse: 0.945624\tvalid_1's rmse: 1.02908\n",
      "[718]\ttraining's rmse: 0.945527\tvalid_1's rmse: 1.02909\n",
      "[719]\ttraining's rmse: 0.945442\tvalid_1's rmse: 1.02912\n",
      "[720]\ttraining's rmse: 0.94539\tvalid_1's rmse: 1.0291\n",
      "[721]\ttraining's rmse: 0.9453\tvalid_1's rmse: 1.02909\n",
      "[722]\ttraining's rmse: 0.945232\tvalid_1's rmse: 1.02911\n",
      "Early stopping, best iteration is:\n",
      "[712]\ttraining's rmse: 0.945978\tvalid_1's rmse: 1.02904\n",
      "fold_2 coefficients:  [0.5317976  1.64526633 2.20308407]\n",
      "[1]\ttraining's rmse: 1.25305\tvalid_1's rmse: 1.30182\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.24941\tvalid_1's rmse: 1.29859\n",
      "[3]\ttraining's rmse: 1.24583\tvalid_1's rmse: 1.29544\n",
      "[4]\ttraining's rmse: 1.24232\tvalid_1's rmse: 1.29233\n",
      "[5]\ttraining's rmse: 1.23886\tvalid_1's rmse: 1.28929\n",
      "[6]\ttraining's rmse: 1.23547\tvalid_1's rmse: 1.2863\n",
      "[7]\ttraining's rmse: 1.23207\tvalid_1's rmse: 1.28331\n",
      "[8]\ttraining's rmse: 1.22878\tvalid_1's rmse: 1.28044\n",
      "[9]\ttraining's rmse: 1.22549\tvalid_1's rmse: 1.27756\n",
      "[10]\ttraining's rmse: 1.22232\tvalid_1's rmse: 1.27477\n",
      "[11]\ttraining's rmse: 1.21914\tvalid_1's rmse: 1.272\n",
      "[12]\ttraining's rmse: 1.21606\tvalid_1's rmse: 1.26933\n",
      "[13]\ttraining's rmse: 1.21298\tvalid_1's rmse: 1.26666\n",
      "[14]\ttraining's rmse: 1.21001\tvalid_1's rmse: 1.26407\n",
      "[15]\ttraining's rmse: 1.20702\tvalid_1's rmse: 1.26149\n",
      "[16]\ttraining's rmse: 1.20415\tvalid_1's rmse: 1.25899\n",
      "[17]\ttraining's rmse: 1.20126\tvalid_1's rmse: 1.25651\n",
      "[18]\ttraining's rmse: 1.19847\tvalid_1's rmse: 1.25422\n",
      "[19]\ttraining's rmse: 1.19568\tvalid_1's rmse: 1.25183\n",
      "[20]\ttraining's rmse: 1.19295\tvalid_1's rmse: 1.24958\n",
      "[21]\ttraining's rmse: 1.19029\tvalid_1's rmse: 1.24731\n",
      "[22]\ttraining's rmse: 1.18763\tvalid_1's rmse: 1.24513\n",
      "[23]\ttraining's rmse: 1.18507\tvalid_1's rmse: 1.24292\n",
      "[24]\ttraining's rmse: 1.1825\tvalid_1's rmse: 1.24085\n",
      "[25]\ttraining's rmse: 1.18001\tvalid_1's rmse: 1.23876\n",
      "[26]\ttraining's rmse: 1.17752\tvalid_1's rmse: 1.23674\n",
      "[27]\ttraining's rmse: 1.17512\tvalid_1's rmse: 1.23472\n",
      "[28]\ttraining's rmse: 1.17275\tvalid_1's rmse: 1.23283\n",
      "[29]\ttraining's rmse: 1.17044\tvalid_1's rmse: 1.23086\n",
      "[30]\ttraining's rmse: 1.16864\tvalid_1's rmse: 1.22931\n",
      "[31]\ttraining's rmse: 1.16688\tvalid_1's rmse: 1.22779\n",
      "[32]\ttraining's rmse: 1.16515\tvalid_1's rmse: 1.22631\n",
      "[33]\ttraining's rmse: 1.16345\tvalid_1's rmse: 1.22478\n",
      "[34]\ttraining's rmse: 1.16178\tvalid_1's rmse: 1.22336\n",
      "[35]\ttraining's rmse: 1.16015\tvalid_1's rmse: 1.22192\n",
      "[36]\ttraining's rmse: 1.15854\tvalid_1's rmse: 1.22057\n",
      "[37]\ttraining's rmse: 1.15696\tvalid_1's rmse: 1.21916\n",
      "[38]\ttraining's rmse: 1.1554\tvalid_1's rmse: 1.2179\n",
      "[39]\ttraining's rmse: 1.15387\tvalid_1's rmse: 1.21661\n",
      "[40]\ttraining's rmse: 1.15213\tvalid_1's rmse: 1.21526\n",
      "[41]\ttraining's rmse: 1.15064\tvalid_1's rmse: 1.21408\n",
      "[42]\ttraining's rmse: 1.14895\tvalid_1's rmse: 1.21281\n",
      "[43]\ttraining's rmse: 1.14752\tvalid_1's rmse: 1.21163\n",
      "[44]\ttraining's rmse: 1.14587\tvalid_1's rmse: 1.21033\n",
      "[45]\ttraining's rmse: 1.14448\tvalid_1's rmse: 1.20924\n",
      "[46]\ttraining's rmse: 1.14289\tvalid_1's rmse: 1.20804\n",
      "[47]\ttraining's rmse: 1.14168\tvalid_1's rmse: 1.20722\n",
      "[48]\ttraining's rmse: 1.14012\tvalid_1's rmse: 1.20606\n",
      "[49]\ttraining's rmse: 1.13859\tvalid_1's rmse: 1.20488\n",
      "[50]\ttraining's rmse: 1.13743\tvalid_1's rmse: 1.20408\n",
      "[51]\ttraining's rmse: 1.13618\tvalid_1's rmse: 1.20307\n",
      "[52]\ttraining's rmse: 1.13472\tvalid_1's rmse: 1.20202\n",
      "[53]\ttraining's rmse: 1.1335\tvalid_1's rmse: 1.20104\n",
      "[54]\ttraining's rmse: 1.13208\tvalid_1's rmse: 1.20004\n",
      "[55]\ttraining's rmse: 1.1309\tvalid_1's rmse: 1.19909\n",
      "[56]\ttraining's rmse: 1.12949\tvalid_1's rmse: 1.1981\n",
      "[57]\ttraining's rmse: 1.12834\tvalid_1's rmse: 1.1972\n",
      "[58]\ttraining's rmse: 1.12669\tvalid_1's rmse: 1.1959\n",
      "[59]\ttraining's rmse: 1.12536\tvalid_1's rmse: 1.19493\n",
      "[60]\ttraining's rmse: 1.12426\tvalid_1's rmse: 1.19405\n",
      "[61]\ttraining's rmse: 1.12264\tvalid_1's rmse: 1.19283\n",
      "[62]\ttraining's rmse: 1.12122\tvalid_1's rmse: 1.19187\n",
      "[63]\ttraining's rmse: 1.12016\tvalid_1's rmse: 1.19104\n",
      "[64]\ttraining's rmse: 1.11888\tvalid_1's rmse: 1.19011\n",
      "[65]\ttraining's rmse: 1.11777\tvalid_1's rmse: 1.18926\n",
      "[66]\ttraining's rmse: 1.11667\tvalid_1's rmse: 1.18837\n",
      "[67]\ttraining's rmse: 1.11533\tvalid_1's rmse: 1.18746\n",
      "[68]\ttraining's rmse: 1.11436\tvalid_1's rmse: 1.18678\n",
      "[69]\ttraining's rmse: 1.11289\tvalid_1's rmse: 1.18562\n",
      "[70]\ttraining's rmse: 1.11198\tvalid_1's rmse: 1.18504\n",
      "[71]\ttraining's rmse: 1.11054\tvalid_1's rmse: 1.18399\n",
      "[72]\ttraining's rmse: 1.10968\tvalid_1's rmse: 1.18348\n",
      "[73]\ttraining's rmse: 1.10841\tvalid_1's rmse: 1.18267\n",
      "[74]\ttraining's rmse: 1.10758\tvalid_1's rmse: 1.18216\n",
      "[75]\ttraining's rmse: 1.10647\tvalid_1's rmse: 1.18133\n",
      "[76]\ttraining's rmse: 1.10553\tvalid_1's rmse: 1.18074\n",
      "[77]\ttraining's rmse: 1.10461\tvalid_1's rmse: 1.18011\n",
      "[78]\ttraining's rmse: 1.1034\tvalid_1's rmse: 1.17933\n",
      "[79]\ttraining's rmse: 1.1025\tvalid_1's rmse: 1.17881\n",
      "[80]\ttraining's rmse: 1.10166\tvalid_1's rmse: 1.17819\n",
      "[81]\ttraining's rmse: 1.10058\tvalid_1's rmse: 1.17744\n",
      "[82]\ttraining's rmse: 1.09971\tvalid_1's rmse: 1.17688\n",
      "[83]\ttraining's rmse: 1.09886\tvalid_1's rmse: 1.17634\n",
      "[84]\ttraining's rmse: 1.09782\tvalid_1's rmse: 1.17564\n",
      "[85]\ttraining's rmse: 1.097\tvalid_1's rmse: 1.17525\n",
      "[86]\ttraining's rmse: 1.09621\tvalid_1's rmse: 1.17474\n",
      "[87]\ttraining's rmse: 1.0954\tvalid_1's rmse: 1.17422\n",
      "[88]\ttraining's rmse: 1.09416\tvalid_1's rmse: 1.17325\n",
      "[89]\ttraining's rmse: 1.09336\tvalid_1's rmse: 1.17274\n",
      "[90]\ttraining's rmse: 1.09214\tvalid_1's rmse: 1.17181\n",
      "[91]\ttraining's rmse: 1.09138\tvalid_1's rmse: 1.17137\n",
      "[92]\ttraining's rmse: 1.09018\tvalid_1's rmse: 1.17049\n",
      "[93]\ttraining's rmse: 1.08943\tvalid_1's rmse: 1.17014\n",
      "[94]\ttraining's rmse: 1.08826\tvalid_1's rmse: 1.16931\n",
      "[95]\ttraining's rmse: 1.08752\tvalid_1's rmse: 1.1688\n",
      "[96]\ttraining's rmse: 1.08637\tvalid_1's rmse: 1.16787\n",
      "[97]\ttraining's rmse: 1.0856\tvalid_1's rmse: 1.16732\n",
      "[98]\ttraining's rmse: 1.08488\tvalid_1's rmse: 1.16694\n",
      "[99]\ttraining's rmse: 1.08375\tvalid_1's rmse: 1.16613\n",
      "[100]\ttraining's rmse: 1.08301\tvalid_1's rmse: 1.1656\n",
      "[101]\ttraining's rmse: 1.0819\tvalid_1's rmse: 1.16483\n",
      "[102]\ttraining's rmse: 1.0811\tvalid_1's rmse: 1.16428\n",
      "[103]\ttraining's rmse: 1.0804\tvalid_1's rmse: 1.16384\n",
      "[104]\ttraining's rmse: 1.07933\tvalid_1's rmse: 1.1631\n",
      "[105]\ttraining's rmse: 1.07865\tvalid_1's rmse: 1.16267\n",
      "[106]\ttraining's rmse: 1.07789\tvalid_1's rmse: 1.16215\n",
      "[107]\ttraining's rmse: 1.07683\tvalid_1's rmse: 1.1615\n",
      "[108]\ttraining's rmse: 1.07617\tvalid_1's rmse: 1.1611\n",
      "[109]\ttraining's rmse: 1.07546\tvalid_1's rmse: 1.16076\n",
      "[110]\ttraining's rmse: 1.07482\tvalid_1's rmse: 1.16037\n",
      "[111]\ttraining's rmse: 1.07381\tvalid_1's rmse: 1.15958\n",
      "[112]\ttraining's rmse: 1.07309\tvalid_1's rmse: 1.15915\n",
      "[113]\ttraining's rmse: 1.07241\tvalid_1's rmse: 1.15883\n",
      "[114]\ttraining's rmse: 1.07172\tvalid_1's rmse: 1.15835\n",
      "[115]\ttraining's rmse: 1.0708\tvalid_1's rmse: 1.15764\n",
      "[116]\ttraining's rmse: 1.0702\tvalid_1's rmse: 1.15727\n",
      "[117]\ttraining's rmse: 1.06966\tvalid_1's rmse: 1.15693\n",
      "[118]\ttraining's rmse: 1.06877\tvalid_1's rmse: 1.15625\n",
      "[119]\ttraining's rmse: 1.06811\tvalid_1's rmse: 1.1558\n",
      "[120]\ttraining's rmse: 1.06759\tvalid_1's rmse: 1.15546\n",
      "[121]\ttraining's rmse: 1.06672\tvalid_1's rmse: 1.15478\n",
      "[122]\ttraining's rmse: 1.0662\tvalid_1's rmse: 1.15448\n",
      "[123]\ttraining's rmse: 1.06564\tvalid_1's rmse: 1.15415\n",
      "[124]\ttraining's rmse: 1.06503\tvalid_1's rmse: 1.15388\n",
      "[125]\ttraining's rmse: 1.06419\tvalid_1's rmse: 1.15326\n",
      "[126]\ttraining's rmse: 1.06358\tvalid_1's rmse: 1.15297\n",
      "[127]\ttraining's rmse: 1.06304\tvalid_1's rmse: 1.15266\n",
      "[128]\ttraining's rmse: 1.06256\tvalid_1's rmse: 1.1523\n",
      "[129]\ttraining's rmse: 1.06174\tvalid_1's rmse: 1.1517\n",
      "[130]\ttraining's rmse: 1.06122\tvalid_1's rmse: 1.15143\n",
      "[131]\ttraining's rmse: 1.06043\tvalid_1's rmse: 1.15079\n",
      "[132]\ttraining's rmse: 1.05998\tvalid_1's rmse: 1.15046\n",
      "[133]\ttraining's rmse: 1.0592\tvalid_1's rmse: 1.14986\n",
      "[134]\ttraining's rmse: 1.05855\tvalid_1's rmse: 1.14951\n",
      "[135]\ttraining's rmse: 1.05806\tvalid_1's rmse: 1.1491\n",
      "[136]\ttraining's rmse: 1.05751\tvalid_1's rmse: 1.14886\n",
      "[137]\ttraining's rmse: 1.05696\tvalid_1's rmse: 1.14858\n",
      "[138]\ttraining's rmse: 1.05652\tvalid_1's rmse: 1.14825\n",
      "[139]\ttraining's rmse: 1.05579\tvalid_1's rmse: 1.14761\n",
      "[140]\ttraining's rmse: 1.05518\tvalid_1's rmse: 1.14728\n",
      "[141]\ttraining's rmse: 1.05437\tvalid_1's rmse: 1.14672\n",
      "[142]\ttraining's rmse: 1.0539\tvalid_1's rmse: 1.14652\n",
      "[143]\ttraining's rmse: 1.05341\tvalid_1's rmse: 1.14619\n",
      "[144]\ttraining's rmse: 1.05263\tvalid_1's rmse: 1.14567\n",
      "[145]\ttraining's rmse: 1.05222\tvalid_1's rmse: 1.14536\n",
      "[146]\ttraining's rmse: 1.05164\tvalid_1's rmse: 1.14504\n",
      "[147]\ttraining's rmse: 1.05088\tvalid_1's rmse: 1.14452\n",
      "[148]\ttraining's rmse: 1.05039\tvalid_1's rmse: 1.14432\n",
      "[149]\ttraining's rmse: 1.04994\tvalid_1's rmse: 1.14396\n",
      "[150]\ttraining's rmse: 1.04954\tvalid_1's rmse: 1.14366\n",
      "[151]\ttraining's rmse: 1.04879\tvalid_1's rmse: 1.14315\n",
      "[152]\ttraining's rmse: 1.04814\tvalid_1's rmse: 1.14258\n",
      "[153]\ttraining's rmse: 1.0476\tvalid_1's rmse: 1.14229\n",
      "[154]\ttraining's rmse: 1.04713\tvalid_1's rmse: 1.14205\n",
      "[155]\ttraining's rmse: 1.0467\tvalid_1's rmse: 1.14183\n",
      "[156]\ttraining's rmse: 1.04599\tvalid_1's rmse: 1.1414\n",
      "[157]\ttraining's rmse: 1.04555\tvalid_1's rmse: 1.14111\n",
      "[158]\ttraining's rmse: 1.04486\tvalid_1's rmse: 1.14066\n",
      "[159]\ttraining's rmse: 1.04449\tvalid_1's rmse: 1.1405\n",
      "[160]\ttraining's rmse: 1.04389\tvalid_1's rmse: 1.14021\n",
      "[161]\ttraining's rmse: 1.04349\tvalid_1's rmse: 1.13998\n",
      "[162]\ttraining's rmse: 1.04289\tvalid_1's rmse: 1.13946\n",
      "[163]\ttraining's rmse: 1.04247\tvalid_1's rmse: 1.13928\n",
      "[164]\ttraining's rmse: 1.04189\tvalid_1's rmse: 1.13877\n",
      "[165]\ttraining's rmse: 1.04128\tvalid_1's rmse: 1.13843\n",
      "[166]\ttraining's rmse: 1.04088\tvalid_1's rmse: 1.13821\n",
      "[167]\ttraining's rmse: 1.04048\tvalid_1's rmse: 1.13806\n",
      "[168]\ttraining's rmse: 1.04008\tvalid_1's rmse: 1.1378\n",
      "[169]\ttraining's rmse: 1.03974\tvalid_1's rmse: 1.13765\n",
      "[170]\ttraining's rmse: 1.03918\tvalid_1's rmse: 1.13715\n",
      "[171]\ttraining's rmse: 1.0386\tvalid_1's rmse: 1.13683\n",
      "[172]\ttraining's rmse: 1.03821\tvalid_1's rmse: 1.13669\n",
      "[173]\ttraining's rmse: 1.03767\tvalid_1's rmse: 1.13633\n",
      "[174]\ttraining's rmse: 1.03728\tvalid_1's rmse: 1.13607\n",
      "[175]\ttraining's rmse: 1.03695\tvalid_1's rmse: 1.13594\n",
      "[176]\ttraining's rmse: 1.0364\tvalid_1's rmse: 1.13551\n",
      "[177]\ttraining's rmse: 1.03603\tvalid_1's rmse: 1.13535\n",
      "[178]\ttraining's rmse: 1.03548\tvalid_1's rmse: 1.13503\n",
      "[179]\ttraining's rmse: 1.03513\tvalid_1's rmse: 1.13475\n",
      "[180]\ttraining's rmse: 1.03477\tvalid_1's rmse: 1.13459\n",
      "[181]\ttraining's rmse: 1.03418\tvalid_1's rmse: 1.13421\n",
      "[182]\ttraining's rmse: 1.03382\tvalid_1's rmse: 1.13405\n",
      "[183]\ttraining's rmse: 1.03331\tvalid_1's rmse: 1.13366\n",
      "[184]\ttraining's rmse: 1.03296\tvalid_1's rmse: 1.13356\n",
      "[185]\ttraining's rmse: 1.0326\tvalid_1's rmse: 1.13338\n",
      "[186]\ttraining's rmse: 1.03203\tvalid_1's rmse: 1.133\n",
      "[187]\ttraining's rmse: 1.03154\tvalid_1's rmse: 1.13263\n",
      "[188]\ttraining's rmse: 1.03124\tvalid_1's rmse: 1.13251\n",
      "[189]\ttraining's rmse: 1.03073\tvalid_1's rmse: 1.13223\n",
      "[190]\ttraining's rmse: 1.03025\tvalid_1's rmse: 1.13192\n",
      "[191]\ttraining's rmse: 1.0299\tvalid_1's rmse: 1.13178\n",
      "[192]\ttraining's rmse: 1.02961\tvalid_1's rmse: 1.13167\n",
      "[193]\ttraining's rmse: 1.02912\tvalid_1's rmse: 1.13132\n",
      "[194]\ttraining's rmse: 1.02878\tvalid_1's rmse: 1.13117\n",
      "[195]\ttraining's rmse: 1.02844\tvalid_1's rmse: 1.13101\n",
      "[196]\ttraining's rmse: 1.02798\tvalid_1's rmse: 1.13067\n",
      "[197]\ttraining's rmse: 1.02766\tvalid_1's rmse: 1.13053\n",
      "[198]\ttraining's rmse: 1.02712\tvalid_1's rmse: 1.13023\n",
      "[199]\ttraining's rmse: 1.02665\tvalid_1's rmse: 1.12997\n",
      "[200]\ttraining's rmse: 1.02619\tvalid_1's rmse: 1.12974\n",
      "[201]\ttraining's rmse: 1.02586\tvalid_1's rmse: 1.12956\n",
      "[202]\ttraining's rmse: 1.02559\tvalid_1's rmse: 1.12945\n",
      "[203]\ttraining's rmse: 1.02514\tvalid_1's rmse: 1.12917\n",
      "[204]\ttraining's rmse: 1.02483\tvalid_1's rmse: 1.12901\n",
      "[205]\ttraining's rmse: 1.02439\tvalid_1's rmse: 1.12879\n",
      "[206]\ttraining's rmse: 1.02407\tvalid_1's rmse: 1.12868\n",
      "[207]\ttraining's rmse: 1.02366\tvalid_1's rmse: 1.1285\n",
      "[208]\ttraining's rmse: 1.02334\tvalid_1's rmse: 1.12835\n",
      "[209]\ttraining's rmse: 1.02306\tvalid_1's rmse: 1.12823\n",
      "[210]\ttraining's rmse: 1.02273\tvalid_1's rmse: 1.12797\n",
      "[211]\ttraining's rmse: 1.02232\tvalid_1's rmse: 1.12766\n",
      "[212]\ttraining's rmse: 1.02207\tvalid_1's rmse: 1.12749\n",
      "[213]\ttraining's rmse: 1.02165\tvalid_1's rmse: 1.12727\n",
      "[214]\ttraining's rmse: 1.02124\tvalid_1's rmse: 1.12707\n",
      "[215]\ttraining's rmse: 1.02092\tvalid_1's rmse: 1.12681\n",
      "[216]\ttraining's rmse: 1.02064\tvalid_1's rmse: 1.12667\n",
      "[217]\ttraining's rmse: 1.02035\tvalid_1's rmse: 1.1265\n",
      "[218]\ttraining's rmse: 1.01994\tvalid_1's rmse: 1.12628\n",
      "[219]\ttraining's rmse: 1.01966\tvalid_1's rmse: 1.12609\n",
      "[220]\ttraining's rmse: 1.01938\tvalid_1's rmse: 1.1259\n",
      "[221]\ttraining's rmse: 1.01892\tvalid_1's rmse: 1.12565\n",
      "[222]\ttraining's rmse: 1.01852\tvalid_1's rmse: 1.12548\n",
      "[223]\ttraining's rmse: 1.01814\tvalid_1's rmse: 1.12531\n",
      "[224]\ttraining's rmse: 1.01785\tvalid_1's rmse: 1.12506\n",
      "[225]\ttraining's rmse: 1.01754\tvalid_1's rmse: 1.12486\n",
      "[226]\ttraining's rmse: 1.01717\tvalid_1's rmse: 1.12457\n",
      "[227]\ttraining's rmse: 1.0169\tvalid_1's rmse: 1.1244\n",
      "[228]\ttraining's rmse: 1.01663\tvalid_1's rmse: 1.12426\n",
      "[229]\ttraining's rmse: 1.01627\tvalid_1's rmse: 1.12399\n",
      "[230]\ttraining's rmse: 1.0159\tvalid_1's rmse: 1.12383\n",
      "[231]\ttraining's rmse: 1.01564\tvalid_1's rmse: 1.12367\n",
      "[232]\ttraining's rmse: 1.0153\tvalid_1's rmse: 1.12345\n",
      "[233]\ttraining's rmse: 1.01502\tvalid_1's rmse: 1.1232\n",
      "[234]\ttraining's rmse: 1.01477\tvalid_1's rmse: 1.12308\n",
      "[235]\ttraining's rmse: 1.01451\tvalid_1's rmse: 1.12291\n",
      "[236]\ttraining's rmse: 1.01417\tvalid_1's rmse: 1.12265\n",
      "[237]\ttraining's rmse: 1.01396\tvalid_1's rmse: 1.12266\n",
      "[238]\ttraining's rmse: 1.01361\tvalid_1's rmse: 1.12246\n",
      "[239]\ttraining's rmse: 1.01326\tvalid_1's rmse: 1.12231\n",
      "[240]\ttraining's rmse: 1.013\tvalid_1's rmse: 1.12207\n",
      "[241]\ttraining's rmse: 1.01267\tvalid_1's rmse: 1.1218\n",
      "[242]\ttraining's rmse: 1.01242\tvalid_1's rmse: 1.12159\n",
      "[243]\ttraining's rmse: 1.01214\tvalid_1's rmse: 1.12142\n",
      "[244]\ttraining's rmse: 1.0119\tvalid_1's rmse: 1.12127\n",
      "[245]\ttraining's rmse: 1.01158\tvalid_1's rmse: 1.12103\n",
      "[246]\ttraining's rmse: 1.01125\tvalid_1's rmse: 1.1209\n",
      "[247]\ttraining's rmse: 1.01101\tvalid_1's rmse: 1.12074\n",
      "[248]\ttraining's rmse: 1.01069\tvalid_1's rmse: 1.12059\n",
      "[249]\ttraining's rmse: 1.0104\tvalid_1's rmse: 1.12042\n",
      "[250]\ttraining's rmse: 1.01021\tvalid_1's rmse: 1.12043\n",
      "[251]\ttraining's rmse: 1.00987\tvalid_1's rmse: 1.12021\n",
      "[252]\ttraining's rmse: 1.00962\tvalid_1's rmse: 1.1201\n",
      "[253]\ttraining's rmse: 1.00938\tvalid_1's rmse: 1.11989\n",
      "[254]\ttraining's rmse: 1.00916\tvalid_1's rmse: 1.11977\n",
      "[255]\ttraining's rmse: 1.00896\tvalid_1's rmse: 1.11958\n",
      "[256]\ttraining's rmse: 1.00868\tvalid_1's rmse: 1.11942\n",
      "[257]\ttraining's rmse: 1.00838\tvalid_1's rmse: 1.11928\n",
      "[258]\ttraining's rmse: 1.00814\tvalid_1's rmse: 1.11918\n",
      "[259]\ttraining's rmse: 1.00789\tvalid_1's rmse: 1.11897\n",
      "[260]\ttraining's rmse: 1.00767\tvalid_1's rmse: 1.11885\n",
      "[261]\ttraining's rmse: 1.00737\tvalid_1's rmse: 1.11864\n",
      "[262]\ttraining's rmse: 1.00711\tvalid_1's rmse: 1.11851\n",
      "[263]\ttraining's rmse: 1.00683\tvalid_1's rmse: 1.11832\n",
      "[264]\ttraining's rmse: 1.00661\tvalid_1's rmse: 1.11817\n",
      "[265]\ttraining's rmse: 1.00643\tvalid_1's rmse: 1.1182\n",
      "[266]\ttraining's rmse: 1.00619\tvalid_1's rmse: 1.11812\n",
      "[267]\ttraining's rmse: 1.00588\tvalid_1's rmse: 1.1179\n",
      "[268]\ttraining's rmse: 1.00565\tvalid_1's rmse: 1.1178\n",
      "[269]\ttraining's rmse: 1.00544\tvalid_1's rmse: 1.11762\n",
      "[270]\ttraining's rmse: 1.00519\tvalid_1's rmse: 1.11747\n",
      "[271]\ttraining's rmse: 1.00491\tvalid_1's rmse: 1.11731\n",
      "[272]\ttraining's rmse: 1.0047\tvalid_1's rmse: 1.11714\n",
      "[273]\ttraining's rmse: 1.00447\tvalid_1's rmse: 1.11707\n",
      "[274]\ttraining's rmse: 1.00413\tvalid_1's rmse: 1.11692\n",
      "[275]\ttraining's rmse: 1.00385\tvalid_1's rmse: 1.11677\n",
      "[276]\ttraining's rmse: 1.00366\tvalid_1's rmse: 1.11666\n",
      "[277]\ttraining's rmse: 1.00345\tvalid_1's rmse: 1.11652\n",
      "[278]\ttraining's rmse: 1.00313\tvalid_1's rmse: 1.11629\n",
      "[279]\ttraining's rmse: 1.00286\tvalid_1's rmse: 1.11614\n",
      "[280]\ttraining's rmse: 1.00263\tvalid_1's rmse: 1.11599\n",
      "[281]\ttraining's rmse: 1.0024\tvalid_1's rmse: 1.11581\n",
      "[282]\ttraining's rmse: 1.00207\tvalid_1's rmse: 1.11566\n",
      "[283]\ttraining's rmse: 1.00181\tvalid_1's rmse: 1.11546\n",
      "[284]\ttraining's rmse: 1.00158\tvalid_1's rmse: 1.11531\n",
      "[285]\ttraining's rmse: 1.00127\tvalid_1's rmse: 1.11516\n",
      "[286]\ttraining's rmse: 1.00104\tvalid_1's rmse: 1.1149\n",
      "[287]\ttraining's rmse: 1.00083\tvalid_1's rmse: 1.11484\n",
      "[288]\ttraining's rmse: 1.00052\tvalid_1's rmse: 1.11467\n",
      "[289]\ttraining's rmse: 1.00027\tvalid_1's rmse: 1.11458\n",
      "[290]\ttraining's rmse: 1.00007\tvalid_1's rmse: 1.11452\n",
      "[291]\ttraining's rmse: 0.999893\tvalid_1's rmse: 1.11441\n",
      "[292]\ttraining's rmse: 0.999677\tvalid_1's rmse: 1.11427\n",
      "[293]\ttraining's rmse: 0.999427\tvalid_1's rmse: 1.11413\n",
      "[294]\ttraining's rmse: 0.999136\tvalid_1's rmse: 1.11397\n",
      "[295]\ttraining's rmse: 0.99892\tvalid_1's rmse: 1.11371\n",
      "[296]\ttraining's rmse: 0.998708\tvalid_1's rmse: 1.11358\n",
      "[297]\ttraining's rmse: 0.998419\tvalid_1's rmse: 1.11344\n",
      "[298]\ttraining's rmse: 0.998218\tvalid_1's rmse: 1.11333\n",
      "[299]\ttraining's rmse: 0.998009\tvalid_1's rmse: 1.11307\n",
      "[300]\ttraining's rmse: 0.99772\tvalid_1's rmse: 1.1129\n",
      "[301]\ttraining's rmse: 0.997515\tvalid_1's rmse: 1.11277\n",
      "[302]\ttraining's rmse: 0.997277\tvalid_1's rmse: 1.1126\n",
      "[303]\ttraining's rmse: 0.997\tvalid_1's rmse: 1.11247\n",
      "[304]\ttraining's rmse: 0.99681\tvalid_1's rmse: 1.11236\n",
      "[305]\ttraining's rmse: 0.996621\tvalid_1's rmse: 1.1122\n",
      "[306]\ttraining's rmse: 0.996414\tvalid_1's rmse: 1.11218\n",
      "[307]\ttraining's rmse: 0.99614\tvalid_1's rmse: 1.11203\n",
      "[308]\ttraining's rmse: 0.995947\tvalid_1's rmse: 1.11191\n",
      "[309]\ttraining's rmse: 0.995669\tvalid_1's rmse: 1.11174\n",
      "[310]\ttraining's rmse: 0.99549\tvalid_1's rmse: 1.11161\n",
      "[311]\ttraining's rmse: 0.995294\tvalid_1's rmse: 1.11154\n",
      "[312]\ttraining's rmse: 0.995073\tvalid_1's rmse: 1.11146\n",
      "[313]\ttraining's rmse: 0.994879\tvalid_1's rmse: 1.11133\n",
      "[314]\ttraining's rmse: 0.994669\tvalid_1's rmse: 1.11117\n",
      "[315]\ttraining's rmse: 0.994474\tvalid_1's rmse: 1.11115\n",
      "[316]\ttraining's rmse: 0.994221\tvalid_1's rmse: 1.11099\n",
      "[317]\ttraining's rmse: 0.994044\tvalid_1's rmse: 1.11084\n",
      "[318]\ttraining's rmse: 0.993841\tvalid_1's rmse: 1.11072\n",
      "[319]\ttraining's rmse: 0.993579\tvalid_1's rmse: 1.1106\n",
      "[320]\ttraining's rmse: 0.993393\tvalid_1's rmse: 1.11053\n",
      "[321]\ttraining's rmse: 0.993209\tvalid_1's rmse: 1.11048\n",
      "[322]\ttraining's rmse: 0.992962\tvalid_1's rmse: 1.11036\n",
      "[323]\ttraining's rmse: 0.992775\tvalid_1's rmse: 1.11026\n",
      "[324]\ttraining's rmse: 0.992579\tvalid_1's rmse: 1.11011\n",
      "[325]\ttraining's rmse: 0.992334\tvalid_1's rmse: 1.11003\n",
      "[326]\ttraining's rmse: 0.992146\tvalid_1's rmse: 1.10996\n",
      "[327]\ttraining's rmse: 0.991977\tvalid_1's rmse: 1.10983\n",
      "[328]\ttraining's rmse: 0.991787\tvalid_1's rmse: 1.10972\n",
      "[329]\ttraining's rmse: 0.991547\tvalid_1's rmse: 1.10962\n",
      "[330]\ttraining's rmse: 0.991377\tvalid_1's rmse: 1.1095\n",
      "[331]\ttraining's rmse: 0.991142\tvalid_1's rmse: 1.10941\n",
      "[332]\ttraining's rmse: 0.990959\tvalid_1's rmse: 1.10926\n",
      "[333]\ttraining's rmse: 0.990796\tvalid_1's rmse: 1.10912\n",
      "[334]\ttraining's rmse: 0.990565\tvalid_1's rmse: 1.10902\n",
      "[335]\ttraining's rmse: 0.990389\tvalid_1's rmse: 1.10893\n",
      "[336]\ttraining's rmse: 0.990211\tvalid_1's rmse: 1.10878\n",
      "[337]\ttraining's rmse: 0.990044\tvalid_1's rmse: 1.10874\n",
      "[338]\ttraining's rmse: 0.989814\tvalid_1's rmse: 1.10867\n",
      "[339]\ttraining's rmse: 0.989636\tvalid_1's rmse: 1.1086\n",
      "[340]\ttraining's rmse: 0.989461\tvalid_1's rmse: 1.10849\n",
      "[341]\ttraining's rmse: 0.989239\tvalid_1's rmse: 1.10837\n",
      "[342]\ttraining's rmse: 0.989054\tvalid_1's rmse: 1.10814\n",
      "[343]\ttraining's rmse: 0.988894\tvalid_1's rmse: 1.10808\n",
      "[344]\ttraining's rmse: 0.988713\tvalid_1's rmse: 1.10802\n",
      "[345]\ttraining's rmse: 0.988495\tvalid_1's rmse: 1.10795\n",
      "[346]\ttraining's rmse: 0.988326\tvalid_1's rmse: 1.10787\n",
      "[347]\ttraining's rmse: 0.988147\tvalid_1's rmse: 1.10762\n",
      "[348]\ttraining's rmse: 0.987989\tvalid_1's rmse: 1.10758\n",
      "[349]\ttraining's rmse: 0.987771\tvalid_1's rmse: 1.10745\n",
      "[350]\ttraining's rmse: 0.987599\tvalid_1's rmse: 1.10738\n",
      "[351]\ttraining's rmse: 0.987424\tvalid_1's rmse: 1.10713\n",
      "[352]\ttraining's rmse: 0.987275\tvalid_1's rmse: 1.10707\n",
      "[353]\ttraining's rmse: 0.987065\tvalid_1's rmse: 1.10698\n",
      "[354]\ttraining's rmse: 0.98693\tvalid_1's rmse: 1.10695\n",
      "[355]\ttraining's rmse: 0.986764\tvalid_1's rmse: 1.10684\n",
      "[356]\ttraining's rmse: 0.986557\tvalid_1's rmse: 1.10674\n",
      "[357]\ttraining's rmse: 0.986398\tvalid_1's rmse: 1.1067\n",
      "[358]\ttraining's rmse: 0.986227\tvalid_1's rmse: 1.10666\n",
      "[359]\ttraining's rmse: 0.986065\tvalid_1's rmse: 1.10656\n",
      "[360]\ttraining's rmse: 0.985864\tvalid_1's rmse: 1.10647\n",
      "[361]\ttraining's rmse: 0.985687\tvalid_1's rmse: 1.10638\n",
      "[362]\ttraining's rmse: 0.98553\tvalid_1's rmse: 1.10616\n",
      "[363]\ttraining's rmse: 0.985377\tvalid_1's rmse: 1.10612\n",
      "[364]\ttraining's rmse: 0.985179\tvalid_1's rmse: 1.10599\n",
      "[365]\ttraining's rmse: 0.985021\tvalid_1's rmse: 1.10588\n",
      "[366]\ttraining's rmse: 0.984869\tvalid_1's rmse: 1.10578\n",
      "[367]\ttraining's rmse: 0.984706\tvalid_1's rmse: 1.10557\n",
      "[368]\ttraining's rmse: 0.984507\tvalid_1's rmse: 1.10553\n",
      "[369]\ttraining's rmse: 0.984365\tvalid_1's rmse: 1.10545\n",
      "[370]\ttraining's rmse: 0.984192\tvalid_1's rmse: 1.10533\n",
      "[371]\ttraining's rmse: 0.984044\tvalid_1's rmse: 1.10526\n",
      "[372]\ttraining's rmse: 0.983853\tvalid_1's rmse: 1.10516\n",
      "[373]\ttraining's rmse: 0.983686\tvalid_1's rmse: 1.10505\n",
      "[374]\ttraining's rmse: 0.983531\tvalid_1's rmse: 1.10507\n",
      "[375]\ttraining's rmse: 0.983383\tvalid_1's rmse: 1.10498\n",
      "[376]\ttraining's rmse: 0.983194\tvalid_1's rmse: 1.10489\n",
      "[377]\ttraining's rmse: 0.983057\tvalid_1's rmse: 1.10481\n",
      "[378]\ttraining's rmse: 0.982962\tvalid_1's rmse: 1.10477\n",
      "[379]\ttraining's rmse: 0.9828\tvalid_1's rmse: 1.10466\n",
      "[380]\ttraining's rmse: 0.982612\tvalid_1's rmse: 1.10459\n",
      "[381]\ttraining's rmse: 0.982449\tvalid_1's rmse: 1.10448\n",
      "[382]\ttraining's rmse: 0.982309\tvalid_1's rmse: 1.10444\n",
      "[383]\ttraining's rmse: 0.98213\tvalid_1's rmse: 1.10439\n",
      "[384]\ttraining's rmse: 0.982039\tvalid_1's rmse: 1.10435\n",
      "[385]\ttraining's rmse: 0.981906\tvalid_1's rmse: 1.10427\n",
      "[386]\ttraining's rmse: 0.981733\tvalid_1's rmse: 1.10416\n",
      "[387]\ttraining's rmse: 0.981579\tvalid_1's rmse: 1.10406\n",
      "[388]\ttraining's rmse: 0.981427\tvalid_1's rmse: 1.10402\n",
      "[389]\ttraining's rmse: 0.981328\tvalid_1's rmse: 1.10396\n",
      "[390]\ttraining's rmse: 0.981148\tvalid_1's rmse: 1.10388\n",
      "[391]\ttraining's rmse: 0.981016\tvalid_1's rmse: 1.10378\n",
      "[392]\ttraining's rmse: 0.980866\tvalid_1's rmse: 1.10373\n",
      "[393]\ttraining's rmse: 0.980739\tvalid_1's rmse: 1.10366\n",
      "[394]\ttraining's rmse: 0.980637\tvalid_1's rmse: 1.1036\n",
      "[395]\ttraining's rmse: 0.980464\tvalid_1's rmse: 1.10351\n",
      "[396]\ttraining's rmse: 0.980312\tvalid_1's rmse: 1.10339\n",
      "[397]\ttraining's rmse: 0.980138\tvalid_1's rmse: 1.1033\n",
      "[398]\ttraining's rmse: 0.979982\tvalid_1's rmse: 1.10314\n",
      "[399]\ttraining's rmse: 0.979847\tvalid_1's rmse: 1.10313\n",
      "[400]\ttraining's rmse: 0.979685\tvalid_1's rmse: 1.10306\n",
      "[401]\ttraining's rmse: 0.97958\tvalid_1's rmse: 1.10297\n",
      "[402]\ttraining's rmse: 0.979413\tvalid_1's rmse: 1.10292\n",
      "[403]\ttraining's rmse: 0.979287\tvalid_1's rmse: 1.10286\n",
      "[404]\ttraining's rmse: 0.979118\tvalid_1's rmse: 1.10281\n",
      "[405]\ttraining's rmse: 0.978971\tvalid_1's rmse: 1.10271\n",
      "[406]\ttraining's rmse: 0.978889\tvalid_1's rmse: 1.10267\n",
      "[407]\ttraining's rmse: 0.978773\tvalid_1's rmse: 1.10261\n",
      "[408]\ttraining's rmse: 0.978608\tvalid_1's rmse: 1.10249\n",
      "[409]\ttraining's rmse: 0.978468\tvalid_1's rmse: 1.10246\n",
      "[410]\ttraining's rmse: 0.978324\tvalid_1's rmse: 1.10235\n",
      "[411]\ttraining's rmse: 0.978186\tvalid_1's rmse: 1.10227\n",
      "[412]\ttraining's rmse: 0.978064\tvalid_1's rmse: 1.1022\n",
      "[413]\ttraining's rmse: 0.977924\tvalid_1's rmse: 1.10219\n",
      "[414]\ttraining's rmse: 0.977763\tvalid_1's rmse: 1.10214\n",
      "[415]\ttraining's rmse: 0.977626\tvalid_1's rmse: 1.10213\n",
      "[416]\ttraining's rmse: 0.977548\tvalid_1's rmse: 1.1021\n",
      "[417]\ttraining's rmse: 0.977412\tvalid_1's rmse: 1.10206\n",
      "[418]\ttraining's rmse: 0.977246\tvalid_1's rmse: 1.10197\n",
      "[419]\ttraining's rmse: 0.97715\tvalid_1's rmse: 1.10188\n",
      "[420]\ttraining's rmse: 0.977032\tvalid_1's rmse: 1.10182\n",
      "[421]\ttraining's rmse: 0.976871\tvalid_1's rmse: 1.10174\n",
      "[422]\ttraining's rmse: 0.976738\tvalid_1's rmse: 1.10173\n",
      "[423]\ttraining's rmse: 0.976602\tvalid_1's rmse: 1.10163\n",
      "[424]\ttraining's rmse: 0.976516\tvalid_1's rmse: 1.1016\n",
      "[425]\ttraining's rmse: 0.976387\tvalid_1's rmse: 1.10158\n",
      "[426]\ttraining's rmse: 0.976228\tvalid_1's rmse: 1.1015\n",
      "[427]\ttraining's rmse: 0.976093\tvalid_1's rmse: 1.10148\n",
      "[428]\ttraining's rmse: 0.975953\tvalid_1's rmse: 1.10137\n",
      "[429]\ttraining's rmse: 0.975825\tvalid_1's rmse: 1.10136\n",
      "[430]\ttraining's rmse: 0.97574\tvalid_1's rmse: 1.10127\n",
      "[431]\ttraining's rmse: 0.975584\tvalid_1's rmse: 1.10118\n",
      "[432]\ttraining's rmse: 0.975472\tvalid_1's rmse: 1.1011\n",
      "[433]\ttraining's rmse: 0.975345\tvalid_1's rmse: 1.10101\n",
      "[434]\ttraining's rmse: 0.975193\tvalid_1's rmse: 1.101\n",
      "[435]\ttraining's rmse: 0.975103\tvalid_1's rmse: 1.10093\n",
      "[436]\ttraining's rmse: 0.974979\tvalid_1's rmse: 1.10092\n",
      "[437]\ttraining's rmse: 0.974854\tvalid_1's rmse: 1.1009\n",
      "[438]\ttraining's rmse: 0.974706\tvalid_1's rmse: 1.10085\n",
      "[439]\ttraining's rmse: 0.974589\tvalid_1's rmse: 1.10086\n",
      "[440]\ttraining's rmse: 0.974494\tvalid_1's rmse: 1.10081\n",
      "[441]\ttraining's rmse: 0.974365\tvalid_1's rmse: 1.10079\n",
      "[442]\ttraining's rmse: 0.974217\tvalid_1's rmse: 1.10074\n",
      "[443]\ttraining's rmse: 0.974095\tvalid_1's rmse: 1.10067\n",
      "[444]\ttraining's rmse: 0.973956\tvalid_1's rmse: 1.10054\n",
      "[445]\ttraining's rmse: 0.973871\tvalid_1's rmse: 1.10044\n",
      "[446]\ttraining's rmse: 0.973751\tvalid_1's rmse: 1.10043\n",
      "[447]\ttraining's rmse: 0.973604\tvalid_1's rmse: 1.10039\n",
      "[448]\ttraining's rmse: 0.973484\tvalid_1's rmse: 1.10031\n",
      "[449]\ttraining's rmse: 0.973365\tvalid_1's rmse: 1.1003\n",
      "[450]\ttraining's rmse: 0.973222\tvalid_1's rmse: 1.10026\n",
      "[451]\ttraining's rmse: 0.973139\tvalid_1's rmse: 1.10022\n",
      "[452]\ttraining's rmse: 0.973002\tvalid_1's rmse: 1.10006\n",
      "[453]\ttraining's rmse: 0.972861\tvalid_1's rmse: 1.1\n",
      "[454]\ttraining's rmse: 0.972737\tvalid_1's rmse: 1.09999\n",
      "[455]\ttraining's rmse: 0.972589\tvalid_1's rmse: 1.09986\n",
      "[456]\ttraining's rmse: 0.972499\tvalid_1's rmse: 1.09981\n",
      "[457]\ttraining's rmse: 0.972384\tvalid_1's rmse: 1.09978\n",
      "[458]\ttraining's rmse: 0.972242\tvalid_1's rmse: 1.0997\n",
      "[459]\ttraining's rmse: 0.972125\tvalid_1's rmse: 1.09964\n",
      "[460]\ttraining's rmse: 0.971999\tvalid_1's rmse: 1.09953\n",
      "[461]\ttraining's rmse: 0.971882\tvalid_1's rmse: 1.09953\n",
      "[462]\ttraining's rmse: 0.971765\tvalid_1's rmse: 1.09947\n",
      "[463]\ttraining's rmse: 0.971627\tvalid_1's rmse: 1.09944\n",
      "[464]\ttraining's rmse: 0.971547\tvalid_1's rmse: 1.09939\n",
      "[465]\ttraining's rmse: 0.971412\tvalid_1's rmse: 1.09929\n",
      "[466]\ttraining's rmse: 0.971279\tvalid_1's rmse: 1.09927\n",
      "[467]\ttraining's rmse: 0.971144\tvalid_1's rmse: 1.09927\n",
      "[468]\ttraining's rmse: 0.97103\tvalid_1's rmse: 1.09926\n",
      "[469]\ttraining's rmse: 0.970915\tvalid_1's rmse: 1.09918\n",
      "[470]\ttraining's rmse: 0.970851\tvalid_1's rmse: 1.09919\n",
      "[471]\ttraining's rmse: 0.97074\tvalid_1's rmse: 1.09914\n",
      "[472]\ttraining's rmse: 0.970602\tvalid_1's rmse: 1.09908\n",
      "[473]\ttraining's rmse: 0.97047\tvalid_1's rmse: 1.09895\n",
      "[474]\ttraining's rmse: 0.970359\tvalid_1's rmse: 1.09891\n",
      "[475]\ttraining's rmse: 0.970222\tvalid_1's rmse: 1.09879\n",
      "[476]\ttraining's rmse: 0.970088\tvalid_1's rmse: 1.09874\n",
      "[477]\ttraining's rmse: 0.969979\tvalid_1's rmse: 1.09864\n",
      "[478]\ttraining's rmse: 0.969859\tvalid_1's rmse: 1.09861\n",
      "[479]\ttraining's rmse: 0.969797\tvalid_1's rmse: 1.09862\n",
      "[480]\ttraining's rmse: 0.969663\tvalid_1's rmse: 1.0985\n",
      "[481]\ttraining's rmse: 0.969516\tvalid_1's rmse: 1.09849\n",
      "[482]\ttraining's rmse: 0.969408\tvalid_1's rmse: 1.0985\n",
      "[483]\ttraining's rmse: 0.96929\tvalid_1's rmse: 1.09837\n",
      "[484]\ttraining's rmse: 0.969218\tvalid_1's rmse: 1.09831\n",
      "[485]\ttraining's rmse: 0.969113\tvalid_1's rmse: 1.0982\n",
      "[486]\ttraining's rmse: 0.96898\tvalid_1's rmse: 1.09823\n",
      "[487]\ttraining's rmse: 0.968873\tvalid_1's rmse: 1.09814\n",
      "[488]\ttraining's rmse: 0.968752\tvalid_1's rmse: 1.09813\n",
      "[489]\ttraining's rmse: 0.968633\tvalid_1's rmse: 1.09807\n",
      "[490]\ttraining's rmse: 0.968486\tvalid_1's rmse: 1.09806\n",
      "[491]\ttraining's rmse: 0.968414\tvalid_1's rmse: 1.09805\n",
      "[492]\ttraining's rmse: 0.968298\tvalid_1's rmse: 1.098\n",
      "[493]\ttraining's rmse: 0.968172\tvalid_1's rmse: 1.09789\n",
      "[494]\ttraining's rmse: 0.96804\tvalid_1's rmse: 1.09787\n",
      "[495]\ttraining's rmse: 0.967928\tvalid_1's rmse: 1.09785\n",
      "[496]\ttraining's rmse: 0.967827\tvalid_1's rmse: 1.09786\n",
      "[497]\ttraining's rmse: 0.967756\tvalid_1's rmse: 1.09783\n",
      "[498]\ttraining's rmse: 0.96765\tvalid_1's rmse: 1.09776\n",
      "[499]\ttraining's rmse: 0.967526\tvalid_1's rmse: 1.09759\n",
      "[500]\ttraining's rmse: 0.967387\tvalid_1's rmse: 1.09759\n",
      "[501]\ttraining's rmse: 0.967264\tvalid_1's rmse: 1.09745\n",
      "[502]\ttraining's rmse: 0.967151\tvalid_1's rmse: 1.0974\n",
      "[503]\ttraining's rmse: 0.967014\tvalid_1's rmse: 1.0974\n",
      "[504]\ttraining's rmse: 0.966884\tvalid_1's rmse: 1.09735\n",
      "[505]\ttraining's rmse: 0.966823\tvalid_1's rmse: 1.09732\n",
      "[506]\ttraining's rmse: 0.966715\tvalid_1's rmse: 1.0973\n",
      "[507]\ttraining's rmse: 0.966603\tvalid_1's rmse: 1.09728\n",
      "[508]\ttraining's rmse: 0.966506\tvalid_1's rmse: 1.09729\n",
      "[509]\ttraining's rmse: 0.96638\tvalid_1's rmse: 1.09728\n",
      "[510]\ttraining's rmse: 0.966269\tvalid_1's rmse: 1.09723\n",
      "[511]\ttraining's rmse: 0.96617\tvalid_1's rmse: 1.09717\n",
      "[512]\ttraining's rmse: 0.966115\tvalid_1's rmse: 1.09715\n",
      "[513]\ttraining's rmse: 0.965996\tvalid_1's rmse: 1.097\n",
      "[514]\ttraining's rmse: 0.965879\tvalid_1's rmse: 1.09688\n",
      "[515]\ttraining's rmse: 0.96576\tvalid_1's rmse: 1.09688\n",
      "[516]\ttraining's rmse: 0.965655\tvalid_1's rmse: 1.09687\n",
      "[517]\ttraining's rmse: 0.965581\tvalid_1's rmse: 1.09686\n",
      "[518]\ttraining's rmse: 0.965467\tvalid_1's rmse: 1.09684\n",
      "[519]\ttraining's rmse: 0.965356\tvalid_1's rmse: 1.09682\n",
      "[520]\ttraining's rmse: 0.965292\tvalid_1's rmse: 1.09682\n",
      "[521]\ttraining's rmse: 0.965196\tvalid_1's rmse: 1.09679\n",
      "[522]\ttraining's rmse: 0.965062\tvalid_1's rmse: 1.09678\n",
      "[523]\ttraining's rmse: 0.964955\tvalid_1's rmse: 1.09673\n",
      "[524]\ttraining's rmse: 0.964834\tvalid_1's rmse: 1.09663\n",
      "[525]\ttraining's rmse: 0.964715\tvalid_1's rmse: 1.09662\n",
      "[526]\ttraining's rmse: 0.964635\tvalid_1's rmse: 1.09662\n",
      "[527]\ttraining's rmse: 0.964531\tvalid_1's rmse: 1.09657\n",
      "[528]\ttraining's rmse: 0.964429\tvalid_1's rmse: 1.09654\n",
      "[529]\ttraining's rmse: 0.96432\tvalid_1's rmse: 1.09652\n",
      "[530]\ttraining's rmse: 0.964211\tvalid_1's rmse: 1.09645\n",
      "[531]\ttraining's rmse: 0.964153\tvalid_1's rmse: 1.09643\n",
      "[532]\ttraining's rmse: 0.964024\tvalid_1's rmse: 1.09642\n",
      "[533]\ttraining's rmse: 0.963915\tvalid_1's rmse: 1.09641\n",
      "[534]\ttraining's rmse: 0.963813\tvalid_1's rmse: 1.0964\n",
      "[535]\ttraining's rmse: 0.963733\tvalid_1's rmse: 1.09642\n",
      "[536]\ttraining's rmse: 0.963628\tvalid_1's rmse: 1.09633\n",
      "[537]\ttraining's rmse: 0.963512\tvalid_1's rmse: 1.09631\n",
      "[538]\ttraining's rmse: 0.963399\tvalid_1's rmse: 1.09625\n",
      "[539]\ttraining's rmse: 0.963312\tvalid_1's rmse: 1.09626\n",
      "[540]\ttraining's rmse: 0.963193\tvalid_1's rmse: 1.09613\n",
      "[541]\ttraining's rmse: 0.963062\tvalid_1's rmse: 1.09608\n",
      "[542]\ttraining's rmse: 0.962962\tvalid_1's rmse: 1.09607\n",
      "[543]\ttraining's rmse: 0.962858\tvalid_1's rmse: 1.09602\n",
      "[544]\ttraining's rmse: 0.962742\tvalid_1's rmse: 1.09595\n",
      "[545]\ttraining's rmse: 0.962634\tvalid_1's rmse: 1.09585\n",
      "[546]\ttraining's rmse: 0.962524\tvalid_1's rmse: 1.09584\n",
      "[547]\ttraining's rmse: 0.962413\tvalid_1's rmse: 1.09571\n",
      "[548]\ttraining's rmse: 0.962308\tvalid_1's rmse: 1.09561\n",
      "[549]\ttraining's rmse: 0.962211\tvalid_1's rmse: 1.0956\n",
      "[550]\ttraining's rmse: 0.962112\tvalid_1's rmse: 1.09557\n",
      "[551]\ttraining's rmse: 0.962012\tvalid_1's rmse: 1.09556\n",
      "[552]\ttraining's rmse: 0.961962\tvalid_1's rmse: 1.09556\n",
      "[553]\ttraining's rmse: 0.961855\tvalid_1's rmse: 1.09552\n",
      "[554]\ttraining's rmse: 0.961732\tvalid_1's rmse: 1.09552\n",
      "[555]\ttraining's rmse: 0.961622\tvalid_1's rmse: 1.09543\n",
      "[556]\ttraining's rmse: 0.961506\tvalid_1's rmse: 1.09542\n",
      "[557]\ttraining's rmse: 0.9614\tvalid_1's rmse: 1.09538\n",
      "[558]\ttraining's rmse: 0.961287\tvalid_1's rmse: 1.09529\n",
      "[559]\ttraining's rmse: 0.961173\tvalid_1's rmse: 1.09519\n",
      "[560]\ttraining's rmse: 0.96107\tvalid_1's rmse: 1.09516\n",
      "[561]\ttraining's rmse: 0.961011\tvalid_1's rmse: 1.09512\n",
      "[562]\ttraining's rmse: 0.960894\tvalid_1's rmse: 1.09514\n",
      "[563]\ttraining's rmse: 0.960791\tvalid_1's rmse: 1.09504\n",
      "[564]\ttraining's rmse: 0.960674\tvalid_1's rmse: 1.09503\n",
      "[565]\ttraining's rmse: 0.960579\tvalid_1's rmse: 1.09499\n",
      "[566]\ttraining's rmse: 0.960486\tvalid_1's rmse: 1.09498\n",
      "[567]\ttraining's rmse: 0.960435\tvalid_1's rmse: 1.09498\n",
      "[568]\ttraining's rmse: 0.960346\tvalid_1's rmse: 1.09496\n",
      "[569]\ttraining's rmse: 0.960234\tvalid_1's rmse: 1.09483\n",
      "[570]\ttraining's rmse: 0.960121\tvalid_1's rmse: 1.09472\n",
      "[571]\ttraining's rmse: 0.960017\tvalid_1's rmse: 1.09472\n",
      "[572]\ttraining's rmse: 0.959913\tvalid_1's rmse: 1.09464\n",
      "[573]\ttraining's rmse: 0.959866\tvalid_1's rmse: 1.09466\n",
      "[574]\ttraining's rmse: 0.959757\tvalid_1's rmse: 1.09457\n",
      "[575]\ttraining's rmse: 0.959652\tvalid_1's rmse: 1.09455\n",
      "[576]\ttraining's rmse: 0.959554\tvalid_1's rmse: 1.09449\n",
      "[577]\ttraining's rmse: 0.959452\tvalid_1's rmse: 1.0944\n",
      "[578]\ttraining's rmse: 0.959381\tvalid_1's rmse: 1.0944\n",
      "[579]\ttraining's rmse: 0.959275\tvalid_1's rmse: 1.09436\n",
      "[580]\ttraining's rmse: 0.959165\tvalid_1's rmse: 1.09424\n",
      "[581]\ttraining's rmse: 0.95907\tvalid_1's rmse: 1.09424\n",
      "[582]\ttraining's rmse: 0.958976\tvalid_1's rmse: 1.09416\n",
      "[583]\ttraining's rmse: 0.958864\tvalid_1's rmse: 1.09418\n",
      "[584]\ttraining's rmse: 0.958771\tvalid_1's rmse: 1.09417\n",
      "[585]\ttraining's rmse: 0.958689\tvalid_1's rmse: 1.09418\n",
      "[586]\ttraining's rmse: 0.958593\tvalid_1's rmse: 1.09412\n",
      "[587]\ttraining's rmse: 0.958486\tvalid_1's rmse: 1.09401\n",
      "[588]\ttraining's rmse: 0.958391\tvalid_1's rmse: 1.09399\n",
      "[589]\ttraining's rmse: 0.958284\tvalid_1's rmse: 1.09388\n",
      "[590]\ttraining's rmse: 0.958236\tvalid_1's rmse: 1.09388\n",
      "[591]\ttraining's rmse: 0.958135\tvalid_1's rmse: 1.09386\n",
      "[592]\ttraining's rmse: 0.958042\tvalid_1's rmse: 1.09386\n",
      "[593]\ttraining's rmse: 0.95794\tvalid_1's rmse: 1.09372\n",
      "[594]\ttraining's rmse: 0.957838\tvalid_1's rmse: 1.09368\n",
      "[595]\ttraining's rmse: 0.957733\tvalid_1's rmse: 1.09355\n",
      "[596]\ttraining's rmse: 0.957635\tvalid_1's rmse: 1.09356\n",
      "[597]\ttraining's rmse: 0.957527\tvalid_1's rmse: 1.09356\n",
      "[598]\ttraining's rmse: 0.957434\tvalid_1's rmse: 1.09351\n",
      "[599]\ttraining's rmse: 0.95739\tvalid_1's rmse: 1.09353\n",
      "[600]\ttraining's rmse: 0.957308\tvalid_1's rmse: 1.09351\n",
      "[601]\ttraining's rmse: 0.957205\tvalid_1's rmse: 1.09343\n",
      "[602]\ttraining's rmse: 0.957107\tvalid_1's rmse: 1.09347\n",
      "[603]\ttraining's rmse: 0.95702\tvalid_1's rmse: 1.09342\n",
      "[604]\ttraining's rmse: 0.956973\tvalid_1's rmse: 1.09342\n",
      "[605]\ttraining's rmse: 0.956877\tvalid_1's rmse: 1.09334\n",
      "[606]\ttraining's rmse: 0.956784\tvalid_1's rmse: 1.09332\n",
      "[607]\ttraining's rmse: 0.956665\tvalid_1's rmse: 1.09324\n",
      "[608]\ttraining's rmse: 0.956567\tvalid_1's rmse: 1.09319\n",
      "[609]\ttraining's rmse: 0.956477\tvalid_1's rmse: 1.09309\n",
      "[610]\ttraining's rmse: 0.956385\tvalid_1's rmse: 1.09303\n",
      "[611]\ttraining's rmse: 0.956295\tvalid_1's rmse: 1.09305\n",
      "[612]\ttraining's rmse: 0.956192\tvalid_1's rmse: 1.09309\n",
      "[613]\ttraining's rmse: 0.956104\tvalid_1's rmse: 1.09306\n",
      "[614]\ttraining's rmse: 0.956054\tvalid_1's rmse: 1.09303\n",
      "[615]\ttraining's rmse: 0.955967\tvalid_1's rmse: 1.09296\n",
      "[616]\ttraining's rmse: 0.955894\tvalid_1's rmse: 1.09293\n",
      "[617]\ttraining's rmse: 0.955811\tvalid_1's rmse: 1.09288\n",
      "[618]\ttraining's rmse: 0.955727\tvalid_1's rmse: 1.09278\n",
      "[619]\ttraining's rmse: 0.955656\tvalid_1's rmse: 1.09273\n",
      "[620]\ttraining's rmse: 0.955556\tvalid_1's rmse: 1.09272\n",
      "[621]\ttraining's rmse: 0.95546\tvalid_1's rmse: 1.09261\n",
      "[622]\ttraining's rmse: 0.955415\tvalid_1's rmse: 1.09261\n",
      "[623]\ttraining's rmse: 0.955314\tvalid_1's rmse: 1.09249\n",
      "[624]\ttraining's rmse: 0.955226\tvalid_1's rmse: 1.09246\n",
      "[625]\ttraining's rmse: 0.955139\tvalid_1's rmse: 1.09246\n",
      "[626]\ttraining's rmse: 0.955042\tvalid_1's rmse: 1.09247\n",
      "[627]\ttraining's rmse: 0.954955\tvalid_1's rmse: 1.09245\n",
      "[628]\ttraining's rmse: 0.954862\tvalid_1's rmse: 1.09241\n",
      "[629]\ttraining's rmse: 0.95477\tvalid_1's rmse: 1.09235\n",
      "[630]\ttraining's rmse: 0.954709\tvalid_1's rmse: 1.0923\n",
      "[631]\ttraining's rmse: 0.954625\tvalid_1's rmse: 1.09227\n",
      "[632]\ttraining's rmse: 0.954525\tvalid_1's rmse: 1.0923\n",
      "[633]\ttraining's rmse: 0.954479\tvalid_1's rmse: 1.09229\n",
      "[634]\ttraining's rmse: 0.954397\tvalid_1's rmse: 1.09228\n",
      "[635]\ttraining's rmse: 0.954304\tvalid_1's rmse: 1.09223\n",
      "[636]\ttraining's rmse: 0.954214\tvalid_1's rmse: 1.09225\n",
      "[637]\ttraining's rmse: 0.954115\tvalid_1's rmse: 1.09214\n",
      "[638]\ttraining's rmse: 0.954021\tvalid_1's rmse: 1.09218\n",
      "[639]\ttraining's rmse: 0.953926\tvalid_1's rmse: 1.09211\n",
      "[640]\ttraining's rmse: 0.953841\tvalid_1's rmse: 1.09209\n",
      "[641]\ttraining's rmse: 0.953772\tvalid_1's rmse: 1.09205\n",
      "[642]\ttraining's rmse: 0.953729\tvalid_1's rmse: 1.09205\n",
      "[643]\ttraining's rmse: 0.953641\tvalid_1's rmse: 1.09207\n",
      "[644]\ttraining's rmse: 0.95354\tvalid_1's rmse: 1.09197\n",
      "[645]\ttraining's rmse: 0.953473\tvalid_1's rmse: 1.09193\n",
      "[646]\ttraining's rmse: 0.953371\tvalid_1's rmse: 1.09193\n",
      "[647]\ttraining's rmse: 0.953285\tvalid_1's rmse: 1.09188\n",
      "[648]\ttraining's rmse: 0.953194\tvalid_1's rmse: 1.09184\n",
      "[649]\ttraining's rmse: 0.953102\tvalid_1's rmse: 1.09175\n",
      "[650]\ttraining's rmse: 0.953014\tvalid_1's rmse: 1.09178\n",
      "[651]\ttraining's rmse: 0.952931\tvalid_1's rmse: 1.09176\n",
      "[652]\ttraining's rmse: 0.952886\tvalid_1's rmse: 1.09175\n",
      "[653]\ttraining's rmse: 0.95282\tvalid_1's rmse: 1.0917\n",
      "[654]\ttraining's rmse: 0.952721\tvalid_1's rmse: 1.09167\n",
      "[655]\ttraining's rmse: 0.952679\tvalid_1's rmse: 1.09168\n",
      "[656]\ttraining's rmse: 0.952625\tvalid_1's rmse: 1.09164\n",
      "[657]\ttraining's rmse: 0.952545\tvalid_1's rmse: 1.09158\n",
      "[658]\ttraining's rmse: 0.952456\tvalid_1's rmse: 1.0915\n",
      "[659]\ttraining's rmse: 0.952373\tvalid_1's rmse: 1.09152\n",
      "[660]\ttraining's rmse: 0.952343\tvalid_1's rmse: 1.09153\n",
      "[661]\ttraining's rmse: 0.952259\tvalid_1's rmse: 1.0915\n",
      "[662]\ttraining's rmse: 0.95218\tvalid_1's rmse: 1.0914\n",
      "[663]\ttraining's rmse: 0.952116\tvalid_1's rmse: 1.09136\n",
      "[664]\ttraining's rmse: 0.952021\tvalid_1's rmse: 1.09136\n",
      "[665]\ttraining's rmse: 0.951992\tvalid_1's rmse: 1.09138\n",
      "[666]\ttraining's rmse: 0.951905\tvalid_1's rmse: 1.09131\n",
      "[667]\ttraining's rmse: 0.951822\tvalid_1's rmse: 1.09128\n",
      "[668]\ttraining's rmse: 0.951713\tvalid_1's rmse: 1.09122\n",
      "[669]\ttraining's rmse: 0.951625\tvalid_1's rmse: 1.09122\n",
      "[670]\ttraining's rmse: 0.951544\tvalid_1's rmse: 1.09117\n",
      "[671]\ttraining's rmse: 0.951518\tvalid_1's rmse: 1.0912\n",
      "[672]\ttraining's rmse: 0.951468\tvalid_1's rmse: 1.09115\n",
      "[673]\ttraining's rmse: 0.951406\tvalid_1's rmse: 1.09112\n",
      "[674]\ttraining's rmse: 0.951328\tvalid_1's rmse: 1.09105\n",
      "[675]\ttraining's rmse: 0.951249\tvalid_1's rmse: 1.09107\n",
      "[676]\ttraining's rmse: 0.951223\tvalid_1's rmse: 1.09106\n",
      "[677]\ttraining's rmse: 0.95114\tvalid_1's rmse: 1.09101\n",
      "[678]\ttraining's rmse: 0.951057\tvalid_1's rmse: 1.09102\n",
      "[679]\ttraining's rmse: 0.950974\tvalid_1's rmse: 1.09098\n",
      "[680]\ttraining's rmse: 0.950935\tvalid_1's rmse: 1.09096\n",
      "[681]\ttraining's rmse: 0.95086\tvalid_1's rmse: 1.09087\n",
      "[682]\ttraining's rmse: 0.950782\tvalid_1's rmse: 1.09088\n",
      "[683]\ttraining's rmse: 0.950704\tvalid_1's rmse: 1.09082\n",
      "[684]\ttraining's rmse: 0.950643\tvalid_1's rmse: 1.09078\n",
      "[685]\ttraining's rmse: 0.950562\tvalid_1's rmse: 1.09073\n",
      "[686]\ttraining's rmse: 0.950477\tvalid_1's rmse: 1.09066\n",
      "[687]\ttraining's rmse: 0.950396\tvalid_1's rmse: 1.09067\n",
      "[688]\ttraining's rmse: 0.9503\tvalid_1's rmse: 1.09067\n",
      "[689]\ttraining's rmse: 0.950256\tvalid_1's rmse: 1.09065\n",
      "[690]\ttraining's rmse: 0.950221\tvalid_1's rmse: 1.09066\n",
      "[691]\ttraining's rmse: 0.950142\tvalid_1's rmse: 1.09061\n",
      "[692]\ttraining's rmse: 0.950075\tvalid_1's rmse: 1.0906\n",
      "[693]\ttraining's rmse: 0.950016\tvalid_1's rmse: 1.09056\n",
      "[694]\ttraining's rmse: 0.949942\tvalid_1's rmse: 1.09047\n",
      "[695]\ttraining's rmse: 0.949858\tvalid_1's rmse: 1.09049\n",
      "[696]\ttraining's rmse: 0.949769\tvalid_1's rmse: 1.09039\n",
      "[697]\ttraining's rmse: 0.949694\tvalid_1's rmse: 1.09031\n",
      "[698]\ttraining's rmse: 0.949619\tvalid_1's rmse: 1.09024\n",
      "[699]\ttraining's rmse: 0.94954\tvalid_1's rmse: 1.09025\n",
      "[700]\ttraining's rmse: 0.949466\tvalid_1's rmse: 1.09027\n",
      "[701]\ttraining's rmse: 0.949432\tvalid_1's rmse: 1.0903\n",
      "[702]\ttraining's rmse: 0.949341\tvalid_1's rmse: 1.09021\n",
      "[703]\ttraining's rmse: 0.949293\tvalid_1's rmse: 1.09016\n",
      "[704]\ttraining's rmse: 0.949216\tvalid_1's rmse: 1.0901\n",
      "[705]\ttraining's rmse: 0.94915\tvalid_1's rmse: 1.09009\n",
      "[706]\ttraining's rmse: 0.949091\tvalid_1's rmse: 1.09008\n",
      "[707]\ttraining's rmse: 0.949012\tvalid_1's rmse: 1.09005\n",
      "[708]\ttraining's rmse: 0.948935\tvalid_1's rmse: 1.09002\n",
      "[709]\ttraining's rmse: 0.948871\tvalid_1's rmse: 1.09004\n",
      "[710]\ttraining's rmse: 0.948785\tvalid_1's rmse: 1.09004\n",
      "[711]\ttraining's rmse: 0.948699\tvalid_1's rmse: 1.09003\n",
      "[712]\ttraining's rmse: 0.948623\tvalid_1's rmse: 1.09005\n",
      "[713]\ttraining's rmse: 0.948529\tvalid_1's rmse: 1.08998\n",
      "[714]\ttraining's rmse: 0.948457\tvalid_1's rmse: 1.08991\n",
      "[715]\ttraining's rmse: 0.948383\tvalid_1's rmse: 1.08985\n",
      "[716]\ttraining's rmse: 0.948348\tvalid_1's rmse: 1.0898\n",
      "[717]\ttraining's rmse: 0.948263\tvalid_1's rmse: 1.08984\n",
      "[718]\ttraining's rmse: 0.948178\tvalid_1's rmse: 1.08978\n",
      "[719]\ttraining's rmse: 0.948097\tvalid_1's rmse: 1.08979\n",
      "[720]\ttraining's rmse: 0.948006\tvalid_1's rmse: 1.08972\n",
      "[721]\ttraining's rmse: 0.947964\tvalid_1's rmse: 1.0897\n",
      "[722]\ttraining's rmse: 0.947901\tvalid_1's rmse: 1.08968\n",
      "[723]\ttraining's rmse: 0.947854\tvalid_1's rmse: 1.08965\n",
      "[724]\ttraining's rmse: 0.947774\tvalid_1's rmse: 1.08969\n",
      "[725]\ttraining's rmse: 0.947691\tvalid_1's rmse: 1.08966\n",
      "[726]\ttraining's rmse: 0.947615\tvalid_1's rmse: 1.08961\n",
      "[727]\ttraining's rmse: 0.947537\tvalid_1's rmse: 1.08962\n",
      "[728]\ttraining's rmse: 0.947449\tvalid_1's rmse: 1.08956\n",
      "[729]\ttraining's rmse: 0.947426\tvalid_1's rmse: 1.08956\n",
      "[730]\ttraining's rmse: 0.947346\tvalid_1's rmse: 1.08959\n",
      "[731]\ttraining's rmse: 0.947276\tvalid_1's rmse: 1.08952\n",
      "[732]\ttraining's rmse: 0.947187\tvalid_1's rmse: 1.08945\n",
      "[733]\ttraining's rmse: 0.947113\tvalid_1's rmse: 1.08934\n",
      "[734]\ttraining's rmse: 0.947015\tvalid_1's rmse: 1.08932\n",
      "[735]\ttraining's rmse: 0.94694\tvalid_1's rmse: 1.08932\n",
      "[736]\ttraining's rmse: 0.946912\tvalid_1's rmse: 1.08933\n",
      "[737]\ttraining's rmse: 0.946874\tvalid_1's rmse: 1.0893\n",
      "[738]\ttraining's rmse: 0.9468\tvalid_1's rmse: 1.08929\n",
      "[739]\ttraining's rmse: 0.946743\tvalid_1's rmse: 1.08929\n",
      "[740]\ttraining's rmse: 0.946667\tvalid_1's rmse: 1.08924\n",
      "[741]\ttraining's rmse: 0.946641\tvalid_1's rmse: 1.08924\n",
      "[742]\ttraining's rmse: 0.946541\tvalid_1's rmse: 1.08918\n",
      "[743]\ttraining's rmse: 0.946454\tvalid_1's rmse: 1.08911\n",
      "[744]\ttraining's rmse: 0.94637\tvalid_1's rmse: 1.0891\n",
      "[745]\ttraining's rmse: 0.946325\tvalid_1's rmse: 1.08909\n",
      "[746]\ttraining's rmse: 0.94625\tvalid_1's rmse: 1.0891\n",
      "[747]\ttraining's rmse: 0.946225\tvalid_1's rmse: 1.08911\n",
      "[748]\ttraining's rmse: 0.946145\tvalid_1's rmse: 1.0891\n",
      "[749]\ttraining's rmse: 0.946061\tvalid_1's rmse: 1.08903\n",
      "[750]\ttraining's rmse: 0.945961\tvalid_1's rmse: 1.08903\n",
      "[751]\ttraining's rmse: 0.94589\tvalid_1's rmse: 1.08903\n",
      "[752]\ttraining's rmse: 0.945806\tvalid_1's rmse: 1.08895\n",
      "[753]\ttraining's rmse: 0.945731\tvalid_1's rmse: 1.08898\n",
      "[754]\ttraining's rmse: 0.945671\tvalid_1's rmse: 1.089\n",
      "[755]\ttraining's rmse: 0.945601\tvalid_1's rmse: 1.08899\n",
      "[756]\ttraining's rmse: 0.945577\tvalid_1's rmse: 1.089\n",
      "[757]\ttraining's rmse: 0.945503\tvalid_1's rmse: 1.08894\n",
      "[758]\ttraining's rmse: 0.945461\tvalid_1's rmse: 1.08891\n",
      "[759]\ttraining's rmse: 0.945391\tvalid_1's rmse: 1.08891\n",
      "[760]\ttraining's rmse: 0.945309\tvalid_1's rmse: 1.08888\n",
      "[761]\ttraining's rmse: 0.945268\tvalid_1's rmse: 1.08887\n",
      "[762]\ttraining's rmse: 0.945195\tvalid_1's rmse: 1.08884\n",
      "[763]\ttraining's rmse: 0.945134\tvalid_1's rmse: 1.08882\n",
      "[764]\ttraining's rmse: 0.945054\tvalid_1's rmse: 1.08881\n",
      "[765]\ttraining's rmse: 0.945013\tvalid_1's rmse: 1.08877\n",
      "[766]\ttraining's rmse: 0.94495\tvalid_1's rmse: 1.08878\n",
      "[767]\ttraining's rmse: 0.944868\tvalid_1's rmse: 1.08878\n",
      "[768]\ttraining's rmse: 0.944771\tvalid_1's rmse: 1.08876\n",
      "[769]\ttraining's rmse: 0.944696\tvalid_1's rmse: 1.08873\n",
      "[770]\ttraining's rmse: 0.944629\tvalid_1's rmse: 1.08867\n",
      "[771]\ttraining's rmse: 0.944563\tvalid_1's rmse: 1.08865\n",
      "[772]\ttraining's rmse: 0.944527\tvalid_1's rmse: 1.08862\n",
      "[773]\ttraining's rmse: 0.944488\tvalid_1's rmse: 1.08861\n",
      "[774]\ttraining's rmse: 0.944426\tvalid_1's rmse: 1.08861\n",
      "[775]\ttraining's rmse: 0.944344\tvalid_1's rmse: 1.08861\n",
      "[776]\ttraining's rmse: 0.944301\tvalid_1's rmse: 1.0886\n",
      "[777]\ttraining's rmse: 0.944219\tvalid_1's rmse: 1.08856\n",
      "[778]\ttraining's rmse: 0.94415\tvalid_1's rmse: 1.08847\n",
      "[779]\ttraining's rmse: 0.944074\tvalid_1's rmse: 1.08851\n",
      "[780]\ttraining's rmse: 0.944021\tvalid_1's rmse: 1.08852\n",
      "[781]\ttraining's rmse: 0.943948\tvalid_1's rmse: 1.08848\n",
      "[782]\ttraining's rmse: 0.943907\tvalid_1's rmse: 1.08848\n",
      "[783]\ttraining's rmse: 0.943824\tvalid_1's rmse: 1.08841\n",
      "[784]\ttraining's rmse: 0.943784\tvalid_1's rmse: 1.08838\n",
      "[785]\ttraining's rmse: 0.943729\tvalid_1's rmse: 1.08837\n",
      "[786]\ttraining's rmse: 0.943655\tvalid_1's rmse: 1.08836\n",
      "[787]\ttraining's rmse: 0.943592\tvalid_1's rmse: 1.0883\n",
      "[788]\ttraining's rmse: 0.943569\tvalid_1's rmse: 1.08829\n",
      "[789]\ttraining's rmse: 0.943529\tvalid_1's rmse: 1.08828\n",
      "[790]\ttraining's rmse: 0.94349\tvalid_1's rmse: 1.08824\n",
      "[791]\ttraining's rmse: 0.94342\tvalid_1's rmse: 1.08817\n",
      "[792]\ttraining's rmse: 0.943355\tvalid_1's rmse: 1.08821\n",
      "[793]\ttraining's rmse: 0.943283\tvalid_1's rmse: 1.08817\n",
      "[794]\ttraining's rmse: 0.943207\tvalid_1's rmse: 1.08819\n",
      "[795]\ttraining's rmse: 0.943153\tvalid_1's rmse: 1.08816\n",
      "[796]\ttraining's rmse: 0.943075\tvalid_1's rmse: 1.08813\n",
      "[797]\ttraining's rmse: 0.942994\tvalid_1's rmse: 1.0881\n",
      "[798]\ttraining's rmse: 0.942926\tvalid_1's rmse: 1.08811\n",
      "[799]\ttraining's rmse: 0.942891\tvalid_1's rmse: 1.08808\n",
      "[800]\ttraining's rmse: 0.942866\tvalid_1's rmse: 1.08808\n",
      "[801]\ttraining's rmse: 0.942822\tvalid_1's rmse: 1.08806\n",
      "[802]\ttraining's rmse: 0.942745\tvalid_1's rmse: 1.08809\n",
      "[803]\ttraining's rmse: 0.942704\tvalid_1's rmse: 1.08809\n",
      "[804]\ttraining's rmse: 0.942629\tvalid_1's rmse: 1.08805\n",
      "[805]\ttraining's rmse: 0.942562\tvalid_1's rmse: 1.08806\n",
      "[806]\ttraining's rmse: 0.942468\tvalid_1's rmse: 1.088\n",
      "[807]\ttraining's rmse: 0.942397\tvalid_1's rmse: 1.08797\n",
      "[808]\ttraining's rmse: 0.942326\tvalid_1's rmse: 1.088\n",
      "[809]\ttraining's rmse: 0.942249\tvalid_1's rmse: 1.088\n",
      "[810]\ttraining's rmse: 0.942176\tvalid_1's rmse: 1.08803\n",
      "[811]\ttraining's rmse: 0.942105\tvalid_1's rmse: 1.08794\n",
      "[812]\ttraining's rmse: 0.942084\tvalid_1's rmse: 1.08794\n",
      "[813]\ttraining's rmse: 0.942015\tvalid_1's rmse: 1.08795\n",
      "[814]\ttraining's rmse: 0.941982\tvalid_1's rmse: 1.08792\n",
      "[815]\ttraining's rmse: 0.941929\tvalid_1's rmse: 1.08792\n",
      "[816]\ttraining's rmse: 0.941899\tvalid_1's rmse: 1.08792\n",
      "[817]\ttraining's rmse: 0.941829\tvalid_1's rmse: 1.08789\n",
      "[818]\ttraining's rmse: 0.941736\tvalid_1's rmse: 1.0879\n",
      "[819]\ttraining's rmse: 0.94167\tvalid_1's rmse: 1.08791\n",
      "[820]\ttraining's rmse: 0.941603\tvalid_1's rmse: 1.08789\n",
      "[821]\ttraining's rmse: 0.941544\tvalid_1's rmse: 1.0879\n",
      "[822]\ttraining's rmse: 0.941455\tvalid_1's rmse: 1.08791\n",
      "[823]\ttraining's rmse: 0.941403\tvalid_1's rmse: 1.08791\n",
      "[824]\ttraining's rmse: 0.941338\tvalid_1's rmse: 1.08792\n",
      "[825]\ttraining's rmse: 0.941298\tvalid_1's rmse: 1.08791\n",
      "[826]\ttraining's rmse: 0.941237\tvalid_1's rmse: 1.08788\n",
      "[827]\ttraining's rmse: 0.941199\tvalid_1's rmse: 1.08785\n",
      "[828]\ttraining's rmse: 0.941122\tvalid_1's rmse: 1.08784\n",
      "[829]\ttraining's rmse: 0.94105\tvalid_1's rmse: 1.08781\n",
      "[830]\ttraining's rmse: 0.940993\tvalid_1's rmse: 1.08782\n",
      "[831]\ttraining's rmse: 0.940902\tvalid_1's rmse: 1.08777\n",
      "[832]\ttraining's rmse: 0.940859\tvalid_1's rmse: 1.08775\n",
      "[833]\ttraining's rmse: 0.940792\tvalid_1's rmse: 1.08777\n",
      "[834]\ttraining's rmse: 0.940727\tvalid_1's rmse: 1.08774\n",
      "[835]\ttraining's rmse: 0.94065\tvalid_1's rmse: 1.08773\n",
      "[836]\ttraining's rmse: 0.940599\tvalid_1's rmse: 1.08773\n",
      "[837]\ttraining's rmse: 0.940542\tvalid_1's rmse: 1.08773\n",
      "[838]\ttraining's rmse: 0.940509\tvalid_1's rmse: 1.0877\n",
      "[839]\ttraining's rmse: 0.940442\tvalid_1's rmse: 1.08771\n",
      "[840]\ttraining's rmse: 0.940377\tvalid_1's rmse: 1.08768\n",
      "[841]\ttraining's rmse: 0.940338\tvalid_1's rmse: 1.08767\n",
      "[842]\ttraining's rmse: 0.940269\tvalid_1's rmse: 1.08758\n",
      "[843]\ttraining's rmse: 0.9402\tvalid_1's rmse: 1.0876\n",
      "[844]\ttraining's rmse: 0.940168\tvalid_1's rmse: 1.08757\n",
      "[845]\ttraining's rmse: 0.940117\tvalid_1's rmse: 1.08759\n",
      "[846]\ttraining's rmse: 0.940048\tvalid_1's rmse: 1.08756\n",
      "[847]\ttraining's rmse: 0.939974\tvalid_1's rmse: 1.08759\n",
      "[848]\ttraining's rmse: 0.939902\tvalid_1's rmse: 1.0876\n",
      "[849]\ttraining's rmse: 0.939812\tvalid_1's rmse: 1.08759\n",
      "[850]\ttraining's rmse: 0.939743\tvalid_1's rmse: 1.08752\n",
      "[851]\ttraining's rmse: 0.939678\tvalid_1's rmse: 1.08752\n",
      "[852]\ttraining's rmse: 0.939622\tvalid_1's rmse: 1.08753\n",
      "[853]\ttraining's rmse: 0.939546\tvalid_1's rmse: 1.08752\n",
      "[854]\ttraining's rmse: 0.939457\tvalid_1's rmse: 1.08755\n",
      "[855]\ttraining's rmse: 0.939394\tvalid_1's rmse: 1.0875\n",
      "[856]\ttraining's rmse: 0.939316\tvalid_1's rmse: 1.0875\n",
      "[857]\ttraining's rmse: 0.939252\tvalid_1's rmse: 1.08751\n",
      "[858]\ttraining's rmse: 0.939201\tvalid_1's rmse: 1.08752\n",
      "[859]\ttraining's rmse: 0.939181\tvalid_1's rmse: 1.08753\n",
      "[860]\ttraining's rmse: 0.939094\tvalid_1's rmse: 1.08751\n",
      "[861]\ttraining's rmse: 0.939017\tvalid_1's rmse: 1.08747\n",
      "[862]\ttraining's rmse: 0.938985\tvalid_1's rmse: 1.08744\n",
      "[863]\ttraining's rmse: 0.938917\tvalid_1's rmse: 1.08742\n",
      "[864]\ttraining's rmse: 0.938859\tvalid_1's rmse: 1.0874\n",
      "[865]\ttraining's rmse: 0.938797\tvalid_1's rmse: 1.08743\n",
      "[866]\ttraining's rmse: 0.938768\tvalid_1's rmse: 1.08743\n",
      "[867]\ttraining's rmse: 0.938733\tvalid_1's rmse: 1.08741\n",
      "[868]\ttraining's rmse: 0.938676\tvalid_1's rmse: 1.08743\n",
      "[869]\ttraining's rmse: 0.938628\tvalid_1's rmse: 1.08743\n",
      "[870]\ttraining's rmse: 0.938553\tvalid_1's rmse: 1.0874\n",
      "[871]\ttraining's rmse: 0.938495\tvalid_1's rmse: 1.08736\n",
      "[872]\ttraining's rmse: 0.93846\tvalid_1's rmse: 1.08735\n",
      "[873]\ttraining's rmse: 0.938384\tvalid_1's rmse: 1.08737\n",
      "[874]\ttraining's rmse: 0.938281\tvalid_1's rmse: 1.08735\n",
      "[875]\ttraining's rmse: 0.938208\tvalid_1's rmse: 1.08735\n",
      "[876]\ttraining's rmse: 0.938122\tvalid_1's rmse: 1.08737\n",
      "[877]\ttraining's rmse: 0.938093\tvalid_1's rmse: 1.08735\n",
      "[878]\ttraining's rmse: 0.938006\tvalid_1's rmse: 1.08737\n",
      "[879]\ttraining's rmse: 0.937936\tvalid_1's rmse: 1.08732\n",
      "[880]\ttraining's rmse: 0.937869\tvalid_1's rmse: 1.08724\n",
      "[881]\ttraining's rmse: 0.937793\tvalid_1's rmse: 1.08725\n",
      "[882]\ttraining's rmse: 0.937719\tvalid_1's rmse: 1.08725\n",
      "[883]\ttraining's rmse: 0.937671\tvalid_1's rmse: 1.08725\n",
      "[884]\ttraining's rmse: 0.937608\tvalid_1's rmse: 1.08723\n",
      "[885]\ttraining's rmse: 0.937568\tvalid_1's rmse: 1.08722\n",
      "[886]\ttraining's rmse: 0.937487\tvalid_1's rmse: 1.08723\n",
      "[887]\ttraining's rmse: 0.937428\tvalid_1's rmse: 1.08723\n",
      "[888]\ttraining's rmse: 0.937371\tvalid_1's rmse: 1.08717\n",
      "[889]\ttraining's rmse: 0.937325\tvalid_1's rmse: 1.08717\n",
      "[890]\ttraining's rmse: 0.937271\tvalid_1's rmse: 1.08714\n",
      "[891]\ttraining's rmse: 0.937241\tvalid_1's rmse: 1.08711\n",
      "[892]\ttraining's rmse: 0.937178\tvalid_1's rmse: 1.0871\n",
      "[893]\ttraining's rmse: 0.937145\tvalid_1's rmse: 1.0871\n",
      "[894]\ttraining's rmse: 0.93707\tvalid_1's rmse: 1.08708\n",
      "[895]\ttraining's rmse: 0.937034\tvalid_1's rmse: 1.08707\n",
      "[896]\ttraining's rmse: 0.936951\tvalid_1's rmse: 1.08706\n",
      "[897]\ttraining's rmse: 0.936884\tvalid_1's rmse: 1.08697\n",
      "[898]\ttraining's rmse: 0.936834\tvalid_1's rmse: 1.08699\n",
      "[899]\ttraining's rmse: 0.93676\tvalid_1's rmse: 1.08694\n",
      "[900]\ttraining's rmse: 0.93673\tvalid_1's rmse: 1.08691\n",
      "[901]\ttraining's rmse: 0.936664\tvalid_1's rmse: 1.08695\n",
      "[902]\ttraining's rmse: 0.936597\tvalid_1's rmse: 1.08697\n",
      "[903]\ttraining's rmse: 0.936543\tvalid_1's rmse: 1.08694\n",
      "[904]\ttraining's rmse: 0.936513\tvalid_1's rmse: 1.08695\n",
      "[905]\ttraining's rmse: 0.936441\tvalid_1's rmse: 1.08696\n",
      "[906]\ttraining's rmse: 0.936412\tvalid_1's rmse: 1.08693\n",
      "[907]\ttraining's rmse: 0.936313\tvalid_1's rmse: 1.08692\n",
      "[908]\ttraining's rmse: 0.936267\tvalid_1's rmse: 1.08691\n",
      "[909]\ttraining's rmse: 0.936215\tvalid_1's rmse: 1.08693\n",
      "[910]\ttraining's rmse: 0.93616\tvalid_1's rmse: 1.08691\n",
      "[911]\ttraining's rmse: 0.936093\tvalid_1's rmse: 1.08692\n",
      "[912]\ttraining's rmse: 0.936009\tvalid_1's rmse: 1.08695\n",
      "[913]\ttraining's rmse: 0.935944\tvalid_1's rmse: 1.08697\n",
      "[914]\ttraining's rmse: 0.935869\tvalid_1's rmse: 1.08697\n",
      "[915]\ttraining's rmse: 0.935796\tvalid_1's rmse: 1.08693\n",
      "[916]\ttraining's rmse: 0.935761\tvalid_1's rmse: 1.08692\n",
      "[917]\ttraining's rmse: 0.935691\tvalid_1's rmse: 1.08689\n",
      "[918]\ttraining's rmse: 0.935628\tvalid_1's rmse: 1.08684\n",
      "[919]\ttraining's rmse: 0.93559\tvalid_1's rmse: 1.08683\n",
      "[920]\ttraining's rmse: 0.935521\tvalid_1's rmse: 1.08685\n",
      "[921]\ttraining's rmse: 0.935436\tvalid_1's rmse: 1.08686\n",
      "[922]\ttraining's rmse: 0.93535\tvalid_1's rmse: 1.08682\n",
      "[923]\ttraining's rmse: 0.935325\tvalid_1's rmse: 1.08682\n",
      "[924]\ttraining's rmse: 0.935294\tvalid_1's rmse: 1.08681\n",
      "[925]\ttraining's rmse: 0.935222\tvalid_1's rmse: 1.08681\n",
      "[926]\ttraining's rmse: 0.935156\tvalid_1's rmse: 1.08679\n",
      "[927]\ttraining's rmse: 0.935082\tvalid_1's rmse: 1.08676\n",
      "[928]\ttraining's rmse: 0.935035\tvalid_1's rmse: 1.08676\n",
      "[929]\ttraining's rmse: 0.934979\tvalid_1's rmse: 1.0867\n",
      "[930]\ttraining's rmse: 0.934935\tvalid_1's rmse: 1.08669\n",
      "[931]\ttraining's rmse: 0.934898\tvalid_1's rmse: 1.08662\n",
      "[932]\ttraining's rmse: 0.934869\tvalid_1's rmse: 1.08662\n",
      "[933]\ttraining's rmse: 0.934806\tvalid_1's rmse: 1.08667\n",
      "[934]\ttraining's rmse: 0.934774\tvalid_1's rmse: 1.08666\n",
      "[935]\ttraining's rmse: 0.934706\tvalid_1's rmse: 1.0866\n",
      "[936]\ttraining's rmse: 0.934611\tvalid_1's rmse: 1.08659\n",
      "[937]\ttraining's rmse: 0.934533\tvalid_1's rmse: 1.08657\n",
      "[938]\ttraining's rmse: 0.93449\tvalid_1's rmse: 1.08655\n",
      "[939]\ttraining's rmse: 0.934423\tvalid_1's rmse: 1.08654\n",
      "[940]\ttraining's rmse: 0.934368\tvalid_1's rmse: 1.08654\n",
      "[941]\ttraining's rmse: 0.934332\tvalid_1's rmse: 1.08647\n",
      "[942]\ttraining's rmse: 0.934282\tvalid_1's rmse: 1.08648\n",
      "[943]\ttraining's rmse: 0.934218\tvalid_1's rmse: 1.0865\n",
      "[944]\ttraining's rmse: 0.934147\tvalid_1's rmse: 1.08647\n",
      "[945]\ttraining's rmse: 0.934105\tvalid_1's rmse: 1.08647\n",
      "[946]\ttraining's rmse: 0.934054\tvalid_1's rmse: 1.08646\n",
      "[947]\ttraining's rmse: 0.933964\tvalid_1's rmse: 1.08644\n",
      "[948]\ttraining's rmse: 0.933915\tvalid_1's rmse: 1.08645\n",
      "[949]\ttraining's rmse: 0.933839\tvalid_1's rmse: 1.08643\n",
      "[950]\ttraining's rmse: 0.933788\tvalid_1's rmse: 1.08645\n",
      "[951]\ttraining's rmse: 0.933745\tvalid_1's rmse: 1.08644\n",
      "[952]\ttraining's rmse: 0.933662\tvalid_1's rmse: 1.0864\n",
      "[953]\ttraining's rmse: 0.933598\tvalid_1's rmse: 1.08636\n",
      "[954]\ttraining's rmse: 0.933509\tvalid_1's rmse: 1.08638\n",
      "[955]\ttraining's rmse: 0.933452\tvalid_1's rmse: 1.08638\n",
      "[956]\ttraining's rmse: 0.933389\tvalid_1's rmse: 1.08633\n",
      "[957]\ttraining's rmse: 0.933327\tvalid_1's rmse: 1.08627\n",
      "[958]\ttraining's rmse: 0.933258\tvalid_1's rmse: 1.08626\n",
      "[959]\ttraining's rmse: 0.933222\tvalid_1's rmse: 1.08625\n",
      "[960]\ttraining's rmse: 0.933136\tvalid_1's rmse: 1.08622\n",
      "[961]\ttraining's rmse: 0.933054\tvalid_1's rmse: 1.08623\n",
      "[962]\ttraining's rmse: 0.932981\tvalid_1's rmse: 1.08618\n",
      "[963]\ttraining's rmse: 0.932907\tvalid_1's rmse: 1.08617\n",
      "[964]\ttraining's rmse: 0.932831\tvalid_1's rmse: 1.08618\n",
      "[965]\ttraining's rmse: 0.932789\tvalid_1's rmse: 1.08618\n",
      "[966]\ttraining's rmse: 0.932726\tvalid_1's rmse: 1.08616\n",
      "[967]\ttraining's rmse: 0.9327\tvalid_1's rmse: 1.08616\n",
      "[968]\ttraining's rmse: 0.932618\tvalid_1's rmse: 1.08612\n",
      "[969]\ttraining's rmse: 0.932535\tvalid_1's rmse: 1.08609\n",
      "[970]\ttraining's rmse: 0.932473\tvalid_1's rmse: 1.08608\n",
      "[971]\ttraining's rmse: 0.932412\tvalid_1's rmse: 1.08608\n",
      "[972]\ttraining's rmse: 0.932384\tvalid_1's rmse: 1.08607\n",
      "[973]\ttraining's rmse: 0.932323\tvalid_1's rmse: 1.08606\n",
      "[974]\ttraining's rmse: 0.93227\tvalid_1's rmse: 1.08607\n",
      "[975]\ttraining's rmse: 0.932211\tvalid_1's rmse: 1.08599\n",
      "[976]\ttraining's rmse: 0.932142\tvalid_1's rmse: 1.08601\n",
      "[977]\ttraining's rmse: 0.932074\tvalid_1's rmse: 1.08598\n",
      "[978]\ttraining's rmse: 0.932031\tvalid_1's rmse: 1.08599\n",
      "[979]\ttraining's rmse: 0.931967\tvalid_1's rmse: 1.08596\n",
      "[980]\ttraining's rmse: 0.931933\tvalid_1's rmse: 1.08597\n",
      "[981]\ttraining's rmse: 0.931903\tvalid_1's rmse: 1.08595\n",
      "[982]\ttraining's rmse: 0.931863\tvalid_1's rmse: 1.08595\n",
      "[983]\ttraining's rmse: 0.931802\tvalid_1's rmse: 1.08591\n",
      "[984]\ttraining's rmse: 0.931731\tvalid_1's rmse: 1.08587\n",
      "[985]\ttraining's rmse: 0.931683\tvalid_1's rmse: 1.08587\n",
      "[986]\ttraining's rmse: 0.93161\tvalid_1's rmse: 1.08589\n",
      "[987]\ttraining's rmse: 0.931587\tvalid_1's rmse: 1.08589\n",
      "[988]\ttraining's rmse: 0.931507\tvalid_1's rmse: 1.08592\n",
      "[989]\ttraining's rmse: 0.931448\tvalid_1's rmse: 1.08591\n",
      "[990]\ttraining's rmse: 0.931409\tvalid_1's rmse: 1.0859\n",
      "[991]\ttraining's rmse: 0.931337\tvalid_1's rmse: 1.08588\n",
      "[992]\ttraining's rmse: 0.9313\tvalid_1's rmse: 1.08587\n",
      "[993]\ttraining's rmse: 0.931253\tvalid_1's rmse: 1.08589\n",
      "[994]\ttraining's rmse: 0.931195\tvalid_1's rmse: 1.08585\n",
      "[995]\ttraining's rmse: 0.931161\tvalid_1's rmse: 1.08584\n",
      "[996]\ttraining's rmse: 0.931092\tvalid_1's rmse: 1.08587\n",
      "[997]\ttraining's rmse: 0.931015\tvalid_1's rmse: 1.0858\n",
      "[998]\ttraining's rmse: 0.930966\tvalid_1's rmse: 1.08579\n",
      "[999]\ttraining's rmse: 0.930905\tvalid_1's rmse: 1.08576\n",
      "[1000]\ttraining's rmse: 0.93086\tvalid_1's rmse: 1.08578\n",
      "[1001]\ttraining's rmse: 0.9308\tvalid_1's rmse: 1.08582\n",
      "[1002]\ttraining's rmse: 0.930765\tvalid_1's rmse: 1.0858\n",
      "[1003]\ttraining's rmse: 0.930695\tvalid_1's rmse: 1.08582\n",
      "[1004]\ttraining's rmse: 0.930612\tvalid_1's rmse: 1.08579\n",
      "[1005]\ttraining's rmse: 0.930558\tvalid_1's rmse: 1.08577\n",
      "[1006]\ttraining's rmse: 0.9305\tvalid_1's rmse: 1.08576\n",
      "[1007]\ttraining's rmse: 0.930461\tvalid_1's rmse: 1.08576\n",
      "[1008]\ttraining's rmse: 0.930396\tvalid_1's rmse: 1.0858\n",
      "[1009]\ttraining's rmse: 0.930325\tvalid_1's rmse: 1.08581\n",
      "[1010]\ttraining's rmse: 0.930262\tvalid_1's rmse: 1.08581\n",
      "[1011]\ttraining's rmse: 0.930233\tvalid_1's rmse: 1.08579\n",
      "[1012]\ttraining's rmse: 0.930172\tvalid_1's rmse: 1.08584\n",
      "[1013]\ttraining's rmse: 0.930099\tvalid_1's rmse: 1.0858\n",
      "[1014]\ttraining's rmse: 0.930035\tvalid_1's rmse: 1.08575\n",
      "[1015]\ttraining's rmse: 0.929997\tvalid_1's rmse: 1.08576\n",
      "[1016]\ttraining's rmse: 0.929928\tvalid_1's rmse: 1.0857\n",
      "[1017]\ttraining's rmse: 0.9299\tvalid_1's rmse: 1.08569\n",
      "[1018]\ttraining's rmse: 0.929848\tvalid_1's rmse: 1.08568\n",
      "[1019]\ttraining's rmse: 0.929795\tvalid_1's rmse: 1.08569\n",
      "[1020]\ttraining's rmse: 0.929717\tvalid_1's rmse: 1.08572\n",
      "[1021]\ttraining's rmse: 0.929643\tvalid_1's rmse: 1.08565\n",
      "[1022]\ttraining's rmse: 0.929598\tvalid_1's rmse: 1.08567\n",
      "[1023]\ttraining's rmse: 0.929553\tvalid_1's rmse: 1.08568\n",
      "[1024]\ttraining's rmse: 0.929486\tvalid_1's rmse: 1.08569\n",
      "[1025]\ttraining's rmse: 0.929433\tvalid_1's rmse: 1.08566\n",
      "[1026]\ttraining's rmse: 0.92938\tvalid_1's rmse: 1.08565\n",
      "[1027]\ttraining's rmse: 0.929297\tvalid_1's rmse: 1.08568\n",
      "[1028]\ttraining's rmse: 0.929225\tvalid_1's rmse: 1.08566\n",
      "[1029]\ttraining's rmse: 0.929156\tvalid_1's rmse: 1.08563\n",
      "[1030]\ttraining's rmse: 0.929116\tvalid_1's rmse: 1.08561\n",
      "[1031]\ttraining's rmse: 0.929089\tvalid_1's rmse: 1.08561\n",
      "[1032]\ttraining's rmse: 0.929033\tvalid_1's rmse: 1.0856\n",
      "[1033]\ttraining's rmse: 0.929005\tvalid_1's rmse: 1.08559\n",
      "[1034]\ttraining's rmse: 0.928983\tvalid_1's rmse: 1.08559\n",
      "[1035]\ttraining's rmse: 0.928941\tvalid_1's rmse: 1.08559\n",
      "[1036]\ttraining's rmse: 0.928875\tvalid_1's rmse: 1.08559\n",
      "[1037]\ttraining's rmse: 0.928807\tvalid_1's rmse: 1.0856\n",
      "[1038]\ttraining's rmse: 0.928773\tvalid_1's rmse: 1.08559\n",
      "[1039]\ttraining's rmse: 0.928702\tvalid_1's rmse: 1.08557\n",
      "[1040]\ttraining's rmse: 0.928627\tvalid_1's rmse: 1.08553\n",
      "[1041]\ttraining's rmse: 0.928601\tvalid_1's rmse: 1.08553\n",
      "[1042]\ttraining's rmse: 0.928519\tvalid_1's rmse: 1.0855\n",
      "[1043]\ttraining's rmse: 0.928449\tvalid_1's rmse: 1.08545\n",
      "[1044]\ttraining's rmse: 0.928396\tvalid_1's rmse: 1.08543\n",
      "[1045]\ttraining's rmse: 0.92834\tvalid_1's rmse: 1.08542\n",
      "[1046]\ttraining's rmse: 0.928272\tvalid_1's rmse: 1.0854\n",
      "[1047]\ttraining's rmse: 0.928196\tvalid_1's rmse: 1.08544\n",
      "[1048]\ttraining's rmse: 0.928134\tvalid_1's rmse: 1.08547\n",
      "[1049]\ttraining's rmse: 0.928107\tvalid_1's rmse: 1.08546\n",
      "[1050]\ttraining's rmse: 0.928039\tvalid_1's rmse: 1.08545\n",
      "[1051]\ttraining's rmse: 0.927973\tvalid_1's rmse: 1.08543\n",
      "[1052]\ttraining's rmse: 0.927929\tvalid_1's rmse: 1.08543\n",
      "[1053]\ttraining's rmse: 0.927891\tvalid_1's rmse: 1.08541\n",
      "[1054]\ttraining's rmse: 0.92782\tvalid_1's rmse: 1.08539\n",
      "[1055]\ttraining's rmse: 0.927775\tvalid_1's rmse: 1.0854\n",
      "[1056]\ttraining's rmse: 0.927729\tvalid_1's rmse: 1.0854\n",
      "[1057]\ttraining's rmse: 0.927679\tvalid_1's rmse: 1.08541\n",
      "[1058]\ttraining's rmse: 0.927594\tvalid_1's rmse: 1.08541\n",
      "[1059]\ttraining's rmse: 0.927541\tvalid_1's rmse: 1.08541\n",
      "[1060]\ttraining's rmse: 0.927471\tvalid_1's rmse: 1.08539\n",
      "[1061]\ttraining's rmse: 0.927412\tvalid_1's rmse: 1.08536\n",
      "[1062]\ttraining's rmse: 0.927343\tvalid_1's rmse: 1.08534\n",
      "[1063]\ttraining's rmse: 0.927251\tvalid_1's rmse: 1.08537\n",
      "[1064]\ttraining's rmse: 0.927188\tvalid_1's rmse: 1.08538\n",
      "[1065]\ttraining's rmse: 0.927108\tvalid_1's rmse: 1.08537\n",
      "[1066]\ttraining's rmse: 0.927081\tvalid_1's rmse: 1.08536\n",
      "[1067]\ttraining's rmse: 0.927016\tvalid_1's rmse: 1.08533\n",
      "[1068]\ttraining's rmse: 0.926979\tvalid_1's rmse: 1.08531\n",
      "[1069]\ttraining's rmse: 0.926894\tvalid_1's rmse: 1.08527\n",
      "[1070]\ttraining's rmse: 0.926873\tvalid_1's rmse: 1.08526\n",
      "[1071]\ttraining's rmse: 0.926798\tvalid_1's rmse: 1.08525\n",
      "[1072]\ttraining's rmse: 0.926754\tvalid_1's rmse: 1.08527\n",
      "[1073]\ttraining's rmse: 0.926676\tvalid_1's rmse: 1.08528\n",
      "[1074]\ttraining's rmse: 0.926592\tvalid_1's rmse: 1.08527\n",
      "[1075]\ttraining's rmse: 0.926524\tvalid_1's rmse: 1.08522\n",
      "[1076]\ttraining's rmse: 0.926465\tvalid_1's rmse: 1.0852\n",
      "[1077]\ttraining's rmse: 0.926423\tvalid_1's rmse: 1.08519\n",
      "[1078]\ttraining's rmse: 0.926341\tvalid_1's rmse: 1.08514\n",
      "[1079]\ttraining's rmse: 0.926268\tvalid_1's rmse: 1.08515\n",
      "[1080]\ttraining's rmse: 0.926206\tvalid_1's rmse: 1.08518\n",
      "[1081]\ttraining's rmse: 0.92614\tvalid_1's rmse: 1.08517\n",
      "[1082]\ttraining's rmse: 0.926103\tvalid_1's rmse: 1.08515\n",
      "[1083]\ttraining's rmse: 0.92602\tvalid_1's rmse: 1.08517\n",
      "[1084]\ttraining's rmse: 0.925962\tvalid_1's rmse: 1.08521\n",
      "[1085]\ttraining's rmse: 0.925909\tvalid_1's rmse: 1.08522\n",
      "[1086]\ttraining's rmse: 0.925861\tvalid_1's rmse: 1.08521\n",
      "[1087]\ttraining's rmse: 0.925787\tvalid_1's rmse: 1.08524\n",
      "[1088]\ttraining's rmse: 0.925737\tvalid_1's rmse: 1.08526\n",
      "Early stopping, best iteration is:\n",
      "[1078]\ttraining's rmse: 0.926341\tvalid_1's rmse: 1.08514\n",
      "fold_3 coefficients:  [0.55097641 1.60169904 2.19681125]\n",
      "[1]\ttraining's rmse: 1.2539\tvalid_1's rmse: 1.25582\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.25104\tvalid_1's rmse: 1.25345\n",
      "[3]\ttraining's rmse: 1.24821\tvalid_1's rmse: 1.25114\n",
      "[4]\ttraining's rmse: 1.24546\tvalid_1's rmse: 1.24886\n",
      "[5]\ttraining's rmse: 1.24273\tvalid_1's rmse: 1.24664\n",
      "[6]\ttraining's rmse: 1.24007\tvalid_1's rmse: 1.24442\n",
      "[7]\ttraining's rmse: 1.23743\tvalid_1's rmse: 1.24227\n",
      "[8]\ttraining's rmse: 1.23485\tvalid_1's rmse: 1.24013\n",
      "[9]\ttraining's rmse: 1.23232\tvalid_1's rmse: 1.23803\n",
      "[10]\ttraining's rmse: 1.22983\tvalid_1's rmse: 1.23596\n",
      "[11]\ttraining's rmse: 1.22738\tvalid_1's rmse: 1.23394\n",
      "[12]\ttraining's rmse: 1.22499\tvalid_1's rmse: 1.23194\n",
      "[13]\ttraining's rmse: 1.22254\tvalid_1's rmse: 1.22991\n",
      "[14]\ttraining's rmse: 1.22013\tvalid_1's rmse: 1.22791\n",
      "[15]\ttraining's rmse: 1.21775\tvalid_1's rmse: 1.22597\n",
      "[16]\ttraining's rmse: 1.21539\tvalid_1's rmse: 1.22408\n",
      "[17]\ttraining's rmse: 1.21309\tvalid_1's rmse: 1.22219\n",
      "[18]\ttraining's rmse: 1.21085\tvalid_1's rmse: 1.22033\n",
      "[19]\ttraining's rmse: 1.20864\tvalid_1's rmse: 1.21852\n",
      "[20]\ttraining's rmse: 1.20647\tvalid_1's rmse: 1.21673\n",
      "[21]\ttraining's rmse: 1.20436\tvalid_1's rmse: 1.215\n",
      "[22]\ttraining's rmse: 1.20226\tvalid_1's rmse: 1.21332\n",
      "[23]\ttraining's rmse: 1.20019\tvalid_1's rmse: 1.21164\n",
      "[24]\ttraining's rmse: 1.19816\tvalid_1's rmse: 1.21003\n",
      "[25]\ttraining's rmse: 1.19617\tvalid_1's rmse: 1.20842\n",
      "[26]\ttraining's rmse: 1.19421\tvalid_1's rmse: 1.20689\n",
      "[27]\ttraining's rmse: 1.19228\tvalid_1's rmse: 1.20533\n",
      "[28]\ttraining's rmse: 1.18967\tvalid_1's rmse: 1.20324\n",
      "[29]\ttraining's rmse: 1.1878\tvalid_1's rmse: 1.20178\n",
      "[30]\ttraining's rmse: 1.18526\tvalid_1's rmse: 1.19977\n",
      "[31]\ttraining's rmse: 1.18277\tvalid_1's rmse: 1.19779\n",
      "[32]\ttraining's rmse: 1.18032\tvalid_1's rmse: 1.19585\n",
      "[33]\ttraining's rmse: 1.17791\tvalid_1's rmse: 1.19394\n",
      "[34]\ttraining's rmse: 1.17555\tvalid_1's rmse: 1.19207\n",
      "[35]\ttraining's rmse: 1.17363\tvalid_1's rmse: 1.19045\n",
      "[36]\ttraining's rmse: 1.17136\tvalid_1's rmse: 1.18862\n",
      "[37]\ttraining's rmse: 1.16911\tvalid_1's rmse: 1.18685\n",
      "[38]\ttraining's rmse: 1.16749\tvalid_1's rmse: 1.18565\n",
      "[39]\ttraining's rmse: 1.16569\tvalid_1's rmse: 1.18414\n",
      "[40]\ttraining's rmse: 1.16354\tvalid_1's rmse: 1.18245\n",
      "[41]\ttraining's rmse: 1.16144\tvalid_1's rmse: 1.18077\n",
      "[42]\ttraining's rmse: 1.15936\tvalid_1's rmse: 1.17913\n",
      "[43]\ttraining's rmse: 1.15733\tvalid_1's rmse: 1.17753\n",
      "[44]\ttraining's rmse: 1.15568\tvalid_1's rmse: 1.17614\n",
      "[45]\ttraining's rmse: 1.1537\tvalid_1's rmse: 1.17459\n",
      "[46]\ttraining's rmse: 1.1519\tvalid_1's rmse: 1.17325\n",
      "[47]\ttraining's rmse: 1.14998\tvalid_1's rmse: 1.17174\n",
      "[48]\ttraining's rmse: 1.14822\tvalid_1's rmse: 1.1703\n",
      "[49]\ttraining's rmse: 1.14697\tvalid_1's rmse: 1.16947\n",
      "[50]\ttraining's rmse: 1.14546\tvalid_1's rmse: 1.16813\n",
      "[51]\ttraining's rmse: 1.14425\tvalid_1's rmse: 1.16733\n",
      "[52]\ttraining's rmse: 1.14257\tvalid_1's rmse: 1.16608\n",
      "[53]\ttraining's rmse: 1.1411\tvalid_1's rmse: 1.165\n",
      "[54]\ttraining's rmse: 1.13945\tvalid_1's rmse: 1.16368\n",
      "[55]\ttraining's rmse: 1.13802\tvalid_1's rmse: 1.16263\n",
      "[56]\ttraining's rmse: 1.1368\tvalid_1's rmse: 1.16165\n",
      "[57]\ttraining's rmse: 1.13568\tvalid_1's rmse: 1.16092\n",
      "[58]\ttraining's rmse: 1.13411\tvalid_1's rmse: 1.15966\n",
      "[59]\ttraining's rmse: 1.13274\tvalid_1's rmse: 1.15865\n",
      "[60]\ttraining's rmse: 1.13157\tvalid_1's rmse: 1.15771\n",
      "[61]\ttraining's rmse: 1.13005\tvalid_1's rmse: 1.15651\n",
      "[62]\ttraining's rmse: 1.12874\tvalid_1's rmse: 1.15542\n",
      "[63]\ttraining's rmse: 1.12727\tvalid_1's rmse: 1.15435\n",
      "[64]\ttraining's rmse: 1.12615\tvalid_1's rmse: 1.15356\n",
      "[65]\ttraining's rmse: 1.12471\tvalid_1's rmse: 1.15241\n",
      "[66]\ttraining's rmse: 1.12346\tvalid_1's rmse: 1.15141\n",
      "[67]\ttraining's rmse: 1.12238\tvalid_1's rmse: 1.15065\n",
      "[68]\ttraining's rmse: 1.12103\tvalid_1's rmse: 1.14956\n",
      "[69]\ttraining's rmse: 1.11983\tvalid_1's rmse: 1.14856\n",
      "[70]\ttraining's rmse: 1.11887\tvalid_1's rmse: 1.14785\n",
      "[71]\ttraining's rmse: 1.11746\tvalid_1's rmse: 1.14687\n",
      "[72]\ttraining's rmse: 1.11645\tvalid_1's rmse: 1.14617\n",
      "[73]\ttraining's rmse: 1.1153\tvalid_1's rmse: 1.14522\n",
      "[74]\ttraining's rmse: 1.11404\tvalid_1's rmse: 1.14413\n",
      "[75]\ttraining's rmse: 1.11314\tvalid_1's rmse: 1.14348\n",
      "[76]\ttraining's rmse: 1.11241\tvalid_1's rmse: 1.14302\n",
      "[77]\ttraining's rmse: 1.11153\tvalid_1's rmse: 1.14238\n",
      "[78]\ttraining's rmse: 1.11044\tvalid_1's rmse: 1.14152\n",
      "[79]\ttraining's rmse: 1.10951\tvalid_1's rmse: 1.1408\n",
      "[80]\ttraining's rmse: 1.10832\tvalid_1's rmse: 1.13976\n",
      "[81]\ttraining's rmse: 1.10704\tvalid_1's rmse: 1.13889\n",
      "[82]\ttraining's rmse: 1.10615\tvalid_1's rmse: 1.13829\n",
      "[83]\ttraining's rmse: 1.10512\tvalid_1's rmse: 1.13746\n",
      "[84]\ttraining's rmse: 1.10389\tvalid_1's rmse: 1.13663\n",
      "[85]\ttraining's rmse: 1.10303\tvalid_1's rmse: 1.13592\n",
      "[86]\ttraining's rmse: 1.10181\tvalid_1's rmse: 1.13496\n",
      "[87]\ttraining's rmse: 1.10098\tvalid_1's rmse: 1.13439\n",
      "[88]\ttraining's rmse: 1.10001\tvalid_1's rmse: 1.1337\n",
      "[89]\ttraining's rmse: 1.09883\tvalid_1's rmse: 1.13277\n",
      "[90]\ttraining's rmse: 1.09803\tvalid_1's rmse: 1.13219\n",
      "[91]\ttraining's rmse: 1.09688\tvalid_1's rmse: 1.13142\n",
      "[92]\ttraining's rmse: 1.09575\tvalid_1's rmse: 1.13054\n",
      "[93]\ttraining's rmse: 1.09482\tvalid_1's rmse: 1.12984\n",
      "[94]\ttraining's rmse: 1.09406\tvalid_1's rmse: 1.12915\n",
      "[95]\ttraining's rmse: 1.09297\tvalid_1's rmse: 1.12841\n",
      "[96]\ttraining's rmse: 1.09222\tvalid_1's rmse: 1.12788\n",
      "[97]\ttraining's rmse: 1.09115\tvalid_1's rmse: 1.12697\n",
      "[98]\ttraining's rmse: 1.09027\tvalid_1's rmse: 1.12633\n",
      "[99]\ttraining's rmse: 1.08955\tvalid_1's rmse: 1.12569\n",
      "[100]\ttraining's rmse: 1.08852\tvalid_1's rmse: 1.12481\n",
      "[101]\ttraining's rmse: 1.08749\tvalid_1's rmse: 1.12413\n",
      "[102]\ttraining's rmse: 1.08678\tvalid_1's rmse: 1.12365\n",
      "[103]\ttraining's rmse: 1.08592\tvalid_1's rmse: 1.123\n",
      "[104]\ttraining's rmse: 1.08493\tvalid_1's rmse: 1.12218\n",
      "[105]\ttraining's rmse: 1.08421\tvalid_1's rmse: 1.12168\n",
      "[106]\ttraining's rmse: 1.08323\tvalid_1's rmse: 1.12104\n",
      "[107]\ttraining's rmse: 1.0822\tvalid_1's rmse: 1.12021\n",
      "[108]\ttraining's rmse: 1.08125\tvalid_1's rmse: 1.11941\n",
      "[109]\ttraining's rmse: 1.08061\tvalid_1's rmse: 1.11899\n",
      "[110]\ttraining's rmse: 1.07967\tvalid_1's rmse: 1.11836\n",
      "[111]\ttraining's rmse: 1.07869\tvalid_1's rmse: 1.11762\n",
      "[112]\ttraining's rmse: 1.078\tvalid_1's rmse: 1.117\n",
      "[113]\ttraining's rmse: 1.0771\tvalid_1's rmse: 1.11641\n",
      "[114]\ttraining's rmse: 1.07649\tvalid_1's rmse: 1.11601\n",
      "[115]\ttraining's rmse: 1.0756\tvalid_1's rmse: 1.11532\n",
      "[116]\ttraining's rmse: 1.07467\tvalid_1's rmse: 1.11462\n",
      "[117]\ttraining's rmse: 1.0738\tvalid_1's rmse: 1.11403\n",
      "[118]\ttraining's rmse: 1.07308\tvalid_1's rmse: 1.11364\n",
      "[119]\ttraining's rmse: 1.07242\tvalid_1's rmse: 1.11313\n",
      "[120]\ttraining's rmse: 1.07159\tvalid_1's rmse: 1.11249\n",
      "[121]\ttraining's rmse: 1.07107\tvalid_1's rmse: 1.11202\n",
      "[122]\ttraining's rmse: 1.07025\tvalid_1's rmse: 1.11139\n",
      "[123]\ttraining's rmse: 1.06937\tvalid_1's rmse: 1.11074\n",
      "[124]\ttraining's rmse: 1.06875\tvalid_1's rmse: 1.11026\n",
      "[125]\ttraining's rmse: 1.06795\tvalid_1's rmse: 1.10965\n",
      "[126]\ttraining's rmse: 1.06728\tvalid_1's rmse: 1.10931\n",
      "[127]\ttraining's rmse: 1.06668\tvalid_1's rmse: 1.10885\n",
      "[128]\ttraining's rmse: 1.06591\tvalid_1's rmse: 1.10825\n",
      "[129]\ttraining's rmse: 1.06514\tvalid_1's rmse: 1.10761\n",
      "[130]\ttraining's rmse: 1.06457\tvalid_1's rmse: 1.10717\n",
      "[131]\ttraining's rmse: 1.06376\tvalid_1's rmse: 1.10661\n",
      "[132]\ttraining's rmse: 1.06313\tvalid_1's rmse: 1.10629\n",
      "[133]\ttraining's rmse: 1.0624\tvalid_1's rmse: 1.10571\n",
      "[134]\ttraining's rmse: 1.06166\tvalid_1's rmse: 1.10514\n",
      "[135]\ttraining's rmse: 1.06112\tvalid_1's rmse: 1.10475\n",
      "[136]\ttraining's rmse: 1.06042\tvalid_1's rmse: 1.10416\n",
      "[137]\ttraining's rmse: 1.05966\tvalid_1's rmse: 1.10364\n",
      "[138]\ttraining's rmse: 1.05914\tvalid_1's rmse: 1.10327\n",
      "[139]\ttraining's rmse: 1.05843\tvalid_1's rmse: 1.10274\n",
      "[140]\ttraining's rmse: 1.05785\tvalid_1's rmse: 1.10245\n",
      "[141]\ttraining's rmse: 1.05735\tvalid_1's rmse: 1.10209\n",
      "[142]\ttraining's rmse: 1.05668\tvalid_1's rmse: 1.10153\n",
      "[143]\ttraining's rmse: 1.056\tvalid_1's rmse: 1.101\n",
      "[144]\ttraining's rmse: 1.05554\tvalid_1's rmse: 1.10064\n",
      "[145]\ttraining's rmse: 1.05483\tvalid_1's rmse: 1.10019\n",
      "[146]\ttraining's rmse: 1.05442\tvalid_1's rmse: 1.09986\n",
      "[147]\ttraining's rmse: 1.05379\tvalid_1's rmse: 1.09943\n",
      "[148]\ttraining's rmse: 1.0531\tvalid_1's rmse: 1.09899\n",
      "[149]\ttraining's rmse: 1.0527\tvalid_1's rmse: 1.09871\n",
      "[150]\ttraining's rmse: 1.05206\tvalid_1's rmse: 1.09821\n",
      "[151]\ttraining's rmse: 1.05158\tvalid_1's rmse: 1.0978\n",
      "[152]\ttraining's rmse: 1.05092\tvalid_1's rmse: 1.0974\n",
      "[153]\ttraining's rmse: 1.05048\tvalid_1's rmse: 1.09707\n",
      "[154]\ttraining's rmse: 1.04987\tvalid_1's rmse: 1.09646\n",
      "[155]\ttraining's rmse: 1.0495\tvalid_1's rmse: 1.09628\n",
      "[156]\ttraining's rmse: 1.04885\tvalid_1's rmse: 1.0959\n",
      "[157]\ttraining's rmse: 1.04849\tvalid_1's rmse: 1.09566\n",
      "[158]\ttraining's rmse: 1.04804\tvalid_1's rmse: 1.09525\n",
      "[159]\ttraining's rmse: 1.04741\tvalid_1's rmse: 1.09484\n",
      "[160]\ttraining's rmse: 1.04681\tvalid_1's rmse: 1.09435\n",
      "[161]\ttraining's rmse: 1.04621\tvalid_1's rmse: 1.09381\n",
      "[162]\ttraining's rmse: 1.04586\tvalid_1's rmse: 1.09363\n",
      "[163]\ttraining's rmse: 1.04542\tvalid_1's rmse: 1.09328\n",
      "[164]\ttraining's rmse: 1.04482\tvalid_1's rmse: 1.09289\n",
      "[165]\ttraining's rmse: 1.04424\tvalid_1's rmse: 1.09234\n",
      "[166]\ttraining's rmse: 1.04382\tvalid_1's rmse: 1.09199\n",
      "[167]\ttraining's rmse: 1.04342\tvalid_1's rmse: 1.09169\n",
      "[168]\ttraining's rmse: 1.04284\tvalid_1's rmse: 1.09131\n",
      "[169]\ttraining's rmse: 1.04227\tvalid_1's rmse: 1.09088\n",
      "[170]\ttraining's rmse: 1.0418\tvalid_1's rmse: 1.09059\n",
      "[171]\ttraining's rmse: 1.04143\tvalid_1's rmse: 1.09034\n",
      "[172]\ttraining's rmse: 1.04088\tvalid_1's rmse: 1.08981\n",
      "[173]\ttraining's rmse: 1.04032\tvalid_1's rmse: 1.08945\n",
      "[174]\ttraining's rmse: 1.03979\tvalid_1's rmse: 1.08904\n",
      "[175]\ttraining's rmse: 1.03943\tvalid_1's rmse: 1.08886\n",
      "[176]\ttraining's rmse: 1.03888\tvalid_1's rmse: 1.08845\n",
      "[177]\ttraining's rmse: 1.03849\tvalid_1's rmse: 1.08814\n",
      "[178]\ttraining's rmse: 1.03796\tvalid_1's rmse: 1.08781\n",
      "[179]\ttraining's rmse: 1.03761\tvalid_1's rmse: 1.08757\n",
      "[180]\ttraining's rmse: 1.0371\tvalid_1's rmse: 1.08717\n",
      "[181]\ttraining's rmse: 1.03659\tvalid_1's rmse: 1.08677\n",
      "[182]\ttraining's rmse: 1.03622\tvalid_1's rmse: 1.08643\n",
      "[183]\ttraining's rmse: 1.0357\tvalid_1's rmse: 1.08612\n",
      "[184]\ttraining's rmse: 1.03536\tvalid_1's rmse: 1.08595\n",
      "[185]\ttraining's rmse: 1.03506\tvalid_1's rmse: 1.08582\n",
      "[186]\ttraining's rmse: 1.03455\tvalid_1's rmse: 1.08551\n",
      "[187]\ttraining's rmse: 1.03407\tvalid_1's rmse: 1.08514\n",
      "[188]\ttraining's rmse: 1.03358\tvalid_1's rmse: 1.08473\n",
      "[189]\ttraining's rmse: 1.03329\tvalid_1's rmse: 1.08451\n",
      "[190]\ttraining's rmse: 1.03294\tvalid_1's rmse: 1.08422\n",
      "[191]\ttraining's rmse: 1.03245\tvalid_1's rmse: 1.08394\n",
      "[192]\ttraining's rmse: 1.03198\tvalid_1's rmse: 1.08365\n",
      "[193]\ttraining's rmse: 1.03164\tvalid_1's rmse: 1.08353\n",
      "[194]\ttraining's rmse: 1.03129\tvalid_1's rmse: 1.08326\n",
      "[195]\ttraining's rmse: 1.03092\tvalid_1's rmse: 1.08301\n",
      "[196]\ttraining's rmse: 1.03061\tvalid_1's rmse: 1.08278\n",
      "[197]\ttraining's rmse: 1.03025\tvalid_1's rmse: 1.0825\n",
      "[198]\ttraining's rmse: 1.02991\tvalid_1's rmse: 1.08224\n",
      "[199]\ttraining's rmse: 1.02959\tvalid_1's rmse: 1.08198\n",
      "[200]\ttraining's rmse: 1.02924\tvalid_1's rmse: 1.0817\n",
      "[201]\ttraining's rmse: 1.02895\tvalid_1's rmse: 1.08148\n",
      "[202]\ttraining's rmse: 1.02865\tvalid_1's rmse: 1.08134\n",
      "[203]\ttraining's rmse: 1.02832\tvalid_1's rmse: 1.08104\n",
      "[204]\ttraining's rmse: 1.02787\tvalid_1's rmse: 1.08074\n",
      "[205]\ttraining's rmse: 1.0276\tvalid_1's rmse: 1.08063\n",
      "[206]\ttraining's rmse: 1.02727\tvalid_1's rmse: 1.08037\n",
      "[207]\ttraining's rmse: 1.0269\tvalid_1's rmse: 1.08015\n",
      "[208]\ttraining's rmse: 1.02663\tvalid_1's rmse: 1.07994\n",
      "[209]\ttraining's rmse: 1.0262\tvalid_1's rmse: 1.07964\n",
      "[210]\ttraining's rmse: 1.02592\tvalid_1's rmse: 1.0795\n",
      "[211]\ttraining's rmse: 1.0256\tvalid_1's rmse: 1.07924\n",
      "[212]\ttraining's rmse: 1.0253\tvalid_1's rmse: 1.079\n",
      "[213]\ttraining's rmse: 1.02487\tvalid_1's rmse: 1.07873\n",
      "[214]\ttraining's rmse: 1.02456\tvalid_1's rmse: 1.07851\n",
      "[215]\ttraining's rmse: 1.02431\tvalid_1's rmse: 1.07832\n",
      "[216]\ttraining's rmse: 1.02398\tvalid_1's rmse: 1.07808\n",
      "[217]\ttraining's rmse: 1.02368\tvalid_1's rmse: 1.07782\n",
      "[218]\ttraining's rmse: 1.02329\tvalid_1's rmse: 1.07755\n",
      "[219]\ttraining's rmse: 1.02299\tvalid_1's rmse: 1.07729\n",
      "[220]\ttraining's rmse: 1.02258\tvalid_1's rmse: 1.07703\n",
      "[221]\ttraining's rmse: 1.02227\tvalid_1's rmse: 1.07679\n",
      "[222]\ttraining's rmse: 1.02198\tvalid_1's rmse: 1.07659\n",
      "[223]\ttraining's rmse: 1.02168\tvalid_1's rmse: 1.07638\n",
      "[224]\ttraining's rmse: 1.02143\tvalid_1's rmse: 1.07621\n",
      "[225]\ttraining's rmse: 1.02106\tvalid_1's rmse: 1.07596\n",
      "[226]\ttraining's rmse: 1.02072\tvalid_1's rmse: 1.07561\n",
      "[227]\ttraining's rmse: 1.02041\tvalid_1's rmse: 1.07536\n",
      "[228]\ttraining's rmse: 1.02015\tvalid_1's rmse: 1.07517\n",
      "[229]\ttraining's rmse: 1.01989\tvalid_1's rmse: 1.07504\n",
      "[230]\ttraining's rmse: 1.0196\tvalid_1's rmse: 1.07484\n",
      "[231]\ttraining's rmse: 1.01922\tvalid_1's rmse: 1.0746\n",
      "[232]\ttraining's rmse: 1.01885\tvalid_1's rmse: 1.07434\n",
      "[233]\ttraining's rmse: 1.0186\tvalid_1's rmse: 1.07422\n",
      "[234]\ttraining's rmse: 1.01825\tvalid_1's rmse: 1.074\n",
      "[235]\ttraining's rmse: 1.01792\tvalid_1's rmse: 1.07364\n",
      "[236]\ttraining's rmse: 1.01767\tvalid_1's rmse: 1.07352\n",
      "[237]\ttraining's rmse: 1.01739\tvalid_1's rmse: 1.0733\n",
      "[238]\ttraining's rmse: 1.01706\tvalid_1's rmse: 1.07307\n",
      "[239]\ttraining's rmse: 1.01682\tvalid_1's rmse: 1.07296\n",
      "[240]\ttraining's rmse: 1.01649\tvalid_1's rmse: 1.07261\n",
      "[241]\ttraining's rmse: 1.01613\tvalid_1's rmse: 1.07241\n",
      "[242]\ttraining's rmse: 1.01585\tvalid_1's rmse: 1.07224\n",
      "[243]\ttraining's rmse: 1.0156\tvalid_1's rmse: 1.07212\n",
      "[244]\ttraining's rmse: 1.01528\tvalid_1's rmse: 1.0719\n",
      "[245]\ttraining's rmse: 1.01501\tvalid_1's rmse: 1.07172\n",
      "[246]\ttraining's rmse: 1.01476\tvalid_1's rmse: 1.07151\n",
      "[247]\ttraining's rmse: 1.01444\tvalid_1's rmse: 1.07132\n",
      "[248]\ttraining's rmse: 1.01421\tvalid_1's rmse: 1.07122\n",
      "[249]\ttraining's rmse: 1.01399\tvalid_1's rmse: 1.07107\n",
      "[250]\ttraining's rmse: 1.01365\tvalid_1's rmse: 1.07088\n",
      "[251]\ttraining's rmse: 1.01342\tvalid_1's rmse: 1.07074\n",
      "[252]\ttraining's rmse: 1.01318\tvalid_1's rmse: 1.07057\n",
      "[253]\ttraining's rmse: 1.01296\tvalid_1's rmse: 1.07046\n",
      "[254]\ttraining's rmse: 1.01266\tvalid_1's rmse: 1.07026\n",
      "[255]\ttraining's rmse: 1.01236\tvalid_1's rmse: 1.06989\n",
      "[256]\ttraining's rmse: 1.0121\tvalid_1's rmse: 1.06982\n",
      "[257]\ttraining's rmse: 1.01187\tvalid_1's rmse: 1.06959\n",
      "[258]\ttraining's rmse: 1.01158\tvalid_1's rmse: 1.06941\n",
      "[259]\ttraining's rmse: 1.01133\tvalid_1's rmse: 1.06934\n",
      "[260]\ttraining's rmse: 1.0111\tvalid_1's rmse: 1.06923\n",
      "[261]\ttraining's rmse: 1.01086\tvalid_1's rmse: 1.06908\n",
      "[262]\ttraining's rmse: 1.01053\tvalid_1's rmse: 1.06889\n",
      "[263]\ttraining's rmse: 1.01025\tvalid_1's rmse: 1.06856\n",
      "[264]\ttraining's rmse: 1.01004\tvalid_1's rmse: 1.06847\n",
      "[265]\ttraining's rmse: 1.00976\tvalid_1's rmse: 1.06822\n",
      "[266]\ttraining's rmse: 1.00955\tvalid_1's rmse: 1.06817\n",
      "[267]\ttraining's rmse: 1.00934\tvalid_1's rmse: 1.06797\n",
      "[268]\ttraining's rmse: 1.00911\tvalid_1's rmse: 1.06783\n",
      "[269]\ttraining's rmse: 1.0089\tvalid_1's rmse: 1.0677\n",
      "[270]\ttraining's rmse: 1.00866\tvalid_1's rmse: 1.06753\n",
      "[271]\ttraining's rmse: 1.00845\tvalid_1's rmse: 1.06749\n",
      "[272]\ttraining's rmse: 1.00824\tvalid_1's rmse: 1.06734\n",
      "[273]\ttraining's rmse: 1.00801\tvalid_1's rmse: 1.06719\n",
      "[274]\ttraining's rmse: 1.00781\tvalid_1's rmse: 1.06707\n",
      "[275]\ttraining's rmse: 1.00754\tvalid_1's rmse: 1.06684\n",
      "[276]\ttraining's rmse: 1.00733\tvalid_1's rmse: 1.0667\n",
      "[277]\ttraining's rmse: 1.00709\tvalid_1's rmse: 1.06658\n",
      "[278]\ttraining's rmse: 1.00679\tvalid_1's rmse: 1.06641\n",
      "[279]\ttraining's rmse: 1.00648\tvalid_1's rmse: 1.06612\n",
      "[280]\ttraining's rmse: 1.00626\tvalid_1's rmse: 1.06606\n",
      "[281]\ttraining's rmse: 1.006\tvalid_1's rmse: 1.06588\n",
      "[282]\ttraining's rmse: 1.00575\tvalid_1's rmse: 1.06567\n",
      "[283]\ttraining's rmse: 1.00549\tvalid_1's rmse: 1.06545\n",
      "[284]\ttraining's rmse: 1.00525\tvalid_1's rmse: 1.06532\n",
      "[285]\ttraining's rmse: 1.00505\tvalid_1's rmse: 1.06518\n",
      "[286]\ttraining's rmse: 1.00485\tvalid_1's rmse: 1.06504\n",
      "[287]\ttraining's rmse: 1.00461\tvalid_1's rmse: 1.06488\n",
      "[288]\ttraining's rmse: 1.00436\tvalid_1's rmse: 1.06473\n",
      "[289]\ttraining's rmse: 1.00414\tvalid_1's rmse: 1.06456\n",
      "[290]\ttraining's rmse: 1.00396\tvalid_1's rmse: 1.06453\n",
      "[291]\ttraining's rmse: 1.00377\tvalid_1's rmse: 1.06442\n",
      "[292]\ttraining's rmse: 1.00354\tvalid_1's rmse: 1.0644\n",
      "[293]\ttraining's rmse: 1.00335\tvalid_1's rmse: 1.06424\n",
      "[294]\ttraining's rmse: 1.00311\tvalid_1's rmse: 1.06404\n",
      "[295]\ttraining's rmse: 1.00291\tvalid_1's rmse: 1.06388\n",
      "[296]\ttraining's rmse: 1.0027\tvalid_1's rmse: 1.06373\n",
      "[297]\ttraining's rmse: 1.00252\tvalid_1's rmse: 1.06367\n",
      "[298]\ttraining's rmse: 1.00228\tvalid_1's rmse: 1.06354\n",
      "[299]\ttraining's rmse: 1.00207\tvalid_1's rmse: 1.06348\n",
      "[300]\ttraining's rmse: 1.00188\tvalid_1's rmse: 1.06337\n",
      "[301]\ttraining's rmse: 1.00166\tvalid_1's rmse: 1.06332\n",
      "[302]\ttraining's rmse: 1.00139\tvalid_1's rmse: 1.0632\n",
      "[303]\ttraining's rmse: 1.00111\tvalid_1's rmse: 1.06296\n",
      "[304]\ttraining's rmse: 1.00093\tvalid_1's rmse: 1.06289\n",
      "[305]\ttraining's rmse: 1.00074\tvalid_1's rmse: 1.06272\n",
      "[306]\ttraining's rmse: 1.00053\tvalid_1's rmse: 1.06255\n",
      "[307]\ttraining's rmse: 1.0003\tvalid_1's rmse: 1.06235\n",
      "[308]\ttraining's rmse: 1.00012\tvalid_1's rmse: 1.06225\n",
      "[309]\ttraining's rmse: 0.999923\tvalid_1's rmse: 1.06224\n",
      "[310]\ttraining's rmse: 0.999629\tvalid_1's rmse: 1.06197\n",
      "[311]\ttraining's rmse: 0.999413\tvalid_1's rmse: 1.06183\n",
      "[312]\ttraining's rmse: 0.999231\tvalid_1's rmse: 1.06175\n",
      "[313]\ttraining's rmse: 0.999034\tvalid_1's rmse: 1.06165\n",
      "[314]\ttraining's rmse: 0.998745\tvalid_1's rmse: 1.06135\n",
      "[315]\ttraining's rmse: 0.998575\tvalid_1's rmse: 1.06122\n",
      "[316]\ttraining's rmse: 0.99838\tvalid_1's rmse: 1.06121\n",
      "[317]\ttraining's rmse: 0.998101\tvalid_1's rmse: 1.06091\n",
      "[318]\ttraining's rmse: 0.9979\tvalid_1's rmse: 1.06082\n",
      "[319]\ttraining's rmse: 0.997737\tvalid_1's rmse: 1.06073\n",
      "[320]\ttraining's rmse: 0.997535\tvalid_1's rmse: 1.06058\n",
      "[321]\ttraining's rmse: 0.997373\tvalid_1's rmse: 1.0605\n",
      "[322]\ttraining's rmse: 0.997098\tvalid_1's rmse: 1.06021\n",
      "[323]\ttraining's rmse: 0.996914\tvalid_1's rmse: 1.0602\n",
      "[324]\ttraining's rmse: 0.99675\tvalid_1's rmse: 1.0601\n",
      "[325]\ttraining's rmse: 0.996557\tvalid_1's rmse: 1.06001\n",
      "[326]\ttraining's rmse: 0.996384\tvalid_1's rmse: 1.05991\n",
      "[327]\ttraining's rmse: 0.996203\tvalid_1's rmse: 1.05981\n",
      "[328]\ttraining's rmse: 0.996008\tvalid_1's rmse: 1.0597\n",
      "[329]\ttraining's rmse: 0.995816\tvalid_1's rmse: 1.05959\n",
      "[330]\ttraining's rmse: 0.995551\tvalid_1's rmse: 1.05933\n",
      "[331]\ttraining's rmse: 0.995361\tvalid_1's rmse: 1.05922\n",
      "[332]\ttraining's rmse: 0.995164\tvalid_1's rmse: 1.05904\n",
      "[333]\ttraining's rmse: 0.994969\tvalid_1's rmse: 1.05895\n",
      "[334]\ttraining's rmse: 0.994795\tvalid_1's rmse: 1.05887\n",
      "[335]\ttraining's rmse: 0.994609\tvalid_1's rmse: 1.05877\n",
      "[336]\ttraining's rmse: 0.99439\tvalid_1's rmse: 1.05868\n",
      "[337]\ttraining's rmse: 0.994239\tvalid_1's rmse: 1.05858\n",
      "[338]\ttraining's rmse: 0.994067\tvalid_1's rmse: 1.05846\n",
      "[339]\ttraining's rmse: 0.993878\tvalid_1's rmse: 1.05837\n",
      "[340]\ttraining's rmse: 0.993698\tvalid_1's rmse: 1.05827\n",
      "[341]\ttraining's rmse: 0.99351\tvalid_1's rmse: 1.0581\n",
      "[342]\ttraining's rmse: 0.993342\tvalid_1's rmse: 1.05804\n",
      "[343]\ttraining's rmse: 0.993153\tvalid_1's rmse: 1.05783\n",
      "[344]\ttraining's rmse: 0.992905\tvalid_1's rmse: 1.0576\n",
      "[345]\ttraining's rmse: 0.99273\tvalid_1's rmse: 1.05752\n",
      "[346]\ttraining's rmse: 0.992581\tvalid_1's rmse: 1.05744\n",
      "[347]\ttraining's rmse: 0.992338\tvalid_1's rmse: 1.05725\n",
      "[348]\ttraining's rmse: 0.992155\tvalid_1's rmse: 1.05703\n",
      "[349]\ttraining's rmse: 0.99198\tvalid_1's rmse: 1.05692\n",
      "[350]\ttraining's rmse: 0.991702\tvalid_1's rmse: 1.05678\n",
      "[351]\ttraining's rmse: 0.991527\tvalid_1's rmse: 1.05668\n",
      "[352]\ttraining's rmse: 0.991352\tvalid_1's rmse: 1.05648\n",
      "[353]\ttraining's rmse: 0.991174\tvalid_1's rmse: 1.05639\n",
      "[354]\ttraining's rmse: 0.991013\tvalid_1's rmse: 1.05632\n",
      "[355]\ttraining's rmse: 0.990765\tvalid_1's rmse: 1.05618\n",
      "[356]\ttraining's rmse: 0.990621\tvalid_1's rmse: 1.05611\n",
      "[357]\ttraining's rmse: 0.990369\tvalid_1's rmse: 1.0559\n",
      "[358]\ttraining's rmse: 0.990199\tvalid_1's rmse: 1.0557\n",
      "[359]\ttraining's rmse: 0.990017\tvalid_1's rmse: 1.05563\n",
      "[360]\ttraining's rmse: 0.989855\tvalid_1's rmse: 1.05558\n",
      "[361]\ttraining's rmse: 0.989689\tvalid_1's rmse: 1.05536\n",
      "[362]\ttraining's rmse: 0.989432\tvalid_1's rmse: 1.05523\n",
      "[363]\ttraining's rmse: 0.989261\tvalid_1's rmse: 1.05517\n",
      "[364]\ttraining's rmse: 0.989103\tvalid_1's rmse: 1.05509\n",
      "[365]\ttraining's rmse: 0.988866\tvalid_1's rmse: 1.05499\n",
      "[366]\ttraining's rmse: 0.988735\tvalid_1's rmse: 1.05494\n",
      "[367]\ttraining's rmse: 0.988487\tvalid_1's rmse: 1.05482\n",
      "[368]\ttraining's rmse: 0.988335\tvalid_1's rmse: 1.05473\n",
      "[369]\ttraining's rmse: 0.988099\tvalid_1's rmse: 1.05455\n",
      "[370]\ttraining's rmse: 0.987933\tvalid_1's rmse: 1.05436\n",
      "[371]\ttraining's rmse: 0.98776\tvalid_1's rmse: 1.0543\n",
      "[372]\ttraining's rmse: 0.987596\tvalid_1's rmse: 1.05411\n",
      "[373]\ttraining's rmse: 0.987443\tvalid_1's rmse: 1.05404\n",
      "[374]\ttraining's rmse: 0.987274\tvalid_1's rmse: 1.05399\n",
      "[375]\ttraining's rmse: 0.987056\tvalid_1's rmse: 1.05392\n",
      "[376]\ttraining's rmse: 0.98681\tvalid_1's rmse: 1.0538\n",
      "[377]\ttraining's rmse: 0.986645\tvalid_1's rmse: 1.05375\n",
      "[378]\ttraining's rmse: 0.986486\tvalid_1's rmse: 1.05357\n",
      "[379]\ttraining's rmse: 0.986339\tvalid_1's rmse: 1.05347\n",
      "[380]\ttraining's rmse: 0.986183\tvalid_1's rmse: 1.05328\n",
      "[381]\ttraining's rmse: 0.986002\tvalid_1's rmse: 1.0532\n",
      "[382]\ttraining's rmse: 0.985855\tvalid_1's rmse: 1.05314\n",
      "[383]\ttraining's rmse: 0.985726\tvalid_1's rmse: 1.05311\n",
      "[384]\ttraining's rmse: 0.985504\tvalid_1's rmse: 1.05291\n",
      "[385]\ttraining's rmse: 0.985351\tvalid_1's rmse: 1.05272\n",
      "[386]\ttraining's rmse: 0.985201\tvalid_1's rmse: 1.05266\n",
      "[387]\ttraining's rmse: 0.984985\tvalid_1's rmse: 1.05257\n",
      "[388]\ttraining's rmse: 0.98483\tvalid_1's rmse: 1.05246\n",
      "[389]\ttraining's rmse: 0.984686\tvalid_1's rmse: 1.05233\n",
      "[390]\ttraining's rmse: 0.984554\tvalid_1's rmse: 1.05226\n",
      "[391]\ttraining's rmse: 0.984328\tvalid_1's rmse: 1.05217\n",
      "[392]\ttraining's rmse: 0.984189\tvalid_1's rmse: 1.05208\n",
      "[393]\ttraining's rmse: 0.984042\tvalid_1's rmse: 1.05192\n",
      "[394]\ttraining's rmse: 0.983887\tvalid_1's rmse: 1.05186\n",
      "[395]\ttraining's rmse: 0.983727\tvalid_1's rmse: 1.05178\n",
      "[396]\ttraining's rmse: 0.983577\tvalid_1's rmse: 1.05169\n",
      "[397]\ttraining's rmse: 0.983368\tvalid_1's rmse: 1.05153\n",
      "[398]\ttraining's rmse: 0.983217\tvalid_1's rmse: 1.05141\n",
      "[399]\ttraining's rmse: 0.983081\tvalid_1's rmse: 1.05131\n",
      "[400]\ttraining's rmse: 0.982931\tvalid_1's rmse: 1.05125\n",
      "[401]\ttraining's rmse: 0.982707\tvalid_1's rmse: 1.05113\n",
      "[402]\ttraining's rmse: 0.98251\tvalid_1's rmse: 1.05105\n",
      "[403]\ttraining's rmse: 0.982376\tvalid_1's rmse: 1.05093\n",
      "[404]\ttraining's rmse: 0.982227\tvalid_1's rmse: 1.05085\n",
      "[405]\ttraining's rmse: 0.982094\tvalid_1's rmse: 1.05079\n",
      "[406]\ttraining's rmse: 0.981881\tvalid_1's rmse: 1.05069\n",
      "[407]\ttraining's rmse: 0.981743\tvalid_1's rmse: 1.05063\n",
      "[408]\ttraining's rmse: 0.981591\tvalid_1's rmse: 1.05056\n",
      "[409]\ttraining's rmse: 0.981385\tvalid_1's rmse: 1.05044\n",
      "[410]\ttraining's rmse: 0.981247\tvalid_1's rmse: 1.05033\n",
      "[411]\ttraining's rmse: 0.98111\tvalid_1's rmse: 1.05031\n",
      "[412]\ttraining's rmse: 0.980978\tvalid_1's rmse: 1.05015\n",
      "[413]\ttraining's rmse: 0.980815\tvalid_1's rmse: 1.05008\n",
      "[414]\ttraining's rmse: 0.980668\tvalid_1's rmse: 1.05001\n",
      "[415]\ttraining's rmse: 0.980534\tvalid_1's rmse: 1.04991\n",
      "[416]\ttraining's rmse: 0.980346\tvalid_1's rmse: 1.04981\n",
      "[417]\ttraining's rmse: 0.980227\tvalid_1's rmse: 1.04973\n",
      "[418]\ttraining's rmse: 0.980096\tvalid_1's rmse: 1.04962\n",
      "[419]\ttraining's rmse: 0.979885\tvalid_1's rmse: 1.04951\n",
      "[420]\ttraining's rmse: 0.979753\tvalid_1's rmse: 1.04937\n",
      "[421]\ttraining's rmse: 0.979634\tvalid_1's rmse: 1.04931\n",
      "[422]\ttraining's rmse: 0.979508\tvalid_1's rmse: 1.04928\n",
      "[423]\ttraining's rmse: 0.97932\tvalid_1's rmse: 1.04916\n",
      "[424]\ttraining's rmse: 0.979188\tvalid_1's rmse: 1.04912\n",
      "[425]\ttraining's rmse: 0.979064\tvalid_1's rmse: 1.04901\n",
      "[426]\ttraining's rmse: 0.978884\tvalid_1's rmse: 1.04894\n",
      "[427]\ttraining's rmse: 0.978741\tvalid_1's rmse: 1.04889\n",
      "[428]\ttraining's rmse: 0.978603\tvalid_1's rmse: 1.04886\n",
      "[429]\ttraining's rmse: 0.978477\tvalid_1's rmse: 1.04886\n",
      "[430]\ttraining's rmse: 0.978357\tvalid_1's rmse: 1.04879\n",
      "[431]\ttraining's rmse: 0.978231\tvalid_1's rmse: 1.0487\n",
      "[432]\ttraining's rmse: 0.978051\tvalid_1's rmse: 1.04863\n",
      "[433]\ttraining's rmse: 0.977896\tvalid_1's rmse: 1.04859\n",
      "[434]\ttraining's rmse: 0.97771\tvalid_1's rmse: 1.0484\n",
      "[435]\ttraining's rmse: 0.977582\tvalid_1's rmse: 1.04836\n",
      "[436]\ttraining's rmse: 0.97746\tvalid_1's rmse: 1.04828\n",
      "[437]\ttraining's rmse: 0.977328\tvalid_1's rmse: 1.04812\n",
      "[438]\ttraining's rmse: 0.977212\tvalid_1's rmse: 1.04806\n",
      "[439]\ttraining's rmse: 0.97702\tvalid_1's rmse: 1.048\n",
      "[440]\ttraining's rmse: 0.976902\tvalid_1's rmse: 1.04789\n",
      "[441]\ttraining's rmse: 0.97675\tvalid_1's rmse: 1.04784\n",
      "[442]\ttraining's rmse: 0.976628\tvalid_1's rmse: 1.04775\n",
      "[443]\ttraining's rmse: 0.976513\tvalid_1's rmse: 1.04774\n",
      "[444]\ttraining's rmse: 0.976339\tvalid_1's rmse: 1.04766\n",
      "[445]\ttraining's rmse: 0.976218\tvalid_1's rmse: 1.04756\n",
      "[446]\ttraining's rmse: 0.976074\tvalid_1's rmse: 1.04752\n",
      "[447]\ttraining's rmse: 0.97594\tvalid_1's rmse: 1.04744\n",
      "[448]\ttraining's rmse: 0.97577\tvalid_1's rmse: 1.04739\n",
      "[449]\ttraining's rmse: 0.975655\tvalid_1's rmse: 1.0473\n",
      "[450]\ttraining's rmse: 0.975538\tvalid_1's rmse: 1.04723\n",
      "[451]\ttraining's rmse: 0.975359\tvalid_1's rmse: 1.0471\n",
      "[452]\ttraining's rmse: 0.975237\tvalid_1's rmse: 1.04706\n",
      "[453]\ttraining's rmse: 0.975101\tvalid_1's rmse: 1.04702\n",
      "[454]\ttraining's rmse: 0.974976\tvalid_1's rmse: 1.04688\n",
      "[455]\ttraining's rmse: 0.97486\tvalid_1's rmse: 1.04679\n",
      "[456]\ttraining's rmse: 0.974695\tvalid_1's rmse: 1.04672\n",
      "[457]\ttraining's rmse: 0.974584\tvalid_1's rmse: 1.04665\n",
      "[458]\ttraining's rmse: 0.974477\tvalid_1's rmse: 1.04661\n",
      "[459]\ttraining's rmse: 0.974306\tvalid_1's rmse: 1.04654\n",
      "[460]\ttraining's rmse: 0.974197\tvalid_1's rmse: 1.04645\n",
      "[461]\ttraining's rmse: 0.974055\tvalid_1's rmse: 1.04642\n",
      "[462]\ttraining's rmse: 0.973898\tvalid_1's rmse: 1.04633\n",
      "[463]\ttraining's rmse: 0.97378\tvalid_1's rmse: 1.04629\n",
      "[464]\ttraining's rmse: 0.973664\tvalid_1's rmse: 1.0463\n",
      "[465]\ttraining's rmse: 0.973551\tvalid_1's rmse: 1.04619\n",
      "[466]\ttraining's rmse: 0.973435\tvalid_1's rmse: 1.0461\n",
      "[467]\ttraining's rmse: 0.973327\tvalid_1's rmse: 1.04604\n",
      "[468]\ttraining's rmse: 0.97322\tvalid_1's rmse: 1.04598\n",
      "[469]\ttraining's rmse: 0.973058\tvalid_1's rmse: 1.0459\n",
      "[470]\ttraining's rmse: 0.97293\tvalid_1's rmse: 1.04585\n",
      "[471]\ttraining's rmse: 0.972817\tvalid_1's rmse: 1.0458\n",
      "[472]\ttraining's rmse: 0.972712\tvalid_1's rmse: 1.04576\n",
      "[473]\ttraining's rmse: 0.972553\tvalid_1's rmse: 1.04569\n",
      "[474]\ttraining's rmse: 0.972406\tvalid_1's rmse: 1.0457\n",
      "[475]\ttraining's rmse: 0.972286\tvalid_1's rmse: 1.04558\n",
      "[476]\ttraining's rmse: 0.972179\tvalid_1's rmse: 1.04551\n",
      "[477]\ttraining's rmse: 0.97202\tvalid_1's rmse: 1.04544\n",
      "[478]\ttraining's rmse: 0.971911\tvalid_1's rmse: 1.04538\n",
      "[479]\ttraining's rmse: 0.971787\tvalid_1's rmse: 1.04532\n",
      "[480]\ttraining's rmse: 0.971646\tvalid_1's rmse: 1.04526\n",
      "[481]\ttraining's rmse: 0.971532\tvalid_1's rmse: 1.04522\n",
      "[482]\ttraining's rmse: 0.971422\tvalid_1's rmse: 1.04517\n",
      "[483]\ttraining's rmse: 0.971318\tvalid_1's rmse: 1.04512\n",
      "[484]\ttraining's rmse: 0.971166\tvalid_1's rmse: 1.04509\n",
      "[485]\ttraining's rmse: 0.971064\tvalid_1's rmse: 1.04502\n",
      "[486]\ttraining's rmse: 0.970925\tvalid_1's rmse: 1.04503\n",
      "[487]\ttraining's rmse: 0.970806\tvalid_1's rmse: 1.04498\n",
      "[488]\ttraining's rmse: 0.970661\tvalid_1's rmse: 1.04486\n",
      "[489]\ttraining's rmse: 0.970579\tvalid_1's rmse: 1.04481\n",
      "[490]\ttraining's rmse: 0.970472\tvalid_1's rmse: 1.04477\n",
      "[491]\ttraining's rmse: 0.970353\tvalid_1's rmse: 1.0447\n",
      "[492]\ttraining's rmse: 0.970209\tvalid_1's rmse: 1.04467\n",
      "[493]\ttraining's rmse: 0.970094\tvalid_1's rmse: 1.04454\n",
      "[494]\ttraining's rmse: 0.96994\tvalid_1's rmse: 1.04448\n",
      "[495]\ttraining's rmse: 0.969821\tvalid_1's rmse: 1.04445\n",
      "[496]\ttraining's rmse: 0.969686\tvalid_1's rmse: 1.04446\n",
      "[497]\ttraining's rmse: 0.969573\tvalid_1's rmse: 1.04451\n",
      "[498]\ttraining's rmse: 0.969493\tvalid_1's rmse: 1.04446\n",
      "[499]\ttraining's rmse: 0.969394\tvalid_1's rmse: 1.0444\n",
      "[500]\ttraining's rmse: 0.969277\tvalid_1's rmse: 1.04432\n",
      "[501]\ttraining's rmse: 0.969175\tvalid_1's rmse: 1.04425\n",
      "[502]\ttraining's rmse: 0.969036\tvalid_1's rmse: 1.04418\n",
      "[503]\ttraining's rmse: 0.968921\tvalid_1's rmse: 1.0441\n",
      "[504]\ttraining's rmse: 0.968796\tvalid_1's rmse: 1.04409\n",
      "[505]\ttraining's rmse: 0.968698\tvalid_1's rmse: 1.04409\n",
      "[506]\ttraining's rmse: 0.968585\tvalid_1's rmse: 1.04404\n",
      "[507]\ttraining's rmse: 0.968508\tvalid_1's rmse: 1.044\n",
      "[508]\ttraining's rmse: 0.968408\tvalid_1's rmse: 1.0439\n",
      "[509]\ttraining's rmse: 0.968265\tvalid_1's rmse: 1.04379\n",
      "[510]\ttraining's rmse: 0.968154\tvalid_1's rmse: 1.04373\n",
      "[511]\ttraining's rmse: 0.968022\tvalid_1's rmse: 1.04365\n",
      "[512]\ttraining's rmse: 0.967885\tvalid_1's rmse: 1.04356\n",
      "[513]\ttraining's rmse: 0.967789\tvalid_1's rmse: 1.04349\n",
      "[514]\ttraining's rmse: 0.967675\tvalid_1's rmse: 1.04344\n",
      "[515]\ttraining's rmse: 0.96752\tvalid_1's rmse: 1.04336\n",
      "[516]\ttraining's rmse: 0.967406\tvalid_1's rmse: 1.04331\n",
      "[517]\ttraining's rmse: 0.967296\tvalid_1's rmse: 1.04323\n",
      "[518]\ttraining's rmse: 0.967222\tvalid_1's rmse: 1.04319\n",
      "[519]\ttraining's rmse: 0.967087\tvalid_1's rmse: 1.04315\n",
      "[520]\ttraining's rmse: 0.966978\tvalid_1's rmse: 1.04308\n",
      "[521]\ttraining's rmse: 0.966878\tvalid_1's rmse: 1.04304\n",
      "[522]\ttraining's rmse: 0.966745\tvalid_1's rmse: 1.04298\n",
      "[523]\ttraining's rmse: 0.966652\tvalid_1's rmse: 1.04292\n",
      "[524]\ttraining's rmse: 0.966561\tvalid_1's rmse: 1.04285\n",
      "[525]\ttraining's rmse: 0.966422\tvalid_1's rmse: 1.0428\n",
      "[526]\ttraining's rmse: 0.966324\tvalid_1's rmse: 1.04275\n",
      "[527]\ttraining's rmse: 0.966199\tvalid_1's rmse: 1.04277\n",
      "[528]\ttraining's rmse: 0.96609\tvalid_1's rmse: 1.04277\n",
      "[529]\ttraining's rmse: 0.966018\tvalid_1's rmse: 1.04273\n",
      "[530]\ttraining's rmse: 0.965877\tvalid_1's rmse: 1.04265\n",
      "[531]\ttraining's rmse: 0.965784\tvalid_1's rmse: 1.0426\n",
      "[532]\ttraining's rmse: 0.965677\tvalid_1's rmse: 1.04252\n",
      "[533]\ttraining's rmse: 0.965561\tvalid_1's rmse: 1.04252\n",
      "[534]\ttraining's rmse: 0.965468\tvalid_1's rmse: 1.04247\n",
      "[535]\ttraining's rmse: 0.96535\tvalid_1's rmse: 1.04251\n",
      "[536]\ttraining's rmse: 0.965245\tvalid_1's rmse: 1.04246\n",
      "[537]\ttraining's rmse: 0.965118\tvalid_1's rmse: 1.04241\n",
      "[538]\ttraining's rmse: 0.965023\tvalid_1's rmse: 1.04235\n",
      "[539]\ttraining's rmse: 0.964911\tvalid_1's rmse: 1.04236\n",
      "[540]\ttraining's rmse: 0.964801\tvalid_1's rmse: 1.04225\n",
      "[541]\ttraining's rmse: 0.964697\tvalid_1's rmse: 1.04217\n",
      "[542]\ttraining's rmse: 0.964585\tvalid_1's rmse: 1.04212\n",
      "[543]\ttraining's rmse: 0.964475\tvalid_1's rmse: 1.04212\n",
      "[544]\ttraining's rmse: 0.96436\tvalid_1's rmse: 1.04213\n",
      "[545]\ttraining's rmse: 0.964259\tvalid_1's rmse: 1.04206\n",
      "[546]\ttraining's rmse: 0.964175\tvalid_1's rmse: 1.04206\n",
      "[547]\ttraining's rmse: 0.96405\tvalid_1's rmse: 1.042\n",
      "[548]\ttraining's rmse: 0.963964\tvalid_1's rmse: 1.04199\n",
      "[549]\ttraining's rmse: 0.963871\tvalid_1's rmse: 1.04195\n",
      "[550]\ttraining's rmse: 0.963758\tvalid_1's rmse: 1.04187\n",
      "[551]\ttraining's rmse: 0.963651\tvalid_1's rmse: 1.04188\n",
      "[552]\ttraining's rmse: 0.963543\tvalid_1's rmse: 1.04183\n",
      "[553]\ttraining's rmse: 0.96345\tvalid_1's rmse: 1.0418\n",
      "[554]\ttraining's rmse: 0.963326\tvalid_1's rmse: 1.04182\n",
      "[555]\ttraining's rmse: 0.963227\tvalid_1's rmse: 1.04181\n",
      "[556]\ttraining's rmse: 0.963123\tvalid_1's rmse: 1.04175\n",
      "[557]\ttraining's rmse: 0.963\tvalid_1's rmse: 1.04172\n",
      "[558]\ttraining's rmse: 0.962883\tvalid_1's rmse: 1.04167\n",
      "[559]\ttraining's rmse: 0.962793\tvalid_1's rmse: 1.04163\n",
      "[560]\ttraining's rmse: 0.962702\tvalid_1's rmse: 1.04158\n",
      "[561]\ttraining's rmse: 0.962592\tvalid_1's rmse: 1.04153\n",
      "[562]\ttraining's rmse: 0.962489\tvalid_1's rmse: 1.04149\n",
      "[563]\ttraining's rmse: 0.962392\tvalid_1's rmse: 1.04149\n",
      "[564]\ttraining's rmse: 0.962283\tvalid_1's rmse: 1.04142\n",
      "[565]\ttraining's rmse: 0.9622\tvalid_1's rmse: 1.0414\n",
      "[566]\ttraining's rmse: 0.962117\tvalid_1's rmse: 1.04139\n",
      "[567]\ttraining's rmse: 0.961997\tvalid_1's rmse: 1.04142\n",
      "[568]\ttraining's rmse: 0.961878\tvalid_1's rmse: 1.04134\n",
      "[569]\ttraining's rmse: 0.961786\tvalid_1's rmse: 1.04129\n",
      "[570]\ttraining's rmse: 0.961692\tvalid_1's rmse: 1.04128\n",
      "[571]\ttraining's rmse: 0.961604\tvalid_1's rmse: 1.04124\n",
      "[572]\ttraining's rmse: 0.961544\tvalid_1's rmse: 1.04125\n",
      "[573]\ttraining's rmse: 0.961426\tvalid_1's rmse: 1.04129\n",
      "[574]\ttraining's rmse: 0.961323\tvalid_1's rmse: 1.04125\n",
      "[575]\ttraining's rmse: 0.961217\tvalid_1's rmse: 1.04119\n",
      "[576]\ttraining's rmse: 0.961134\tvalid_1's rmse: 1.0412\n",
      "[577]\ttraining's rmse: 0.961049\tvalid_1's rmse: 1.04116\n",
      "[578]\ttraining's rmse: 0.960957\tvalid_1's rmse: 1.0411\n",
      "[579]\ttraining's rmse: 0.96084\tvalid_1's rmse: 1.04113\n",
      "[580]\ttraining's rmse: 0.960757\tvalid_1's rmse: 1.04109\n",
      "[581]\ttraining's rmse: 0.960654\tvalid_1's rmse: 1.04095\n",
      "[582]\ttraining's rmse: 0.960569\tvalid_1's rmse: 1.0409\n",
      "[583]\ttraining's rmse: 0.960479\tvalid_1's rmse: 1.04085\n",
      "[584]\ttraining's rmse: 0.960383\tvalid_1's rmse: 1.04081\n",
      "[585]\ttraining's rmse: 0.960276\tvalid_1's rmse: 1.04072\n",
      "[586]\ttraining's rmse: 0.960162\tvalid_1's rmse: 1.04075\n",
      "[587]\ttraining's rmse: 0.960055\tvalid_1's rmse: 1.04073\n",
      "[588]\ttraining's rmse: 0.959954\tvalid_1's rmse: 1.04072\n",
      "[589]\ttraining's rmse: 0.959872\tvalid_1's rmse: 1.04072\n",
      "[590]\ttraining's rmse: 0.959772\tvalid_1's rmse: 1.04058\n",
      "[591]\ttraining's rmse: 0.959668\tvalid_1's rmse: 1.04055\n",
      "[592]\ttraining's rmse: 0.959578\tvalid_1's rmse: 1.04057\n",
      "[593]\ttraining's rmse: 0.959528\tvalid_1's rmse: 1.04057\n",
      "[594]\ttraining's rmse: 0.959417\tvalid_1's rmse: 1.04058\n",
      "[595]\ttraining's rmse: 0.959341\tvalid_1's rmse: 1.04053\n",
      "[596]\ttraining's rmse: 0.95926\tvalid_1's rmse: 1.0405\n",
      "[597]\ttraining's rmse: 0.959162\tvalid_1's rmse: 1.04036\n",
      "[598]\ttraining's rmse: 0.959047\tvalid_1's rmse: 1.04034\n",
      "[599]\ttraining's rmse: 0.958965\tvalid_1's rmse: 1.04029\n",
      "[600]\ttraining's rmse: 0.958864\tvalid_1's rmse: 1.04027\n",
      "[601]\ttraining's rmse: 0.958758\tvalid_1's rmse: 1.04023\n",
      "[602]\ttraining's rmse: 0.958653\tvalid_1's rmse: 1.04017\n",
      "[603]\ttraining's rmse: 0.958565\tvalid_1's rmse: 1.04012\n",
      "[604]\ttraining's rmse: 0.958469\tvalid_1's rmse: 1.04011\n",
      "[605]\ttraining's rmse: 0.95837\tvalid_1's rmse: 1.04008\n",
      "[606]\ttraining's rmse: 0.958307\tvalid_1's rmse: 1.04004\n",
      "[607]\ttraining's rmse: 0.958202\tvalid_1's rmse: 1.04003\n",
      "[608]\ttraining's rmse: 0.958117\tvalid_1's rmse: 1.03998\n",
      "[609]\ttraining's rmse: 0.95802\tvalid_1's rmse: 1.03998\n",
      "[610]\ttraining's rmse: 0.957914\tvalid_1's rmse: 1.03997\n",
      "[611]\ttraining's rmse: 0.957822\tvalid_1's rmse: 1.03996\n",
      "[612]\ttraining's rmse: 0.95774\tvalid_1's rmse: 1.03991\n",
      "[613]\ttraining's rmse: 0.957642\tvalid_1's rmse: 1.03987\n",
      "[614]\ttraining's rmse: 0.957581\tvalid_1's rmse: 1.03984\n",
      "[615]\ttraining's rmse: 0.957469\tvalid_1's rmse: 1.03979\n",
      "[616]\ttraining's rmse: 0.957377\tvalid_1's rmse: 1.03981\n",
      "[617]\ttraining's rmse: 0.957264\tvalid_1's rmse: 1.03978\n",
      "[618]\ttraining's rmse: 0.957174\tvalid_1's rmse: 1.03979\n",
      "[619]\ttraining's rmse: 0.957077\tvalid_1's rmse: 1.03977\n",
      "[620]\ttraining's rmse: 0.956999\tvalid_1's rmse: 1.03973\n",
      "[621]\ttraining's rmse: 0.956921\tvalid_1's rmse: 1.03972\n",
      "[622]\ttraining's rmse: 0.956828\tvalid_1's rmse: 1.03969\n",
      "[623]\ttraining's rmse: 0.956729\tvalid_1's rmse: 1.03964\n",
      "[624]\ttraining's rmse: 0.956636\tvalid_1's rmse: 1.03964\n",
      "[625]\ttraining's rmse: 0.956551\tvalid_1's rmse: 1.03965\n",
      "[626]\ttraining's rmse: 0.956451\tvalid_1's rmse: 1.03968\n",
      "[627]\ttraining's rmse: 0.956404\tvalid_1's rmse: 1.03968\n",
      "[628]\ttraining's rmse: 0.956313\tvalid_1's rmse: 1.03968\n",
      "[629]\ttraining's rmse: 0.956204\tvalid_1's rmse: 1.03964\n",
      "[630]\ttraining's rmse: 0.956134\tvalid_1's rmse: 1.03963\n",
      "[631]\ttraining's rmse: 0.956059\tvalid_1's rmse: 1.03962\n",
      "[632]\ttraining's rmse: 0.955976\tvalid_1's rmse: 1.03963\n",
      "[633]\ttraining's rmse: 0.95587\tvalid_1's rmse: 1.03955\n",
      "[634]\ttraining's rmse: 0.955778\tvalid_1's rmse: 1.0395\n",
      "[635]\ttraining's rmse: 0.955693\tvalid_1's rmse: 1.03956\n",
      "[636]\ttraining's rmse: 0.955617\tvalid_1's rmse: 1.03955\n",
      "[637]\ttraining's rmse: 0.955528\tvalid_1's rmse: 1.03954\n",
      "[638]\ttraining's rmse: 0.955455\tvalid_1's rmse: 1.03952\n",
      "[639]\ttraining's rmse: 0.955396\tvalid_1's rmse: 1.03949\n",
      "[640]\ttraining's rmse: 0.955315\tvalid_1's rmse: 1.0394\n",
      "[641]\ttraining's rmse: 0.955237\tvalid_1's rmse: 1.03936\n",
      "[642]\ttraining's rmse: 0.955145\tvalid_1's rmse: 1.03934\n",
      "[643]\ttraining's rmse: 0.955065\tvalid_1's rmse: 1.03933\n",
      "[644]\ttraining's rmse: 0.95498\tvalid_1's rmse: 1.03929\n",
      "[645]\ttraining's rmse: 0.954905\tvalid_1's rmse: 1.03929\n",
      "[646]\ttraining's rmse: 0.954819\tvalid_1's rmse: 1.03931\n",
      "[647]\ttraining's rmse: 0.954731\tvalid_1's rmse: 1.03919\n",
      "[648]\ttraining's rmse: 0.954674\tvalid_1's rmse: 1.03916\n",
      "[649]\ttraining's rmse: 0.954577\tvalid_1's rmse: 1.03912\n",
      "[650]\ttraining's rmse: 0.954482\tvalid_1's rmse: 1.0391\n",
      "[651]\ttraining's rmse: 0.954414\tvalid_1's rmse: 1.03908\n",
      "[652]\ttraining's rmse: 0.95433\tvalid_1's rmse: 1.03907\n",
      "[653]\ttraining's rmse: 0.954232\tvalid_1's rmse: 1.03905\n",
      "[654]\ttraining's rmse: 0.95413\tvalid_1's rmse: 1.03901\n",
      "[655]\ttraining's rmse: 0.954057\tvalid_1's rmse: 1.03898\n",
      "[656]\ttraining's rmse: 0.953965\tvalid_1's rmse: 1.03897\n",
      "[657]\ttraining's rmse: 0.95386\tvalid_1's rmse: 1.03892\n",
      "[658]\ttraining's rmse: 0.953765\tvalid_1's rmse: 1.03889\n",
      "[659]\ttraining's rmse: 0.95373\tvalid_1's rmse: 1.0389\n",
      "[660]\ttraining's rmse: 0.953658\tvalid_1's rmse: 1.03886\n",
      "[661]\ttraining's rmse: 0.953565\tvalid_1's rmse: 1.03886\n",
      "[662]\ttraining's rmse: 0.95348\tvalid_1's rmse: 1.03875\n",
      "[663]\ttraining's rmse: 0.953409\tvalid_1's rmse: 1.03873\n",
      "[664]\ttraining's rmse: 0.953337\tvalid_1's rmse: 1.03872\n",
      "[665]\ttraining's rmse: 0.953256\tvalid_1's rmse: 1.0387\n",
      "[666]\ttraining's rmse: 0.953201\tvalid_1's rmse: 1.03867\n",
      "[667]\ttraining's rmse: 0.953099\tvalid_1's rmse: 1.03863\n",
      "[668]\ttraining's rmse: 0.953065\tvalid_1's rmse: 1.03863\n",
      "[669]\ttraining's rmse: 0.952979\tvalid_1's rmse: 1.03866\n",
      "[670]\ttraining's rmse: 0.952899\tvalid_1's rmse: 1.03865\n",
      "[671]\ttraining's rmse: 0.952805\tvalid_1's rmse: 1.03863\n",
      "[672]\ttraining's rmse: 0.952708\tvalid_1's rmse: 1.03862\n",
      "[673]\ttraining's rmse: 0.952623\tvalid_1's rmse: 1.03861\n",
      "[674]\ttraining's rmse: 0.952543\tvalid_1's rmse: 1.03857\n",
      "[675]\ttraining's rmse: 0.95247\tvalid_1's rmse: 1.03849\n",
      "[676]\ttraining's rmse: 0.952427\tvalid_1's rmse: 1.03849\n",
      "[677]\ttraining's rmse: 0.952363\tvalid_1's rmse: 1.03846\n",
      "[678]\ttraining's rmse: 0.95227\tvalid_1's rmse: 1.03843\n",
      "[679]\ttraining's rmse: 0.9522\tvalid_1's rmse: 1.03839\n",
      "[680]\ttraining's rmse: 0.952121\tvalid_1's rmse: 1.03836\n",
      "[681]\ttraining's rmse: 0.952027\tvalid_1's rmse: 1.0383\n",
      "[682]\ttraining's rmse: 0.951948\tvalid_1's rmse: 1.0383\n",
      "[683]\ttraining's rmse: 0.951854\tvalid_1's rmse: 1.03828\n",
      "[684]\ttraining's rmse: 0.951821\tvalid_1's rmse: 1.03828\n",
      "[685]\ttraining's rmse: 0.951736\tvalid_1's rmse: 1.03828\n",
      "[686]\ttraining's rmse: 0.951643\tvalid_1's rmse: 1.03828\n",
      "[687]\ttraining's rmse: 0.951563\tvalid_1's rmse: 1.03828\n",
      "[688]\ttraining's rmse: 0.951509\tvalid_1's rmse: 1.03822\n",
      "[689]\ttraining's rmse: 0.951456\tvalid_1's rmse: 1.03819\n",
      "[690]\ttraining's rmse: 0.951394\tvalid_1's rmse: 1.03819\n",
      "[691]\ttraining's rmse: 0.951304\tvalid_1's rmse: 1.03814\n",
      "[692]\ttraining's rmse: 0.951197\tvalid_1's rmse: 1.03812\n",
      "[693]\ttraining's rmse: 0.951108\tvalid_1's rmse: 1.0381\n",
      "[694]\ttraining's rmse: 0.951045\tvalid_1's rmse: 1.03807\n",
      "[695]\ttraining's rmse: 0.951013\tvalid_1's rmse: 1.03809\n",
      "[696]\ttraining's rmse: 0.95093\tvalid_1's rmse: 1.03806\n",
      "[697]\ttraining's rmse: 0.950877\tvalid_1's rmse: 1.038\n",
      "[698]\ttraining's rmse: 0.950784\tvalid_1's rmse: 1.03797\n",
      "[699]\ttraining's rmse: 0.950714\tvalid_1's rmse: 1.03795\n",
      "[700]\ttraining's rmse: 0.950659\tvalid_1's rmse: 1.03795\n",
      "[701]\ttraining's rmse: 0.950598\tvalid_1's rmse: 1.03792\n",
      "[702]\ttraining's rmse: 0.950508\tvalid_1's rmse: 1.03791\n",
      "[703]\ttraining's rmse: 0.950418\tvalid_1's rmse: 1.03788\n",
      "[704]\ttraining's rmse: 0.950387\tvalid_1's rmse: 1.03788\n",
      "[705]\ttraining's rmse: 0.950335\tvalid_1's rmse: 1.03782\n",
      "[706]\ttraining's rmse: 0.950245\tvalid_1's rmse: 1.03783\n",
      "[707]\ttraining's rmse: 0.950194\tvalid_1's rmse: 1.0378\n",
      "[708]\ttraining's rmse: 0.950121\tvalid_1's rmse: 1.03779\n",
      "[709]\ttraining's rmse: 0.95007\tvalid_1's rmse: 1.03779\n",
      "[710]\ttraining's rmse: 0.949985\tvalid_1's rmse: 1.03785\n",
      "[711]\ttraining's rmse: 0.949954\tvalid_1's rmse: 1.03786\n",
      "[712]\ttraining's rmse: 0.949868\tvalid_1's rmse: 1.03773\n",
      "[713]\ttraining's rmse: 0.949796\tvalid_1's rmse: 1.03772\n",
      "[714]\ttraining's rmse: 0.949725\tvalid_1's rmse: 1.03768\n",
      "[715]\ttraining's rmse: 0.949633\tvalid_1's rmse: 1.03769\n",
      "[716]\ttraining's rmse: 0.949563\tvalid_1's rmse: 1.03766\n",
      "[717]\ttraining's rmse: 0.949512\tvalid_1's rmse: 1.03761\n",
      "[718]\ttraining's rmse: 0.949431\tvalid_1's rmse: 1.03765\n",
      "[719]\ttraining's rmse: 0.949395\tvalid_1's rmse: 1.03766\n",
      "[720]\ttraining's rmse: 0.949312\tvalid_1's rmse: 1.03772\n",
      "[721]\ttraining's rmse: 0.949225\tvalid_1's rmse: 1.03769\n",
      "[722]\ttraining's rmse: 0.949205\tvalid_1's rmse: 1.03769\n",
      "[723]\ttraining's rmse: 0.949156\tvalid_1's rmse: 1.03763\n",
      "[724]\ttraining's rmse: 0.949097\tvalid_1's rmse: 1.0376\n",
      "[725]\ttraining's rmse: 0.949005\tvalid_1's rmse: 1.03763\n",
      "[726]\ttraining's rmse: 0.94897\tvalid_1's rmse: 1.03764\n",
      "[727]\ttraining's rmse: 0.94889\tvalid_1's rmse: 1.03767\n",
      "[728]\ttraining's rmse: 0.948808\tvalid_1's rmse: 1.03766\n",
      "[729]\ttraining's rmse: 0.948741\tvalid_1's rmse: 1.03765\n",
      "[730]\ttraining's rmse: 0.948666\tvalid_1's rmse: 1.03766\n",
      "[731]\ttraining's rmse: 0.948616\tvalid_1's rmse: 1.03766\n",
      "[732]\ttraining's rmse: 0.948548\tvalid_1's rmse: 1.03765\n",
      "[733]\ttraining's rmse: 0.948515\tvalid_1's rmse: 1.03766\n",
      "[734]\ttraining's rmse: 0.948435\tvalid_1's rmse: 1.03766\n",
      "Early stopping, best iteration is:\n",
      "[724]\ttraining's rmse: 0.949097\tvalid_1's rmse: 1.0376\n",
      "fold_4 coefficients:  [0.49945411 1.76798106 2.0838814 ]\n",
      "training qwk     :  [0.60808019, 0.61163911, 0.6058771, 0.62391041, 0.59534796] 0.6089709539999999\n",
      "validation qwk   :  [0.50893048, 0.53000111, 0.50803946, 0.49367607, 0.49988773] 0.50810697\n",
      "train qwk by dist:  [0.6424732, 0.63984979, 0.63707597, 0.65416632, 0.63283941] 0.641280938\n",
      "valid qwk by dist:  [0.53389113, 0.54637533, 0.51719436, 0.52276715, 0.52170688] 0.52838697\n",
      "fold_0 coefficients:  [0.52205978 1.54825671 2.31756386]\n",
      "fold_1 coefficients:  [0.60325183 1.40788633 2.18839178]\n",
      "fold_2 coefficients:  [0.52444521 1.68857812 2.1212896 ]\n",
      "fold_3 coefficients:  [0.55920877 1.56089017 2.17278801]\n",
      "fold_4 coefficients:  [0.68645423 1.40993657 2.12137196]\n",
      "training qwk     :  [0.53462721, 0.55061874, 0.55226813, 0.55654159, 0.56106215] 0.5510235640000001\n",
      "validation qwk   :  [0.46292211, 0.49927909, 0.49283453, 0.42696872, 0.47825] 0.47205089000000006\n",
      "train qwk by dist:  [0.57849331, 0.57758365, 0.58158728, 0.58205769, 0.57969774] 0.5798839339999999\n",
      "valid qwk by dist:  [0.49740511, 0.48515513, 0.50255156, 0.46235792, 0.4866402] 0.486821984\n",
      "Train on 14151 samples, validate on 683 samples\n",
      "Epoch 1/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 1.5987\n",
      "Epoch 00001: val_loss improved from inf to 1.14969, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 4s 250us/sample - loss: 1.5947 - val_loss: 1.1497\n",
      "Epoch 2/100\n",
      "13920/14151 [============================>.] - ETA: 0s - loss: 1.3081\n",
      "Epoch 00002: val_loss improved from 1.14969 to 1.14769, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 170us/sample - loss: 1.3075 - val_loss: 1.1477\n",
      "Epoch 3/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.2266\n",
      "Epoch 00003: val_loss improved from 1.14769 to 1.12079, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 167us/sample - loss: 1.2243 - val_loss: 1.1208\n",
      "Epoch 4/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.1624\n",
      "Epoch 00004: val_loss did not improve from 1.12079\n",
      "14151/14151 [==============================] - 2s 165us/sample - loss: 1.1613 - val_loss: 1.1260\n",
      "Epoch 5/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1373\n",
      "Epoch 00005: val_loss improved from 1.12079 to 1.10415, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 164us/sample - loss: 1.1364 - val_loss: 1.1041\n",
      "Epoch 6/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1200\n",
      "Epoch 00006: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 164us/sample - loss: 1.1198 - val_loss: 1.1166\n",
      "Epoch 7/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0892\n",
      "Epoch 00007: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 165us/sample - loss: 1.0890 - val_loss: 1.1171\n",
      "Epoch 8/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0798\n",
      "Epoch 00008: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 166us/sample - loss: 1.0795 - val_loss: 1.1050\n",
      "Epoch 9/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0583\n",
      "Epoch 00009: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 164us/sample - loss: 1.0584 - val_loss: 1.1095\n",
      "Epoch 10/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0487\n",
      "Epoch 00010: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 165us/sample - loss: 1.0497 - val_loss: 1.1254\n",
      "Epoch 11/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0243\n",
      "Epoch 00011: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 1.0255 - val_loss: 1.1252\n",
      "Epoch 12/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0118\n",
      "Epoch 00012: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 161us/sample - loss: 1.0097 - val_loss: 1.1135\n",
      "Epoch 13/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9983\n",
      "Epoch 00013: val_loss did not improve from 1.10415\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.9998 - val_loss: 1.1120\n",
      "Epoch 14/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.9898\n",
      "Epoch 00014: val_loss improved from 1.10415 to 1.09923, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 164us/sample - loss: 0.9884 - val_loss: 1.0992\n",
      "Epoch 15/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.9645\n",
      "Epoch 00015: val_loss did not improve from 1.09923\n",
      "14151/14151 [==============================] - 2s 161us/sample - loss: 0.9644 - val_loss: 1.1017\n",
      "Epoch 16/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9717\n",
      "Epoch 00016: val_loss did not improve from 1.09923\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.9715 - val_loss: 1.1280\n",
      "Epoch 17/100\n",
      "13824/14151 [============================>.] - ETA: 0s - loss: 0.9327\n",
      "Epoch 00017: val_loss did not improve from 1.09923\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.9307 - val_loss: 1.1101\n",
      "Epoch 18/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9447\n",
      "Epoch 00018: val_loss improved from 1.09923 to 1.07948, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 163us/sample - loss: 0.9429 - val_loss: 1.0795\n",
      "Epoch 19/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.9333\n",
      "Epoch 00019: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 3s 233us/sample - loss: 0.9338 - val_loss: 1.1374\n",
      "Epoch 20/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9090\n",
      "Epoch 00020: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.9089 - val_loss: 1.1026\n",
      "Epoch 21/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8968\n",
      "Epoch 00021: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 163us/sample - loss: 0.8969 - val_loss: 1.1281\n",
      "Epoch 22/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8886\n",
      "Epoch 00022: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 160us/sample - loss: 0.8875 - val_loss: 1.1200\n",
      "Epoch 23/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8789\n",
      "Epoch 00023: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 159us/sample - loss: 0.8784 - val_loss: 1.1546\n",
      "Epoch 24/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.8738\n",
      "Epoch 00024: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 161us/sample - loss: 0.8712 - val_loss: 1.1359\n",
      "Epoch 25/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8546\n",
      "Epoch 00025: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.8563 - val_loss: 1.1364\n",
      "Epoch 26/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.8444\n",
      "Epoch 00026: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.8442 - val_loss: 1.1544\n",
      "Epoch 27/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.8423\n",
      "Epoch 00027: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 162us/sample - loss: 0.8414 - val_loss: 1.1505\n",
      "Epoch 28/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.8310\n",
      "Epoch 00028: val_loss did not improve from 1.07948\n",
      "14151/14151 [==============================] - 2s 165us/sample - loss: 0.8322 - val_loss: 1.1521\n",
      "fold_0 coefficients:  [0.5592793  1.56612676 2.18924706]\n",
      "Train on 14152 samples, validate on 728 samples\n",
      "Epoch 1/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.7268\n",
      "Epoch 00001: val_loss improved from inf to 1.15388, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 242us/sample - loss: 1.7244 - val_loss: 1.1539\n",
      "Epoch 2/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.3438\n",
      "Epoch 00002: val_loss improved from 1.15388 to 1.12842, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.3443 - val_loss: 1.1284\n",
      "Epoch 3/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.2405\n",
      "Epoch 00003: val_loss improved from 1.12842 to 1.10077, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 1.2415 - val_loss: 1.1008\n",
      "Epoch 4/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.1913\n",
      "Epoch 00004: val_loss did not improve from 1.10077\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.1905 - val_loss: 1.1041\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1491\n",
      "Epoch 00005: val_loss improved from 1.10077 to 1.09230, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.1462 - val_loss: 1.0923\n",
      "Epoch 6/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1142\n",
      "Epoch 00006: val_loss did not improve from 1.09230\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.1126 - val_loss: 1.1169\n",
      "Epoch 7/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0953\n",
      "Epoch 00007: val_loss improved from 1.09230 to 1.08559, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 1.0979 - val_loss: 1.0856\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0816\n",
      "Epoch 00008: val_loss did not improve from 1.08559\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0822 - val_loss: 1.1035\n",
      "Epoch 9/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0630\n",
      "Epoch 00009: val_loss improved from 1.08559 to 1.08183, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0634 - val_loss: 1.0818\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0419\n",
      "Epoch 00010: val_loss did not improve from 1.08183\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0418 - val_loss: 1.0888\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0188\n",
      "Epoch 00011: val_loss improved from 1.08183 to 1.08011, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0189 - val_loss: 1.0801\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0089\n",
      "Epoch 00012: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 1.0088 - val_loss: 1.0965\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0010\n",
      "Epoch 00013: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 1.0012 - val_loss: 1.0929\n",
      "Epoch 14/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9758\n",
      "Epoch 00014: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9752 - val_loss: 1.0974\n",
      "Epoch 15/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9738\n",
      "Epoch 00015: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9738 - val_loss: 1.0924\n",
      "Epoch 16/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9597\n",
      "Epoch 00016: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9586 - val_loss: 1.0999\n",
      "Epoch 17/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9297\n",
      "Epoch 00017: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9291 - val_loss: 1.1062\n",
      "Epoch 18/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9272\n",
      "Epoch 00018: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9274 - val_loss: 1.1220\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9159\n",
      "Epoch 00019: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9153 - val_loss: 1.1567\n",
      "Epoch 20/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8917\n",
      "Epoch 00020: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.8921 - val_loss: 1.1137\n",
      "Epoch 21/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00021: val_loss did not improve from 1.08011\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.8910 - val_loss: 1.1016\n",
      "fold_1 coefficients:  [0.48201509 1.74168457 2.30166814]\n",
      "Train on 14152 samples, validate on 789 samples\n",
      "Epoch 1/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.6340\n",
      "Epoch 00001: val_loss improved from inf to 1.14797, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 234us/sample - loss: 1.6329 - val_loss: 1.1480\n",
      "Epoch 2/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.2841\n",
      "Epoch 00002: val_loss improved from 1.14797 to 1.13126, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.2841 - val_loss: 1.1313\n",
      "Epoch 3/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2106\n",
      "Epoch 00003: val_loss did not improve from 1.13126\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.2096 - val_loss: 1.1332\n",
      "Epoch 4/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1581\n",
      "Epoch 00004: val_loss improved from 1.13126 to 1.11584, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.1570 - val_loss: 1.1158\n",
      "Epoch 5/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1278\n",
      "Epoch 00005: val_loss improved from 1.11584 to 1.09236, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.1269 - val_loss: 1.0924\n",
      "Epoch 6/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1128\n",
      "Epoch 00006: val_loss did not improve from 1.09236\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.1145 - val_loss: 1.1005\n",
      "Epoch 7/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0853\n",
      "Epoch 00007: val_loss improved from 1.09236 to 1.08179, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.0847 - val_loss: 1.0818\n",
      "Epoch 8/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0578\n",
      "Epoch 00008: val_loss did not improve from 1.08179\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0591 - val_loss: 1.0885\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0540\n",
      "Epoch 00009: val_loss did not improve from 1.08179\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0537 - val_loss: 1.0930\n",
      "Epoch 10/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0255\n",
      "Epoch 00010: val_loss improved from 1.08179 to 1.07242, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0256 - val_loss: 1.0724\n",
      "Epoch 11/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0126\n",
      "Epoch 00011: val_loss did not improve from 1.07242\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0123 - val_loss: 1.1146\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0070\n",
      "Epoch 00012: val_loss did not improve from 1.07242\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 1.0062 - val_loss: 1.0927\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00013: val_loss improved from 1.07242 to 1.06521, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9866 - val_loss: 1.0652\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9729\n",
      "Epoch 00014: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 3s 209us/sample - loss: 0.9739 - val_loss: 1.0763\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 00015: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.9568 - val_loss: 1.0853\n",
      "Epoch 16/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9566\n",
      "Epoch 00016: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9565 - val_loss: 1.0855\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9314\n",
      "Epoch 00017: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9317 - val_loss: 1.0860\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9251\n",
      "Epoch 00018: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 164us/sample - loss: 0.9249 - val_loss: 1.0859\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9126\n",
      "Epoch 00019: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9132 - val_loss: 1.0824\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9104\n",
      "Epoch 00020: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.9098 - val_loss: 1.0960\n",
      "Epoch 21/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8886\n",
      "Epoch 00021: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.8885 - val_loss: 1.1158\n",
      "Epoch 22/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8793\n",
      "Epoch 00022: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 162us/sample - loss: 0.8788 - val_loss: 1.1041\n",
      "Epoch 23/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.8745\n",
      "Epoch 00023: val_loss did not improve from 1.06521\n",
      "14152/14152 [==============================] - 2s 163us/sample - loss: 0.8752 - val_loss: 1.0870\n",
      "fold_2 coefficients:  [0.58071468 1.50355968 2.11810233]\n",
      "Train on 14152 samples, validate on 807 samples\n",
      "Epoch 1/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.6239\n",
      "Epoch 00001: val_loss improved from inf to 1.32523, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 237us/sample - loss: 1.6241 - val_loss: 1.3252\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.2921\n",
      "Epoch 00002: val_loss improved from 1.32523 to 1.29025, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.2881 - val_loss: 1.2902\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.2184\n",
      "Epoch 00003: val_loss improved from 1.29025 to 1.25958, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.2177 - val_loss: 1.2596\n",
      "Epoch 4/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.1697\n",
      "Epoch 00004: val_loss did not improve from 1.25958\n",
      "14152/14152 [==============================] - 2s 168us/sample - loss: 1.1701 - val_loss: 1.2704\n",
      "Epoch 5/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.1401\n",
      "Epoch 00005: val_loss improved from 1.25958 to 1.21704, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 172us/sample - loss: 1.1391 - val_loss: 1.2170\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1172\n",
      "Epoch 00006: val_loss did not improve from 1.21704\n",
      "14152/14152 [==============================] - 2s 174us/sample - loss: 1.1168 - val_loss: 1.2642\n",
      "Epoch 7/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0916\n",
      "Epoch 00007: val_loss improved from 1.21704 to 1.21599, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 170us/sample - loss: 1.0913 - val_loss: 1.2160\n",
      "Epoch 8/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0718\n",
      "Epoch 00008: val_loss improved from 1.21599 to 1.19237, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 1.0741 - val_loss: 1.1924\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0633\n",
      "Epoch 00009: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 1.0633 - val_loss: 1.2257\n",
      "Epoch 10/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0440\n",
      "Epoch 00010: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 1.0431 - val_loss: 1.2212\n",
      "Epoch 11/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0339\n",
      "Epoch 00011: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0337 - val_loss: 1.3406\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0077\n",
      "Epoch 00012: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 1.0072 - val_loss: 1.2358\n",
      "Epoch 13/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9857\n",
      "Epoch 00013: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 167us/sample - loss: 0.9855 - val_loss: 1.2535\n",
      "Epoch 14/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9750\n",
      "Epoch 00014: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9769 - val_loss: 1.2330\n",
      "Epoch 15/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9649\n",
      "Epoch 00015: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 166us/sample - loss: 0.9647 - val_loss: 1.2135\n",
      "Epoch 16/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9597\n",
      "Epoch 00016: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 3s 194us/sample - loss: 0.9579 - val_loss: 1.2364\n",
      "Epoch 17/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9417\n",
      "Epoch 00017: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 169us/sample - loss: 0.9426 - val_loss: 1.2342\n",
      "Epoch 18/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9227\n",
      "Epoch 00018: val_loss did not improve from 1.19237\n",
      "14152/14152 [==============================] - 2s 165us/sample - loss: 0.9247 - val_loss: 1.2310\n",
      "fold_3 coefficients:  [0.84283178 1.08586393 2.16298707]\n",
      "Train on 14153 samples, validate on 775 samples\n",
      "Epoch 1/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.6916\n",
      "Epoch 00001: val_loss improved from inf to 1.18139, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 4s 269us/sample - loss: 1.6897 - val_loss: 1.1814\n",
      "Epoch 2/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.3157\n",
      "Epoch 00002: val_loss improved from 1.18139 to 1.13872, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 185us/sample - loss: 1.3146 - val_loss: 1.1387\n",
      "Epoch 3/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.2136\n",
      "Epoch 00003: val_loss improved from 1.13872 to 1.11537, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 191us/sample - loss: 1.2150 - val_loss: 1.1154\n",
      "Epoch 4/100\n",
      "14112/14153 [============================>.] - ETA: 0s - loss: 1.1762\n",
      "Epoch 00004: val_loss did not improve from 1.11537\n",
      "14153/14153 [==============================] - 3s 179us/sample - loss: 1.1764 - val_loss: 1.1203\n",
      "Epoch 5/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.1358\n",
      "Epoch 00005: val_loss improved from 1.11537 to 1.10871, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 171us/sample - loss: 1.1356 - val_loss: 1.1087\n",
      "Epoch 6/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.1017\n",
      "Epoch 00006: val_loss did not improve from 1.10871\n",
      "14153/14153 [==============================] - 2s 169us/sample - loss: 1.1021 - val_loss: 1.1088\n",
      "Epoch 7/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.0817\n",
      "Epoch 00007: val_loss did not improve from 1.10871\n",
      "14153/14153 [==============================] - 2s 169us/sample - loss: 1.0813 - val_loss: 1.1114\n",
      "Epoch 8/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0617\n",
      "Epoch 00008: val_loss improved from 1.10871 to 1.10506, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 170us/sample - loss: 1.0628 - val_loss: 1.1051\n",
      "Epoch 9/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 1.0482\n",
      "Epoch 00009: val_loss did not improve from 1.10506\n",
      "14153/14153 [==============================] - 2s 171us/sample - loss: 1.0462 - val_loss: 1.1092\n",
      "Epoch 10/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 1.0361\n",
      "Epoch 00010: val_loss improved from 1.10506 to 1.09011, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 172us/sample - loss: 1.0357 - val_loss: 1.0901\n",
      "Epoch 11/100\n",
      "14016/14153 [============================>.] - ETA: 0s - loss: 1.0244\n",
      "Epoch 00011: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 166us/sample - loss: 1.0236 - val_loss: 1.0919\n",
      "Epoch 12/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 1.0143\n",
      "Epoch 00012: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 165us/sample - loss: 1.0127 - val_loss: 1.1029\n",
      "Epoch 13/100\n",
      "14048/14153 [============================>.] - ETA: 0s - loss: 0.9874\n",
      "Epoch 00013: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 175us/sample - loss: 0.9870 - val_loss: 1.0984\n",
      "Epoch 14/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9738\n",
      "Epoch 00014: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 3s 177us/sample - loss: 0.9735 - val_loss: 1.1056\n",
      "Epoch 15/100\n",
      "14080/14153 [============================>.] - ETA: 0s - loss: 0.9683\n",
      "Epoch 00015: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 171us/sample - loss: 0.9686 - val_loss: 1.1070\n",
      "Epoch 16/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 0.9462\n",
      "Epoch 00016: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 165us/sample - loss: 0.9465 - val_loss: 1.1112\n",
      "Epoch 17/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 0.9420\n",
      "Epoch 00017: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 167us/sample - loss: 0.9434 - val_loss: 1.1091\n",
      "Epoch 18/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.9218\n",
      "Epoch 00018: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 166us/sample - loss: 0.9214 - val_loss: 1.1213\n",
      "Epoch 19/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 0.9152\n",
      "Epoch 00019: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 167us/sample - loss: 0.9170 - val_loss: 1.1054\n",
      "Epoch 20/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9113\n",
      "Epoch 00020: val_loss did not improve from 1.09011\n",
      "14153/14153 [==============================] - 2s 166us/sample - loss: 0.9135 - val_loss: 1.1176\n",
      "fold_4 coefficients:  [0.56690297 1.5547309  2.18781346]\n",
      "training qwk     :  [0.65200493, 0.62325117, 0.6278408, 0.60004412, 0.60097303] 0.6208228100000001\n",
      "validation qwk   :  [0.54263666, 0.57791638, 0.53466617, 0.50775633, 0.52074563] 0.5367442339999999\n",
      "train qwk by dist:  [0.66936265, 0.63193096, 0.65327154, 0.612201, 0.62644273] 0.638641776\n",
      "valid qwk by dist:  [0.55030984, 0.55912954, 0.55461485, 0.51446088, 0.53067742] 0.541838506\n"
     ]
    }
   ],
   "source": [
    "lgb_preds = models(\"lgb\", X_train_lgb, y_train_lgb, X_test_lgb)\n",
    "lr_preds = models(\"lr\", X_train_lr, y_train_lr, X_test_lr)\n",
    "nn_preds = models(\"nn\", X_train_lr, y_train_lr, X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_class = eval_qwk_lgb_regr(lgb_preds * 0.4 + nn_preds * 0.45 + lr_preds * 0.15, new_train) # threshold by distribution\n",
    "sample_submission[\"accuracy_group\"] = test_pred_class\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission[\"accuracy_group\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "034ac59a2bbd417ea00533e9c9c47ec1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_81d3fe1a8bca40cbace6242cfaa0efad",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_518e228904bf4d68a80e2a56f1300440",
       "value": 3614
      }
     },
     "04f4a409db4149eda2c3a6c1b6a9e998": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e605889e6c22416888545c2238330b28",
        "IPY_MODEL_759cece4a4294ab586a6c242d4d7a268"
       ],
       "layout": "IPY_MODEL_5395f6ee710e4276bb0ebf53108d63d9"
      }
     },
     "09629245b09645709228e368adfaf6e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_034ac59a2bbd417ea00533e9c9c47ec1",
        "IPY_MODEL_c07df710da1348dcab9f3cf82942bab7"
       ],
       "layout": "IPY_MODEL_87e98c594ebf4e42a809fe45e66a74df"
      }
     },
     "0d641fc342df4e81a1a9f2b623a169b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "518e228904bf4d68a80e2a56f1300440": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5395f6ee710e4276bb0ebf53108d63d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "759cece4a4294ab586a6c242d4d7a268": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_964df059d9f14ab08e567fbf1a1b9f25",
       "placeholder": "​",
       "style": "IPY_MODEL_9252a8489f584af4a48d88daeafb0e4a",
       "value": " 1000/1000 [02:10&lt;00:00,  7.65it/s]"
      }
     },
     "7fc00ee60fce429c8fc77f6a0873212f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81d3fe1a8bca40cbace6242cfaa0efad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87e98c594ebf4e42a809fe45e66a74df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9252a8489f584af4a48d88daeafb0e4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "964df059d9f14ab08e567fbf1a1b9f25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c07df710da1348dcab9f3cf82942bab7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7fc00ee60fce429c8fc77f6a0873212f",
       "placeholder": "​",
       "style": "IPY_MODEL_d73d1cb4cf324ae5b6955118c4008a8e",
       "value": " 3614/3614 [07:21&lt;00:00,  8.19it/s]"
      }
     },
     "d73d1cb4cf324ae5b6955118c4008a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e605889e6c22416888545c2238330b28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f988a2539c0f496abf58e3368c3bb3da",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0d641fc342df4e81a1a9f2b623a169b4",
       "value": 1000
      }
     },
     "f988a2539c0f496abf58e3368c3bb3da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
