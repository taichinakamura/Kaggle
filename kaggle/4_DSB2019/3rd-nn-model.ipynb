{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- incorporate different threshold optimization scheme by distribution and removed eventid count and add game, activity duration, highest level, round in game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from functools import partial\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "np.set_printoptions(precision=8)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "        return -qwk(y, X_p)\n",
    "        \n",
    "    def fit(self, X, y,random_flg=False):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        if random_flg:\n",
    "            initial_coef = [np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7), np.random.uniform(0.8,0.9)]\n",
    "        else:\n",
    "            initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        \n",
    "    def predict(self, X, coef):\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwk(a1, a2):\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "    e = e / a1.shape[0]\n",
    "    return np.round(1 - o / e, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 11.4 s, total: 1min 18s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n",
    "train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n",
    "test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n",
    "#specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n",
    "sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\n",
    "train = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\n",
    "train = train[train.installation_id.isin(train_labels.installation_id.unique())]\n",
    "assess_title = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
    "       'Cauldron Filler (Assessment)', 'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)']\n",
    "def remove_index_calc(df):\n",
    "    additional_remove_index = []\n",
    "    for i, session in df.groupby('installation_id', sort=False):\n",
    "        last_row = session.index[-1]\n",
    "        session = session[session.title.isin(assess_title)]\n",
    "        first_row = session.index[-1] + 1\n",
    "        for j in range(first_row, last_row+1):\n",
    "            additional_remove_index.append(j)                \n",
    "    return additional_remove_index\n",
    "additional_remove_index = remove_index_calc(train)\n",
    "train = train[~train.index.isin(additional_remove_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 2.92 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def encode_title(train, test):\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    list_of_title_eventcode = list(set(train['title_event_code'].unique()).union(set(test['title_event_code'].unique())))\n",
    "    \n",
    "    list_of_eventid = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    \n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"level\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    test[\"level\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    \n",
    "    train[\"round\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "    test[\"round\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "    \n",
    "    return train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid\n",
    "\n",
    "train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_eventid = encode_title(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    last_activity = 0\n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n",
    "    title_eventcode_count = {str(ele): 0 for ele in list_of_title_eventcode}\n",
    "    #eventid_count = {str(ele): 0 for ele in list_of_eventid}\n",
    "    user_world_count = {\"world_\"+str(wor) : 0 for wor in activities_world.values()}\n",
    "    eventcode_count = {str(ele): 0 for ele in list_of_event_code}\n",
    "    \n",
    "    last_session_time_sec = 0\n",
    "    all_assessments = []\n",
    "    accuracy_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0 \n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    crys_game_level = np.array([]); tree_game_level = np.array([]); magma_game_level = np.array([])\n",
    "    crys_game_round = np.array([]); tree_game_round = np.array([]); magma_game_round = np.array([])    \n",
    "    crys_act_true = 0; crys_act_false = 0\n",
    "    tree_act_true = 0; tree_act_false = 0\n",
    "    magma_act_true = 0; magma_act_false = 0\n",
    "    crys_act_acc = []; tree_act_acc = []; magma_act_acc = []\n",
    "    durations_game = []; durations_activity = []\n",
    "        \n",
    "    for i, session in user_sample.groupby('game_session', sort=False):      \n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "        \n",
    "        # get current session time in seconds\n",
    "        if session_type != 'Assessment':\n",
    "            time_spent = int(session['game_time'].iloc[-1] / 1000)\n",
    "            time_spent_each_act[activities_labels[session_title]] += time_spent   \n",
    "            \n",
    "            if session_type == \"Game\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_game_true += true\n",
    "                    crys_game_false += false\n",
    "                    crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_game_true += true\n",
    "                    tree_game_false += false\n",
    "                    tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_game_true += true\n",
    "                    magma_game_false += false\n",
    "                    magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            if session_type == \"Activity\":\n",
    "                true = session['event_data'].str.contains('true').sum()\n",
    "                false = session['event_data'].str.contains('false').sum() \n",
    "                durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_act_true += true\n",
    "                    crys_act_false += false\n",
    "                    crys_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_act_true += true\n",
    "                    tree_act_false += false\n",
    "                    tree_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_act_true += true\n",
    "                    magma_act_false += false\n",
    "                    magma_act_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1): # test set or session in train_label\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum() # true in target assess\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum() # false in target assessment\n",
    "            \n",
    "            # from start of installation_id to the start of target assessment ------------------------\n",
    "            features = user_activities_count.copy() # appearance of each type without duplicates\n",
    "            features.update(time_spent_each_act.copy()) # cumulative gameplay time in each title\n",
    "            features.update(title_eventcode_count.copy()) # apperance of combi of title and event_code\n",
    "            #features.update(eventid_count.copy()) # apperance of eventid\n",
    "            features.update(user_world_count.copy()) # appearance of world with duplicates\n",
    "            features.update(eventcode_count.copy()) # appearance of world with duplicates\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            features[\"misses\"] = miss\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = crys_act_true\n",
    "                features[\"act_false\"] = crys_act_false\n",
    "                features['act_accuracy'] = crys_act_true / (crys_act_true + crys_act_false) if (crys_act_true + crys_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(crys_act_acc) if len(crys_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = crys_act_acc[-1] if len(crys_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(crys_game_level) if len(crys_game_level[~np.isnan(crys_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(crys_game_round) if len(crys_game_round[~np.isnan(crys_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = tree_act_true\n",
    "                features[\"act_false\"] = tree_act_false\n",
    "                features['act_accuracy'] = tree_act_true / (tree_act_true + tree_act_false) if (tree_act_true + tree_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(tree_act_acc) if len(tree_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = tree_act_acc[-1] if len(tree_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(tree_game_level) if len(tree_game_level[~np.isnan(tree_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(tree_game_round) if len(tree_game_round[~np.isnan(tree_game_round)]) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "                features[\"act_true\"] = magma_act_true\n",
    "                features[\"act_false\"] = magma_act_false\n",
    "                features['act_accuracy'] = magma_act_true / (magma_act_true + magma_act_false) if (magma_act_true + magma_act_false) != 0 else 0\n",
    "                features[\"act_accuracy_std\"] = np.std(magma_act_acc) if len(magma_act_acc) >=1 else 0\n",
    "                features[\"last_act_acc\"] = magma_act_acc[-1] if len(magma_act_acc) >=1 else 0\n",
    "                features[\"hightest_level\"] = np.nanmax(magma_game_level) if len(magma_game_level[~np.isnan(magma_game_level)]) >=1 else 0\n",
    "                features[\"hightest_round\"] = np.nanmax(magma_game_round) if len(magma_game_round[~np.isnan(magma_game_round)]) >=1 else 0\n",
    "\n",
    "            if durations_game == []:\n",
    "                features['duration_game_mean'] = 0\n",
    "                features['duration_game_std'] = 0\n",
    "                features['game_last_duration'] = 0\n",
    "                features['game_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_game_mean'] = np.mean(durations_game)\n",
    "                features['duration_game_std'] = np.std(durations_game)\n",
    "                features['game_last_duration'] = durations_game[-1]\n",
    "                features['game_max_duration'] = np.max(durations_game)\n",
    "                \n",
    "            if durations_activity == []:\n",
    "                features['duration_activity_mean'] = 0\n",
    "                features['duration_activity_std'] = 0\n",
    "                features['activity_last_duration'] = 0\n",
    "                features['activity_max_duration'] = 0\n",
    "            else:\n",
    "                features['duration_activity_mean'] = np.mean(durations_activity)\n",
    "                features['duration_activity_std'] = np.std(durations_activity)\n",
    "                features['activity_last_duration'] = durations_activity[-1]\n",
    "                features['activity_max_duration'] = np.max(durations_activity)\n",
    "            \n",
    "            # unique type --------------------------------------------------------\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['session_title'] = session_title\n",
    "            \n",
    "            # nums in target assessment data ------------------------------------------\n",
    "            if durations == []: #span of timestamp in target assessment\n",
    "                features['duration_mean'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds) \n",
    "            \n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[str(features['accuracy_group'])] += 1\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            \n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "                        \n",
    "        n_of_title_eventcode = Counter(session['title_event_code']) \n",
    "        for key in n_of_title_eventcode.keys():\n",
    "            title_eventcode_count[str(key)] += n_of_title_eventcode[key]\n",
    "        miss += np.sum(session[\"misses\"])\n",
    "        #n_of_eventid = Counter(session['event_id']) \n",
    "        #for key in n_of_eventid.keys():\n",
    "        #    eventid_count[str(key)] += n_of_eventid[key]\n",
    "            \n",
    "        user_world_count[\"world_\"+str(session_world)] += session.shape[0]\n",
    "        \n",
    "        n_of_eventcode = Counter(session['event_code']) \n",
    "        for key in n_of_eventcode.keys():\n",
    "            eventcode_count[str(key)] += n_of_eventcode[key]\n",
    "        \n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d125c03a471740a1a52b4f1c6f44ff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=3614, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(17690, 524)\n"
     ]
    }
   ],
   "source": [
    "new_train = []\n",
    "for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "    new_train += get_data(user_sample)\n",
    "new_train = pd.DataFrame(new_train)\n",
    "print(new_train.shape)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa31dbe8e87474d93986faf0a0dc878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=1000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1000, 524)\n"
     ]
    }
   ],
   "source": [
    "new_test = []\n",
    "for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "    a = get_data(user_sample, test_set=True)\n",
    "    new_test.append(a)   \n",
    "new_test = pd.DataFrame(new_test)\n",
    "print(new_test.shape)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = new_train.corr().abs()\n",
    "correlations = correlations.mask(np.tril(np.ones(correlations.shape)).astype(np.bool))\n",
    "correlations = correlations.stack().reset_index()\n",
    "corr_columns = [\"level_0\", \"level_1\", \"value\"]\n",
    "correlations.columns = corr_columns\n",
    "correlations = correlations.sort_values(\"value\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "high_corr = correlations[correlations[\"value\"] >= 0.995]\n",
    "\n",
    "high_corr_features = []\n",
    "for i in range(high_corr.shape[0]):\n",
    "    if high_corr.iloc[i][\"level_0\"] not in high_corr_features and high_corr.iloc[i][\"level_1\"] not in high_corr_features:\n",
    "        high_corr_features.append(high_corr.iloc[i][\"level_0\"])\n",
    "    elif high_corr.iloc[i][\"level_0\"] in high_corr_features and high_corr.iloc[i][\"level_1\"] not in high_corr_features:\n",
    "        high_corr_features.append(high_corr.iloc[i][\"level_1\"])\n",
    "    elif high_corr.iloc[i][\"level_0\"] not in high_corr_features and high_corr.iloc[i][\"level_1\"] in high_corr_features:\n",
    "        high_corr_features.append(high_corr.iloc[i][\"level_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "features = [i for i in new_train.columns if i not in [\"installation_id\", \"accuracy_group\"]]\n",
    "categoricals = ['session_title']\n",
    "\n",
    "features = features.copy()\n",
    "new_train_nn = new_train.copy()\n",
    "new_test_nn = new_test.copy()\n",
    "if len(categoricals) > 0:\n",
    "    for cat in categoricals:\n",
    "        enc = OneHotEncoder()\n",
    "        train_cats = enc.fit_transform(new_train_nn[[cat]])\n",
    "        test_cats = enc.transform(new_test_nn[[cat]])\n",
    "        cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "        features += cat_cols\n",
    "        train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "        test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "        new_train_nn = pd.concat([new_train_nn, train_cats], axis=1)\n",
    "        new_test_nn = pd.concat([new_test_nn, test_cats], axis=1)\n",
    "    scalar = MinMaxScaler()\n",
    "    new_train_nn[features] = scalar.fit_transform(new_train_nn[features])\n",
    "    new_test_nn[features] = scalar.transform(new_test_nn[features])\n",
    "    \n",
    "X_train = new_train_nn.drop(['accuracy_group'],axis=1) \n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(X_train[\"installation_id\"]))\n",
    "X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "remove_features = []\n",
    "for i in categoricals:\n",
    "    remove_features.append(i)\n",
    "for i in X_train.columns:\n",
    "    if X_train[i].std() == 0 and i not in remove_features:\n",
    "        remove_features.append(i)\n",
    "for i in high_corr_features:\n",
    "    if i not in remove_features:\n",
    "        remove_features.append(i)\n",
    "X_train = X_train.drop(remove_features, axis=1)\n",
    "X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "y_train = new_train.accuracy_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               75000     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 98,301\n",
      "Trainable params: 98,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14151 samples, validate on 683 samples\n",
      "Epoch 1/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.6691\n",
      "Epoch 00001: val_loss improved from inf to 1.17367, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 3s 232us/sample - loss: 1.6679 - val_loss: 1.1737\n",
      "Epoch 2/100\n",
      "13824/14151 [============================>.] - ETA: 0s - loss: 1.3085\n",
      "Epoch 00002: val_loss improved from 1.17367 to 1.13248, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 142us/sample - loss: 1.3053 - val_loss: 1.1325\n",
      "Epoch 3/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.2107\n",
      "Epoch 00003: val_loss improved from 1.13248 to 1.12387, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 142us/sample - loss: 1.2107 - val_loss: 1.1239\n",
      "Epoch 4/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1719\n",
      "Epoch 00004: val_loss did not improve from 1.12387\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 1.1724 - val_loss: 1.1285\n",
      "Epoch 5/100\n",
      "13792/14151 [============================>.] - ETA: 0s - loss: 1.1515\n",
      "Epoch 00005: val_loss did not improve from 1.12387\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 1.1495 - val_loss: 1.1386\n",
      "Epoch 6/100\n",
      "13824/14151 [============================>.] - ETA: 0s - loss: 1.1248\n",
      "Epoch 00006: val_loss improved from 1.12387 to 1.11969, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 1.1264 - val_loss: 1.1197\n",
      "Epoch 7/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 1.1000\n",
      "Epoch 00007: val_loss improved from 1.11969 to 1.10791, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 141us/sample - loss: 1.1019 - val_loss: 1.1079\n",
      "Epoch 8/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0769\n",
      "Epoch 00008: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 142us/sample - loss: 1.0767 - val_loss: 1.1231\n",
      "Epoch 9/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0693\n",
      "Epoch 00009: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 142us/sample - loss: 1.0687 - val_loss: 1.1303\n",
      "Epoch 10/100\n",
      "13792/14151 [============================>.] - ETA: 0s - loss: 1.0428\n",
      "Epoch 00010: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 1.0449 - val_loss: 1.1207\n",
      "Epoch 11/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0450\n",
      "Epoch 00011: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 1.0452 - val_loss: 1.1284\n",
      "Epoch 12/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0315\n",
      "Epoch 00012: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 1.0311 - val_loss: 1.1158\n",
      "Epoch 13/100\n",
      "13792/14151 [============================>.] - ETA: 0s - loss: 1.0092\n",
      "Epoch 00013: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 1.0101 - val_loss: 1.1276\n",
      "Epoch 14/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0065\n",
      "Epoch 00014: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 1.0051 - val_loss: 1.1507\n",
      "Epoch 15/100\n",
      "13792/14151 [============================>.] - ETA: 0s - loss: 0.9847\n",
      "Epoch 00015: val_loss did not improve from 1.10791\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 0.9869 - val_loss: 1.1161\n",
      "Epoch 16/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9733\n",
      "Epoch 00016: val_loss improved from 1.10791 to 1.10787, saving model to ./nn_model.w8\n",
      "14151/14151 [==============================] - 2s 141us/sample - loss: 0.9731 - val_loss: 1.1079\n",
      "Epoch 17/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9739\n",
      "Epoch 00017: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 139us/sample - loss: 0.9739 - val_loss: 1.1245\n",
      "Epoch 18/100\n",
      "13792/14151 [============================>.] - ETA: 0s - loss: 0.9659\n",
      "Epoch 00018: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 0.9662 - val_loss: 1.1185\n",
      "Epoch 19/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.9462\n",
      "Epoch 00019: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 0.9485 - val_loss: 1.1250\n",
      "Epoch 20/100\n",
      "13824/14151 [============================>.] - ETA: 0s - loss: 0.9412\n",
      "Epoch 00020: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 138us/sample - loss: 0.9405 - val_loss: 1.1103\n",
      "Epoch 21/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9345\n",
      "Epoch 00021: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 140us/sample - loss: 0.9346 - val_loss: 1.1291\n",
      "Epoch 22/100\n",
      "13888/14151 [============================>.] - ETA: 0s - loss: 0.9138\n",
      "Epoch 00022: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 152us/sample - loss: 0.9139 - val_loss: 1.1309\n",
      "Epoch 23/100\n",
      "13856/14151 [============================>.] - ETA: 0s - loss: 0.9061\n",
      "Epoch 00023: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 158us/sample - loss: 0.9045 - val_loss: 1.1399\n",
      "Epoch 24/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8909\n",
      "Epoch 00024: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 159us/sample - loss: 0.8897 - val_loss: 1.1710\n",
      "Epoch 25/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8804\n",
      "Epoch 00025: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 3s 180us/sample - loss: 0.8802 - val_loss: 1.1568\n",
      "Epoch 26/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.8808\n",
      "Epoch 00026: val_loss did not improve from 1.10787\n",
      "14151/14151 [==============================] - 2s 161us/sample - loss: 0.8810 - val_loss: 1.1588\n",
      "fold_0 coefficients:  [1.04761851 1.64835883 2.27971363]\n",
      "training qwk:  0.65838827\n",
      "validation qwk:  0.54992591\n",
      "train qwk by dist: 0.64672361\n",
      "valid qwk by dist 0.53115468\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 200)               75000     \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 98,301\n",
      "Trainable params: 98,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 728 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.8435\n",
      "Epoch 00001: val_loss improved from inf to 1.18424, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 205us/sample - loss: 1.8393 - val_loss: 1.1842\n",
      "Epoch 2/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.4001\n",
      "Epoch 00002: val_loss improved from 1.18424 to 1.14783, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 145us/sample - loss: 1.3965 - val_loss: 1.1478\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.2765\n",
      "Epoch 00003: val_loss improved from 1.14783 to 1.12059, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.2783 - val_loss: 1.1206\n",
      "Epoch 4/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.2173\n",
      "Epoch 00004: val_loss improved from 1.12059 to 1.11762, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.2167 - val_loss: 1.1176\n",
      "Epoch 5/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1660\n",
      "Epoch 00005: val_loss improved from 1.11762 to 1.11307, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.1655 - val_loss: 1.1131\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1485\n",
      "Epoch 00006: val_loss improved from 1.11307 to 1.10030, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.1484 - val_loss: 1.1003\n",
      "Epoch 7/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.1257\n",
      "Epoch 00007: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.1243 - val_loss: 1.1086\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0985\n",
      "Epoch 00008: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.1028 - val_loss: 1.1197\n",
      "Epoch 9/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0833\n",
      "Epoch 00009: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0824 - val_loss: 1.1222\n",
      "Epoch 10/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0745\n",
      "Epoch 00010: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0738 - val_loss: 1.1174\n",
      "Epoch 11/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0487\n",
      "Epoch 00011: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0467 - val_loss: 1.1296\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0378\n",
      "Epoch 00012: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0373 - val_loss: 1.1055\n",
      "Epoch 13/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0202\n",
      "Epoch 00013: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0208 - val_loss: 1.1053\n",
      "Epoch 14/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0080\n",
      "Epoch 00014: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0087 - val_loss: 1.1049\n",
      "Epoch 15/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9991\n",
      "Epoch 00015: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 1.0024 - val_loss: 1.1139\n",
      "Epoch 16/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9716\n",
      "Epoch 00016: val_loss did not improve from 1.10030\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9753 - val_loss: 1.1248\n",
      "fold_1 coefficients:  [0.5870668  1.5197544  2.12127299]\n",
      "training qwk:  0.58005228\n",
      "validation qwk:  0.52894441\n",
      "train qwk by dist: 0.58570019\n",
      "valid qwk by dist 0.52766916\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 200)               75000     \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "layer_normalization_8 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 98,301\n",
      "Trainable params: 98,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 789 samples\n",
      "Epoch 1/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.7254\n",
      "Epoch 00001: val_loss improved from inf to 1.17447, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 209us/sample - loss: 1.7161 - val_loss: 1.1745\n",
      "Epoch 2/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.3116\n",
      "Epoch 00002: val_loss improved from 1.17447 to 1.15298, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.3132 - val_loss: 1.1530\n",
      "Epoch 3/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.2362\n",
      "Epoch 00003: val_loss improved from 1.15298 to 1.14490, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.2339 - val_loss: 1.1449\n",
      "Epoch 4/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1997\n",
      "Epoch 00004: val_loss improved from 1.14490 to 1.12291, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 142us/sample - loss: 1.1999 - val_loss: 1.1229\n",
      "Epoch 5/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1603\n",
      "Epoch 00005: val_loss improved from 1.12291 to 1.11143, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.1606 - val_loss: 1.1114\n",
      "Epoch 6/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.1411\n",
      "Epoch 00006: val_loss did not improve from 1.11143\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.1400 - val_loss: 1.1120\n",
      "Epoch 7/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.1137\n",
      "Epoch 00007: val_loss improved from 1.11143 to 1.10044, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.1145 - val_loss: 1.1004\n",
      "Epoch 8/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0978\n",
      "Epoch 00008: val_loss did not improve from 1.10044\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0989 - val_loss: 1.1048\n",
      "Epoch 9/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0837\n",
      "Epoch 00009: val_loss improved from 1.10044 to 1.09622, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0882 - val_loss: 1.0962\n",
      "Epoch 10/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0688\n",
      "Epoch 00010: val_loss improved from 1.09622 to 1.08077, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0682 - val_loss: 1.0808\n",
      "Epoch 11/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.0582\n",
      "Epoch 00011: val_loss improved from 1.08077 to 1.07936, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0590 - val_loss: 1.0794\n",
      "Epoch 12/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 00012: val_loss did not improve from 1.07936\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0429 - val_loss: 1.0797\n",
      "Epoch 13/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 1.0316\n",
      "Epoch 00013: val_loss did not improve from 1.07936\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0311 - val_loss: 1.0929\n",
      "Epoch 14/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.0205\n",
      "Epoch 00014: val_loss did not improve from 1.07936\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0196 - val_loss: 1.0914\n",
      "Epoch 15/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0103\n",
      "Epoch 00015: val_loss improved from 1.07936 to 1.06720, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0102 - val_loss: 1.0672\n",
      "Epoch 16/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 00016: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9972 - val_loss: 1.0974\n",
      "Epoch 17/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9766\n",
      "Epoch 00017: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9765 - val_loss: 1.0922\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9818\n",
      "Epoch 00018: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9819 - val_loss: 1.0955\n",
      "Epoch 19/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9632\n",
      "Epoch 00019: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9670 - val_loss: 1.0881\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9645\n",
      "Epoch 00020: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9649 - val_loss: 1.0784\n",
      "Epoch 21/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9514\n",
      "Epoch 00021: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 153us/sample - loss: 0.9514 - val_loss: 1.0872\n",
      "Epoch 22/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9395\n",
      "Epoch 00022: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 151us/sample - loss: 0.9385 - val_loss: 1.0904\n",
      "Epoch 23/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9359\n",
      "Epoch 00023: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 143us/sample - loss: 0.9355 - val_loss: 1.0876\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9343\n",
      "Epoch 00024: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 0.9341 - val_loss: 1.1155\n",
      "Epoch 25/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9104\n",
      "Epoch 00025: val_loss did not improve from 1.06720\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9108 - val_loss: 1.1099\n",
      "fold_2 coefficients:  [0.90723714 1.73165186 2.2677272 ]\n",
      "training qwk:  0.64051375\n",
      "validation qwk:  0.5441553\n",
      "train qwk by dist: 0.62732283\n",
      "valid qwk by dist 0.53265065\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 200)               75000     \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 98,301\n",
      "Trainable params: 98,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 807 samples\n",
      "Epoch 1/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 1.6104\n",
      "Epoch 00001: val_loss improved from inf to 1.30072, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 3s 207us/sample - loss: 1.6075 - val_loss: 1.3007\n",
      "Epoch 2/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 1.2800\n",
      "Epoch 00002: val_loss improved from 1.30072 to 1.28387, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.2776 - val_loss: 1.2839\n",
      "Epoch 3/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.2127\n",
      "Epoch 00003: val_loss improved from 1.28387 to 1.24530, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.2120 - val_loss: 1.2453\n",
      "Epoch 4/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1638\n",
      "Epoch 00004: val_loss improved from 1.24530 to 1.24044, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.1614 - val_loss: 1.2404\n",
      "Epoch 5/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.1454\n",
      "Epoch 00005: val_loss did not improve from 1.24044\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.1433 - val_loss: 1.2481\n",
      "Epoch 6/100\n",
      "13760/14152 [============================>.] - ETA: 0s - loss: 1.1306\n",
      "Epoch 00006: val_loss did not improve from 1.24044\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.1268 - val_loss: 1.2472\n",
      "Epoch 7/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.1027\n",
      "Epoch 00007: val_loss did not improve from 1.24044\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.1019 - val_loss: 1.2578\n",
      "Epoch 8/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0858\n",
      "Epoch 00008: val_loss improved from 1.24044 to 1.23646, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0863 - val_loss: 1.2365\n",
      "Epoch 9/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 1.0600\n",
      "Epoch 00009: val_loss did not improve from 1.23646\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0605 - val_loss: 1.2436\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0553\n",
      "Epoch 00010: val_loss improved from 1.23646 to 1.21538, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 141us/sample - loss: 1.0550 - val_loss: 1.2154\n",
      "Epoch 11/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0333\n",
      "Epoch 00011: val_loss did not improve from 1.21538\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 1.0343 - val_loss: 1.2245\n",
      "Epoch 12/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0268\n",
      "Epoch 00012: val_loss did not improve from 1.21538\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0290 - val_loss: 1.2411\n",
      "Epoch 13/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 1.0148\n",
      "Epoch 00013: val_loss improved from 1.21538 to 1.21115, saving model to ./nn_model.w8\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0166 - val_loss: 1.2112\n",
      "Epoch 14/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00014: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 140us/sample - loss: 1.0058 - val_loss: 1.2126\n",
      "Epoch 15/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Epoch 00015: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9976 - val_loss: 1.2176\n",
      "Epoch 16/100\n",
      "13952/14152 [============================>.] - ETA: 0s - loss: 0.9788\n",
      "Epoch 00016: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9816 - val_loss: 1.2212\n",
      "Epoch 17/100\n",
      "13888/14152 [============================>.] - ETA: 0s - loss: 0.9672\n",
      "Epoch 00017: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 138us/sample - loss: 0.9666 - val_loss: 1.2229\n",
      "Epoch 18/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9589\n",
      "Epoch 00018: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9591 - val_loss: 1.2921\n",
      "Epoch 19/100\n",
      "13920/14152 [============================>.] - ETA: 0s - loss: 0.9437\n",
      "Epoch 00019: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9446 - val_loss: 1.2467\n",
      "Epoch 20/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9298\n",
      "Epoch 00020: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9311 - val_loss: 1.2511\n",
      "Epoch 21/100\n",
      "13792/14152 [============================>.] - ETA: 0s - loss: 0.9308\n",
      "Epoch 00021: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9313 - val_loss: 1.2460\n",
      "Epoch 22/100\n",
      "13856/14152 [============================>.] - ETA: 0s - loss: 0.9122\n",
      "Epoch 00022: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9133 - val_loss: 1.2722\n",
      "Epoch 23/100\n",
      "13824/14152 [============================>.] - ETA: 0s - loss: 0.9090\n",
      "Epoch 00023: val_loss did not improve from 1.21115\n",
      "14152/14152 [==============================] - 2s 139us/sample - loss: 0.9111 - val_loss: 1.2640\n",
      "fold_3 coefficients:  [0.71795065 1.26512994 2.16536811]\n",
      "training qwk:  0.63672681\n",
      "validation qwk:  0.49077128\n",
      "train qwk by dist: 0.62741231\n",
      "valid qwk by dist 0.50615461\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 200)               75000     \n",
      "_________________________________________________________________\n",
      "layer_normalization_12 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_13 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                2525      \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 98,301\n",
      "Trainable params: 98,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14153 samples, validate on 775 samples\n",
      "Epoch 1/100\n",
      "14144/14153 [============================>.] - ETA: 0s - loss: 1.6700\n",
      "Epoch 00001: val_loss improved from inf to 1.19394, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 3s 208us/sample - loss: 1.6700 - val_loss: 1.1939\n",
      "Epoch 2/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.3200\n",
      "Epoch 00002: val_loss improved from 1.19394 to 1.15495, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 142us/sample - loss: 1.3202 - val_loss: 1.1549\n",
      "Epoch 3/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 1.2236\n",
      "Epoch 00003: val_loss improved from 1.15495 to 1.14553, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 141us/sample - loss: 1.2237 - val_loss: 1.1455\n",
      "Epoch 4/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.1831\n",
      "Epoch 00004: val_loss improved from 1.14553 to 1.14033, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.1829 - val_loss: 1.1403\n",
      "Epoch 5/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.1489\n",
      "Epoch 00005: val_loss improved from 1.14033 to 1.12443, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.1502 - val_loss: 1.1244\n",
      "Epoch 6/100\n",
      "13984/14153 [============================>.] - ETA: 0s - loss: 1.1325\n",
      "Epoch 00006: val_loss improved from 1.12443 to 1.11931, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.1323 - val_loss: 1.1193\n",
      "Epoch 7/100\n",
      "13792/14153 [============================>.] - ETA: 0s - loss: 1.1008\n",
      "Epoch 00007: val_loss improved from 1.11931 to 1.11770, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.0983 - val_loss: 1.1177\n",
      "Epoch 8/100\n",
      "13824/14153 [============================>.] - ETA: 0s - loss: 1.0925\n",
      "Epoch 00008: val_loss did not improve from 1.11770\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.0913 - val_loss: 1.1211\n",
      "Epoch 9/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0675\n",
      "Epoch 00009: val_loss did not improve from 1.11770\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 1.0661 - val_loss: 1.1228\n",
      "Epoch 10/100\n",
      "13920/14153 [============================>.] - ETA: 0s - loss: 1.0532\n",
      "Epoch 00010: val_loss improved from 1.11770 to 1.11227, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.0520 - val_loss: 1.1123\n",
      "Epoch 11/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0465\n",
      "Epoch 00011: val_loss improved from 1.11227 to 1.10517, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 1.0466 - val_loss: 1.1052\n",
      "Epoch 12/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0415\n",
      "Epoch 00012: val_loss improved from 1.10517 to 1.08168, saving model to ./nn_model.w8\n",
      "14153/14153 [==============================] - 2s 141us/sample - loss: 1.0407 - val_loss: 1.0817\n",
      "Epoch 13/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 1.0114\n",
      "Epoch 00013: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 1.0114 - val_loss: 1.0903\n",
      "Epoch 14/100\n",
      "13792/14153 [============================>.] - ETA: 0s - loss: 1.0052\n",
      "Epoch 00014: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 1.0042 - val_loss: 1.1088\n",
      "Epoch 15/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00015: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9947 - val_loss: 1.0891\n",
      "Epoch 16/100\n",
      "13824/14153 [============================>.] - ETA: 0s - loss: 0.9896\n",
      "Epoch 00016: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9889 - val_loss: 1.0934\n",
      "Epoch 17/100\n",
      "13824/14153 [============================>.] - ETA: 0s - loss: 0.9689\n",
      "Epoch 00017: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 140us/sample - loss: 0.9675 - val_loss: 1.0992\n",
      "Epoch 18/100\n",
      "13792/14153 [============================>.] - ETA: 0s - loss: 0.9580\n",
      "Epoch 00018: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9582 - val_loss: 1.1021\n",
      "Epoch 19/100\n",
      "13952/14153 [============================>.] - ETA: 0s - loss: 0.9499\n",
      "Epoch 00019: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9515 - val_loss: 1.1057\n",
      "Epoch 20/100\n",
      "13856/14153 [============================>.] - ETA: 0s - loss: 0.9441\n",
      "Epoch 00020: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9416 - val_loss: 1.1055\n",
      "Epoch 21/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9257\n",
      "Epoch 00021: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 139us/sample - loss: 0.9259 - val_loss: 1.1141\n",
      "Epoch 22/100\n",
      "13888/14153 [============================>.] - ETA: 0s - loss: 0.9217\n",
      "Epoch 00022: val_loss did not improve from 1.08168\n",
      "14153/14153 [==============================] - 2s 141us/sample - loss: 0.9224 - val_loss: 1.0958\n",
      "fold_4 coefficients:  [1.12132248 1.77612229 2.24651846]\n",
      "training qwk:  0.62883467\n",
      "validation qwk:  0.54725928\n",
      "train qwk by dist: 0.61950926\n",
      "valid qwk by dist 0.53067742\n",
      "                             \n",
      "-----------------------------\n",
      "coefficients:  [array([1.04761851, 1.64835883, 2.27971363]), array([0.5870668 , 1.5197544 , 2.12127299]), array([0.90723714, 1.73165186, 2.2677272 ]), array([0.71795065, 1.26512994, 2.16536811]), array([1.12132248, 1.77612229, 2.24651846])]\n",
      "train qwk list: [0.65838827, 0.58005228, 0.64051375, 0.63672681, 0.62883467]\n",
      "train qwk average score: 0.628903156\n",
      "train qwk average score by dist: 0.6213336399999999\n",
      "                             \n",
      "valid qwk list:  [0.54992591, 0.52894441, 0.5441553, 0.49077128, 0.54725928]\n",
      "valid qwk average score: 0.532211236\n",
      "valid qwk average score by dist: 0.525661304\n",
      "oof qwk:  0.53083511\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_folds=5\n",
    "skf=StratifiedKFold(n_splits = n_folds, random_state=1223)\n",
    "coefficients = []\n",
    "models = []\n",
    "train_qwk_scores = []\n",
    "test_qwk_scores = []\n",
    "train_qwk_dist = []\n",
    "test_qwk_dist = []\n",
    "\n",
    "valid = np.array([])\n",
    "real = np.array([])\n",
    "features_list = [i for i in X_train.columns if i != \"installation_id\"]\n",
    "feature_importance_df = pd.DataFrame(features_list, columns=[\"Feature\"])\n",
    "for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    optR = OptimizedRounder()\n",
    "    X_train2 = X_train.iloc[train_index,:]\n",
    "    y_train2 = y_train.iloc[train_index]\n",
    "    X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "    X_test2 = X_train.iloc[test_index,:]\n",
    "    y_test2 = y_train.iloc[test_index]\n",
    "    test2 = pd.concat([X_test2, y_test2], axis=1)\n",
    "    test2 = test2.groupby('installation_id').apply(lambda x: x.sample(1, random_state=1223)).reset_index(drop=True)\n",
    "    X_test2 = test2.drop([\"accuracy_group\", \"installation_id\"], axis=1)\n",
    "    y_test2 = test2[\"accuracy_group\"]\n",
    "        \n",
    "    verbosity = 100\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(X_train2.shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='tanh'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            #tf.keras.layers.Dense(50, activation='relu'),\n",
    "            #tf.keras.layers.LayerNormalization(),\n",
    "            #tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "    print(model.summary())\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "    \n",
    "    model.fit(X_train2, \n",
    "                y_train2, \n",
    "                validation_data=(X_test2, y_test2),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "    model.load_weights('./nn_model.w8')\n",
    "\n",
    "    models.append(model)\n",
    "    train_predict = model.predict(X_train2)\n",
    "    test_predict = model.predict(X_test2)\n",
    "    \n",
    "    valid = np.concatenate((valid, np.array(test_predict.reshape(test_predict.shape[0],))), axis=0) \n",
    "    real = np.concatenate((real, np.array(y_test2)), axis=0)\n",
    "    \n",
    "    optR.fit(train_predict.reshape(-1,), y_train2)\n",
    "    tmp_coefficients = optR.coefficients()\n",
    "    print(\"fold_\"+str(i)+\" coefficients: \", tmp_coefficients)\n",
    "    opt_train_preds = optR.predict(train_predict.reshape(-1, ), tmp_coefficients)\n",
    "    train_qwk_score = qwk(y_train2, opt_train_preds)\n",
    "    print(\"training qwk: \", train_qwk_score)\n",
    "    opt_test_preds = optR.predict(test_predict.reshape(-1, ), tmp_coefficients)\n",
    "    test_qwk_score = qwk(y_test2, opt_test_preds)\n",
    "    print(\"validation qwk: \", test_qwk_score)\n",
    "    train_qwk_scores.append(train_qwk_score)\n",
    "    test_qwk_scores.append(test_qwk_score)\n",
    "    coefficients.append(tmp_coefficients)\n",
    "    \n",
    "    train_qwk_d = qwk(y_train2, eval_qwk_lgb_regr(train_predict, new_train))\n",
    "    print(\"train qwk by dist:\", train_qwk_d)\n",
    "    test_qwk_d = qwk(y_test2, eval_qwk_lgb_regr(test_predict, new_train))\n",
    "    print(\"valid qwk by dist\", test_qwk_d)\n",
    "    train_qwk_dist.append(train_qwk_d)\n",
    "    test_qwk_dist.append(test_qwk_d)\n",
    "                           \n",
    "    \n",
    "print(\"                             \")\n",
    "print(\"-----------------------------\")\n",
    "print('coefficients: ', coefficients)\n",
    "print('train qwk list:', train_qwk_scores)\n",
    "print('train qwk average score:',np.mean(train_qwk_scores))\n",
    "print('train qwk average score by dist:',np.mean(train_qwk_dist))\n",
    "print(\"                             \")\n",
    "print('valid qwk list: ', test_qwk_scores)\n",
    "print('valid qwk average score:',np.mean(test_qwk_scores))\n",
    "print('valid qwk average score by dist:',np.mean(test_qwk_dist))\n",
    "print(\"oof qwk: \", qwk(real, eval_qwk_lgb_regr(valid, new_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = new_test_nn.drop([\"installation_id\", \"accuracy_group\"], axis=1)\n",
    "X_test = X_test.drop(remove_features, axis=1)\n",
    "X_test = X_test[sorted(X_test.columns.tolist())]\n",
    "pred_value = np.zeros([X_test.shape[0]])\n",
    "test_coefficients = np.mean(coefficients, axis=0)\n",
    "for model in models:\n",
    "    pred_value += model.predict(X_test).reshape(X_test.shape[0],) / n_folds\n",
    "#test_pred_class = optR.predict(pred_value.reshape(-1, ), test_coefficients) # threshold by nelder-mead\n",
    "test_pred_class = eval_qwk_lgb_regr(pred_value, new_train) # threshold by distribution\n",
    "sample_submission[\"accuracy_group\"] = test_pred_class\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission[\"accuracy_group\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "075b5f9c0ce044b884c64899b2665ba8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37d2b3d355b241d6893df44d7b3bd231": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4f3a189e8d94f2da253e4c5c67a869d",
       "placeholder": "​",
       "style": "IPY_MODEL_3d55aa8ead344d5e8d0944e8b0a905f7",
       "value": " 1000/1000 [01:33&lt;00:00, 10.74it/s]"
      }
     },
     "3a7fbe2bdf95486cbf857db0d68c14db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3aa31dbe8e87474d93986faf0a0dc878": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_84747e73902441849e80f707d13a4435",
        "IPY_MODEL_37d2b3d355b241d6893df44d7b3bd231"
       ],
       "layout": "IPY_MODEL_7c1a6e7b5ff84b588b296a23aa4b2b6e"
      }
     },
     "3d55aa8ead344d5e8d0944e8b0a905f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "53b42cffe7454b9395af9a7674788728": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70d08a53d74049b7b2ed76f33f858461": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78c9a5e58c364c2a8c5c208d341b061b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c1a6e7b5ff84b588b296a23aa4b2b6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "84747e73902441849e80f707d13a4435": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c7ee1448b71d49ed8711672889e66749",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a7fbe2bdf95486cbf857db0d68c14db",
       "value": 1000
      }
     },
     "a53f246ab9914bffa1b5a75c84887ef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53b42cffe7454b9395af9a7674788728",
       "placeholder": "​",
       "style": "IPY_MODEL_78c9a5e58c364c2a8c5c208d341b061b",
       "value": " 3614/3614 [06:39&lt;00:00,  9.04it/s]"
      }
     },
     "aca24ab584584aa4afc0b3dcaa8876b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c7ee1448b71d49ed8711672889e66749": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d125c03a471740a1a52b4f1c6f44ff79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d70f7b78c19b4876a692352fcbf98865",
        "IPY_MODEL_a53f246ab9914bffa1b5a75c84887ef4"
       ],
       "layout": "IPY_MODEL_70d08a53d74049b7b2ed76f33f858461"
      }
     },
     "d70f7b78c19b4876a692352fcbf98865": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_075b5f9c0ce044b884c64899b2665ba8",
       "max": 3614,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aca24ab584584aa4afc0b3dcaa8876b7",
       "value": 3614
      }
     },
     "e4f3a189e8d94f2da253e4c5c67a869d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
