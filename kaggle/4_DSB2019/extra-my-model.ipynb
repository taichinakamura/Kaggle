{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "- try truncated validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss, roc_auc_score, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, cohen_kappa_score\n",
    "import optuna.integration.lightgbm as lgb_opt\n",
    "import lightgbm as lgb\n",
    "from functools import partial\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from hyperopt import hp, tpe, Trials, fmin, space_eval\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "np.set_printoptions(precision=8)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwk(a1, a2):\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "    e = e / a1.shape[0]\n",
    "    return np.round(1 - o / e, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "        return -qwk(y, X_p)\n",
    "        #return -mod_qwk(y, X_p, weights=weights)\n",
    "    \n",
    "    def fit(self, X, y, random_flg = False):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        if random_flg:\n",
    "            initial_coef = [np.random.uniform(0.4,0.5), np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7)]\n",
    "        else:\n",
    "            initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') #Powell\n",
    "        \n",
    "    def predict(self, X, coef):\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_pred, train_t):\n",
    "    dist = Counter(train_t['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(train_t)\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred)))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 11.4 s, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n",
    "train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n",
    "test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n",
    "sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 9.69 s, total: 1min 57s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def encode_title(train, test):\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    list_of_title_eventcode = sorted(list(set(train['title_event_code'].unique()).union(set(test['title_event_code'].unique()))))\n",
    "    \n",
    "    train['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), train['type'], train['world']))\n",
    "    test['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), test['type'], test['world']))\n",
    "    list_of_type_world = sorted(list(set(train['type_world'].unique()).union(set(test['type_world'].unique()))))\n",
    "    \n",
    "    list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
    "    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
    "    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n",
    "\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    train[\"misses\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "    test[\"misses\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"misses\"] if \"\\\"misses\\\"\" in x else np.nan)\n",
    "        \n",
    "    train[\"true\"] = train[\"event_data\"].apply(lambda x: 1 if \"true\" in x and \"correct\" in x else 0)\n",
    "    test[\"true\"] = test[\"event_data\"].apply(lambda x: 1 if \"true\" in x and \"correct\" in x else 0)\n",
    "\n",
    "    train[\"false\"] = train[\"event_data\"].apply(lambda x: 1 if \"false\" in x and \"correct\" in x else 0)\n",
    "    test[\"false\"] = test[\"event_data\"].apply(lambda x: 1 if \"false\" in x and \"correct\" in x else 0)\n",
    "    \n",
    "    train[\"game_complete\"] = train[\"event_data\"].apply(lambda x: 1 if \"game_completed\" in x else 0)\n",
    "    test[\"game_complete\"] = test[\"event_data\"].apply(lambda x: 1 if \"game_completed\" in x else 0)\n",
    "    \n",
    "    #train[\"level\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    #test[\"level\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"level\"] if \"\\\"level\\\"\" in x else np.nan)\n",
    "    \n",
    "    #train[\"round\"] = train[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "    #test[\"round\"] = test[\"event_data\"].apply(lambda x: json.loads(x)[\"round\"] if \"\\\"round\\\"\" in x else np.nan)\n",
    "               \n",
    "    return train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_type_world\n",
    "\n",
    "train, test, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, activities_world, list_of_title_eventcode, list_of_type_world = encode_title(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def make_ratio(features, dic):\n",
    "    total = sum(dic.values())\n",
    "    if total != 0:\n",
    "        for key in dic.keys():\n",
    "            features[str(key)] = features[str(key)] / total\n",
    "    else:\n",
    "        pass\n",
    "    return features\n",
    "\n",
    "def get_data(user_sample, test_set=False):\n",
    "    last_activity = 0\n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    title_eventcode_count = {str(ele): 0 for ele in list_of_title_eventcode}\n",
    "    user_world_count = {\"world_\"+str(wor) : 0 for wor in activities_world.values()}\n",
    "    event_code_count = {str(ev): 0 for ev in list_of_event_code}\n",
    "    title_count = {actv: 0 for actv in list_of_user_activities}\n",
    "    type_world_count = {str(ev): 0 for ev in list_of_type_world}\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n",
    "    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n",
    "    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n",
    "    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n",
    "    \n",
    "    all_assessments = []\n",
    "    accuracy_groups = {\"0\":0, \"1\":0, \"2\":0, \"3\":0}\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0 \n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = user_sample.iloc[0]['timestamp']\n",
    "    miss = 0\n",
    "    crys_game_true = 0; crys_game_false = 0\n",
    "    tree_game_true = 0; tree_game_false = 0\n",
    "    magma_game_true = 0; magma_game_false = 0\n",
    "    crys_game_acc = []; tree_game_acc = []; magma_game_acc = []\n",
    "    durations = []\n",
    "    prev_assess_title = -999\n",
    "    assess_count = 1\n",
    "    last_accuracy = -999\n",
    "    prev_assess_start = -999; prev_assess_end = -999\n",
    "    real_prev_assess_start = -999; real_prev_assess_end = -999\n",
    "    real_assess_start = -999; real_assess_end = -999\n",
    "    complete_games = 0\n",
    "    no_result_count = 0\n",
    "    crys_game_level = np.array([]); tree_game_level = np.array([]); magma_game_level = np.array([])\n",
    "    crys_game_round = np.array([]); tree_game_round = np.array([]); magma_game_round = np.array([])\n",
    "    \n",
    "    for i, session in user_sample.groupby('game_session', sort=False):      \n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        session_world = session[\"world\"].iloc[0]\n",
    "        \n",
    "        if session_type != 'Assessment':\n",
    "            if session_type == \"Game\":\n",
    "                true = session['true'].sum()\n",
    "                false = session['false'].sum() \n",
    "                if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                    crys_game_true += true\n",
    "                    crys_game_false += false\n",
    "                    crys_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    #crys_game_level = np.concatenate([crys_game_level, session[\"level\"]], axis=0)\n",
    "                    #crys_game_round = np.concatenate([crys_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                    tree_game_true += true\n",
    "                    tree_game_false += false\n",
    "                    tree_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    #tree_game_level = np.concatenate([tree_game_level, session[\"level\"]], axis=0)\n",
    "                    #tree_game_round = np.concatenate([tree_game_round, session[\"round\"]], axis=0)\n",
    "                elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                    magma_game_true += true\n",
    "                    magma_game_false += false\n",
    "                    magma_game_acc.append(true / (true + false) if (true + false) != 0 else 0)\n",
    "                    #magma_game_level = np.concatenate([magma_game_level, session[\"level\"]], axis=0)\n",
    "                    #magma_game_round = np.concatenate([magma_game_round, session[\"round\"]], axis=0)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1): \n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum() # true in target assess\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum() # false in target assessment\n",
    "            assess_start = session.iloc[0,2]\n",
    "            assess_end = session.iloc[-1,2]\n",
    "            \n",
    "            # from start of installation_id to the start of target assessment ------------------------\n",
    "            features = user_activities_count.copy() # appearance of each type without duplicates\n",
    "            features = make_ratio(features, user_activities_count)\n",
    "            features.update(title_eventcode_count.copy()) # apperance of combi of title and event_code\n",
    "            features = make_ratio(features, title_eventcode_count)\n",
    "            features.update(user_world_count.copy()) # appearance of world with duplicates\n",
    "            features = make_ratio(features, user_world_count)\n",
    "            features.update(event_code_count.copy())\n",
    "            features = make_ratio(features, event_code_count)\n",
    "            features.update(title_count.copy())\n",
    "            features = make_ratio(features, title_count)\n",
    "            features.update(type_world_count.copy())\n",
    "            features = make_ratio(features, type_world_count)\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(last_game_time_title.copy())\n",
    "            features.update(ac_game_time_title.copy())\n",
    "            features.update(ac_true_attempts_title.copy())\n",
    "            features.update(ac_false_attempts_title.copy())\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n",
    "            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n",
    "            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n",
    "            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n",
    "            features[\"misses\"] = miss\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            features[\"no_complete_game\"] = complete_games\n",
    "            features[\"no_result_count\"] = no_result_count \n",
    "            \n",
    "            if true_attempts + false_attempts == 0:\n",
    "                no_result_count += 1\n",
    "            else:\n",
    "                real_assess_start = session.iloc[0,2]\n",
    "                real_assess_end = session.iloc[-1,2]\n",
    "             \n",
    "            #features[\"crys_game_true\"] = crys_game_true\n",
    "            #features[\"crys_game_false\"] = crys_game_false\n",
    "            #features['crys_game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "            #features[\"crys_game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "            #features[\"cryslast_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "            #features[\"tree_game_true\"] = tree_game_true\n",
    "            #features[\"tree_game_false\"] = tree_game_false\n",
    "            #features['tree_game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "            #features[\"tree_game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "            #features[\"tree_last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "            #features[\"magma_game_true\"] = magma_game_true\n",
    "            #features[\"magma_game_false\"] = magma_game_false\n",
    "            #features['magma_game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "            #features[\"magma_game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "            #features[\"magma_last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "            \n",
    "            if session_world == activities_world[\"CRYSTALCAVES\"]:\n",
    "                features[\"game_true\"] = crys_game_true\n",
    "                features[\"game_false\"] = crys_game_false\n",
    "                features['game_accuracy'] = crys_game_true / (crys_game_true + crys_game_false) if (crys_game_true + crys_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(crys_game_acc) if len(crys_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = crys_game_acc[-1] if len(crys_game_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"TREETOPCITY\"]:\n",
    "                features[\"game_true\"] = tree_game_true\n",
    "                features[\"game_false\"] = tree_game_false\n",
    "                features['game_accuracy'] = tree_game_true / (tree_game_true + tree_game_false) if (tree_game_true + tree_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(tree_game_acc) if len(tree_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = tree_game_acc[-1] if len(tree_game_acc) >=1 else 0\n",
    "            elif session_world == activities_world[\"MAGMAPEAK\"]:\n",
    "                features[\"game_true\"] = magma_game_true\n",
    "                features[\"game_false\"] = magma_game_false\n",
    "                features['game_accuracy'] = magma_game_true / (magma_game_true + magma_game_false) if (magma_game_true + magma_game_false) != 0 else 0\n",
    "                features[\"game_accuracy_std\"] = np.std(magma_game_acc) if len(magma_game_acc) >=1 else 0\n",
    "                features[\"last_game_acc\"] = magma_game_acc[-1] if len(magma_game_acc) >=1 else 0\n",
    "            \n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['session_title'] = session_title\n",
    "            features[\"prev_assess_title\"] = prev_assess_title\n",
    "            prev_assess_title = session_title\n",
    "            features[\"first_assessment\"] = 1 if assess_count == 1 else 0\n",
    "            #features[\"assess_count\"] = assess_count\n",
    "            assess_count += 1\n",
    "            features[\"time_from_start\"] = (assess_start - time_first_activity).seconds\n",
    "\n",
    "            if prev_assess_end == -999:\n",
    "                features[\"time_bet_assess\"] = -999\n",
    "            else:\n",
    "                features[\"time_bet_assess\"] = (assess_start - prev_assess_end).seconds\n",
    "            prev_assess_start = assess_start\n",
    "            prev_assess_end = assess_end\n",
    "            if real_prev_assess_end == -999:\n",
    "                features[\"time_bet_real_assess\"] = -999\n",
    "            else:\n",
    "                features[\"time_bet_real_assess\"] = (real_assess_start - real_prev_assess_end).seconds\n",
    "            real_prev_assess_start = real_assess_start\n",
    "            real_prev_assess_end = real_assess_end\n",
    "            \n",
    "            if durations == []: #span of timestamp in target assessment\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "                features['duration_max'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "                features['duration_max'] = np.max(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "            \n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            features['last_assess_acc'] = last_accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            last_accuracy = accuracy\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[str(features['accuracy_group'])] += 1\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            \n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "            \n",
    "        complete_games += np.sum(session[\"game_complete\"])\n",
    "        miss += np.sum(session[\"misses\"])\n",
    "        user_world_count[\"world_\"+str(session_world)] += session.shape[0]\n",
    "        \n",
    "        n_of_type_world = Counter(session['type_world']) \n",
    "        for key in n_of_type_world.keys():\n",
    "            type_world_count[str(key)] += n_of_type_world[key]\n",
    "            \n",
    "        n_of_title = Counter(session['title']) \n",
    "        for key in n_of_title.keys():\n",
    "            title_count[activities_labels[key]] += n_of_title[key]\n",
    "            \n",
    "        n_of_eventcode = Counter(session['event_code']) \n",
    "        for key in n_of_eventcode.keys():\n",
    "            event_code_count[str(key)] += n_of_eventcode[key]\n",
    "                        \n",
    "        n_of_title_eventcode = Counter(session['title_event_code']) \n",
    "        for key in n_of_title_eventcode.keys():\n",
    "            title_eventcode_count[str(key)] += n_of_title_eventcode[key]\n",
    "        \n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "    if test_set:\n",
    "        return all_assessments[-1], all_assessments[:-1] # test previous data to incorporate into training\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69917e47998d48058a43be1c5b9cc26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=17000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef53f1c2306c4fc7a87cb8afc119a297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Installation_id', max=1000, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    compiled_val = []\n",
    "\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    del train\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n",
    "        test_data, val_data = get_data(user_sample, test_set=True)\n",
    "        compiled_test.append(test_data)\n",
    "        compiled_val += val_data\n",
    "    del test\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    reduce_val = pd.DataFrame(compiled_val)\n",
    "\n",
    "    categoricals = ['session_title']\n",
    "    return reduce_train, reduce_test, reduce_val, categoricals\n",
    "new_train, new_test, new_val, categoricals = get_train_and_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = new_train[new_train.Game==0].copy()\n",
    "tmp = tmp[tmp.Activity == 0].copy()\n",
    "tmp = tmp[tmp.Clip == 0].copy()\n",
    "tmp = tmp[tmp.Assessment ==0].copy()\n",
    "remove_train_index = tmp.index\n",
    "new_train = new_train[~new_train.index.isin(remove_train_index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17577, 563)\n",
      "(1000, 563)\n",
      "(2347, 563)\n"
     ]
    }
   ],
   "source": [
    "print(new_train.shape)\n",
    "print(new_test.shape)\n",
    "print(new_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 563)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data augmentation\n",
    "tmp = new_val[new_val.Game==0].copy()\n",
    "tmp = tmp[tmp.Activity == 0].copy()\n",
    "tmp = tmp[tmp.Clip == 0].copy()\n",
    "tmp = tmp[tmp.Assessment ==0].copy()\n",
    "remove_val_index = tmp.index\n",
    "add_val = new_val[~new_val.index.isin(remove_val_index)].copy()\n",
    "\n",
    "#tmp = add_val.installation_id.value_counts().reset_index(drop=False) # include some part of new_test installation_id\n",
    "#val_id = list(tmp[tmp.installation_id >= 20][\"index\"])\n",
    "#add_val = add_val[add_val.installation_id.isin(val_id)]\n",
    "\n",
    "mod_train = pd.concat([new_train, add_val], sort=False).reset_index(drop=True)\n",
    "mod_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Show_4080\n",
      "Bottle Filler (Activity)_2010\n",
      "Bubble Bath_4080\n",
      "Bubble Bath_4090\n",
      "Bug Measurer (Activity)_4080\n",
      "Cart Balancer (Assessment)_4080\n",
      "Chest Sorter (Assessment)_4080\n",
      "Crystals Rule_2010\n",
      "Dino Dive_4080\n",
      "Dino Drink_4080\n",
      "Egg Dropper (Activity)_4080\n",
      "Fireworks (Activity)_4080\n",
      "Happy Camel_4080\n",
      "Leaf Leader_4080\n",
      "Mushroom Sorter (Assessment)_4080\n",
      "Pan Balance_2010\n",
      "Pan Balance_4080\n",
      "Sandcastle Builder (Activity)_2010\n",
      "Scrub-A-Dub_4080\n",
      "Watering Hole (Activity)_2010\n",
      "acc_Cart Balancer (Assessment)\n"
     ]
    }
   ],
   "source": [
    "def exclude(reduce_train, reduce_test, features):\n",
    "    to_exclude = [] \n",
    "    ajusted_test = reduce_test.copy()\n",
    "    for feature in features:\n",
    "        if feature not in ['accuracy_group', 'installation_id', 'session_title', 'hightest_level']:\n",
    "            data = reduce_train[feature]\n",
    "            train_mean = data.mean()\n",
    "            data = ajusted_test[feature] \n",
    "            test_mean = data.mean()\n",
    "            try:\n",
    "                ajust_factor = train_mean / test_mean\n",
    "                if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                    to_exclude.append(feature)\n",
    "                    print(feature)\n",
    "                else:\n",
    "                    ajusted_test[feature] *= ajust_factor\n",
    "            except:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature)\n",
    "    return to_exclude, ajusted_test\n",
    "features = [i for i in new_train.columns if i not in [\"game_session\"]]\n",
    "to_exclude, ajusted_test = exclude(new_train, new_test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(train):\n",
    "    X_train = train.drop(['accuracy_group'],axis=1) \n",
    "    y_train = train.accuracy_group.copy()\n",
    "    y_train.loc[y_train <=1] = 0\n",
    "    y_train.loc[y_train >=2] = 1\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = [i for i in X_train.columns if \"_4235\" in i or i == \"world_\"+str(activities_world[\"NONE\"])\n",
    "                      or i in to_exclude]\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "\n",
    "    n_folds=5\n",
    "    skf=GroupKFold(n_splits = n_folds)\n",
    "    models = []\n",
    "    lgbm_params = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial','bagging_fraction': 0.9605425291685099,'bagging_freq': 4,'colsample_bytree': 0.6784238046856443,\n",
    " 'feature_fraction': 1,'learning_rate': 0.017891320270412462,'max_depth': 7,\n",
    " 'min_data_in_leaf': 8,'min_sum_hessian_in_leaf': 17,'num_leaves': 17}\n",
    "\n",
    "    valid = pd.DataFrame(np.zeros([X_train.shape[0]]))\n",
    "    features_list = [i for i in X_train.columns if i != \"installation_id\"]\n",
    "    feature_importance_df = pd.DataFrame(features_list, columns=[\"Feature\"])\n",
    "    for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train, X_train[\"installation_id\"])):\n",
    "        print(\"Fold \"+str(i+1))\n",
    "        X_train2 = X_train.iloc[train_index,:]\n",
    "        y_train2 = y_train.iloc[train_index]\n",
    "        X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "        X_test2 = X_train.iloc[test_index,:]\n",
    "        y_test2 = y_train.iloc[test_index]\n",
    "        X_test2 = X_test2.drop(['installation_id'],axis=1)\n",
    "            \n",
    "        lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "        lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "        clf = lgb.train(lgbm_params, lgb_train,valid_sets=[lgb_train, lgb_eval],\n",
    "            num_boost_round=10000,early_stopping_rounds=100,verbose_eval = 500,categorical_feature = categoricals)\n",
    "        test_predict = clf.predict(X_test2, num_iteration = clf.best_iteration)\n",
    "        valid.iloc[test_index] = test_predict.reshape(X_test2.shape[0], 1)\n",
    "        feature_importance_df[\"Fold_\"+str(i+1)] = clf.feature_importance()\n",
    "                \n",
    "    print(\"logloss = \\t {}\".format(log_loss(y_train, valid)))\n",
    "    print(\"ROC = \\t {}\".format(roc_auc_score(y_train, valid)))\n",
    "    feature_importance_df[\"Average\"] = np.mean(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "    feature_importance_df[\"Std\"] = np.std(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "    feature_importance_df[\"Cv\"] = feature_importance_df[\"Std\"] / feature_importance_df[\"Average\"]\n",
    "    return feature_importance_df\n",
    "#df_for_classification = feature_selection(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_class(train, test, fea, select_flg):\n",
    "    X_train = train.drop(['accuracy_group'],axis=1) \n",
    "    y_train = train.accuracy_group.copy()\n",
    "    y_train.loc[y_train <=1] = 0\n",
    "    y_train.loc[y_train >=2] = 1\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = [i for i in X_train.columns if \"_4235\" in i or i == \"world_\"+str(activities_world[\"NONE\"])\n",
    "                      or i in to_exclude ]\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    if select_flg == True:\n",
    "        X_train = X_train[fea]\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "\n",
    "    X_test = test.drop([\"installation_id\",\"accuracy_group\"], axis=1)\n",
    "    X_test = X_test.drop(remove_features, axis=1)\n",
    "    if select_flg == True:\n",
    "        X_test = X_test[fea]\n",
    "    X_test = X_test[sorted(X_test.columns.tolist())]\n",
    "    print(X_test.shape[1])\n",
    "    \n",
    "    n_folds=5\n",
    "    skf=GroupKFold(n_splits = n_folds)\n",
    "    models = []\n",
    "    lgbm_params = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial','bagging_fraction': 0.9605425291685099,'bagging_freq': 4,'colsample_bytree': 0.6784238046856443,\n",
    " 'feature_fraction': 1,'learning_rate': 0.017891320270412462,'max_depth': 7,\n",
    " 'min_data_in_leaf': 8,'min_sum_hessian_in_leaf': 17,'num_leaves': 17}\n",
    "\n",
    "    valid = pd.DataFrame(np.zeros([X_train.shape[0]]))\n",
    "    for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train, X_train[\"installation_id\"])):\n",
    "        print(\"Fold \"+str(i+1))\n",
    "        X_train2 = X_train.iloc[train_index,:]\n",
    "        y_train2 = y_train.iloc[train_index]\n",
    "        X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "        X_test2 = X_train.iloc[test_index,:]\n",
    "        y_test2 = y_train.iloc[test_index]\n",
    "        X_test2 = X_test2.drop(['installation_id'],axis=1)\n",
    "        if select_flg == True:\n",
    "            X_train2 = X_train2[fea] \n",
    "            X_test2 = X_test2[fea]\n",
    "            \n",
    "        lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "        lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "        \n",
    "        clf = lgb.train(lgbm_params, lgb_train,valid_sets=[lgb_train, lgb_eval],\n",
    "            num_boost_round=10000,early_stopping_rounds=100,verbose_eval = 500,categorical_feature = categoricals)\n",
    "        test_predict = clf.predict(X_test2, num_iteration = clf.best_iteration)\n",
    "        models.append(clf)\n",
    "        valid.iloc[test_index] = test_predict.reshape(X_test2.shape[0], 1)\n",
    "                \n",
    "    print(\"logloss = \\t {}\".format(log_loss(y_train, valid)))\n",
    "    print(\"ROC = \\t {}\".format(roc_auc_score(y_train, valid)))\n",
    "    print('Accuracy score = \\t {}'.format(accuracy_score(y_train, np.round(valid))))\n",
    "    print('Precision score = \\t {}'.format(precision_score(y_train, np.round(valid))))\n",
    "    print('Recall score =   \\t {}'.format(recall_score(y_train, np.round(valid))))\n",
    "    print('F1 score =      \\t {}'.format(f1_score(y_train, np.round(valid))))\n",
    "    print(confusion_matrix(y_train, np.round(valid)))\n",
    "    pred_value = np.zeros([X_test.shape[0]])\n",
    "    for model in models:\n",
    "        pred_value += model.predict(X_test, num_iteration = model.best_iteration) / len(models)\n",
    "    return pred_value, valid\n",
    "\n",
    "#tmp = df_for_classification.sort_values(\"Cv\", ascending = True).reset_index(drop=True).copy()\n",
    "#feat = tmp[tmp.index <= 120][\"Feature\"]\n",
    "#feat = []\n",
    "#pred_value, valid = accuracy_class(new_train, new_test, feat, False)\n",
    "#pred_value, valid = accuracy_class(mod_train, new_test, feat, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score = 0\n",
    "#for i in range(10):\n",
    "#    optR = OptimizedRounder()\n",
    "#    #optR.fit(np.array(valid).reshape(-1,), new_train.accuracy_group, random_flg=True)\n",
    "#    optR.fit(np.array(valid).reshape(-1,), mod_train.accuracy_group, random_flg=True)\n",
    "#    coefficients = optR.coefficients()\n",
    "#    final_valid_pred = optR.predict(np.array(valid).reshape(-1,), coefficients)\n",
    "    #score = qwk(new_train.accuracy_group, final_valid_pred)\n",
    "    #score = qwk(mod_train.accuracy_group, final_valid_pred)\n",
    "#    print(i, np.sort(coefficients), score)\n",
    "#    if score > best_score:\n",
    "#        best_score = score\n",
    "#        best_coefficients = coefficients\n",
    "#final_test_pred = pd.cut(np.array(pred_value).reshape(-1,), [-np.inf] + list(np.sort(best_coefficients)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "#final_test_pred = pd.cut(np.array(pred_value).reshape(-1,), [-np.inf] + list(np.sort([0.31635244, 0.54903181, 0.74679975])) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "#sample_submission[\"accuracy_group\"] = final_test_pred.astype(int)\n",
    "#sample_submission.to_csv('submission.csv', index=False)\n",
    "#sample_submission[\"accuracy_group\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# truncated version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_mod(train):\n",
    "    X_train = train.drop(['accuracy_group'],axis=1) \n",
    "    y_train = train.accuracy_group.copy()\n",
    "    y_train.loc[y_train <=1] = 0\n",
    "    y_train.loc[y_train >=2] = 1\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = [i for i in X_train.columns if \"_4235\" in i or i == \"world_\"+str(activities_world[\"NONE\"])\n",
    "                      or i in to_exclude]\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "\n",
    "    n_folds=5\n",
    "    skf=GroupKFold(n_splits = n_folds)\n",
    "    models = []\n",
    "    lgbm_params = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial','bagging_fraction': 0.9605425291685099,'bagging_freq': 4,'colsample_bytree': 0.6784238046856443,\n",
    " 'feature_fraction': 1,'learning_rate': 0.017891320270412462,'max_depth': 7,\n",
    " 'min_data_in_leaf': 8,'min_sum_hessian_in_leaf': 17,'num_leaves': 17}\n",
    "\n",
    "    valid = pd.DataFrame(np.zeros([X_train.shape[0]]))\n",
    "    features_list = [i for i in X_train.columns if i != \"installation_id\"]\n",
    "    feature_importance_df = pd.DataFrame(features_list, columns=[\"Feature\"])\n",
    "    random_try = 10\n",
    "    for try_time in range(random_try):\n",
    "        valid = np.array([])\n",
    "        real = np.array([])\n",
    "        for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train, X_train[\"installation_id\"])):\n",
    "            print(\"Fold \"+str(i+1))\n",
    "            X_train2 = X_train.iloc[train_index,:]\n",
    "            y_train2 = y_train.iloc[train_index]\n",
    "            X_train2 = X_train2.drop(['installation_id'],axis=1)\n",
    "    \n",
    "            X_test2 = X_train.iloc[test_index,:]\n",
    "            y_test2 = y_train.iloc[test_index]\n",
    "            print(\"Before truncation:\", (X_test2.shape, y_test2.shape))\n",
    "            X_test2['accuracy_group'] = y_test2\n",
    "            np.random.seed(try_time)\n",
    "            X_test2 = X_test2.groupby('installation_id').agg(np.random.choice).reset_index(drop=False)\n",
    "            y_test2 = X_test2.accuracy_group.copy()\n",
    "            X_test2.drop([\"accuracy_group\"], axis=1, inplace=True)\n",
    "            print(\"After truncation:\", (X_test2.shape, y_test2.shape))\n",
    "            X_test2 = X_test2.drop(['installation_id'],axis=1)\n",
    "            \n",
    "            lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "            lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "            clf = lgb.train(lgbm_params, lgb_train,valid_sets=[lgb_train, lgb_eval],\n",
    "                num_boost_round=10000,early_stopping_rounds=100,verbose_eval = 500,categorical_feature = categoricals)\n",
    "            valid_predict = clf.predict(X_test2, num_iteration = clf.best_iteration).reshape(X_test2.shape[0], )\n",
    "            #mean_score += average_precision_score(y_test2,valid_predict) / n_folds\n",
    "            valid = np.concatenate([valid, valid_predict])\n",
    "            real = np.concatenate([real, y_test2])\n",
    "            feature_importance_df[\"Fold_\"+str(i+1)] = clf.feature_importance()\n",
    "                \n",
    "        print(\"logloss = \\t {}\".format(log_loss(real, valid)))\n",
    "        print(\"ROC = \\t {}\".format(roc_auc_score(real, valid)))\n",
    "        feature_importance_df[\"Average\"] = np.mean(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "        feature_importance_df[\"Std\"] = np.std(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "        feature_importance_df[\"Cv\"] = feature_importance_df[\"Std\"] / feature_importance_df[\"Average\"]\n",
    "    return feature_importance_df\n",
    "#df_for_classification = feature_selection_mod(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try 1\n",
      "Fold 1\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((355, 540), (355,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's auc: 0.871178\tvalid_1's auc: 0.727001\n",
      "Fold 2\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((356, 540), (356,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.860579\tvalid_1's auc: 0.725982\n",
      "Fold 3\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((357, 540), (357,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.895116\tvalid_1's auc: 0.737428\n",
      "Fold 4\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's auc: 0.869223\tvalid_1's auc: 0.708753\n",
      "Fold 5\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[281]\ttraining's auc: 0.90965\tvalid_1's auc: 0.722635\n",
      "Fold 6\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.834709\tvalid_1's auc: 0.740752\n",
      "Fold 7\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's auc: 0.885678\tvalid_1's auc: 0.718902\n",
      "Fold 8\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.832011\tvalid_1's auc: 0.72588\n",
      "Fold 9\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's auc: 0.907476\tvalid_1's auc: 0.738733\n",
      "Fold 10\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's auc: 0.860223\tvalid_1's auc: 0.758113\n",
      "logloss = \t 0.6082791961970074\n",
      "ROC = \t 0.7013076978440279\n",
      "0 [0.54357084 0.61391246 0.62987259] 0.36023729\n",
      "1 [0.41237714 0.61307201 0.62991683] 0.34599991\n",
      "2 [0.5091422  0.61763879 0.62326887] 0.35296828\n",
      "3 [0.53708265 0.61348182 0.63054658] 0.35984234\n",
      "4 [0.44296821 0.61326316 0.62988524] 0.34981004\n",
      "5 [0.5254721  0.61316416 0.62814008] 0.35852999\n",
      "6 [0.51608868 0.61316682 0.62940208] 0.35877888\n",
      "7 [0.5195477  0.61406536 0.63079668] 0.35846782\n",
      "8 [0.48373473 0.61329673 0.63112796] 0.35228748\n",
      "9 [0.51862736 0.609487   0.63049324] 0.35754059\n",
      "try 2\n",
      "Fold 1\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((355, 540), (355,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's auc: 0.872561\tvalid_1's auc: 0.722871\n",
      "Fold 2\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((356, 540), (356,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's auc: 0.88258\tvalid_1's auc: 0.736832\n",
      "Fold 3\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((357, 540), (357,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.895354\tvalid_1's auc: 0.734207\n",
      "Fold 4\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[129]\ttraining's auc: 0.88311\tvalid_1's auc: 0.686822\n",
      "Fold 5\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's auc: 0.889766\tvalid_1's auc: 0.730928\n",
      "Fold 6\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.901068\tvalid_1's auc: 0.755564\n",
      "Fold 7\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.850729\tvalid_1's auc: 0.678865\n",
      "Fold 8\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's auc: 0.847228\tvalid_1's auc: 0.728141\n",
      "Fold 9\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[360]\ttraining's auc: 0.918822\tvalid_1's auc: 0.680775\n",
      "Fold 10\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.892931\tvalid_1's auc: 0.769562\n",
      "logloss = \t 0.6024988379870089\n",
      "ROC = \t 0.7075995974106846\n",
      "0 [0.43995652 0.6138209  0.6481638 ] 0.36597982\n",
      "1 [0.40232066 0.61006179 0.64817934] 0.36229272\n",
      "2 [0.43214576 0.60841269 0.65156351] 0.36419826\n",
      "3 [0.38673702 0.60711972 0.71651176] 0.36175508\n",
      "4 [0.48459264 0.61382382 0.64959202] 0.35641751\n",
      "5 [0.43155723 0.61381146 0.64727327] 0.3647997\n",
      "6 [0.44499406 0.61363448 0.70741578] 0.36863788\n",
      "7 [0.43169526 0.60903479 0.70659441] 0.36836791\n",
      "8 [0.44003073 0.60904029 0.70698815] 0.36914038\n",
      "9 [0.4321045  0.60611187 0.65158067] 0.36446826\n",
      "try 3\n",
      "Fold 1\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((355, 540), (355,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's auc: 0.830699\tvalid_1's auc: 0.704358\n",
      "Fold 2\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((356, 540), (356,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's auc: 0.87002\tvalid_1's auc: 0.675916\n",
      "Fold 3\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((357, 540), (357,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.834808\tvalid_1's auc: 0.725379\n",
      "Fold 4\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's auc: 0.883877\tvalid_1's auc: 0.730112\n",
      "Fold 5\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's auc: 0.902589\tvalid_1's auc: 0.731664\n",
      "Fold 6\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's auc: 0.896999\tvalid_1's auc: 0.757964\n",
      "Fold 7\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's auc: 0.871894\tvalid_1's auc: 0.673091\n",
      "Fold 8\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.870125\tvalid_1's auc: 0.715852\n",
      "Fold 9\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[332]\ttraining's auc: 0.915728\tvalid_1's auc: 0.681371\n",
      "Fold 10\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's auc: 0.860223\tvalid_1's auc: 0.740185\n",
      "logloss = \t 0.6175201350731193\n",
      "ROC = \t 0.6876820492506768\n",
      "0 [0.4480525  0.6195585  0.68528899] 0.33861534\n",
      "1 [0.5068698  0.6200983  0.69429025] 0.3441374\n",
      "2 [0.43957807 0.54272277 0.63100381] 0.30983064\n",
      "3 [0.46502196 0.56886821 0.68800297] 0.31596551\n",
      "4 [0.39601927 0.61954145 0.62723589] 0.33295926\n",
      "5 [0.48933445 0.61991466 0.6763819 ] 0.34287422\n",
      "6 [0.47203677 0.52765101 0.67455044] 0.30927072\n",
      "7 [0.43328081 0.61051144 0.63037002] 0.33448131\n",
      "8 [0.43912442 0.62291707 0.67638147] 0.33646813\n",
      "9 [0.5078728  0.53044828 0.68685642] 0.31510348\n",
      "try 4\n",
      "Fold 1\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((355, 540), (355,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's auc: 0.855373\tvalid_1's auc: 0.722309\n",
      "Fold 2\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((356, 540), (356,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[129]\ttraining's auc: 0.883374\tvalid_1's auc: 0.702384\n",
      "Fold 3\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((357, 540), (357,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's auc: 0.899801\tvalid_1's auc: 0.724532\n",
      "Fold 4\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's auc: 0.860444\tvalid_1's auc: 0.708532\n",
      "Fold 5\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttraining's auc: 0.910169\tvalid_1's auc: 0.748042\n",
      "Fold 6\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttraining's auc: 0.899107\tvalid_1's auc: 0.739591\n",
      "Fold 7\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's auc: 0.870077\tvalid_1's auc: 0.695194\n",
      "Fold 8\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's auc: 0.883646\tvalid_1's auc: 0.750833\n",
      "Fold 9\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[386]\ttraining's auc: 0.921812\tvalid_1's auc: 0.700019\n",
      "Fold 10\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's auc: 0.860223\tvalid_1's auc: 0.77517\n",
      "logloss = \t 0.5981744621533952\n",
      "ROC = \t 0.7091059378339111\n",
      "0 [0.53093021 0.60174776 0.66328538] 0.36298597\n",
      "1 [0.45087944 0.60441209 0.66314943] 0.36250108\n",
      "2 [0.45500179 0.60460104 0.67448   ] 0.36206652\n",
      "3 [0.51877754 0.59582466 0.66698214] 0.36092045\n",
      "4 [0.44917848 0.60029405 0.66966598] 0.36290557\n",
      "5 [0.42226955 0.6007953  0.66469695] 0.36125885\n",
      "6 [0.45098139 0.5892454  0.66612822] 0.36245526\n",
      "7 [0.42407153 0.58529273 0.66458773] 0.3588616\n",
      "8 [0.45075911 0.60061035 0.66469827] 0.36415167\n",
      "9 [0.46345494 0.60029861 0.66277367] 0.36243175\n",
      "try 5\n",
      "Fold 1\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((355, 540), (355,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's auc: 0.878303\tvalid_1's auc: 0.726598\n",
      "Fold 2\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((356, 540), (356,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's auc: 0.881124\tvalid_1's auc: 0.719367\n",
      "Fold 3\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((357, 540), (357,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.843914\tvalid_1's auc: 0.741008\n",
      "Fold 4\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's auc: 0.869223\tvalid_1's auc: 0.752866\n",
      "Fold 5\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's auc: 0.89683\tvalid_1's auc: 0.752014\n",
      "Fold 6\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's auc: 0.900003\tvalid_1's auc: 0.731957\n",
      "Fold 7\n",
      "Before truncation: ((1758, 540), (1758,))\n",
      "After truncation: ((359, 540), (359,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's auc: 0.868191\tvalid_1's auc: 0.723872\n",
      "Fold 8\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's auc: 0.876634\tvalid_1's auc: 0.740567\n",
      "Fold 9\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[263]\ttraining's auc: 0.906705\tvalid_1's auc: 0.682747\n",
      "Fold 10\n",
      "Before truncation: ((1757, 540), (1757,))\n",
      "After truncation: ((358, 540), (358,))\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[339]\ttraining's auc: 0.91425\tvalid_1's auc: 0.778408\n",
      "logloss = \t 0.5883185895969566\n",
      "ROC = \t 0.7204816210332412\n",
      "0 [0.47292676 0.59744912 0.71199229] 0.35653506\n",
      "1 [0.41958655 0.60983463 0.71215349] 0.35238078\n",
      "2 [0.46256774 0.59651975 0.72725274] 0.3529561\n",
      "3 [0.4718677  0.59170574 0.67501229] 0.35119534\n",
      "4 [0.47243424 0.59549777 0.71329391] 0.35690592\n",
      "5 [0.43029804 0.59211474 0.71212676] 0.35411286\n",
      "6 [0.47449714 0.57942019 0.7133396 ] 0.35348851\n",
      "7 [0.39718344 0.59567476 0.66014085] 0.34670655\n",
      "8 [0.44073655 0.59503326 0.70085637] 0.35204086\n",
      "9 [0.44744618 0.59501041 0.71174048] 0.35390127\n"
     ]
    }
   ],
   "source": [
    "def accuracy_class_mod(train, test, fea, select_flg):\n",
    "    #X_train = train.drop([\"accuracy_group\"],axis=1) \n",
    "    y_train = train.accuracy_group.copy()\n",
    "    X_train = train.rename(columns={\"accuracy_group\": \"past_target\"})\n",
    "    y_train.loc[y_train <=1] = 0\n",
    "    y_train.loc[y_train >=2] = 1\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(X_train[\"installation_id\"]))\n",
    "    X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "    remove_features = [i for i in X_train.columns if \"_4235\" in i or i == \"world_\"+str(activities_world[\"NONE\"])\n",
    "                      or i in to_exclude]\n",
    "    for i in X_train.columns:\n",
    "        if X_train[i].std() == 0 and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    if select_flg == True:\n",
    "        X_train = X_train[fea]\n",
    "    X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "\n",
    "    X_test = test.drop([\"installation_id\",\"accuracy_group\"], axis=1)\n",
    "    X_test = X_test.drop(remove_features, axis=1)\n",
    "    if select_flg == True:\n",
    "        X_test = X_test[fea]\n",
    "    X_test = X_test[sorted(X_test.columns.tolist())]\n",
    "    \n",
    "    n_folds = 10\n",
    "    skf=GroupKFold(n_splits = n_folds)\n",
    "    models = []\n",
    "    lgbm_params = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial','bagging_fraction': 0.5698056418890787,'bagging_freq': 4,\n",
    " 'colsample_bytree': 0.37564408454469,'learning_rate': 0.015433389422506185,'max_depth': 8,\n",
    " 'min_data_in_leaf': 51,\n",
    " 'min_sum_hessian_in_leaf': 10,\n",
    " 'num_leaves': 48}\n",
    "    valid = pd.DataFrame(np.zeros([X_train.shape[0]]))\n",
    "    random_try = 5\n",
    "    for try_time in range(random_try):\n",
    "        print(\"try\", try_time+1)\n",
    "        valid = np.array([])\n",
    "        real = np.array([])\n",
    "        target = np.array([])\n",
    "        # model learning ---------------------------\n",
    "        for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train, X_train[\"installation_id\"])):\n",
    "            print(\"Fold \"+str(i+1))\n",
    "            X_train2 = X_train.iloc[train_index,:]\n",
    "            y_train2 = y_train.iloc[train_index]\n",
    "            X_train2 = X_train2.drop(['installation_id', 'past_target'],axis=1)\n",
    "    \n",
    "            X_test2 = X_train.iloc[test_index,:]\n",
    "            y_test2 = y_train.iloc[test_index]\n",
    "            print(\"Before truncation:\", (X_test2.shape, y_test2.shape))\n",
    "            X_test2['accuracy_group'] = y_test2\n",
    "            np.random.seed(try_time)\n",
    "            X_test2_mod = X_test2.groupby('installation_id').agg(np.random.choice).reset_index(drop=False)\n",
    "            y_test2_mod = X_test2_mod.accuracy_group.copy()\n",
    "            tmp_target = X_test2_mod.past_target.copy()\n",
    "            X_test2_mod.drop([\"accuracy_group\"], axis=1, inplace=True)\n",
    "            print(\"After truncation:\", (X_test2_mod.shape, y_test2_mod.shape))\n",
    "            X_test2_mod = X_test2_mod.drop(['installation_id', 'past_target'],axis=1)\n",
    "            \n",
    "            lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "            lgb_eval = lgb.Dataset(X_test2_mod, y_test2_mod, reference=lgb_train)\n",
    "            clf = lgb.train(lgbm_params, lgb_train,valid_sets=[lgb_train, lgb_eval],\n",
    "                num_boost_round=10000,early_stopping_rounds=100,verbose_eval = 500,categorical_feature = categoricals)\n",
    "            valid_predict = clf.predict(X_test2_mod, num_iteration = clf.best_iteration).reshape(X_test2_mod.shape[0], )\n",
    "            valid = np.concatenate([valid, valid_predict])\n",
    "            real = np.concatenate([real, y_test2_mod])\n",
    "            target = np.concatenate([target, tmp_target])\n",
    "            \n",
    "            models.append(clf)\n",
    "        print(\"logloss = \\t {}\".format(log_loss(real, valid)))\n",
    "        print(\"ROC = \\t {}\".format(roc_auc_score(real, valid)))\n",
    "        \n",
    "        # threshold optimization --------------\n",
    "        best_score = 0\n",
    "        for i in range(10):\n",
    "            optR = OptimizedRounder()\n",
    "            optR.fit(np.array(valid).reshape(-1,), target, random_flg=True)\n",
    "            coefficients = optR.coefficients()\n",
    "            final_valid_pred = optR.predict(np.array(valid).reshape(-1,), coefficients)\n",
    "            score = qwk(target, final_valid_pred)\n",
    "            print(i, np.sort(coefficients), score)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_coefficients = coefficients\n",
    "        if try_time == 0:\n",
    "            final_coefficients = np.sort(best_coefficients) / random_try\n",
    "        else:\n",
    "            final_coefficients += np.sort(best_coefficients) / random_try\n",
    "            \n",
    "    # test prediction  ------------------------\n",
    "    pred_value = np.zeros([X_test.shape[0]])\n",
    "    for model in models:\n",
    "        pred_value += model.predict(X_test, num_iteration = model.best_iteration) / len(models)\n",
    "    return pred_value, valid, final_coefficients\n",
    "\n",
    "#tmp = df_for_classification.sort_values(\"Cv\", ascending = True).reset_index(drop=True).copy()\n",
    "#feat = tmp[tmp.index <= 120][\"Feature\"]\n",
    "feat = []\n",
    "pred_value, valid, final_coefficients = accuracy_class_mod(new_train, new_test, feat, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hyperopt(X, Y):\n",
    "    def para_tuning_obj(params):\n",
    "        params = {\n",
    "        'boosting_type': 'gbdt', \n",
    "        'metric': \"auc\", \n",
    "        'objective': 'binary', \n",
    "        'eval_metric': 'cappa', \n",
    "        \"tree_learner\": \"serial\",\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'bagging_freq': int(params['bagging_freq']),\n",
    "        'bagging_fraction': float(params['bagging_fraction']),\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'learning_rate': float(params['learning_rate']),\n",
    "        'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "        'min_sum_hessian_in_leaf': int(params['min_sum_hessian_in_leaf']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "}\n",
    "    \n",
    "        real = np.array([])\n",
    "        pred = np.array([])\n",
    "        skf = GroupKFold(n_splits=10)\n",
    "        for trn_idx, val_idx in skf.split(X, Y, X[\"installation_id\"]):\n",
    "            x_train, x_val = X.iloc[trn_idx, :], X.iloc[val_idx, :]\n",
    "            y_train, y_val = Y.iloc[trn_idx], Y.iloc[val_idx]\n",
    "            x_val['accuracy_group'] = y_val\n",
    "            np.random.seed(0)\n",
    "            x_val_mod = x_val.groupby('installation_id').agg(np.random.choice).reset_index(drop=False)\n",
    "            y_val_mod = x_val_mod.accuracy_group.copy()\n",
    "            x_train.drop('installation_id', inplace = True, axis = 1)\n",
    "            x_val_mod.drop(['installation_id', \"accuracy_group\"], inplace = True, axis = 1)\n",
    "            train_set = lgb.Dataset(x_train, y_train, categorical_feature = ['session_title'])\n",
    "            val_set = lgb.Dataset(x_val_mod, y_val_mod, categorical_feature = ['session_title'])\n",
    "        \n",
    "            clf = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = 300)\n",
    "            pred = np.concatenate((pred, np.array(clf.predict(x_val_mod, num_iteration = clf.best_iteration))), axis=0) \n",
    "            real = np.concatenate((real, np.array(y_val_mod)), axis=0) \n",
    "        score = roc_auc_score(real, pred)\n",
    "    \n",
    "        return - score\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    space ={\n",
    "        'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "        'bagging_freq': hp.quniform('bagging_freq', 1, 10, 1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.2, 1.0),\n",
    "        'num_leaves': hp.quniform('num_leaves', 8, 64, 1),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
    "        'min_data_in_leaf': hp.quniform('min_data_in_leaf', 8, 64, 1),\n",
    "        'min_sum_hessian_in_leaf': hp.quniform('min_sum_hessian_in_leaf', 5, 30, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0)\n",
    "    }\n",
    "\n",
    "    best = fmin(para_tuning_obj, space = space, algo=tpe.suggest, max_evals=10, trials=trials, verbose=1)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    return best_params\n",
    "\n",
    "#X_train = new_train.drop([\"accuracy_group\"], axis=1).copy()\n",
    "#Y = new_train.accuracy_group.copy()\n",
    "#Y.loc[Y <=1] = 0\n",
    "#Y.loc[Y >=2] = 1\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "#lbl.fit(list(X_train[\"installation_id\"]))\n",
    "#X_train[\"installation_id\"] = lbl.transform(list(X_train[\"installation_id\"]))\n",
    "#remove_features = [i for i in X_train.columns if \"_4235\" in i or i == \"world_\"+str(activities_world[\"NONE\"])\n",
    "#                      or i in to_exclude]\n",
    "#for i in X_train.columns:\n",
    "#    if X_train[i].std() == 0 and i not in remove_features:\n",
    "#        remove_features.append(i)\n",
    "#X_train = X_train.drop(remove_features, axis=1)\n",
    "#X_train = X_train[sorted(X_train.columns.tolist())]\n",
    "\n",
    "#random_state = 42\n",
    "#my_hyperopt(X_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.511\n",
       "0    0.184\n",
       "2    0.153\n",
       "1    0.152\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_pred = pd.cut(np.array(pred_value).reshape(-1,), [-np.inf] + list(np.sort(final_coefficients)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "#final_test_pred = pd.cut(np.array(pred_value).reshape(-1,), [-np.inf] + list(np.sort([0.32229148, 0.51887455, 0.77529457])) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "sample_submission[\"accuracy_group\"] = final_test_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission[\"accuracy_group\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "22eb6cc37bfe4fa1a502461734d754d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7d8b08ac2ab649bd8586ad390f77c947",
       "placeholder": "​",
       "style": "IPY_MODEL_4e2be24232d746d594d56ca72abad329",
       "value": " 1000/1000 [01:38&lt;00:00, 10.13it/s]"
      }
     },
     "24523b43de08495fb45fd7bca48ec42a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "287d43b4eddc4f728a14068741e14629": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b404355620c143c281fe8d3ad47942d8",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_24523b43de08495fb45fd7bca48ec42a",
       "value": 17000
      }
     },
     "3446938b966d490f8a219c46a9c92d02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "494ae16bc5904d5bb5c907db49bf4bd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Installation_id: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1edf409e71c426bbfa7ac098f8423dd",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b0555251c03f4e0e9a80cd882abc86ec",
       "value": 1000
      }
     },
     "4e2be24232d746d594d56ca72abad329": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "69917e47998d48058a43be1c5b9cc26e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_287d43b4eddc4f728a14068741e14629",
        "IPY_MODEL_bcfe0b6e9fc54abb847ad3df23479ae2"
       ],
       "layout": "IPY_MODEL_c98d61c45dfc47929e07ca71c771fee5"
      }
     },
     "735fc18067064f1da9686cebb187850a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d8b08ac2ab649bd8586ad390f77c947": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9480a687d4e44d6a90d156b292679655": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0555251c03f4e0e9a80cd882abc86ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b1edf409e71c426bbfa7ac098f8423dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b404355620c143c281fe8d3ad47942d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bcfe0b6e9fc54abb847ad3df23479ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_735fc18067064f1da9686cebb187850a",
       "placeholder": "​",
       "style": "IPY_MODEL_3446938b966d490f8a219c46a9c92d02",
       "value": " 17000/17000 [12:56&lt;00:00, 21.89it/s]"
      }
     },
     "c98d61c45dfc47929e07ca71c771fee5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef53f1c2306c4fc7a87cb8afc119a297": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_494ae16bc5904d5bb5c907db49bf4bd8",
        "IPY_MODEL_22eb6cc37bfe4fa1a502461734d754d0"
       ],
       "layout": "IPY_MODEL_9480a687d4e44d6a90d156b292679655"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
