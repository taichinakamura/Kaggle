{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015999,
     "end_time": "2020-10-22T13:30:59.521896",
     "exception": false,
     "start_time": "2020-10-22T13:30:59.505897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- try fold 7\n",
    "- cancel g,c seq again\n",
    "- remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-22T13:30:59.560575Z",
     "iopub.status.busy": "2020-10-22T13:30:59.559788Z",
     "iopub.status.idle": "2020-10-22T13:31:08.098527Z",
     "shell.execute_reply": "2020-10-22T13:31:08.097348Z"
    },
    "papermill": {
     "duration": 8.561972,
     "end_time": "2020-10-22T13:31:08.098664",
     "exception": false,
     "start_time": "2020-10-22T13:30:59.536692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:08.142869Z",
     "iopub.status.busy": "2020-10-22T13:31:08.141966Z",
     "iopub.status.idle": "2020-10-22T13:31:14.921208Z",
     "shell.execute_reply": "2020-10-22T13:31:14.920061Z"
    },
    "papermill": {
     "duration": 6.807652,
     "end_time": "2020-10-22T13:31:14.921348",
     "exception": false,
     "start_time": "2020-10-22T13:31:08.113696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:14.960686Z",
     "iopub.status.busy": "2020-10-22T13:31:14.958663Z",
     "iopub.status.idle": "2020-10-22T13:31:14.961472Z",
     "shell.execute_reply": "2020-10-22T13:31:14.962029Z"
    },
    "papermill": {
     "duration": 0.025281,
     "end_time": "2020-10-22T13:31:14.962161",
     "exception": false,
     "start_time": "2020-10-22T13:31:14.936880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:14.999564Z",
     "iopub.status.busy": "2020-10-22T13:31:14.998737Z",
     "iopub.status.idle": "2020-10-22T13:31:15.003053Z",
     "shell.execute_reply": "2020-10-22T13:31:15.002443Z"
    },
    "papermill": {
     "duration": 0.024549,
     "end_time": "2020-10-22T13:31:15.003163",
     "exception": false,
     "start_time": "2020-10-22T13:31:14.978614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to remove outlier paienr\n",
    "# original_remove_index = [16674] # remove_index = [15361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.047783Z",
     "iopub.status.busy": "2020-10-22T13:31:15.047137Z",
     "iopub.status.idle": "2020-10-22T13:31:15.143554Z",
     "shell.execute_reply": "2020-10-22T13:31:15.143024Z"
    },
    "papermill": {
     "duration": 0.123903,
     "end_time": "2020-10-22T13:31:15.143670",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.019767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015443,
     "end_time": "2020-10-22T13:31:15.174164",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.158721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.219396Z",
     "iopub.status.busy": "2020-10-22T13:31:15.218434Z",
     "iopub.status.idle": "2020-10-22T13:31:15.538663Z",
     "shell.execute_reply": "2020-10-22T13:31:15.541579Z"
    },
    "papermill": {
     "duration": 0.352018,
     "end_time": "2020-10-22T13:31:15.541828",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.189810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.583701Z",
     "iopub.status.busy": "2020-10-22T13:31:15.582786Z",
     "iopub.status.idle": "2020-10-22T13:31:15.585721Z",
     "shell.execute_reply": "2020-10-22T13:31:15.585173Z"
    },
    "papermill": {
     "duration": 0.02217,
     "end_time": "2020-10-22T13:31:15.585818",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.563648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tmp = train.iloc[15361, :].copy().drop([\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"])\n",
    "#tmp1 = np.array(tmp)\n",
    "#np.sum(tmp1 == 10), np.sum(tmp1==-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.650302Z",
     "iopub.status.busy": "2020-10-22T13:31:15.649270Z",
     "iopub.status.idle": "2020-10-22T13:31:15.707496Z",
     "shell.execute_reply": "2020-10-22T13:31:15.706438Z"
    },
    "papermill": {
     "duration": 0.106894,
     "end_time": "2020-10-22T13:31:15.707630",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.600736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)\n",
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "print(len(non_target_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016008,
     "end_time": "2020-10-22T13:31:15.740542",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.724534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.787832Z",
     "iopub.status.busy": "2020-10-22T13:31:15.786697Z",
     "iopub.status.idle": "2020-10-22T13:31:15.914786Z",
     "shell.execute_reply": "2020-10-22T13:31:15.915521Z"
    },
    "papermill": {
     "duration": 0.152029,
     "end_time": "2020-10-22T13:31:15.915706",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.763677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 872) (3982, 872)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    #tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\", \"cp_dose\", \"cp_time\"], axis=1, inplace=True)\n",
    "    #col = list(tmp.columns)\n",
    "    #col = col[1:] + col[:1]\n",
    "    #tmp = tmp[col]\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:15.959248Z",
     "iopub.status.busy": "2020-10-22T13:31:15.958002Z",
     "iopub.status.idle": "2020-10-22T13:31:16.246476Z",
     "shell.execute_reply": "2020-10-22T13:31:16.247048Z"
    },
    "papermill": {
     "duration": 0.312787,
     "end_time": "2020-10-22T13:31:16.247208",
     "exception": false,
     "start_time": "2020-10-22T13:31:15.934421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = f_train.copy().values\n",
    "select = VarianceThreshold(threshold=0.4)\n",
    "X_new = select.fit_transform(X)\n",
    "drop_feats = list(np.array(f_train.columns)[select.get_support()==False])\n",
    "len(drop_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:16.339465Z",
     "iopub.status.busy": "2020-10-22T13:31:16.336316Z",
     "iopub.status.idle": "2020-10-22T13:31:16.442274Z",
     "shell.execute_reply": "2020-10-22T13:31:16.441696Z"
    },
    "papermill": {
     "duration": 0.17724,
     "end_time": "2020-10-22T13:31:16.442383",
     "exception": false,
     "start_time": "2020-10-22T13:31:16.265143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_train.drop(drop_feats, axis = 1, inplace=True)\n",
    "f_test.drop(drop_feats, axis = 1, inplace=True)\n",
    "\n",
    "modg_feats = [i for i in f_train.columns if \"g-\" in i]\n",
    "modc_feats = [i for i in f_train.columns if \"c-\" in i]\n",
    "\n",
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "#ss = preprocessing.RobustScaler()\n",
    "\n",
    "#fn_train= ss.fit_transform(fn_train)\n",
    "#fn_test = ss.transform(fn_test)\n",
    "\n",
    "#fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:16.492310Z",
     "iopub.status.busy": "2020-10-22T13:31:16.490428Z",
     "iopub.status.idle": "2020-10-22T13:31:16.493145Z",
     "shell.execute_reply": "2020-10-22T13:31:16.493627Z"
    },
    "papermill": {
     "duration": 0.033689,
     "end_time": "2020-10-22T13:31:16.493740",
     "exception": false,
     "start_time": "2020-10-22T13:31:16.460051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "#             targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:16.545183Z",
     "iopub.status.busy": "2020-10-22T13:31:16.544468Z",
     "iopub.status.idle": "2020-10-22T13:31:16.548438Z",
     "shell.execute_reply": "2020-10-22T13:31:16.547890Z"
    },
    "papermill": {
     "duration": 0.036971,
     "end_time": "2020-10-22T13:31:16.548547",
     "exception": false,
     "start_time": "2020-10-22T13:31:16.511576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, c_lstm_hidden_size, last_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g_layer_num = 1\n",
    "        self.c_layer_num = 1\n",
    "\n",
    "        self.hidden_dim = 512\n",
    "        self.hidden_dim_c = 10\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_hidden_size, self.hidden_dim, batch_first=True, bidirectional=True, num_layers=self.g_layer_num)\n",
    "        self.c_lstm = nn.LSTM(c_lstm_hidden_size, self.hidden_dim_c, batch_first=True, bidirectional=True, num_layers=self.c_layer_num)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d((self.hidden_dim+self.hidden_dim_c) * 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.utils.weight_norm(nn.Linear((self.hidden_dim+self.hidden_dim_c) * 2, last_num))\n",
    "        \n",
    "    def forward(self, cont_g, cont_c): \n",
    "        cont_g = torch.unsqueeze(cont_g, 1)\n",
    "        h_lstm, lstm_out = self.lstm(cont_g) # h_lstm: 256 * 1 * (2 * 512)\n",
    "        conc_g = h_lstm.view(-1, self.hidden_dim * 2)\n",
    "        \n",
    "        cont_c = torch.unsqueeze(cont_c, 1)\n",
    "        h_lstm_c, lstm_out_c = self.c_lstm(cont_c) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        conc_c = h_lstm_c.view(-1, self.hidden_dim_c * 2)\n",
    "        \n",
    "        conc = torch.cat((conc_g, conc_c),1)\n",
    "        conc = self.batch_norm(conc)\n",
    "        dropped = self.dropout(conc)\n",
    "        out = self.out(dropped)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:16.617399Z",
     "iopub.status.busy": "2020-10-22T13:31:16.596599Z",
     "iopub.status.idle": "2020-10-22T13:31:17.052120Z",
     "shell.execute_reply": "2020-10-22T13:31:17.051044Z"
    },
    "papermill": {
     "duration": 0.486568,
     "end_time": "2020-10-22T13:31:17.052236",
     "exception": false,
     "start_time": "2020-10-22T13:31:16.565668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_epochs = 30\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def modelling_lstm(tr, target, te, sample_seed, last_num):\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "    \n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    X_test_g = torch.tensor(X_test[:,:len(modg_feats)], dtype=torch.float32)\n",
    "    X_test_c = torch.tensor(X_test[:,len(modg_feats):], dtype=torch.float32)\n",
    "\n",
    "    X_test = torch.utils.data.TensorDataset(X_test_g, X_test_c) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2_g = torch.tensor(X_train[train_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_valid2_g = torch.tensor(X_train[valid_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_train2_c = torch.tensor(X_train[train_index,len(modg_feats):], dtype=torch.float32)\n",
    "        X_valid2_c = torch.tensor(X_train[valid_index,len(modg_feats):], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2_g, X_train2_c, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2_g, X_valid2_c, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        #loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.01, weight_decay=1e-5) \n",
    "        #lookahead = Lookahead(optimizer, k=3, alpha=0.5) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        #scheduler2 = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e1, \n",
    "        #                                      max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch_g, x_batch_c, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best: Epoch {} \\t loss={:.6f}  val_loss={:.6f}  sm_loss={:.6f} \\t sm_val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        pred_model = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, ) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "\n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:31:17.098797Z",
     "iopub.status.busy": "2020-10-22T13:31:17.098059Z",
     "iopub.status.idle": "2020-10-22T13:40:54.877220Z",
     "shell.execute_reply": "2020-10-22T13:40:54.877685Z"
    },
    "papermill": {
     "duration": 577.807205,
     "end_time": "2020-10-22T13:40:54.877870",
     "exception": false,
     "start_time": "2020-10-22T13:31:17.070665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.138425  val_loss=0.019388  sm_loss=0.138421 \t sm_val_loss=0.019402 \t time=0.97s\n",
      "Best: Epoch 2 \t loss=0.018247  val_loss=0.018068  sm_loss=0.018316 \t sm_val_loss=0.018146 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017401  val_loss=0.017908  sm_loss=0.017509 \t sm_val_loss=0.017997 \t time=0.79s\n",
      "Best: Epoch 4 \t loss=0.016917  val_loss=0.017591  sm_loss=0.017062 \t sm_val_loss=0.017697 \t time=0.87s\n",
      "Best: Epoch 5 \t loss=0.016624  val_loss=0.017346  sm_loss=0.016787 \t sm_val_loss=0.017480 \t time=0.98s\n",
      "Best: Epoch 6 \t loss=0.016408  val_loss=0.017232  sm_loss=0.016581 \t sm_val_loss=0.017358 \t time=0.77s\n",
      "Best: Epoch 7 \t loss=0.016333  val_loss=0.017218  sm_loss=0.016505 \t sm_val_loss=0.017323 \t time=0.81s\n",
      "Best: Epoch 9 \t loss=0.016340  val_loss=0.017202  sm_loss=0.016505 \t sm_val_loss=0.017331 \t time=0.82s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014608  val_loss=0.016270  sm_loss=0.014805 \t sm_val_loss=0.016436 \t time=0.78s\n",
      "Best: Epoch 15 \t loss=0.013284  val_loss=0.016168  sm_loss=0.013513 \t sm_val_loss=0.016346 \t time=0.79s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.016354043684129477\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.139359  val_loss=0.019579  sm_loss=0.139357 \t sm_val_loss=0.019599 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018613  val_loss=0.018123  sm_loss=0.018678 \t sm_val_loss=0.018201 \t time=0.73s\n",
      "Best: Epoch 3 \t loss=0.017598  val_loss=0.017622  sm_loss=0.017712 \t sm_val_loss=0.017729 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017067  val_loss=0.017359  sm_loss=0.017206 \t sm_val_loss=0.017482 \t time=0.96s\n",
      "Best: Epoch 5 \t loss=0.016661  val_loss=0.017285  sm_loss=0.016829 \t sm_val_loss=0.017398 \t time=0.79s\n",
      "Best: Epoch 6 \t loss=0.016434  val_loss=0.016931  sm_loss=0.016603 \t sm_val_loss=0.017060 \t time=0.74s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014444  val_loss=0.016066  sm_loss=0.014657 \t sm_val_loss=0.016242 \t time=0.87s\n",
      "Best: Epoch 12 \t loss=0.013176  val_loss=0.015952  sm_loss=0.013411 \t sm_val_loss=0.016124 \t time=0.75s\n",
      "Best: Epoch 13 \t loss=0.012441  val_loss=0.015908  sm_loss=0.012681 \t sm_val_loss=0.016100 \t time=0.74s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.016092593299347226\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.136996  val_loss=0.019706  sm_loss=0.136992 \t sm_val_loss=0.019734 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018372  val_loss=0.018191  sm_loss=0.018447 \t sm_val_loss=0.018243 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017390  val_loss=0.017657  sm_loss=0.017513 \t sm_val_loss=0.017760 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017195  val_loss=0.017647  sm_loss=0.017331 \t sm_val_loss=0.017773 \t time=0.74s\n",
      "Best: Epoch 5 \t loss=0.016628  val_loss=0.017424  sm_loss=0.016788 \t sm_val_loss=0.017538 \t time=0.76s\n",
      "Best: Epoch 8 \t loss=0.016422  val_loss=0.017236  sm_loss=0.016581 \t sm_val_loss=0.017343 \t time=0.76s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014529  val_loss=0.016361  sm_loss=0.014738 \t sm_val_loss=0.016528 \t time=0.80s\n",
      "Best: Epoch 14 \t loss=0.013222  val_loss=0.016210  sm_loss=0.013455 \t sm_val_loss=0.016390 \t time=0.86s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.01630203982409531\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.140935  val_loss=0.019605  sm_loss=0.140927 \t sm_val_loss=0.019638 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018515  val_loss=0.018297  sm_loss=0.018578 \t sm_val_loss=0.018359 \t time=0.87s\n",
      "Best: Epoch 3 \t loss=0.017488  val_loss=0.018051  sm_loss=0.017598 \t sm_val_loss=0.018127 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.016921  val_loss=0.017671  sm_loss=0.017065 \t sm_val_loss=0.017809 \t time=0.77s\n",
      "Best: Epoch 5 \t loss=0.016723  val_loss=0.017463  sm_loss=0.016886 \t sm_val_loss=0.017581 \t time=0.75s\n",
      "Best: Epoch 7 \t loss=0.016369  val_loss=0.017400  sm_loss=0.016544 \t sm_val_loss=0.017535 \t time=0.81s\n",
      "Best: Epoch 8 \t loss=0.016294  val_loss=0.017205  sm_loss=0.016468 \t sm_val_loss=0.017336 \t time=0.75s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014606  val_loss=0.016406  sm_loss=0.014816 \t sm_val_loss=0.016560 \t time=0.74s\n",
      "Best: Epoch 14 \t loss=0.013342  val_loss=0.016283  sm_loss=0.013578 \t sm_val_loss=0.016445 \t time=0.74s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.01638419715608156\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.138680  val_loss=0.019689  sm_loss=0.138675 \t sm_val_loss=0.019700 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018304  val_loss=0.018300  sm_loss=0.018375 \t sm_val_loss=0.018367 \t time=0.97s\n",
      "Best: Epoch 3 \t loss=0.017393  val_loss=0.017998  sm_loss=0.017512 \t sm_val_loss=0.018070 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017002  val_loss=0.017706  sm_loss=0.017147 \t sm_val_loss=0.017812 \t time=0.75s\n",
      "Best: Epoch 5 \t loss=0.016669  val_loss=0.017673  sm_loss=0.016828 \t sm_val_loss=0.017795 \t time=0.89s\n",
      "Best: Epoch 6 \t loss=0.016517  val_loss=0.017378  sm_loss=0.016685 \t sm_val_loss=0.017507 \t time=0.74s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014652  val_loss=0.016494  sm_loss=0.014858 \t sm_val_loss=0.016643 \t time=0.74s\n",
      "Best: Epoch 12 \t loss=0.013417  val_loss=0.016327  sm_loss=0.013644 \t sm_val_loss=0.016494 \t time=0.75s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best: Epoch 18 \t loss=0.009329  val_loss=0.016327  sm_loss=0.009591 \t sm_val_loss=0.016485 \t time=0.77s\n",
      "Fold 5 log loss: 0.01632276691796761\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.139245  val_loss=0.019723  sm_loss=0.139243 \t sm_val_loss=0.019746 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018406  val_loss=0.018379  sm_loss=0.018474 \t sm_val_loss=0.018437 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017423  val_loss=0.017981  sm_loss=0.017536 \t sm_val_loss=0.018062 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016953  val_loss=0.017553  sm_loss=0.017096 \t sm_val_loss=0.017673 \t time=0.74s\n",
      "Best: Epoch 5 \t loss=0.016555  val_loss=0.017459  sm_loss=0.016721 \t sm_val_loss=0.017592 \t time=0.74s\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 10 \t loss=0.014529  val_loss=0.016540  sm_loss=0.014734 \t sm_val_loss=0.016697 \t time=0.74s\n",
      "Best: Epoch 11 \t loss=0.013290  val_loss=0.016439  sm_loss=0.013516 \t sm_val_loss=0.016610 \t time=0.75s\n",
      "Best: Epoch 12 \t loss=0.012588  val_loss=0.016387  sm_loss=0.012825 \t sm_val_loss=0.016546 \t time=0.77s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.01650725176883766\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.137244  val_loss=0.019476  sm_loss=0.137241 \t sm_val_loss=0.019514 \t time=0.73s\n",
      "Best: Epoch 2 \t loss=0.018484  val_loss=0.018236  sm_loss=0.018551 \t sm_val_loss=0.018313 \t time=0.74s\n",
      "Best: Epoch 3 \t loss=0.017481  val_loss=0.017860  sm_loss=0.017592 \t sm_val_loss=0.017951 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.017045  val_loss=0.017757  sm_loss=0.017183 \t sm_val_loss=0.017865 \t time=1.03s\n",
      "Best: Epoch 5 \t loss=0.016644  val_loss=0.017304  sm_loss=0.016800 \t sm_val_loss=0.017428 \t time=0.77s\n",
      "Best: Epoch 7 \t loss=0.016481  val_loss=0.017095  sm_loss=0.016650 \t sm_val_loss=0.017233 \t time=0.75s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014709  val_loss=0.016276  sm_loss=0.014916 \t sm_val_loss=0.016428 \t time=0.74s\n",
      "Best: Epoch 13 \t loss=0.013445  val_loss=0.016123  sm_loss=0.013675 \t sm_val_loss=0.016288 \t time=0.88s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.015993800772876057\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016354043684129477\n",
      "Fold 2 log loss: 0.016092593299347226\n",
      "Fold 3 log loss: 0.01630203982409531\n",
      "Fold 4 log loss: 0.01638419715608156\n",
      "Fold 5 log loss: 0.01632276691796761\n",
      "Fold 6 log loss: 0.01650725176883766\n",
      "Fold 7 log loss: 0.015993800772876057\n",
      "Std of log loss: 0.00016358805020450312\n",
      "Total log loss: 0.016279531460458404\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.134456  val_loss=0.019434  sm_loss=0.134453 \t sm_val_loss=0.019462 \t time=0.78s\n",
      "Best: Epoch 2 \t loss=0.018365  val_loss=0.018066  sm_loss=0.018435 \t sm_val_loss=0.018142 \t time=0.96s\n",
      "Best: Epoch 3 \t loss=0.017391  val_loss=0.017723  sm_loss=0.017508 \t sm_val_loss=0.017824 \t time=0.89s\n",
      "Best: Epoch 4 \t loss=0.016932  val_loss=0.017510  sm_loss=0.017077 \t sm_val_loss=0.017618 \t time=0.81s\n",
      "Best: Epoch 5 \t loss=0.016707  val_loss=0.017266  sm_loss=0.016868 \t sm_val_loss=0.017398 \t time=0.80s\n",
      "Best: Epoch 8 \t loss=0.016430  val_loss=0.017249  sm_loss=0.016583 \t sm_val_loss=0.017373 \t time=1.03s\n",
      "Best: Epoch 10 \t loss=0.016442  val_loss=0.017139  sm_loss=0.016595 \t sm_val_loss=0.017268 \t time=0.74s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.015019  val_loss=0.016390  sm_loss=0.015214 \t sm_val_loss=0.016557 \t time=0.75s\n",
      "Best: Epoch 16 \t loss=0.013678  val_loss=0.016216  sm_loss=0.013904 \t sm_val_loss=0.016392 \t time=0.87s\n",
      "Best: Epoch 17 \t loss=0.012886  val_loss=0.016151  sm_loss=0.013126 \t sm_val_loss=0.016323 \t time=0.77s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.016336388839679448\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.140537  val_loss=0.019472  sm_loss=0.140536 \t sm_val_loss=0.019511 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018524  val_loss=0.017922  sm_loss=0.018597 \t sm_val_loss=0.018009 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017493  val_loss=0.017457  sm_loss=0.017608 \t sm_val_loss=0.017549 \t time=1.14s\n",
      "Best: Epoch 4 \t loss=0.017010  val_loss=0.017130  sm_loss=0.017146 \t sm_val_loss=0.017274 \t time=0.80s\n",
      "Best: Epoch 6 \t loss=0.016526  val_loss=0.017101  sm_loss=0.016686 \t sm_val_loss=0.017230 \t time=0.79s\n",
      "Best: Epoch 9 \t loss=0.016425  val_loss=0.017064  sm_loss=0.016585 \t sm_val_loss=0.017189 \t time=0.73s\n",
      "Best: Epoch 10 \t loss=0.016420  val_loss=0.017040  sm_loss=0.016581 \t sm_val_loss=0.017180 \t time=1.09s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014829  val_loss=0.016068  sm_loss=0.015026 \t sm_val_loss=0.016239 \t time=0.74s\n",
      "Best: Epoch 16 \t loss=0.013559  val_loss=0.015973  sm_loss=0.013788 \t sm_val_loss=0.016148 \t time=0.96s\n",
      "Best: Epoch 17 \t loss=0.012758  val_loss=0.015965  sm_loss=0.012995 \t sm_val_loss=0.016141 \t time=0.75s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.016158281290575543\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.139193  val_loss=0.019623  sm_loss=0.139194 \t sm_val_loss=0.019639 \t time=1.04s\n",
      "Best: Epoch 2 \t loss=0.018402  val_loss=0.018221  sm_loss=0.018468 \t sm_val_loss=0.018276 \t time=0.78s\n",
      "Best: Epoch 3 \t loss=0.017516  val_loss=0.017825  sm_loss=0.017623 \t sm_val_loss=0.017897 \t time=0.77s\n",
      "Best: Epoch 5 \t loss=0.016714  val_loss=0.017432  sm_loss=0.016880 \t sm_val_loss=0.017542 \t time=0.75s\n",
      "Best: Epoch 7 \t loss=0.016375  val_loss=0.017146  sm_loss=0.016545 \t sm_val_loss=0.017296 \t time=0.95s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014765  val_loss=0.016404  sm_loss=0.014964 \t sm_val_loss=0.016566 \t time=0.86s\n",
      "Best: Epoch 13 \t loss=0.013460  val_loss=0.016229  sm_loss=0.013682 \t sm_val_loss=0.016401 \t time=0.76s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.016313486379044995\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.141893  val_loss=0.019440  sm_loss=0.141887 \t sm_val_loss=0.019476 \t time=0.76s\n",
      "Best: Epoch 2 \t loss=0.018355  val_loss=0.018052  sm_loss=0.018426 \t sm_val_loss=0.018145 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017358  val_loss=0.017619  sm_loss=0.017474 \t sm_val_loss=0.017716 \t time=0.79s\n",
      "Best: Epoch 4 \t loss=0.016903  val_loss=0.017544  sm_loss=0.017048 \t sm_val_loss=0.017645 \t time=0.93s\n",
      "Best: Epoch 5 \t loss=0.016527  val_loss=0.017272  sm_loss=0.016694 \t sm_val_loss=0.017425 \t time=0.75s\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 10 \t loss=0.014452  val_loss=0.016384  sm_loss=0.014661 \t sm_val_loss=0.016542 \t time=0.88s\n",
      "Best: Epoch 11 \t loss=0.013258  val_loss=0.016300  sm_loss=0.013489 \t sm_val_loss=0.016461 \t time=0.76s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.016413255461526288\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.139115  val_loss=0.019742  sm_loss=0.139116 \t sm_val_loss=0.019767 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018386  val_loss=0.018279  sm_loss=0.018454 \t sm_val_loss=0.018345 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017453  val_loss=0.017934  sm_loss=0.017566 \t sm_val_loss=0.018003 \t time=1.06s\n",
      "Best: Epoch 4 \t loss=0.016926  val_loss=0.017739  sm_loss=0.017071 \t sm_val_loss=0.017857 \t time=0.91s\n",
      "Best: Epoch 5 \t loss=0.016650  val_loss=0.017389  sm_loss=0.016811 \t sm_val_loss=0.017512 \t time=0.76s\n",
      "Best: Epoch 7 \t loss=0.016376  val_loss=0.017303  sm_loss=0.016546 \t sm_val_loss=0.017440 \t time=0.74s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014663  val_loss=0.016500  sm_loss=0.014866 \t sm_val_loss=0.016649 \t time=0.76s\n",
      "Best: Epoch 13 \t loss=0.013452  val_loss=0.016327  sm_loss=0.013681 \t sm_val_loss=0.016490 \t time=0.78s\n",
      "Best: Epoch 14 \t loss=0.012733  val_loss=0.016288  sm_loss=0.012970 \t sm_val_loss=0.016449 \t time=0.87s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.016288205704356323\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.138655  val_loss=0.019656  sm_loss=0.138657 \t sm_val_loss=0.019691 \t time=0.88s\n",
      "Best: Epoch 2 \t loss=0.018354  val_loss=0.018258  sm_loss=0.018419 \t sm_val_loss=0.018345 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017375  val_loss=0.017898  sm_loss=0.017492 \t sm_val_loss=0.017995 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.016880  val_loss=0.017834  sm_loss=0.017024 \t sm_val_loss=0.017954 \t time=0.80s\n",
      "Best: Epoch 5 \t loss=0.016634  val_loss=0.017604  sm_loss=0.016794 \t sm_val_loss=0.017733 \t time=0.98s\n",
      "Best: Epoch 6 \t loss=0.016456  val_loss=0.017417  sm_loss=0.016627 \t sm_val_loss=0.017547 \t time=0.75s\n",
      "Best: Epoch 8 \t loss=0.016369  val_loss=0.017333  sm_loss=0.016535 \t sm_val_loss=0.017466 \t time=0.74s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014797  val_loss=0.016576  sm_loss=0.014995 \t sm_val_loss=0.016723 \t time=0.74s\n",
      "Best: Epoch 14 \t loss=0.013461  val_loss=0.016484  sm_loss=0.013685 \t sm_val_loss=0.016632 \t time=0.74s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.01660317016040389\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.140600  val_loss=0.019739  sm_loss=0.140600 \t sm_val_loss=0.019770 \t time=0.97s\n",
      "Best: Epoch 2 \t loss=0.018464  val_loss=0.018248  sm_loss=0.018529 \t sm_val_loss=0.018336 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017456  val_loss=0.017858  sm_loss=0.017566 \t sm_val_loss=0.017971 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016997  val_loss=0.017377  sm_loss=0.017136 \t sm_val_loss=0.017510 \t time=0.92s\n",
      "Best: Epoch 7 \t loss=0.016384  val_loss=0.017199  sm_loss=0.016552 \t sm_val_loss=0.017341 \t time=0.93s\n",
      "Best: Epoch 8 \t loss=0.016363  val_loss=0.017101  sm_loss=0.016535 \t sm_val_loss=0.017247 \t time=0.90s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014585  val_loss=0.016283  sm_loss=0.014791 \t sm_val_loss=0.016435 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.013333  val_loss=0.016182  sm_loss=0.013562 \t sm_val_loss=0.016337 \t time=0.90s\n",
      "Best: Epoch 15 \t loss=0.012587  val_loss=0.016153  sm_loss=0.012828 \t sm_val_loss=0.016302 \t time=0.85s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.016037966198744393\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016336388839679448\n",
      "Fold 2 log loss: 0.016158281290575543\n",
      "Fold 3 log loss: 0.016313486379044995\n",
      "Fold 4 log loss: 0.016413255461526288\n",
      "Fold 5 log loss: 0.016288205704356323\n",
      "Fold 6 log loss: 0.01660317016040389\n",
      "Fold 7 log loss: 0.016037966198744393\n",
      "Std of log loss: 0.00016680270244848205\n",
      "Total log loss: 0.016307256403990462\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.141212  val_loss=0.019486  sm_loss=0.141211 \t sm_val_loss=0.019518 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018478  val_loss=0.018263  sm_loss=0.018543 \t sm_val_loss=0.018298 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017374  val_loss=0.017734  sm_loss=0.017489 \t sm_val_loss=0.017845 \t time=0.80s\n",
      "Best: Epoch 4 \t loss=0.016922  val_loss=0.017398  sm_loss=0.017062 \t sm_val_loss=0.017516 \t time=0.89s\n",
      "Best: Epoch 5 \t loss=0.016627  val_loss=0.017366  sm_loss=0.016789 \t sm_val_loss=0.017479 \t time=0.75s\n",
      "Best: Epoch 6 \t loss=0.016415  val_loss=0.017360  sm_loss=0.016586 \t sm_val_loss=0.017484 \t time=0.78s\n",
      "Best: Epoch 7 \t loss=0.016395  val_loss=0.017249  sm_loss=0.016564 \t sm_val_loss=0.017395 \t time=0.75s\n",
      "Best: Epoch 8 \t loss=0.016302  val_loss=0.017220  sm_loss=0.016469 \t sm_val_loss=0.017355 \t time=1.02s\n",
      "Best: Epoch 10 \t loss=0.016396  val_loss=0.017165  sm_loss=0.016555 \t sm_val_loss=0.017295 \t time=0.74s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014728  val_loss=0.016342  sm_loss=0.014930 \t sm_val_loss=0.016512 \t time=0.73s\n",
      "Best: Epoch 16 \t loss=0.013386  val_loss=0.016267  sm_loss=0.013616 \t sm_val_loss=0.016443 \t time=0.98s\n",
      "Best: Epoch 17 \t loss=0.012616  val_loss=0.016206  sm_loss=0.012858 \t sm_val_loss=0.016382 \t time=0.74s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.016383371471422006\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.141454  val_loss=0.019357  sm_loss=0.141452 \t sm_val_loss=0.019384 \t time=0.73s\n",
      "Best: Epoch 2 \t loss=0.018421  val_loss=0.017843  sm_loss=0.018485 \t sm_val_loss=0.017900 \t time=0.74s\n",
      "Best: Epoch 3 \t loss=0.017396  val_loss=0.017551  sm_loss=0.017514 \t sm_val_loss=0.017637 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017124  val_loss=0.017190  sm_loss=0.017260 \t sm_val_loss=0.017310 \t time=0.74s\n",
      "Best: Epoch 5 \t loss=0.016699  val_loss=0.017160  sm_loss=0.016860 \t sm_val_loss=0.017284 \t time=0.75s\n",
      "Best: Epoch 6 \t loss=0.016438  val_loss=0.017154  sm_loss=0.016608 \t sm_val_loss=0.017276 \t time=0.75s\n",
      "Best: Epoch 7 \t loss=0.016396  val_loss=0.016939  sm_loss=0.016563 \t sm_val_loss=0.017084 \t time=0.74s\n",
      "Best: Epoch 9 \t loss=0.016410  val_loss=0.016934  sm_loss=0.016577 \t sm_val_loss=0.017078 \t time=0.74s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014680  val_loss=0.016110  sm_loss=0.014883 \t sm_val_loss=0.016291 \t time=0.78s\n",
      "Best: Epoch 15 \t loss=0.013378  val_loss=0.015981  sm_loss=0.013611 \t sm_val_loss=0.016174 \t time=1.14s\n",
      "Best: Epoch 16 \t loss=0.012626  val_loss=0.015952  sm_loss=0.012872 \t sm_val_loss=0.016134 \t time=1.15s\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.016128845235255783\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.143003  val_loss=0.019742  sm_loss=0.143002 \t sm_val_loss=0.019764 \t time=0.74s\n",
      "Best: Epoch 2 \t loss=0.018423  val_loss=0.018307  sm_loss=0.018487 \t sm_val_loss=0.018363 \t time=0.74s\n",
      "Best: Epoch 3 \t loss=0.017601  val_loss=0.017804  sm_loss=0.017701 \t sm_val_loss=0.017863 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017008  val_loss=0.017574  sm_loss=0.017146 \t sm_val_loss=0.017672 \t time=0.75s\n",
      "Best: Epoch 5 \t loss=0.016636  val_loss=0.017245  sm_loss=0.016801 \t sm_val_loss=0.017384 \t time=0.97s\n",
      "Best: Epoch 6 \t loss=0.016461  val_loss=0.017171  sm_loss=0.016631 \t sm_val_loss=0.017292 \t time=0.76s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014539  val_loss=0.016334  sm_loss=0.014745 \t sm_val_loss=0.016507 \t time=0.74s\n",
      "Best: Epoch 12 \t loss=0.013328  val_loss=0.016215  sm_loss=0.013560 \t sm_val_loss=0.016401 \t time=0.78s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.01628756277221166\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.138011  val_loss=0.019580  sm_loss=0.138009 \t sm_val_loss=0.019605 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018389  val_loss=0.018183  sm_loss=0.018460 \t sm_val_loss=0.018244 \t time=0.87s\n",
      "Best: Epoch 3 \t loss=0.017439  val_loss=0.017926  sm_loss=0.017554 \t sm_val_loss=0.018018 \t time=0.73s\n",
      "Best: Epoch 4 \t loss=0.017030  val_loss=0.017817  sm_loss=0.017178 \t sm_val_loss=0.017921 \t time=0.73s\n",
      "Best: Epoch 5 \t loss=0.016587  val_loss=0.017410  sm_loss=0.016747 \t sm_val_loss=0.017529 \t time=0.75s\n",
      "Best: Epoch 6 \t loss=0.016455  val_loss=0.017374  sm_loss=0.016621 \t sm_val_loss=0.017513 \t time=0.75s\n",
      "Best: Epoch 7 \t loss=0.016378  val_loss=0.017231  sm_loss=0.016544 \t sm_val_loss=0.017376 \t time=0.74s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014636  val_loss=0.016395  sm_loss=0.014839 \t sm_val_loss=0.016551 \t time=0.75s\n",
      "Best: Epoch 13 \t loss=0.013346  val_loss=0.016331  sm_loss=0.013583 \t sm_val_loss=0.016494 \t time=0.93s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.01645565477132014\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.139441  val_loss=0.019661  sm_loss=0.139436 \t sm_val_loss=0.019684 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018297  val_loss=0.018338  sm_loss=0.018363 \t sm_val_loss=0.018400 \t time=0.73s\n",
      "Best: Epoch 3 \t loss=0.017492  val_loss=0.018031  sm_loss=0.017600 \t sm_val_loss=0.018109 \t time=0.73s\n",
      "Best: Epoch 4 \t loss=0.016978  val_loss=0.017756  sm_loss=0.017124 \t sm_val_loss=0.017852 \t time=0.80s\n",
      "Best: Epoch 6 \t loss=0.016520  val_loss=0.017437  sm_loss=0.016690 \t sm_val_loss=0.017544 \t time=0.87s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014702  val_loss=0.016476  sm_loss=0.014905 \t sm_val_loss=0.016628 \t time=0.75s\n",
      "Best: Epoch 12 \t loss=0.013450  val_loss=0.016350  sm_loss=0.013675 \t sm_val_loss=0.016505 \t time=0.73s\n",
      "Best: Epoch 13 \t loss=0.012739  val_loss=0.016326  sm_loss=0.012976 \t sm_val_loss=0.016480 \t time=0.73s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.016346867409130383\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.137571  val_loss=0.019689  sm_loss=0.137569 \t sm_val_loss=0.019720 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018411  val_loss=0.018214  sm_loss=0.018469 \t sm_val_loss=0.018286 \t time=0.74s\n",
      "Best: Epoch 3 \t loss=0.017414  val_loss=0.018003  sm_loss=0.017528 \t sm_val_loss=0.018109 \t time=0.95s\n",
      "Best: Epoch 4 \t loss=0.016991  val_loss=0.017665  sm_loss=0.017139 \t sm_val_loss=0.017777 \t time=0.76s\n",
      "Best: Epoch 5 \t loss=0.016611  val_loss=0.017523  sm_loss=0.016771 \t sm_val_loss=0.017650 \t time=0.74s\n",
      "Best: Epoch 6 \t loss=0.016420  val_loss=0.017429  sm_loss=0.016590 \t sm_val_loss=0.017560 \t time=0.72s\n",
      "Best: Epoch 7 \t loss=0.016346  val_loss=0.017308  sm_loss=0.016515 \t sm_val_loss=0.017450 \t time=0.73s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014478  val_loss=0.016535  sm_loss=0.014674 \t sm_val_loss=0.016698 \t time=0.78s\n",
      "Best: Epoch 13 \t loss=0.013186  val_loss=0.016416  sm_loss=0.013421 \t sm_val_loss=0.016564 \t time=0.75s\n",
      "Best: Epoch 14 \t loss=0.012462  val_loss=0.016405  sm_loss=0.012702 \t sm_val_loss=0.016551 \t time=0.75s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.016535436826384444\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.138254  val_loss=0.019314  sm_loss=0.138253 \t sm_val_loss=0.019348 \t time=0.95s\n",
      "Best: Epoch 2 \t loss=0.018331  val_loss=0.018105  sm_loss=0.018401 \t sm_val_loss=0.018174 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017470  val_loss=0.017864  sm_loss=0.017587 \t sm_val_loss=0.017965 \t time=0.85s\n",
      "Best: Epoch 4 \t loss=0.017014  val_loss=0.017550  sm_loss=0.017161 \t sm_val_loss=0.017639 \t time=0.84s\n",
      "Best: Epoch 5 \t loss=0.016825  val_loss=0.017453  sm_loss=0.016982 \t sm_val_loss=0.017573 \t time=0.90s\n",
      "Best: Epoch 6 \t loss=0.016564  val_loss=0.017275  sm_loss=0.016727 \t sm_val_loss=0.017382 \t time=0.92s\n",
      "Best: Epoch 8 \t loss=0.016458  val_loss=0.017212  sm_loss=0.016622 \t sm_val_loss=0.017337 \t time=0.78s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014924  val_loss=0.016321  sm_loss=0.015115 \t sm_val_loss=0.016470 \t time=0.87s\n",
      "Best: Epoch 14 \t loss=0.013645  val_loss=0.016193  sm_loss=0.013871 \t sm_val_loss=0.016338 \t time=0.93s\n",
      "Best: Epoch 15 \t loss=0.012912  val_loss=0.016150  sm_loss=0.013150 \t sm_val_loss=0.016297 \t time=0.84s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.016004378822956133\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.016383371471422006\n",
      "Fold 2 log loss: 0.016128845235255783\n",
      "Fold 3 log loss: 0.01628756277221166\n",
      "Fold 4 log loss: 0.01645565477132014\n",
      "Fold 5 log loss: 0.016346867409130383\n",
      "Fold 6 log loss: 0.016535436826384444\n",
      "Fold 7 log loss: 0.016004378822956133\n",
      "Std of log loss: 0.00017129480523972495\n",
      "Total log loss: 0.016306021000190647\n",
      "Total log loss in targets: 0.01613193459346564\n"
     ]
    }
   ],
   "source": [
    "#print(\"Transfer learning\")\n",
    "#for seed_ in [0]:\n",
    "#    files = modelling_lstm(fn_train, fn_all_targets, fn_test, seed_,  fn_non_targets.shape[1])\n",
    "\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "print(\"Final prediction\")\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_lstm(fn_train, fn_targets, fn_test, seed_, fn_targets.shape[1])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:40:55.118290Z",
     "iopub.status.busy": "2020-10-22T13:40:55.117324Z",
     "iopub.status.idle": "2020-10-22T13:41:01.127875Z",
     "shell.execute_reply": "2020-10-22T13:41:01.127334Z"
    },
    "papermill": {
     "duration": 6.135498,
     "end_time": "2020-10-22T13:41:01.127994",
     "exception": false,
     "start_time": "2020-10-22T13:40:54.992496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.01486788025772175\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T13:41:01.361234Z",
     "iopub.status.busy": "2020-10-22T13:41:01.360356Z",
     "iopub.status.idle": "2020-10-22T13:41:03.834133Z",
     "shell.execute_reply": "2020-10-22T13:41:03.833373Z"
    },
    "papermill": {
     "duration": 2.59277,
     "end_time": "2020-10-22T13:41:03.834255",
     "exception": false,
     "start_time": "2020-10-22T13:41:01.241485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.113243,
     "end_time": "2020-10-22T13:41:04.061341",
     "exception": false,
     "start_time": "2020-10-22T13:41:03.948098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 609.822079,
   "end_time": "2020-10-22T13:41:04.782100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-22T13:30:54.960021",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
