{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016154,
     "end_time": "2020-10-15T04:43:42.003052",
     "exception": false,
     "start_time": "2020-10-15T04:43:41.986898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- implement correct tranfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:42.043160Z",
     "iopub.status.busy": "2020-10-15T04:43:42.042329Z",
     "iopub.status.idle": "2020-10-15T04:43:49.863969Z",
     "shell.execute_reply": "2020-10-15T04:43:49.863118Z"
    },
    "papermill": {
     "duration": 7.84625,
     "end_time": "2020-10-15T04:43:49.864105",
     "exception": false,
     "start_time": "2020-10-15T04:43:42.017855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:49.906338Z",
     "iopub.status.busy": "2020-10-15T04:43:49.905204Z",
     "iopub.status.idle": "2020-10-15T04:43:57.054760Z",
     "shell.execute_reply": "2020-10-15T04:43:57.054090Z"
    },
    "papermill": {
     "duration": 7.175679,
     "end_time": "2020-10-15T04:43:57.054905",
     "exception": false,
     "start_time": "2020-10-15T04:43:49.879226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.094559Z",
     "iopub.status.busy": "2020-10-15T04:43:57.093619Z",
     "iopub.status.idle": "2020-10-15T04:43:57.096174Z",
     "shell.execute_reply": "2020-10-15T04:43:57.096745Z"
    },
    "papermill": {
     "duration": 0.02521,
     "end_time": "2020-10-15T04:43:57.096907",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.071697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.139529Z",
     "iopub.status.busy": "2020-10-15T04:43:57.138523Z",
     "iopub.status.idle": "2020-10-15T04:43:57.275185Z",
     "shell.execute_reply": "2020-10-15T04:43:57.274540Z"
    },
    "papermill": {
     "duration": 0.163123,
     "end_time": "2020-10-15T04:43:57.275308",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.112185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014746,
     "end_time": "2020-10-15T04:43:57.305283",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.290537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.350120Z",
     "iopub.status.busy": "2020-10-15T04:43:57.346057Z",
     "iopub.status.idle": "2020-10-15T04:43:57.701155Z",
     "shell.execute_reply": "2020-10-15T04:43:57.701784Z"
    },
    "papermill": {
     "duration": 0.381611,
     "end_time": "2020-10-15T04:43:57.702005",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.320394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.767825Z",
     "iopub.status.busy": "2020-10-15T04:43:57.741608Z",
     "iopub.status.idle": "2020-10-15T04:43:57.823679Z",
     "shell.execute_reply": "2020-10-15T04:43:57.822942Z"
    },
    "papermill": {
     "duration": 0.106174,
     "end_time": "2020-10-15T04:43:57.823809",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.717635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)\n",
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "print(len(non_target_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.864363Z",
     "iopub.status.busy": "2020-10-15T04:43:57.862744Z",
     "iopub.status.idle": "2020-10-15T04:43:57.899216Z",
     "shell.execute_reply": "2020-10-15T04:43:57.898478Z"
    },
    "papermill": {
     "duration": 0.059371,
     "end_time": "2020-10-15T04:43:57.899336",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.839965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_targets = pd.concat([targets, non_targets], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016713,
     "end_time": "2020-10-15T04:43:57.932620",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.915907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:57.974070Z",
     "iopub.status.busy": "2020-10-15T04:43:57.973266Z",
     "iopub.status.idle": "2020-10-15T04:43:58.113529Z",
     "shell.execute_reply": "2020-10-15T04:43:58.114144Z"
    },
    "papermill": {
     "duration": 0.165498,
     "end_time": "2020-10-15T04:43:58.114306",
     "exception": false,
     "start_time": "2020-10-15T04:43:57.948808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 872) (3982, 872)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    #tmp['g_sum'] = tmp[g_feats].sum(axis = 1)\n",
    "    #tmp['g_mean'] = tmp[g_feats].mean(axis = 1)\n",
    "    #tmp['g_std'] = tmp[g_feats].std(axis = 1)\n",
    "    #tmp['g_kurt'] = tmp[g_feats].kurtosis(axis = 1)\n",
    "    #tmp['g_skew'] = tmp[g_feats].skew(axis = 1)\n",
    "    #tmp['c_sum'] = tmp[c_feats].sum(axis = 1)\n",
    "    #tmp['c_mean'] = tmp[c_feats].mean(axis = 1)\n",
    "    #tmp['c_std'] = tmp[c_feats].std(axis = 1)\n",
    "    #tmp['c_kurt'] = tmp[c_feats].kurtosis(axis = 1)\n",
    "    #tmp['c_skew'] = tmp[c_feats].skew(axis = 1)\n",
    "    #tmp['gc_sum'] = tmp[c_feats + g_feats].sum(axis = 1)\n",
    "    #tmp['gc_mean'] = tmp[c_feats + g_feats].mean(axis = 1)\n",
    "    #tmp['gc_std'] = tmp[c_feats + g_feats].std(axis = 1)\n",
    "    #tmp['gc_kurt'] = tmp[c_feats + g_feats].kurtosis(axis = 1)\n",
    "    #tmp['gc_skew'] = tmp[c_feats + g_feats].skew(axis = 1)\n",
    "    #tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\", \"cp_dose\", \"cp_time\"], axis=1, inplace=True)\n",
    "    #col = list(tmp.columns)\n",
    "    #col = col[1:] + col[:1]\n",
    "    #tmp = tmp[col]\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:58.157130Z",
     "iopub.status.busy": "2020-10-15T04:43:58.156293Z",
     "iopub.status.idle": "2020-10-15T04:43:59.495193Z",
     "shell.execute_reply": "2020-10-15T04:43:59.494463Z"
    },
    "papermill": {
     "duration": 1.363849,
     "end_time": "2020-10-15T04:43:59.495321",
     "exception": false,
     "start_time": "2020-10-15T04:43:58.131472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "ss = preprocessing.RobustScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "#fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_all_targets = all_targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:59.547594Z",
     "iopub.status.busy": "2020-10-15T04:43:59.542807Z",
     "iopub.status.idle": "2020-10-15T04:43:59.550312Z",
     "shell.execute_reply": "2020-10-15T04:43:59.550886Z"
    },
    "papermill": {
     "duration": 0.038809,
     "end_time": "2020-10-15T04:43:59.551053",
     "exception": false,
     "start_time": "2020-10-15T04:43:59.512244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self,target_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_hidden_size = 772\n",
    "        self.c_lstm_hidden_size = 100\n",
    "        self.g_layer_num = 1\n",
    "        self.c_layer_num = 1\n",
    "        \n",
    "        self.hidden_dim = 512\n",
    "        self.hidden_dim_c = 10\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.lstm_hidden_size, self.hidden_dim, batch_first=True, bidirectional=True, num_layers=self.g_layer_num)\n",
    "        self.c_lstm = nn.LSTM(self.c_lstm_hidden_size, self.hidden_dim_c, batch_first=True, bidirectional=True, num_layers=self.c_layer_num)\n",
    "        \n",
    "        #self.hidden_dim_gc = 512        \n",
    "        #self.gc_layer_num = 1\n",
    "        #self.hidden_dim_fgc = 64\n",
    "        #self.hidden_dim_gfc = 256\n",
    "        #self.hidden_dim_fgfc = 256\n",
    "        #self.gc_lstm = nn.LSTM(self.lstm_hidden_size + self.c_lstm_hidden_size, self.hidden_dim_gc, batch_first=True,\n",
    "        #                      bidirectional=True, num_layers=self.gc_layer_num)\n",
    "        #self.fgc_lstm = nn.LSTM(self.lstm_hidden_size + self.c_lstm_hidden_size, self.hidden_dim_fgc, batch_first=True,\n",
    "        #                      bidirectional=True, num_layers=self.gc_layer_num)\n",
    "        #self.gfc_lstm = nn.LSTM(self.lstm_hidden_size + self.c_lstm_hidden_size, self.hidden_dim_gfc, batch_first=True,\n",
    "        #                      bidirectional=True, num_layers=self.gc_layer_num)\n",
    "        #self.fgfc_lstm = nn.LSTM(self.lstm_hidden_size + self.c_lstm_hidden_size, self.hidden_dim_fgfc, batch_first=True,\n",
    "        #                      bidirectional=True, num_layers=self.gc_layer_num)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lr1 = nn.utils.weight_norm(nn.Linear((self.hidden_dim + self.hidden_dim_c) * 2 , target_length))\n",
    "        self.batch_norm1 = nn.BatchNorm1d((self.hidden_dim + self.hidden_dim_c) * 2) \n",
    "\n",
    "    def forward(self, cont_g, cont_c): \n",
    "        cont_g = torch.unsqueeze(cont_g, 1)\n",
    "        h_lstm, lstm_out = self.lstm(cont_g) # h_lstm: 256 * 1 * (2 * 512)\n",
    "        conc_g = h_lstm.view(-1, self.hidden_dim * 2)\n",
    "        \n",
    "        cont_c = torch.unsqueeze(cont_c, 1)\n",
    "        h_lstm_c, lstm_out_c = self.c_lstm(cont_c) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        conc_c = h_lstm_c.view(-1, self.hidden_dim_c * 2)\n",
    "        \n",
    "        #cont_gc = torch.cat((cont_g, cont_c), 2)        \n",
    "        #h_lstm_gc, lstm_out_gc = self.gc_lstm(cont_gc) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        #conc_gc = h_lstm_gc.view(-1, self.hidden_dim_gc * 2)\n",
    "        \n",
    "        #flip_g = torch.fliplr(cont_g)\n",
    "        #cont_fgc = torch.cat((flip_g, cont_c), 2)        \n",
    "        #h_lstm_fgc, lstm_out_fgc = self.fgc_lstm(cont_fgc) \n",
    "        #conc_fgc = h_lstm_fgc.view(-1, self.hidden_dim_fgc * 2)\n",
    "        \n",
    "        #flip_c = torch.fliplr(cont_c)\n",
    "        #cont_gfc = torch.cat((cont_g, flip_c), 2)        \n",
    "        #h_lstm_gfc, lstm_out_gfc = self.gfc_lstm(cont_gfc) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        #conc_gfc = h_lstm_gfc.view(-1, self.hidden_dim_gfc * 2)\n",
    "        \n",
    "        #cont_fgfc = torch.cat((flip_g, flip_c), 2)        \n",
    "        #h_lstm_fgfc, lstm_out_fgfc = self.fgfc_lstm(cont_fgfc) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        #conc_fgfc = h_lstm_fgfc.view(-1, self.hidden_dim_fgfc * 2)\n",
    "        \n",
    "        conc = torch.cat((conc_g, conc_c),1)\n",
    "        conc = self.batch_norm1(conc)\n",
    "                \n",
    "        dropped = self.dropout(conc)\n",
    "        \n",
    "        out = self.lr1(dropped)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:59.622404Z",
     "iopub.status.busy": "2020-10-15T04:43:59.599936Z",
     "iopub.status.idle": "2020-10-15T04:43:59.662620Z",
     "shell.execute_reply": "2020-10-15T04:43:59.662029Z"
    },
    "papermill": {
     "duration": 0.094386,
     "end_time": "2020-10-15T04:43:59.662743",
     "exception": false,
     "start_time": "2020-10-15T04:43:59.568357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def modelling_lstm(tr, target, te, sample_seed, init_num, last_num, files):\n",
    "    if files == []:\n",
    "        train_epochs = 20\n",
    "        ans = []\n",
    "    else:\n",
    "        train_epochs = 30\n",
    "    \n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test_g = torch.tensor(X_test[:,:772], dtype=torch.float32)\n",
    "    X_test_c = torch.tensor(X_test[:,772:872], dtype=torch.float32)\n",
    "    #X_test_num = torch.tensor(X_test[:,872], dtype=torch.float32)\n",
    "\n",
    "    X_test = torch.utils.data.TensorDataset(X_test_g, X_test_c) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2_g = torch.tensor(X_train[train_index,:772], dtype=torch.float32)\n",
    "        X_valid2_g = torch.tensor(X_train[valid_index,:772], dtype=torch.float32)\n",
    "        X_train2_c = torch.tensor(X_train[train_index,772:872], dtype=torch.float32)\n",
    "        X_valid2_c = torch.tensor(X_train[valid_index,772:872], dtype=torch.float32)\n",
    "        #X_train2 = torch.tensor(X_train[train_index,872], dtype=torch.float32)\n",
    "        #X_valid2 = torch.tensor(X_train[valid_index,872], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = myLSTM(len(non_target_feats)+len(target_feats))\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        if files != []:\n",
    "            clf.load_state_dict(torch.load(files[fold]))\n",
    "            for param in clf.parameters():\n",
    "                param.requires_grad = False\n",
    "            # reinitialze\n",
    "            clf.lr1 = nn.utils.weight_norm(nn.Linear((512 + 10) * 2, len(target_feats)))\n",
    "            for param in clf.lr1.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer = optim.Adam(clf.parameters(), lr = 0.01, weight_decay=1e-5) \n",
    "            #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-3, verbose=True)\n",
    "        else:\n",
    "            optimizer = optim.Adam(clf.parameters(), lr = 0.01, weight_decay=1e-5) \n",
    "            #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-3, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2_g, X_train2_c, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2_g, X_valid2_c, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch_g, x_batch_c, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                #x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                #x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                if files == []:\n",
    "                    torch.save(clf.state_dict(), 'parameters'+str(fold+1)+'.pt')\n",
    "                else:    \n",
    "                    torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        if files == []:\n",
    "            pred_model = myLSTM(len(non_target_feats)+len(target_feats))\n",
    "            ans.append('parameters'+str(fold+1)+'.pt')\n",
    "            pred_model.load_state_dict(torch.load('parameters'+str(fold+1)+'.pt'))\n",
    "        else:\n",
    "            pred_model = myLSTM(len(target_feats))\n",
    "            pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        if files != []:\n",
    "            # test predcition --------------\n",
    "            test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "            for i, (x_batch_g, x_batch_c, ) in enumerate(test_loader): \n",
    "                y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach()\n",
    "                test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "            pred_value += test_preds / n_folds\n",
    "            # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "\n",
    "    if files != []:\n",
    "        return oof, oof_targets, pred_value\n",
    "    else:\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T04:43:59.709506Z",
     "iopub.status.busy": "2020-10-15T04:43:59.708650Z",
     "iopub.status.idle": "2020-10-15T05:25:44.005868Z",
     "shell.execute_reply": "2020-10-15T05:25:44.006639Z"
    },
    "papermill": {
     "duration": 2504.326539,
     "end_time": "2020-10-15T05:25:44.006828",
     "exception": false,
     "start_time": "2020-10-15T04:43:59.680289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.143144 \t val_loss=0.011333 \t time=7.92s\n",
      "Best model: Epoch 2 \t loss=0.010692 \t val_loss=0.010511 \t time=8.13s\n",
      "Best model: Epoch 3 \t loss=0.010197 \t val_loss=0.010433 \t time=7.51s\n",
      "Best model: Epoch 4 \t loss=0.010002 \t val_loss=0.010140 \t time=7.81s\n",
      "Best model: Epoch 6 \t loss=0.009831 \t val_loss=0.010101 \t time=7.45s\n",
      "Best model: Epoch 8 \t loss=0.009818 \t val_loss=0.010059 \t time=7.51s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 13 \t loss=0.009315 \t val_loss=0.009556 \t time=8.02s\n",
      "Best model: Epoch 14 \t loss=0.008933 \t val_loss=0.009472 \t time=7.60s\n",
      "Best model: Epoch 15 \t loss=0.008785 \t val_loss=0.009469 \t time=7.62s\n",
      "Best model: Epoch 16 \t loss=0.008698 \t val_loss=0.009465 \t time=7.52s\n",
      "Best model: Epoch 17 \t loss=0.008625 \t val_loss=0.009457 \t time=8.91s\n",
      "Fold 1 log loss: 0.016466286517162285\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.137678 \t val_loss=0.011217 \t time=8.17s\n",
      "Best model: Epoch 2 \t loss=0.010714 \t val_loss=0.010476 \t time=7.93s\n",
      "Best model: Epoch 3 \t loss=0.010175 \t val_loss=0.010323 \t time=8.27s\n",
      "Best model: Epoch 4 \t loss=0.010048 \t val_loss=0.010122 \t time=7.69s\n",
      "Best model: Epoch 5 \t loss=0.009900 \t val_loss=0.010118 \t time=7.48s\n",
      "Best model: Epoch 6 \t loss=0.009863 \t val_loss=0.010063 \t time=7.82s\n",
      "Best model: Epoch 7 \t loss=0.009825 \t val_loss=0.010048 \t time=9.95s\n",
      "Best model: Epoch 8 \t loss=0.009831 \t val_loss=0.009934 \t time=7.53s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 13 \t loss=0.009399 \t val_loss=0.009516 \t time=7.68s\n",
      "Best model: Epoch 14 \t loss=0.009012 \t val_loss=0.009439 \t time=7.61s\n",
      "Best model: Epoch 15 \t loss=0.008863 \t val_loss=0.009383 \t time=7.98s\n",
      "Best model: Epoch 16 \t loss=0.008760 \t val_loss=0.009363 \t time=7.53s\n",
      "Best model: Epoch 20 \t loss=0.008518 \t val_loss=0.009362 \t time=14.14s\n",
      "Fold 2 log loss: 0.016346829526133325\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.141343 \t val_loss=0.011161 \t time=8.22s\n",
      "Best model: Epoch 2 \t loss=0.010722 \t val_loss=0.010464 \t time=7.52s\n",
      "Best model: Epoch 3 \t loss=0.010292 \t val_loss=0.010248 \t time=7.89s\n",
      "Best model: Epoch 5 \t loss=0.009915 \t val_loss=0.010010 \t time=7.89s\n",
      "Best model: Epoch 7 \t loss=0.009784 \t val_loss=0.009918 \t time=7.44s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 12 \t loss=0.009248 \t val_loss=0.009463 \t time=7.80s\n",
      "Best model: Epoch 13 \t loss=0.008889 \t val_loss=0.009384 \t time=7.94s\n",
      "Best model: Epoch 14 \t loss=0.008737 \t val_loss=0.009365 \t time=7.66s\n",
      "Best model: Epoch 15 \t loss=0.008648 \t val_loss=0.009351 \t time=7.54s\n",
      "Best model: Epoch 16 \t loss=0.008573 \t val_loss=0.009348 \t time=7.53s\n",
      "Fold 3 log loss: 0.01639073500804987\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.141056 \t val_loss=0.011158 \t time=7.88s\n",
      "Best model: Epoch 2 \t loss=0.010719 \t val_loss=0.010433 \t time=7.46s\n",
      "Best model: Epoch 3 \t loss=0.010161 \t val_loss=0.010433 \t time=7.81s\n",
      "Best model: Epoch 4 \t loss=0.010007 \t val_loss=0.010170 \t time=8.22s\n",
      "Best model: Epoch 6 \t loss=0.009830 \t val_loss=0.010046 \t time=7.42s\n",
      "Best model: Epoch 7 \t loss=0.009768 \t val_loss=0.009844 \t time=7.74s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 12 \t loss=0.009233 \t val_loss=0.009410 \t time=8.74s\n",
      "Best model: Epoch 13 \t loss=0.008808 \t val_loss=0.009361 \t time=7.71s\n",
      "Best model: Epoch 14 \t loss=0.008649 \t val_loss=0.009335 \t time=7.83s\n",
      "Fold 4 log loss: 0.01643769595238513\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.139970 \t val_loss=0.011425 \t time=7.91s\n",
      "Best model: Epoch 2 \t loss=0.010691 \t val_loss=0.010547 \t time=8.01s\n",
      "Best model: Epoch 3 \t loss=0.010164 \t val_loss=0.010383 \t time=7.50s\n",
      "Best model: Epoch 4 \t loss=0.009988 \t val_loss=0.010178 \t time=7.85s\n",
      "Best model: Epoch 5 \t loss=0.009867 \t val_loss=0.010078 \t time=8.18s\n",
      "Best model: Epoch 7 \t loss=0.009772 \t val_loss=0.010039 \t time=7.50s\n",
      "Best model: Epoch 8 \t loss=0.009765 \t val_loss=0.010035 \t time=7.41s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 13 \t loss=0.009251 \t val_loss=0.009542 \t time=8.19s\n",
      "Best model: Epoch 14 \t loss=0.008879 \t val_loss=0.009504 \t time=8.24s\n",
      "Best model: Epoch 15 \t loss=0.008730 \t val_loss=0.009465 \t time=7.52s\n",
      "Best model: Epoch 16 \t loss=0.008652 \t val_loss=0.009462 \t time=7.47s\n",
      "Best model: Epoch 17 \t loss=0.008601 \t val_loss=0.009432 \t time=8.30s\n",
      "Fold 5 log loss: 0.01663582788745318\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016466286517162285\n",
      "Fold 2 log loss: 0.016346829526133325\n",
      "Fold 3 log loss: 0.01639073500804987\n",
      "Fold 4 log loss: 0.01643769595238513\n",
      "Fold 5 log loss: 0.01663582788745318\n",
      "Std of log loss: 9.894973774554059e-05\n",
      "Total log loss: 0.01645547743533678\n",
      "Final prediction\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.032343 \t val_loss=0.020121 \t time=3.22s\n",
      "Best model: Epoch 2 \t loss=0.019288 \t val_loss=0.018090 \t time=3.34s\n",
      "Best model: Epoch 3 \t loss=0.017518 \t val_loss=0.016862 \t time=3.23s\n",
      "Best model: Epoch 4 \t loss=0.016474 \t val_loss=0.016082 \t time=3.23s\n",
      "Best model: Epoch 5 \t loss=0.015683 \t val_loss=0.015470 \t time=3.30s\n",
      "Best model: Epoch 6 \t loss=0.015120 \t val_loss=0.015028 \t time=3.24s\n",
      "Best model: Epoch 7 \t loss=0.014716 \t val_loss=0.014743 \t time=3.79s\n",
      "Best model: Epoch 8 \t loss=0.014465 \t val_loss=0.014688 \t time=3.24s\n",
      "Best model: Epoch 9 \t loss=0.014332 \t val_loss=0.014661 \t time=3.43s\n",
      "Best model: Epoch 10 \t loss=0.014279 \t val_loss=0.014630 \t time=3.62s\n",
      "Best model: Epoch 11 \t loss=0.014238 \t val_loss=0.014453 \t time=3.44s\n",
      "Best model: Epoch 13 \t loss=0.014114 \t val_loss=0.014401 \t time=3.41s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 18 \t loss=0.013276 \t val_loss=0.013873 \t time=3.37s\n",
      "Best model: Epoch 19 \t loss=0.013121 \t val_loss=0.013837 \t time=3.26s\n",
      "Best model: Epoch 20 \t loss=0.013095 \t val_loss=0.013794 \t time=3.26s\n",
      "Best model: Epoch 23 \t loss=0.013026 \t val_loss=0.013774 \t time=3.29s\n",
      "Best model: Epoch 27 \t loss=0.013019 \t val_loss=0.013747 \t time=3.31s\n",
      "Fold 1 log loss: 0.013977991234896334\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.032300 \t val_loss=0.020015 \t time=3.36s\n",
      "Best model: Epoch 2 \t loss=0.019283 \t val_loss=0.018062 \t time=3.36s\n",
      "Best model: Epoch 3 \t loss=0.017460 \t val_loss=0.016659 \t time=3.34s\n",
      "Best model: Epoch 4 \t loss=0.016303 \t val_loss=0.015709 \t time=3.72s\n",
      "Best model: Epoch 5 \t loss=0.015411 \t val_loss=0.015014 \t time=3.39s\n",
      "Best model: Epoch 6 \t loss=0.014783 \t val_loss=0.014557 \t time=3.36s\n",
      "Best model: Epoch 7 \t loss=0.014362 \t val_loss=0.014285 \t time=3.42s\n",
      "Best model: Epoch 8 \t loss=0.014087 \t val_loss=0.014089 \t time=3.34s\n",
      "Best model: Epoch 9 \t loss=0.013933 \t val_loss=0.014066 \t time=3.33s\n",
      "Best model: Epoch 10 \t loss=0.013826 \t val_loss=0.014013 \t time=3.54s\n",
      "Best model: Epoch 11 \t loss=0.013769 \t val_loss=0.013958 \t time=3.35s\n",
      "Best model: Epoch 13 \t loss=0.013724 \t val_loss=0.013930 \t time=3.51s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 18 \t loss=0.012804 \t val_loss=0.013271 \t time=3.41s\n",
      "Best model: Epoch 19 \t loss=0.012608 \t val_loss=0.013210 \t time=3.43s\n",
      "Best model: Epoch 20 \t loss=0.012557 \t val_loss=0.013207 \t time=3.34s\n",
      "Best model: Epoch 21 \t loss=0.012549 \t val_loss=0.013193 \t time=3.35s\n",
      "Best model: Epoch 22 \t loss=0.012532 \t val_loss=0.013188 \t time=3.40s\n",
      "Best model: Epoch 23 \t loss=0.012509 \t val_loss=0.013167 \t time=3.79s\n",
      "Fold 2 log loss: 0.013272013744050277\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.032363 \t val_loss=0.020019 \t time=3.77s\n",
      "Best model: Epoch 2 \t loss=0.019207 \t val_loss=0.018092 \t time=3.84s\n",
      "Best model: Epoch 3 \t loss=0.017489 \t val_loss=0.016815 \t time=3.42s\n",
      "Best model: Epoch 4 \t loss=0.016439 \t val_loss=0.015933 \t time=3.35s\n",
      "Best model: Epoch 5 \t loss=0.015633 \t val_loss=0.015273 \t time=3.35s\n",
      "Best model: Epoch 6 \t loss=0.015103 \t val_loss=0.014865 \t time=3.33s\n",
      "Best model: Epoch 7 \t loss=0.014708 \t val_loss=0.014602 \t time=3.35s\n",
      "Best model: Epoch 8 \t loss=0.014492 \t val_loss=0.014487 \t time=3.40s\n",
      "Best model: Epoch 9 \t loss=0.014304 \t val_loss=0.014362 \t time=3.33s\n",
      "Best model: Epoch 11 \t loss=0.014186 \t val_loss=0.014238 \t time=3.81s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 16 \t loss=0.013269 \t val_loss=0.013647 \t time=3.31s\n",
      "Best model: Epoch 17 \t loss=0.013101 \t val_loss=0.013624 \t time=3.33s\n",
      "Best model: Epoch 18 \t loss=0.013079 \t val_loss=0.013597 \t time=3.61s\n",
      "Best model: Epoch 19 \t loss=0.013042 \t val_loss=0.013575 \t time=3.35s\n",
      "Best model: Epoch 22 \t loss=0.013034 \t val_loss=0.013550 \t time=3.37s\n",
      "Best model: Epoch 25 \t loss=0.013002 \t val_loss=0.013549 \t time=3.29s\n",
      "Best model: Epoch 26 \t loss=0.013009 \t val_loss=0.013531 \t time=3.28s\n",
      "Fold 3 log loss: 0.013541756306135392\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.032074 \t val_loss=0.019811 \t time=3.51s\n",
      "Best model: Epoch 2 \t loss=0.019154 \t val_loss=0.017807 \t time=3.50s\n",
      "Best model: Epoch 3 \t loss=0.017555 \t val_loss=0.016691 \t time=3.53s\n",
      "Best model: Epoch 4 \t loss=0.016546 \t val_loss=0.015930 \t time=3.46s\n",
      "Best model: Epoch 5 \t loss=0.015770 \t val_loss=0.015431 \t time=3.77s\n",
      "Best model: Epoch 6 \t loss=0.015244 \t val_loss=0.015081 \t time=3.82s\n",
      "Best model: Epoch 7 \t loss=0.014837 \t val_loss=0.014893 \t time=3.55s\n",
      "Best model: Epoch 8 \t loss=0.014622 \t val_loss=0.014745 \t time=3.91s\n",
      "Best model: Epoch 9 \t loss=0.014521 \t val_loss=0.014673 \t time=3.56s\n",
      "Best model: Epoch 10 \t loss=0.014451 \t val_loss=0.014642 \t time=3.47s\n",
      "Best model: Epoch 12 \t loss=0.014336 \t val_loss=0.014604 \t time=3.53s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 17 \t loss=0.013533 \t val_loss=0.014080 \t time=3.93s\n",
      "Best model: Epoch 18 \t loss=0.013354 \t val_loss=0.014038 \t time=3.60s\n",
      "Best model: Epoch 19 \t loss=0.013325 \t val_loss=0.014032 \t time=3.48s\n",
      "Best model: Epoch 20 \t loss=0.013313 \t val_loss=0.014008 \t time=3.46s\n",
      "Best model: Epoch 21 \t loss=0.013284 \t val_loss=0.014002 \t time=3.67s\n",
      "Best model: Epoch 22 \t loss=0.013277 \t val_loss=0.013959 \t time=3.94s\n",
      "Fold 4 log loss: 0.014092989784149175\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.032144 \t val_loss=0.020188 \t time=3.29s\n",
      "Best model: Epoch 2 \t loss=0.019260 \t val_loss=0.018276 \t time=3.37s\n",
      "Best model: Epoch 3 \t loss=0.017502 \t val_loss=0.016977 \t time=3.45s\n",
      "Best model: Epoch 4 \t loss=0.016431 \t val_loss=0.016122 \t time=3.72s\n",
      "Best model: Epoch 5 \t loss=0.015654 \t val_loss=0.015475 \t time=3.31s\n",
      "Best model: Epoch 6 \t loss=0.015082 \t val_loss=0.015086 \t time=3.34s\n",
      "Best model: Epoch 7 \t loss=0.014647 \t val_loss=0.014752 \t time=3.32s\n",
      "Best model: Epoch 8 \t loss=0.014417 \t val_loss=0.014608 \t time=3.59s\n",
      "Best model: Epoch 9 \t loss=0.014247 \t val_loss=0.014524 \t time=3.71s\n",
      "Best model: Epoch 10 \t loss=0.014204 \t val_loss=0.014477 \t time=3.48s\n",
      "Best model: Epoch 12 \t loss=0.014102 \t val_loss=0.014382 \t time=3.44s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 17 \t loss=0.013313 \t val_loss=0.013853 \t time=3.28s\n",
      "Best model: Epoch 18 \t loss=0.013083 \t val_loss=0.013823 \t time=3.36s\n",
      "Best model: Epoch 19 \t loss=0.013049 \t val_loss=0.013807 \t time=3.35s\n",
      "Best model: Epoch 20 \t loss=0.013025 \t val_loss=0.013783 \t time=3.43s\n",
      "Best model: Epoch 23 \t loss=0.013002 \t val_loss=0.013761 \t time=3.29s\n",
      "Fold 5 log loss: 0.013740374919146668\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.013977991234896334\n",
      "Fold 2 log loss: 0.013272013744050277\n",
      "Fold 3 log loss: 0.013541756306135392\n",
      "Fold 4 log loss: 0.014092989784149175\n",
      "Fold 5 log loss: 0.013740374919146668\n",
      "Std of log loss: 0.00029611897290014174\n",
      "Total log loss: 0.013725045138523581\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.032111 \t val_loss=0.020093 \t time=3.29s\n",
      "Best model: Epoch 2 \t loss=0.019318 \t val_loss=0.018210 \t time=3.35s\n",
      "Best model: Epoch 3 \t loss=0.017579 \t val_loss=0.016869 \t time=3.40s\n",
      "Best model: Epoch 4 \t loss=0.016476 \t val_loss=0.016040 \t time=3.36s\n",
      "Best model: Epoch 5 \t loss=0.015690 \t val_loss=0.015442 \t time=3.35s\n",
      "Best model: Epoch 6 \t loss=0.015107 \t val_loss=0.015061 \t time=3.36s\n",
      "Best model: Epoch 7 \t loss=0.014707 \t val_loss=0.014743 \t time=3.45s\n",
      "Best model: Epoch 8 \t loss=0.014501 \t val_loss=0.014657 \t time=3.36s\n",
      "Best model: Epoch 9 \t loss=0.014280 \t val_loss=0.014518 \t time=5.17s\n",
      "Best model: Epoch 11 \t loss=0.014173 \t val_loss=0.014435 \t time=3.56s\n",
      "Best model: Epoch 13 \t loss=0.014073 \t val_loss=0.014428 \t time=3.35s\n",
      "Best model: Epoch 14 \t loss=0.014114 \t val_loss=0.014390 \t time=3.31s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 19 \t loss=0.013307 \t val_loss=0.013877 \t time=3.33s\n",
      "Best model: Epoch 20 \t loss=0.013144 \t val_loss=0.013823 \t time=3.29s\n",
      "Best model: Epoch 21 \t loss=0.013102 \t val_loss=0.013803 \t time=3.36s\n",
      "Best model: Epoch 22 \t loss=0.013060 \t val_loss=0.013789 \t time=3.30s\n",
      "Best model: Epoch 28 \t loss=0.013026 \t val_loss=0.013746 \t time=3.76s\n",
      "Fold 1 log loss: 0.01398886627717238\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.032341 \t val_loss=0.020077 \t time=3.35s\n",
      "Best model: Epoch 2 \t loss=0.019294 \t val_loss=0.018038 \t time=3.34s\n",
      "Best model: Epoch 3 \t loss=0.017447 \t val_loss=0.016639 \t time=3.41s\n",
      "Best model: Epoch 4 \t loss=0.016236 \t val_loss=0.015687 \t time=3.38s\n",
      "Best model: Epoch 5 \t loss=0.015376 \t val_loss=0.014976 \t time=3.46s\n",
      "Best model: Epoch 6 \t loss=0.014731 \t val_loss=0.014596 \t time=3.69s\n",
      "Best model: Epoch 7 \t loss=0.014338 \t val_loss=0.014256 \t time=3.39s\n",
      "Best model: Epoch 8 \t loss=0.014062 \t val_loss=0.014068 \t time=3.37s\n",
      "Best model: Epoch 10 \t loss=0.013828 \t val_loss=0.014012 \t time=3.49s\n",
      "Best model: Epoch 11 \t loss=0.013764 \t val_loss=0.014000 \t time=3.35s\n",
      "Best model: Epoch 12 \t loss=0.013742 \t val_loss=0.013932 \t time=3.33s\n",
      "Best model: Epoch 13 \t loss=0.013627 \t val_loss=0.013768 \t time=3.39s\n",
      "Best model: Epoch 17 \t loss=0.013551 \t val_loss=0.013734 \t time=3.37s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 22 \t loss=0.012703 \t val_loss=0.013236 \t time=3.42s\n",
      "Best model: Epoch 23 \t loss=0.012549 \t val_loss=0.013225 \t time=3.36s\n",
      "Best model: Epoch 24 \t loss=0.012515 \t val_loss=0.013182 \t time=3.73s\n",
      "Best model: Epoch 25 \t loss=0.012503 \t val_loss=0.013170 \t time=3.44s\n",
      "Best model: Epoch 26 \t loss=0.012491 \t val_loss=0.013141 \t time=3.34s\n",
      "Fold 2 log loss: 0.013247298233389933\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.032148 \t val_loss=0.020055 \t time=3.68s\n",
      "Best model: Epoch 2 \t loss=0.019209 \t val_loss=0.018079 \t time=4.07s\n",
      "Best model: Epoch 3 \t loss=0.017486 \t val_loss=0.016788 \t time=3.84s\n",
      "Best model: Epoch 4 \t loss=0.016408 \t val_loss=0.015936 \t time=3.37s\n",
      "Best model: Epoch 5 \t loss=0.015636 \t val_loss=0.015313 \t time=3.35s\n",
      "Best model: Epoch 6 \t loss=0.015079 \t val_loss=0.014898 \t time=3.31s\n",
      "Best model: Epoch 7 \t loss=0.014679 \t val_loss=0.014603 \t time=3.35s\n",
      "Best model: Epoch 8 \t loss=0.014451 \t val_loss=0.014469 \t time=3.28s\n",
      "Best model: Epoch 9 \t loss=0.014327 \t val_loss=0.014447 \t time=3.30s\n",
      "Best model: Epoch 10 \t loss=0.014207 \t val_loss=0.014405 \t time=3.41s\n",
      "Best model: Epoch 11 \t loss=0.014161 \t val_loss=0.014336 \t time=3.32s\n",
      "Best model: Epoch 12 \t loss=0.014133 \t val_loss=0.014280 \t time=3.66s\n",
      "Best model: Epoch 13 \t loss=0.014086 \t val_loss=0.014209 \t time=3.50s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 18 \t loss=0.013280 \t val_loss=0.013635 \t time=3.65s\n",
      "Best model: Epoch 19 \t loss=0.013064 \t val_loss=0.013602 \t time=3.67s\n",
      "Best model: Epoch 21 \t loss=0.013027 \t val_loss=0.013570 \t time=3.77s\n",
      "Best model: Epoch 22 \t loss=0.012999 \t val_loss=0.013548 \t time=3.30s\n",
      "Best model: Epoch 27 \t loss=0.012991 \t val_loss=0.013522 \t time=3.27s\n",
      "Best model: Epoch 29 \t loss=0.013010 \t val_loss=0.013510 \t time=3.35s\n",
      "Fold 3 log loss: 0.013524869799107195\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.032209 \t val_loss=0.019773 \t time=3.57s\n",
      "Best model: Epoch 2 \t loss=0.019187 \t val_loss=0.017841 \t time=3.60s\n",
      "Best model: Epoch 3 \t loss=0.017539 \t val_loss=0.016647 \t time=3.49s\n",
      "Best model: Epoch 4 \t loss=0.016515 \t val_loss=0.015933 \t time=3.78s\n",
      "Best model: Epoch 5 \t loss=0.015772 \t val_loss=0.015424 \t time=3.87s\n",
      "Best model: Epoch 6 \t loss=0.015211 \t val_loss=0.015120 \t time=3.63s\n",
      "Best model: Epoch 7 \t loss=0.014855 \t val_loss=0.014793 \t time=3.52s\n",
      "Best model: Epoch 8 \t loss=0.014637 \t val_loss=0.014729 \t time=3.92s\n",
      "Best model: Epoch 9 \t loss=0.014555 \t val_loss=0.014726 \t time=3.48s\n",
      "Best model: Epoch 10 \t loss=0.014419 \t val_loss=0.014641 \t time=3.48s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 15 \t loss=0.013568 \t val_loss=0.014095 \t time=3.48s\n",
      "Best model: Epoch 16 \t loss=0.013401 \t val_loss=0.014083 \t time=3.60s\n",
      "Best model: Epoch 17 \t loss=0.013371 \t val_loss=0.014053 \t time=3.97s\n",
      "Best model: Epoch 18 \t loss=0.013353 \t val_loss=0.014029 \t time=3.50s\n",
      "Best model: Epoch 20 \t loss=0.013317 \t val_loss=0.014008 \t time=3.54s\n",
      "Best model: Epoch 21 \t loss=0.013319 \t val_loss=0.013989 \t time=4.10s\n",
      "Fold 4 log loss: 0.01412564891551973\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.032105 \t val_loss=0.020213 \t time=3.36s\n",
      "Best model: Epoch 2 \t loss=0.019322 \t val_loss=0.018369 \t time=3.33s\n",
      "Best model: Epoch 3 \t loss=0.017590 \t val_loss=0.017031 \t time=3.47s\n",
      "Best model: Epoch 4 \t loss=0.016462 \t val_loss=0.016114 \t time=3.73s\n",
      "Best model: Epoch 5 \t loss=0.015625 \t val_loss=0.015464 \t time=3.31s\n",
      "Best model: Epoch 6 \t loss=0.015032 \t val_loss=0.015125 \t time=3.29s\n",
      "Best model: Epoch 7 \t loss=0.014622 \t val_loss=0.014787 \t time=3.73s\n",
      "Best model: Epoch 8 \t loss=0.014378 \t val_loss=0.014684 \t time=3.44s\n",
      "Best model: Epoch 9 \t loss=0.014334 \t val_loss=0.014592 \t time=3.57s\n",
      "Best model: Epoch 10 \t loss=0.014160 \t val_loss=0.014574 \t time=3.38s\n",
      "Best model: Epoch 11 \t loss=0.014121 \t val_loss=0.014518 \t time=3.36s\n",
      "Best model: Epoch 13 \t loss=0.014055 \t val_loss=0.014393 \t time=3.57s\n",
      "Best model: Epoch 14 \t loss=0.014025 \t val_loss=0.014376 \t time=3.45s\n",
      "Best model: Epoch 16 \t loss=0.014042 \t val_loss=0.014372 \t time=3.30s\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 21 \t loss=0.013227 \t val_loss=0.013852 \t time=3.38s\n",
      "Best model: Epoch 22 \t loss=0.013034 \t val_loss=0.013809 \t time=3.29s\n",
      "Best model: Epoch 23 \t loss=0.012997 \t val_loss=0.013777 \t time=3.63s\n",
      "Best model: Epoch 24 \t loss=0.012975 \t val_loss=0.013758 \t time=3.66s\n",
      "Best model: Epoch 25 \t loss=0.012963 \t val_loss=0.013751 \t time=3.59s\n",
      "Best model: Epoch 26 \t loss=0.012955 \t val_loss=0.013748 \t time=3.35s\n",
      "Best model: Epoch 27 \t loss=0.012956 \t val_loss=0.013723 \t time=3.48s\n",
      "Fold 5 log loss: 0.01371222265987175\n",
      "Seed 10\n",
      "Fold 1 log loss: 0.01398886627717238\n",
      "Fold 2 log loss: 0.013247298233389933\n",
      "Fold 3 log loss: 0.013524869799107195\n",
      "Fold 4 log loss: 0.01412564891551973\n",
      "Fold 5 log loss: 0.01371222265987175\n",
      "Std of log loss: 0.0003157287192263583\n",
      "Total log loss: 0.013719803048775488\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.032147 \t val_loss=0.020192 \t time=3.54s\n",
      "Best model: Epoch 2 \t loss=0.019441 \t val_loss=0.018282 \t time=3.33s\n",
      "Best model: Epoch 3 \t loss=0.017622 \t val_loss=0.016930 \t time=3.28s\n",
      "Best model: Epoch 4 \t loss=0.016518 \t val_loss=0.016055 \t time=3.29s\n",
      "Best model: Epoch 5 \t loss=0.015696 \t val_loss=0.015438 \t time=3.36s\n",
      "Best model: Epoch 6 \t loss=0.015082 \t val_loss=0.015049 \t time=3.40s\n",
      "Best model: Epoch 7 \t loss=0.014689 \t val_loss=0.014793 \t time=3.28s\n",
      "Best model: Epoch 8 \t loss=0.014453 \t val_loss=0.014570 \t time=3.33s\n",
      "Best model: Epoch 10 \t loss=0.014247 \t val_loss=0.014489 \t time=4.04s\n",
      "Best model: Epoch 14 \t loss=0.014046 \t val_loss=0.014476 \t time=3.37s\n",
      "Best model: Epoch 17 \t loss=0.014012 \t val_loss=0.014438 \t time=3.28s\n",
      "Best model: Epoch 19 \t loss=0.014028 \t val_loss=0.014395 \t time=3.68s\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 24 \t loss=0.013315 \t val_loss=0.013879 \t time=3.38s\n",
      "Best model: Epoch 25 \t loss=0.013135 \t val_loss=0.013844 \t time=3.36s\n",
      "Best model: Epoch 26 \t loss=0.013094 \t val_loss=0.013832 \t time=3.36s\n",
      "Best model: Epoch 27 \t loss=0.013014 \t val_loss=0.013810 \t time=3.49s\n",
      "Best model: Epoch 28 \t loss=0.013030 \t val_loss=0.013777 \t time=4.71s\n",
      "Best model: Epoch 29 \t loss=0.013013 \t val_loss=0.013775 \t time=4.61s\n",
      "Fold 1 log loss: 0.014003015909481074\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.032207 \t val_loss=0.019891 \t time=3.40s\n",
      "Best model: Epoch 2 \t loss=0.019078 \t val_loss=0.017835 \t time=3.45s\n",
      "Best model: Epoch 3 \t loss=0.017369 \t val_loss=0.016625 \t time=3.36s\n",
      "Best model: Epoch 4 \t loss=0.016254 \t val_loss=0.015660 \t time=3.36s\n",
      "Best model: Epoch 5 \t loss=0.015403 \t val_loss=0.014988 \t time=3.49s\n",
      "Best model: Epoch 6 \t loss=0.014758 \t val_loss=0.014552 \t time=3.78s\n",
      "Best model: Epoch 7 \t loss=0.014349 \t val_loss=0.014311 \t time=3.38s\n",
      "Best model: Epoch 8 \t loss=0.014040 \t val_loss=0.014049 \t time=3.45s\n",
      "Best model: Epoch 9 \t loss=0.013913 \t val_loss=0.014008 \t time=3.37s\n",
      "Best model: Epoch 10 \t loss=0.013769 \t val_loss=0.013987 \t time=3.51s\n",
      "Best model: Epoch 12 \t loss=0.013718 \t val_loss=0.013979 \t time=3.35s\n",
      "Best model: Epoch 13 \t loss=0.013700 \t val_loss=0.013841 \t time=3.65s\n",
      "Best model: Epoch 14 \t loss=0.013607 \t val_loss=0.013729 \t time=3.67s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 19 \t loss=0.012729 \t val_loss=0.013264 \t time=3.34s\n",
      "Best model: Epoch 20 \t loss=0.012570 \t val_loss=0.013227 \t time=3.34s\n",
      "Best model: Epoch 21 \t loss=0.012541 \t val_loss=0.013204 \t time=3.50s\n",
      "Best model: Epoch 22 \t loss=0.012518 \t val_loss=0.013188 \t time=3.35s\n",
      "Best model: Epoch 23 \t loss=0.012494 \t val_loss=0.013184 \t time=3.42s\n",
      "Best model: Epoch 24 \t loss=0.012491 \t val_loss=0.013169 \t time=3.77s\n",
      "Best model: Epoch 25 \t loss=0.012508 \t val_loss=0.013167 \t time=3.38s\n",
      "Best model: Epoch 26 \t loss=0.012485 \t val_loss=0.013147 \t time=3.49s\n",
      "Best model: Epoch 28 \t loss=0.012485 \t val_loss=0.013147 \t time=3.33s\n",
      "Best model: Epoch 29 \t loss=0.012476 \t val_loss=0.013143 \t time=3.33s\n",
      "Best model: Epoch 30 \t loss=0.012508 \t val_loss=0.013140 \t time=3.71s\n",
      "Fold 2 log loss: 0.013257163711842851\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.032278 \t val_loss=0.020066 \t time=3.63s\n",
      "Best model: Epoch 2 \t loss=0.019236 \t val_loss=0.018130 \t time=3.55s\n",
      "Best model: Epoch 3 \t loss=0.017524 \t val_loss=0.016790 \t time=3.84s\n",
      "Best model: Epoch 4 \t loss=0.016440 \t val_loss=0.015919 \t time=3.33s\n",
      "Best model: Epoch 5 \t loss=0.015649 \t val_loss=0.015308 \t time=3.31s\n",
      "Best model: Epoch 6 \t loss=0.015060 \t val_loss=0.014850 \t time=3.37s\n",
      "Best model: Epoch 7 \t loss=0.014720 \t val_loss=0.014649 \t time=3.33s\n",
      "Best model: Epoch 8 \t loss=0.014453 \t val_loss=0.014527 \t time=3.32s\n",
      "Best model: Epoch 9 \t loss=0.014335 \t val_loss=0.014382 \t time=3.40s\n",
      "Best model: Epoch 10 \t loss=0.014210 \t val_loss=0.014358 \t time=3.31s\n",
      "Best model: Epoch 11 \t loss=0.014195 \t val_loss=0.014250 \t time=3.41s\n",
      "Best model: Epoch 14 \t loss=0.014077 \t val_loss=0.014234 \t time=3.29s\n",
      "Best model: Epoch 17 \t loss=0.014108 \t val_loss=0.014164 \t time=3.65s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 22 \t loss=0.013261 \t val_loss=0.013630 \t time=3.64s\n",
      "Best model: Epoch 23 \t loss=0.013048 \t val_loss=0.013593 \t time=3.31s\n",
      "Best model: Epoch 24 \t loss=0.013019 \t val_loss=0.013547 \t time=3.31s\n",
      "Best model: Epoch 26 \t loss=0.012994 \t val_loss=0.013532 \t time=3.31s\n",
      "Best model: Epoch 27 \t loss=0.012999 \t val_loss=0.013506 \t time=3.30s\n",
      "Fold 3 log loss: 0.013515223848193056\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.032198 \t val_loss=0.019733 \t time=3.56s\n",
      "Best model: Epoch 2 \t loss=0.019168 \t val_loss=0.017764 \t time=3.48s\n",
      "Best model: Epoch 3 \t loss=0.017476 \t val_loss=0.016656 \t time=3.48s\n",
      "Best model: Epoch 4 \t loss=0.016524 \t val_loss=0.015942 \t time=3.85s\n",
      "Best model: Epoch 5 \t loss=0.015777 \t val_loss=0.015422 \t time=3.79s\n",
      "Best model: Epoch 6 \t loss=0.015213 \t val_loss=0.015065 \t time=3.55s\n",
      "Best model: Epoch 7 \t loss=0.014841 \t val_loss=0.014929 \t time=3.59s\n",
      "Best model: Epoch 8 \t loss=0.014639 \t val_loss=0.014770 \t time=3.48s\n",
      "Best model: Epoch 9 \t loss=0.014533 \t val_loss=0.014715 \t time=3.99s\n",
      "Best model: Epoch 12 \t loss=0.014326 \t val_loss=0.014564 \t time=3.47s\n",
      "Best model: Epoch 14 \t loss=0.014289 \t val_loss=0.014536 \t time=3.47s\n",
      "Best model: Epoch 18 \t loss=0.014330 \t val_loss=0.014513 \t time=3.88s\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 23 \t loss=0.013557 \t val_loss=0.014030 \t time=3.56s\n",
      "Best model: Epoch 24 \t loss=0.013341 \t val_loss=0.014008 \t time=3.49s\n",
      "Best model: Epoch 26 \t loss=0.013279 \t val_loss=0.014008 \t time=3.78s\n",
      "Best model: Epoch 27 \t loss=0.013270 \t val_loss=0.013989 \t time=3.55s\n",
      "Best model: Epoch 28 \t loss=0.013251 \t val_loss=0.013983 \t time=3.65s\n",
      "Best model: Epoch 29 \t loss=0.013256 \t val_loss=0.013961 \t time=3.49s\n",
      "Fold 4 log loss: 0.014090894954272952\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.032482 \t val_loss=0.020250 \t time=3.28s\n",
      "Best model: Epoch 2 \t loss=0.019289 \t val_loss=0.018290 \t time=3.45s\n",
      "Best model: Epoch 3 \t loss=0.017572 \t val_loss=0.017022 \t time=3.41s\n",
      "Best model: Epoch 4 \t loss=0.016505 \t val_loss=0.016147 \t time=3.32s\n",
      "Best model: Epoch 5 \t loss=0.015687 \t val_loss=0.015545 \t time=3.70s\n",
      "Best model: Epoch 6 \t loss=0.015068 \t val_loss=0.015081 \t time=3.59s\n",
      "Best model: Epoch 7 \t loss=0.014660 \t val_loss=0.014800 \t time=3.78s\n",
      "Best model: Epoch 8 \t loss=0.014403 \t val_loss=0.014604 \t time=3.35s\n",
      "Best model: Epoch 9 \t loss=0.014242 \t val_loss=0.014537 \t time=3.36s\n",
      "Best model: Epoch 10 \t loss=0.014154 \t val_loss=0.014472 \t time=3.37s\n",
      "Best model: Epoch 12 \t loss=0.014086 \t val_loss=0.014456 \t time=3.29s\n",
      "Best model: Epoch 13 \t loss=0.014053 \t val_loss=0.014319 \t time=3.36s\n",
      "Best model: Epoch 17 \t loss=0.014057 \t val_loss=0.014266 \t time=3.30s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best model: Epoch 22 \t loss=0.013205 \t val_loss=0.013824 \t time=3.33s\n",
      "Best model: Epoch 23 \t loss=0.013022 \t val_loss=0.013809 \t time=3.64s\n",
      "Best model: Epoch 24 \t loss=0.012998 \t val_loss=0.013798 \t time=3.56s\n",
      "Best model: Epoch 25 \t loss=0.012977 \t val_loss=0.013766 \t time=3.63s\n",
      "Best model: Epoch 26 \t loss=0.012979 \t val_loss=0.013763 \t time=3.47s\n",
      "Best model: Epoch 27 \t loss=0.012954 \t val_loss=0.013747 \t time=3.37s\n",
      "Best model: Epoch 29 \t loss=0.012964 \t val_loss=0.013743 \t time=3.40s\n",
      "Fold 5 log loss: 0.013716953183005226\n",
      "Seed 40\n",
      "Fold 1 log loss: 0.014003015909481074\n",
      "Fold 2 log loss: 0.013257163711842851\n",
      "Fold 3 log loss: 0.013515223848193056\n",
      "Fold 4 log loss: 0.014090894954272952\n",
      "Fold 5 log loss: 0.013716953183005226\n",
      "Std of log loss: 0.000307819737985495\n",
      "Total log loss: 0.013716671242798248\n",
      "Total log loss in targets: 0.013664821164165878\n"
     ]
    }
   ],
   "source": [
    "print(\"Transfer learning\")\n",
    "for seed_ in [0]:\n",
    "    files = modelling_lstm(fn_train, fn_all_targets, fn_test, seed_, fn_train.shape[1], fn_all_targets.shape[1],[])\n",
    "\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [0,10,40]\n",
    "#seeds = [0,1,2,3]\n",
    "\n",
    "print(\"Final prediction\")\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_lstm(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],files)\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T05:25:44.426551Z",
     "iopub.status.busy": "2020-10-15T05:25:44.425519Z",
     "iopub.status.idle": "2020-10-15T05:25:51.661048Z",
     "shell.execute_reply": "2020-10-15T05:25:51.660293Z"
    },
    "papermill": {
     "duration": 7.420893,
     "end_time": "2020-10-15T05:25:51.661168",
     "exception": false,
     "start_time": "2020-10-15T05:25:44.240275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.012594083098644272\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T05:25:52.020730Z",
     "iopub.status.busy": "2020-10-15T05:25:52.019921Z",
     "iopub.status.idle": "2020-10-15T05:25:54.809561Z",
     "shell.execute_reply": "2020-10-15T05:25:54.808894Z"
    },
    "papermill": {
     "duration": 2.97406,
     "end_time": "2020-10-15T05:25:54.809708",
     "exception": false,
     "start_time": "2020-10-15T05:25:51.835648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.17659,
     "end_time": "2020-10-15T05:25:55.161650",
     "exception": false,
     "start_time": "2020-10-15T05:25:54.985060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2538.198286,
   "end_time": "2020-10-15T05:25:55.445496",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-15T04:43:37.247210",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
