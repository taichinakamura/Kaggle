{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01551,
     "end_time": "2020-11-09T23:03:39.000851",
     "exception": false,
     "start_time": "2020-11-09T23:03:38.985341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- incorporate new validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:39.038319Z",
     "iopub.status.busy": "2020-11-09T23:03:39.037577Z",
     "iopub.status.idle": "2020-11-09T23:03:46.579783Z",
     "shell.execute_reply": "2020-11-09T23:03:46.578568Z"
    },
    "papermill": {
     "duration": 7.56466,
     "end_time": "2020-11-09T23:03:46.579923",
     "exception": false,
     "start_time": "2020-11-09T23:03:39.015263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:46.619774Z",
     "iopub.status.busy": "2020-11-09T23:03:46.619133Z",
     "iopub.status.idle": "2020-11-09T23:03:52.903570Z",
     "shell.execute_reply": "2020-11-09T23:03:52.902574Z"
    },
    "papermill": {
     "duration": 6.308641,
     "end_time": "2020-11-09T23:03:52.903697",
     "exception": false,
     "start_time": "2020-11-09T23:03:46.595056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:52.940029Z",
     "iopub.status.busy": "2020-11-09T23:03:52.939327Z",
     "iopub.status.idle": "2020-11-09T23:03:52.942965Z",
     "shell.execute_reply": "2020-11-09T23:03:52.943407Z"
    },
    "papermill": {
     "duration": 0.024828,
     "end_time": "2020-11-09T23:03:52.943531",
     "exception": false,
     "start_time": "2020-11-09T23:03:52.918703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:52.982181Z",
     "iopub.status.busy": "2020-11-09T23:03:52.981428Z",
     "iopub.status.idle": "2020-11-09T23:03:53.083147Z",
     "shell.execute_reply": "2020-11-09T23:03:53.082572Z"
    },
    "papermill": {
     "duration": 0.12511,
     "end_time": "2020-11-09T23:03:53.083257",
     "exception": false,
     "start_time": "2020-11-09T23:03:52.958147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015179,
     "end_time": "2020-11-09T23:03:53.114353",
     "exception": false,
     "start_time": "2020-11-09T23:03:53.099174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:53.163008Z",
     "iopub.status.busy": "2020-11-09T23:03:53.162092Z",
     "iopub.status.idle": "2020-11-09T23:03:53.165688Z",
     "shell.execute_reply": "2020-11-09T23:03:53.165021Z"
    },
    "papermill": {
     "duration": 0.035567,
     "end_time": "2020-11-09T23:03:53.165810",
     "exception": false,
     "start_time": "2020-11-09T23:03:53.130243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_fold(NB_SPLITS, seed):   \n",
    "    folds = []\n",
    "    # LOAD FILES\n",
    "    train_score = targets.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = train_score.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= 19].index.sort_values()\n",
    "    vc2 = vc.loc[vc > 19].index.sort_values()\n",
    "    \n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "    tmp = train_score.groupby('drug_id')[target_feats].mean().loc[vc1]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "    tmp = train_score.loc[train_score.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN FOLDS\n",
    "    train_score['fold'] = train_score.drug_id.map(dct1)\n",
    "    train_score.loc[train_score.fold.isna(),'fold'] = train_score.loc[train_score.fold.isna(),'sig_id'].map(dct2)\n",
    "    train_score.fold = train_score.fold.astype('int8')\n",
    "    folds.append(train_score.fold.values)\n",
    "    \n",
    "    return np.array(folds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:53.210034Z",
     "iopub.status.busy": "2020-11-09T23:03:53.208916Z",
     "iopub.status.idle": "2020-11-09T23:03:53.527487Z",
     "shell.execute_reply": "2020-11-09T23:03:53.526899Z"
    },
    "papermill": {
     "duration": 0.3462,
     "end_time": "2020-11-09T23:03:53.527614",
     "exception": false,
     "start_time": "2020-11-09T23:03:53.181414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:53.580057Z",
     "iopub.status.busy": "2020-11-09T23:03:53.573287Z",
     "iopub.status.idle": "2020-11-09T23:03:54.098804Z",
     "shell.execute_reply": "2020-11-09T23:03:54.097545Z"
    },
    "papermill": {
     "duration": 0.555589,
     "end_time": "2020-11-09T23:03:54.098944",
     "exception": false,
     "start_time": "2020-11-09T23:03:53.543355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_folds = make_fold(7, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015013,
     "end_time": "2020-11-09T23:03:54.129161",
     "exception": false,
     "start_time": "2020-11-09T23:03:54.114148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:54.167635Z",
     "iopub.status.busy": "2020-11-09T23:03:54.166167Z",
     "iopub.status.idle": "2020-11-09T23:03:54.291131Z",
     "shell.execute_reply": "2020-11-09T23:03:54.291633Z"
    },
    "papermill": {
     "duration": 0.147785,
     "end_time": "2020-11-09T23:03:54.291787",
     "exception": false,
     "start_time": "2020-11-09T23:03:54.144002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 872) (3982, 872)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    #tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\", \"cp_dose\", \"cp_time\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:54.329888Z",
     "iopub.status.busy": "2020-11-09T23:03:54.328693Z",
     "iopub.status.idle": "2020-11-09T23:03:54.662463Z",
     "shell.execute_reply": "2020-11-09T23:03:54.669195Z"
    },
    "papermill": {
     "duration": 0.361372,
     "end_time": "2020-11-09T23:03:54.669405",
     "exception": false,
     "start_time": "2020-11-09T23:03:54.308033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = f_train.copy().values\n",
    "select = VarianceThreshold(threshold=0.4)\n",
    "X_new = select.fit_transform(X)\n",
    "drop_feats = list(np.array(f_train.columns)[select.get_support()==False])\n",
    "len(drop_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:54.729559Z",
     "iopub.status.busy": "2020-11-09T23:03:54.722372Z",
     "iopub.status.idle": "2020-11-09T23:03:56.797265Z",
     "shell.execute_reply": "2020-11-09T23:03:56.796637Z"
    },
    "papermill": {
     "duration": 2.105369,
     "end_time": "2020-11-09T23:03:56.797403",
     "exception": false,
     "start_time": "2020-11-09T23:03:54.692034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_train.drop(drop_feats, axis = 1, inplace=True)\n",
    "f_test.drop(drop_feats, axis = 1, inplace=True)\n",
    "\n",
    "modg_feats = [i for i in f_train.columns if \"g-\" in i]\n",
    "modc_feats = [i for i in f_train.columns if \"c-\" in i]\n",
    "\n",
    "for i in modc_feats + modg_feats:\n",
    "    ss = preprocessing.RobustScaler()\n",
    "    ss.fit(f_train[i].values.reshape(-1,1))\n",
    "    f_train[i] = ss.transform(f_train[i].values.reshape(-1,1))\n",
    "    f_test[i] = ss.transform(f_test[i].values.reshape(-1,1))\n",
    "\n",
    "f_train[\"fold\"] = lstm_folds\n",
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:56.842760Z",
     "iopub.status.busy": "2020-11-09T23:03:56.840971Z",
     "iopub.status.idle": "2020-11-09T23:03:56.843457Z",
     "shell.execute_reply": "2020-11-09T23:03:56.843953Z"
    },
    "papermill": {
     "duration": 0.029448,
     "end_time": "2020-11-09T23:03:56.844066",
     "exception": false,
     "start_time": "2020-11-09T23:03:56.814618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "#             targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:56.890848Z",
     "iopub.status.busy": "2020-11-09T23:03:56.889583Z",
     "iopub.status.idle": "2020-11-09T23:03:56.892677Z",
     "shell.execute_reply": "2020-11-09T23:03:56.892213Z"
    },
    "papermill": {
     "duration": 0.03292,
     "end_time": "2020-11-09T23:03:56.892788",
     "exception": false,
     "start_time": "2020-11-09T23:03:56.859868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, c_lstm_hidden_size, last_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g_layer_num = 1\n",
    "        self.c_layer_num = 1\n",
    "\n",
    "        self.hidden_dim = 512\n",
    "        self.hidden_dim_c = 10\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_hidden_size, self.hidden_dim, batch_first=True, bidirectional=True, num_layers=self.g_layer_num)\n",
    "        self.c_lstm = nn.LSTM(c_lstm_hidden_size, self.hidden_dim_c, batch_first=True, bidirectional=True, num_layers=self.c_layer_num)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d((self.hidden_dim+self.hidden_dim_c) * 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.utils.weight_norm(nn.Linear((self.hidden_dim+self.hidden_dim_c) * 2, last_num))\n",
    "        \n",
    "    def forward(self, cont_g, cont_c): \n",
    "        cont_g = torch.unsqueeze(cont_g, 1)\n",
    "        h_lstm, lstm_out = self.lstm(cont_g) # h_lstm: 256 * 1 * (2 * 512)\n",
    "        conc_g = h_lstm.view(-1, self.hidden_dim * 2)\n",
    "        \n",
    "        cont_c = torch.unsqueeze(cont_c, 1)\n",
    "        h_lstm_c, lstm_out_c = self.c_lstm(cont_c) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        conc_c = h_lstm_c.view(-1, self.hidden_dim_c * 2)\n",
    "        \n",
    "        conc = torch.cat((conc_g, conc_c),1)\n",
    "        conc = self.batch_norm(conc)\n",
    "        dropped = self.dropout(conc)\n",
    "        out = self.out(dropped)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:56.960479Z",
     "iopub.status.busy": "2020-11-09T23:03:56.938031Z",
     "iopub.status.idle": "2020-11-09T23:03:57.332590Z",
     "shell.execute_reply": "2020-11-09T23:03:57.332079Z"
    },
    "papermill": {
     "duration": 0.423615,
     "end_time": "2020-11-09T23:03:57.332706",
     "exception": false,
     "start_time": "2020-11-09T23:03:56.909091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_epochs = 30\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def modelling_lstm(tr, target, te, sample_seed, last_num):\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "    \n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    X_test_g = torch.tensor(X_test[:,:len(modg_feats)], dtype=torch.float32)\n",
    "    X_test_c = torch.tensor(X_test[:,len(modg_feats):], dtype=torch.float32)\n",
    "\n",
    "    X_test = torch.utils.data.TensorDataset(X_test_g, X_test_c) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "            \n",
    "    for fold in range(n_folds):\n",
    "        valid_index = X_train[:,-1] == fold\n",
    "        train_index = X_train[:,-1] != fold        \n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2_g = torch.tensor(X_train[train_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_valid2_g = torch.tensor(X_train[valid_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_train2_c = torch.tensor(X_train[train_index,len(modg_feats):-1], dtype=torch.float32)\n",
    "        X_valid2_c = torch.tensor(X_train[valid_index,len(modg_feats):-1], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2_g, X_train2_c, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2_g, X_valid2_c, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        #loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.01, weight_decay=1e-5) \n",
    "        #lookahead = Lookahead(optimizer, k=3, alpha=0.5) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        #scheduler2 = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e1, \n",
    "        #                                      max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch_g, x_batch_c, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best: Epoch {} \\t loss={:.6f}  val_loss={:.6f}  sm_loss={:.6f} \\t sm_val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        pred_model = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, ) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "\n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:03:57.375216Z",
     "iopub.status.busy": "2020-11-09T23:03:57.374444Z",
     "iopub.status.idle": "2020-11-09T23:13:29.001777Z",
     "shell.execute_reply": "2020-11-09T23:13:29.000834Z"
    },
    "papermill": {
     "duration": 571.652015,
     "end_time": "2020-11-09T23:13:29.001903",
     "exception": false,
     "start_time": "2020-11-09T23:03:57.349888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.137525  val_loss=0.019475  sm_loss=0.137525 \t sm_val_loss=0.019481 \t time=0.91s\n",
      "Best: Epoch 2 \t loss=0.018280  val_loss=0.018329  sm_loss=0.018351 \t sm_val_loss=0.018327 \t time=0.72s\n",
      "Best: Epoch 3 \t loss=0.017389  val_loss=0.018294  sm_loss=0.017497 \t sm_val_loss=0.018322 \t time=1.00s\n",
      "Best: Epoch 4 \t loss=0.016932  val_loss=0.018039  sm_loss=0.017079 \t sm_val_loss=0.018018 \t time=1.02s\n",
      "Best: Epoch 5 \t loss=0.016671  val_loss=0.017872  sm_loss=0.016831 \t sm_val_loss=0.017866 \t time=0.72s\n",
      "Best: Epoch 6 \t loss=0.016465  val_loss=0.017683  sm_loss=0.016638 \t sm_val_loss=0.017674 \t time=0.76s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014685  val_loss=0.017163  sm_loss=0.014901 \t sm_val_loss=0.017170 \t time=0.72s\n",
      "Best: Epoch 12 \t loss=0.013396  val_loss=0.017046  sm_loss=0.013629 \t sm_val_loss=0.017050 \t time=0.71s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.017359587598663368\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.138275  val_loss=0.019423  sm_loss=0.138269 \t sm_val_loss=0.019461 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018564  val_loss=0.017938  sm_loss=0.018622 \t sm_val_loss=0.017956 \t time=0.80s\n",
      "Best: Epoch 3 \t loss=0.017465  val_loss=0.017648  sm_loss=0.017575 \t sm_val_loss=0.017649 \t time=0.70s\n",
      "Best: Epoch 4 \t loss=0.017008  val_loss=0.017495  sm_loss=0.017157 \t sm_val_loss=0.017533 \t time=0.71s\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 9 \t loss=0.014582  val_loss=0.016865  sm_loss=0.014801 \t sm_val_loss=0.016895 \t time=0.75s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.017171096857667708\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.135363  val_loss=0.020085  sm_loss=0.135360 \t sm_val_loss=0.020111 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018347  val_loss=0.018869  sm_loss=0.018416 \t sm_val_loss=0.018910 \t time=0.70s\n",
      "Best: Epoch 3 \t loss=0.017426  val_loss=0.018654  sm_loss=0.017549 \t sm_val_loss=0.018684 \t time=0.72s\n",
      "Best: Epoch 4 \t loss=0.016919  val_loss=0.018531  sm_loss=0.017066 \t sm_val_loss=0.018532 \t time=0.70s\n",
      "Best: Epoch 5 \t loss=0.016546  val_loss=0.018460  sm_loss=0.016713 \t sm_val_loss=0.018489 \t time=0.70s\n",
      "Best: Epoch 6 \t loss=0.016420  val_loss=0.018430  sm_loss=0.016596 \t sm_val_loss=0.018422 \t time=0.99s\n",
      "Best: Epoch 7 \t loss=0.016284  val_loss=0.018354  sm_loss=0.016464 \t sm_val_loss=0.018335 \t time=0.79s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014420  val_loss=0.017703  sm_loss=0.014629 \t sm_val_loss=0.017707 \t time=0.72s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017586843070450886\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.138022  val_loss=0.020071  sm_loss=0.138022 \t sm_val_loss=0.020024 \t time=0.85s\n",
      "Best: Epoch 2 \t loss=0.018276  val_loss=0.018817  sm_loss=0.018350 \t sm_val_loss=0.018831 \t time=1.38s\n",
      "Best: Epoch 3 \t loss=0.017287  val_loss=0.018659  sm_loss=0.017403 \t sm_val_loss=0.018585 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016851  val_loss=0.018376  sm_loss=0.017005 \t sm_val_loss=0.018400 \t time=0.70s\n",
      "Best: Epoch 6 \t loss=0.016255  val_loss=0.018268  sm_loss=0.016433 \t sm_val_loss=0.018265 \t time=0.71s\n",
      "Best: Epoch 7 \t loss=0.016201  val_loss=0.018215  sm_loss=0.016383 \t sm_val_loss=0.018189 \t time=0.74s\n",
      "Best: Epoch 10 \t loss=0.016142  val_loss=0.018215  sm_loss=0.016323 \t sm_val_loss=0.018166 \t time=0.72s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014417  val_loss=0.017685  sm_loss=0.014637 \t sm_val_loss=0.017676 \t time=0.77s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.01792602075915021\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.135228  val_loss=0.019851  sm_loss=0.135224 \t sm_val_loss=0.019857 \t time=0.71s\n",
      "Best: Epoch 2 \t loss=0.018218  val_loss=0.018863  sm_loss=0.018291 \t sm_val_loss=0.018840 \t time=0.72s\n",
      "Best: Epoch 3 \t loss=0.017478  val_loss=0.018546  sm_loss=0.017592 \t sm_val_loss=0.018517 \t time=0.74s\n",
      "Best: Epoch 4 \t loss=0.017103  val_loss=0.018496  sm_loss=0.017246 \t sm_val_loss=0.018490 \t time=0.71s\n",
      "Best: Epoch 5 \t loss=0.016720  val_loss=0.018386  sm_loss=0.016886 \t sm_val_loss=0.018357 \t time=0.88s\n",
      "Best: Epoch 7 \t loss=0.016485  val_loss=0.018344  sm_loss=0.016660 \t sm_val_loss=0.018286 \t time=0.77s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014637  val_loss=0.017516  sm_loss=0.014842 \t sm_val_loss=0.017488 \t time=0.79s\n",
      "Best: Epoch 13 \t loss=0.013417  val_loss=0.017495  sm_loss=0.013656 \t sm_val_loss=0.017452 \t time=0.96s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.017449466891217492\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.143866  val_loss=0.020166  sm_loss=0.143861 \t sm_val_loss=0.020154 \t time=0.89s\n",
      "Best: Epoch 2 \t loss=0.018535  val_loss=0.018819  sm_loss=0.018601 \t sm_val_loss=0.018860 \t time=0.74s\n",
      "Best: Epoch 3 \t loss=0.017440  val_loss=0.018428  sm_loss=0.017548 \t sm_val_loss=0.018501 \t time=0.87s\n",
      "Best: Epoch 4 \t loss=0.016871  val_loss=0.018338  sm_loss=0.017020 \t sm_val_loss=0.018404 \t time=0.95s\n",
      "Best: Epoch 5 \t loss=0.016531  val_loss=0.018239  sm_loss=0.016697 \t sm_val_loss=0.018305 \t time=0.75s\n",
      "Best: Epoch 6 \t loss=0.016386  val_loss=0.018121  sm_loss=0.016567 \t sm_val_loss=0.018205 \t time=0.77s\n",
      "Best: Epoch 9 \t loss=0.016165  val_loss=0.017977  sm_loss=0.016348 \t sm_val_loss=0.018072 \t time=1.02s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014496  val_loss=0.017423  sm_loss=0.014712 \t sm_val_loss=0.017518 \t time=0.98s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.01748793710454901\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.138758  val_loss=0.019826  sm_loss=0.138753 \t sm_val_loss=0.019810 \t time=0.91s\n",
      "Best: Epoch 2 \t loss=0.018446  val_loss=0.018655  sm_loss=0.018519 \t sm_val_loss=0.018656 \t time=0.83s\n",
      "Best: Epoch 3 \t loss=0.017520  val_loss=0.018628  sm_loss=0.017629 \t sm_val_loss=0.018647 \t time=0.76s\n",
      "Best: Epoch 4 \t loss=0.016957  val_loss=0.018194  sm_loss=0.017104 \t sm_val_loss=0.018214 \t time=1.10s\n",
      "Best: Epoch 5 \t loss=0.016704  val_loss=0.017990  sm_loss=0.016868 \t sm_val_loss=0.018040 \t time=1.02s\n",
      "Best: Epoch 9 \t loss=0.016366  val_loss=0.017907  sm_loss=0.016538 \t sm_val_loss=0.017934 \t time=1.16s\n",
      "Best: Epoch 10 \t loss=0.016427  val_loss=0.017880  sm_loss=0.016591 \t sm_val_loss=0.017924 \t time=0.81s\n",
      "Best: Epoch 11 \t loss=0.016402  val_loss=0.017868  sm_loss=0.016567 \t sm_val_loss=0.017894 \t time=0.93s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 16 \t loss=0.014871  val_loss=0.017131  sm_loss=0.015071 \t sm_val_loss=0.017218 \t time=0.76s\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.0171725811247773\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.017359587598663368\n",
      "Fold 2 log loss: 0.017171096857667708\n",
      "Fold 3 log loss: 0.017586843070450886\n",
      "Fold 4 log loss: 0.01792602075915021\n",
      "Fold 5 log loss: 0.017449466891217492\n",
      "Fold 6 log loss: 0.01748793710454901\n",
      "Fold 7 log loss: 0.0171725811247773\n",
      "Std of log loss: 0.00024192150765459678\n",
      "Total log loss: 0.01745052200664554\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.138404  val_loss=0.019409  sm_loss=0.138406 \t sm_val_loss=0.019414 \t time=0.82s\n",
      "Best: Epoch 2 \t loss=0.018310  val_loss=0.018310  sm_loss=0.018383 \t sm_val_loss=0.018310 \t time=1.11s\n",
      "Best: Epoch 3 \t loss=0.017382  val_loss=0.018126  sm_loss=0.017502 \t sm_val_loss=0.018144 \t time=0.80s\n",
      "Best: Epoch 6 \t loss=0.016468  val_loss=0.017877  sm_loss=0.016640 \t sm_val_loss=0.017834 \t time=0.74s\n",
      "Best: Epoch 8 \t loss=0.016334  val_loss=0.017734  sm_loss=0.016507 \t sm_val_loss=0.017696 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.016361  val_loss=0.017709  sm_loss=0.016527 \t sm_val_loss=0.017722 \t time=0.79s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 16 \t loss=0.014689  val_loss=0.017178  sm_loss=0.014902 \t sm_val_loss=0.017204 \t time=0.99s\n",
      "Best: Epoch 17 \t loss=0.013384  val_loss=0.017072  sm_loss=0.013624 \t sm_val_loss=0.017097 \t time=0.87s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.01739774664270017\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.136255  val_loss=0.019263  sm_loss=0.136250 \t sm_val_loss=0.019283 \t time=0.79s\n",
      "Best: Epoch 2 \t loss=0.018408  val_loss=0.017904  sm_loss=0.018478 \t sm_val_loss=0.017943 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.017427  val_loss=0.017685  sm_loss=0.017545 \t sm_val_loss=0.017720 \t time=0.75s\n",
      "Best: Epoch 5 \t loss=0.016648  val_loss=0.017548  sm_loss=0.016822 \t sm_val_loss=0.017597 \t time=0.72s\n",
      "Best: Epoch 8 \t loss=0.016384  val_loss=0.017539  sm_loss=0.016558 \t sm_val_loss=0.017500 \t time=0.98s\n",
      "Best: Epoch 9 \t loss=0.016350  val_loss=0.017517  sm_loss=0.016523 \t sm_val_loss=0.017530 \t time=0.73s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014692  val_loss=0.016833  sm_loss=0.014904 \t sm_val_loss=0.016871 \t time=1.25s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.01715905294412915\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.140418  val_loss=0.020017  sm_loss=0.140414 \t sm_val_loss=0.020037 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018420  val_loss=0.019265  sm_loss=0.018488 \t sm_val_loss=0.019269 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017524  val_loss=0.018744  sm_loss=0.017625 \t sm_val_loss=0.018784 \t time=0.77s\n",
      "Best: Epoch 4 \t loss=0.016996  val_loss=0.018539  sm_loss=0.017148 \t sm_val_loss=0.018520 \t time=0.99s\n",
      "Best: Epoch 5 \t loss=0.016570  val_loss=0.018372  sm_loss=0.016740 \t sm_val_loss=0.018406 \t time=0.84s\n",
      "Best: Epoch 6 \t loss=0.016463  val_loss=0.018345  sm_loss=0.016640 \t sm_val_loss=0.018363 \t time=0.71s\n",
      "Best: Epoch 8 \t loss=0.016225  val_loss=0.018239  sm_loss=0.016404 \t sm_val_loss=0.018233 \t time=0.82s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014550  val_loss=0.017694  sm_loss=0.014762 \t sm_val_loss=0.017712 \t time=0.74s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017566428566035356\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.137964  val_loss=0.020050  sm_loss=0.137963 \t sm_val_loss=0.020016 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018365  val_loss=0.018830  sm_loss=0.018438 \t sm_val_loss=0.018788 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017297  val_loss=0.018537  sm_loss=0.017418 \t sm_val_loss=0.018537 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016811  val_loss=0.018392  sm_loss=0.016963 \t sm_val_loss=0.018361 \t time=0.81s\n",
      "Best: Epoch 6 \t loss=0.016211  val_loss=0.018374  sm_loss=0.016395 \t sm_val_loss=0.018318 \t time=0.72s\n",
      "Best: Epoch 8 \t loss=0.016128  val_loss=0.018249  sm_loss=0.016310 \t sm_val_loss=0.018236 \t time=0.72s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014371  val_loss=0.017614  sm_loss=0.014588 \t sm_val_loss=0.017626 \t time=0.70s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.017827480177308862\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.137538  val_loss=0.019992  sm_loss=0.137536 \t sm_val_loss=0.020004 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018419  val_loss=0.018773  sm_loss=0.018488 \t sm_val_loss=0.018785 \t time=0.87s\n",
      "Best: Epoch 3 \t loss=0.017409  val_loss=0.018475  sm_loss=0.017531 \t sm_val_loss=0.018443 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.017029  val_loss=0.018275  sm_loss=0.017179 \t sm_val_loss=0.018161 \t time=0.84s\n",
      "Best: Epoch 8 \t loss=0.016389  val_loss=0.018144  sm_loss=0.016563 \t sm_val_loss=0.018108 \t time=0.71s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014618  val_loss=0.017508  sm_loss=0.014832 \t sm_val_loss=0.017489 \t time=0.70s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.017446243382924886\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.141297  val_loss=0.019986  sm_loss=0.141295 \t sm_val_loss=0.019989 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.018427  val_loss=0.018827  sm_loss=0.018494 \t sm_val_loss=0.018829 \t time=1.07s\n",
      "Best: Epoch 3 \t loss=0.017459  val_loss=0.018380  sm_loss=0.017580 \t sm_val_loss=0.018400 \t time=1.12s\n",
      "Best: Epoch 4 \t loss=0.016887  val_loss=0.018232  sm_loss=0.017042 \t sm_val_loss=0.018246 \t time=0.82s\n",
      "Best: Epoch 5 \t loss=0.016570  val_loss=0.018178  sm_loss=0.016735 \t sm_val_loss=0.018245 \t time=0.74s\n",
      "Best: Epoch 6 \t loss=0.016430  val_loss=0.018101  sm_loss=0.016606 \t sm_val_loss=0.018170 \t time=0.75s\n",
      "Best: Epoch 10 \t loss=0.016336  val_loss=0.018054  sm_loss=0.016501 \t sm_val_loss=0.018141 \t time=0.73s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014679  val_loss=0.017376  sm_loss=0.014882 \t sm_val_loss=0.017469 \t time=0.87s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.017427712428907668\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.135384  val_loss=0.019785  sm_loss=0.135377 \t sm_val_loss=0.019787 \t time=0.71s\n",
      "Best: Epoch 2 \t loss=0.018379  val_loss=0.018620  sm_loss=0.018450 \t sm_val_loss=0.018617 \t time=0.92s\n",
      "Best: Epoch 3 \t loss=0.017407  val_loss=0.018432  sm_loss=0.017518 \t sm_val_loss=0.018429 \t time=0.71s\n",
      "Best: Epoch 4 \t loss=0.016958  val_loss=0.017980  sm_loss=0.017104 \t sm_val_loss=0.018008 \t time=0.71s\n",
      "Best: Epoch 7 \t loss=0.016391  val_loss=0.017934  sm_loss=0.016559 \t sm_val_loss=0.017994 \t time=0.72s\n",
      "Best: Epoch 8 \t loss=0.016378  val_loss=0.017867  sm_loss=0.016544 \t sm_val_loss=0.017918 \t time=0.72s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014604  val_loss=0.017241  sm_loss=0.014820 \t sm_val_loss=0.017329 \t time=0.70s\n",
      "Best: Epoch 14 \t loss=0.013253  val_loss=0.017145  sm_loss=0.013488 \t sm_val_loss=0.017216 \t time=0.79s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.017191118481276656\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.01739774664270017\n",
      "Fold 2 log loss: 0.01715905294412915\n",
      "Fold 3 log loss: 0.017566428566035356\n",
      "Fold 4 log loss: 0.017827480177308862\n",
      "Fold 5 log loss: 0.017446243382924886\n",
      "Fold 6 log loss: 0.017427712428907668\n",
      "Fold 7 log loss: 0.017191118481276656\n",
      "Std of log loss: 0.00020989266461773779\n",
      "Total log loss: 0.017431000893789305\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.137649  val_loss=0.019550  sm_loss=0.137647 \t sm_val_loss=0.019540 \t time=0.72s\n",
      "Best: Epoch 2 \t loss=0.018393  val_loss=0.018369  sm_loss=0.018461 \t sm_val_loss=0.018327 \t time=0.73s\n",
      "Best: Epoch 3 \t loss=0.017345  val_loss=0.018100  sm_loss=0.017466 \t sm_val_loss=0.018065 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.017130  val_loss=0.017897  sm_loss=0.017267 \t sm_val_loss=0.017913 \t time=0.73s\n",
      "Best: Epoch 5 \t loss=0.016682  val_loss=0.017846  sm_loss=0.016846 \t sm_val_loss=0.017817 \t time=0.88s\n",
      "Best: Epoch 7 \t loss=0.016354  val_loss=0.017757  sm_loss=0.016525 \t sm_val_loss=0.017769 \t time=0.76s\n",
      "Best: Epoch 11 \t loss=0.016311  val_loss=0.017712  sm_loss=0.016483 \t sm_val_loss=0.017687 \t time=0.71s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 16 \t loss=0.014655  val_loss=0.017147  sm_loss=0.014867 \t sm_val_loss=0.017191 \t time=0.76s\n",
      "Best: Epoch 17 \t loss=0.013380  val_loss=0.017079  sm_loss=0.013614 \t sm_val_loss=0.017119 \t time=1.55s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.017394354631794198\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.142542  val_loss=0.019374  sm_loss=0.142538 \t sm_val_loss=0.019415 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018590  val_loss=0.017908  sm_loss=0.018657 \t sm_val_loss=0.017958 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017475  val_loss=0.017609  sm_loss=0.017591 \t sm_val_loss=0.017666 \t time=0.77s\n",
      "Best: Epoch 4 \t loss=0.016998  val_loss=0.017539  sm_loss=0.017146 \t sm_val_loss=0.017557 \t time=0.74s\n",
      "Best: Epoch 5 \t loss=0.016624  val_loss=0.017478  sm_loss=0.016801 \t sm_val_loss=0.017461 \t time=0.75s\n",
      "Best: Epoch 8 \t loss=0.016268  val_loss=0.017439  sm_loss=0.016452 \t sm_val_loss=0.017443 \t time=0.72s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014422  val_loss=0.016864  sm_loss=0.014640 \t sm_val_loss=0.016891 \t time=0.70s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.017151502444623386\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.137008  val_loss=0.020101  sm_loss=0.136998 \t sm_val_loss=0.020095 \t time=0.70s\n",
      "Best: Epoch 2 \t loss=0.018311  val_loss=0.018934  sm_loss=0.018382 \t sm_val_loss=0.018961 \t time=0.75s\n",
      "Best: Epoch 3 \t loss=0.017336  val_loss=0.018762  sm_loss=0.017452 \t sm_val_loss=0.018749 \t time=0.71s\n",
      "Best: Epoch 4 \t loss=0.016925  val_loss=0.018532  sm_loss=0.017075 \t sm_val_loss=0.018524 \t time=0.71s\n",
      "Best: Epoch 5 \t loss=0.016561  val_loss=0.018524  sm_loss=0.016735 \t sm_val_loss=0.018548 \t time=0.71s\n",
      "Best: Epoch 6 \t loss=0.016388  val_loss=0.018364  sm_loss=0.016560 \t sm_val_loss=0.018386 \t time=0.98s\n",
      "Best: Epoch 9 \t loss=0.016262  val_loss=0.018346  sm_loss=0.016437 \t sm_val_loss=0.018383 \t time=0.86s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014621  val_loss=0.017709  sm_loss=0.014828 \t sm_val_loss=0.017746 \t time=0.74s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017588761516546227\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.134801  val_loss=0.019991  sm_loss=0.134799 \t sm_val_loss=0.019980 \t time=0.79s\n",
      "Best: Epoch 2 \t loss=0.018270  val_loss=0.018949  sm_loss=0.018347 \t sm_val_loss=0.018954 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017275  val_loss=0.018551  sm_loss=0.017397 \t sm_val_loss=0.018509 \t time=0.78s\n",
      "Best: Epoch 5 \t loss=0.016500  val_loss=0.018370  sm_loss=0.016673 \t sm_val_loss=0.018375 \t time=0.74s\n",
      "Best: Epoch 7 \t loss=0.016302  val_loss=0.018151  sm_loss=0.016479 \t sm_val_loss=0.018121 \t time=0.75s\n",
      "Best: Epoch 10 \t loss=0.016334  val_loss=0.018145  sm_loss=0.016503 \t sm_val_loss=0.018130 \t time=0.73s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014611  val_loss=0.017686  sm_loss=0.014820 \t sm_val_loss=0.017711 \t time=0.77s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.017905397281693443\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.137738  val_loss=0.019957  sm_loss=0.137738 \t sm_val_loss=0.019962 \t time=0.76s\n",
      "Best: Epoch 2 \t loss=0.018449  val_loss=0.018861  sm_loss=0.018518 \t sm_val_loss=0.018866 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.017679  val_loss=0.018407  sm_loss=0.017786 \t sm_val_loss=0.018413 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.017190  val_loss=0.018364  sm_loss=0.017336 \t sm_val_loss=0.018313 \t time=1.08s\n",
      "Best: Epoch 5 \t loss=0.016651  val_loss=0.018150  sm_loss=0.016822 \t sm_val_loss=0.018142 \t time=0.84s\n",
      "Best: Epoch 6 \t loss=0.016424  val_loss=0.018075  sm_loss=0.016600 \t sm_val_loss=0.018036 \t time=0.93s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014605  val_loss=0.017504  sm_loss=0.014819 \t sm_val_loss=0.017482 \t time=0.76s\n",
      "Best: Epoch 12 \t loss=0.013327  val_loss=0.017476  sm_loss=0.013566 \t sm_val_loss=0.017442 \t time=0.74s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.01742710636114265\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.143981  val_loss=0.020028  sm_loss=0.143981 \t sm_val_loss=0.020037 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.018399  val_loss=0.018759  sm_loss=0.018461 \t sm_val_loss=0.018741 \t time=1.09s\n",
      "Best: Epoch 3 \t loss=0.017409  val_loss=0.018464  sm_loss=0.017518 \t sm_val_loss=0.018521 \t time=0.88s\n",
      "Best: Epoch 5 \t loss=0.016699  val_loss=0.018301  sm_loss=0.016863 \t sm_val_loss=0.018352 \t time=0.83s\n",
      "Best: Epoch 6 \t loss=0.016435  val_loss=0.018129  sm_loss=0.016608 \t sm_val_loss=0.018195 \t time=1.02s\n",
      "Best: Epoch 10 \t loss=0.016373  val_loss=0.018028  sm_loss=0.016542 \t sm_val_loss=0.018074 \t time=0.74s\n",
      "Best: Epoch 12 \t loss=0.016428  val_loss=0.018003  sm_loss=0.016595 \t sm_val_loss=0.018068 \t time=1.08s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 17 \t loss=0.014839  val_loss=0.017416  sm_loss=0.015038 \t sm_val_loss=0.017492 \t time=0.83s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.017492217718010895\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.137201  val_loss=0.019831  sm_loss=0.137193 \t sm_val_loss=0.019848 \t time=0.83s\n",
      "Best: Epoch 2 \t loss=0.018421  val_loss=0.018655  sm_loss=0.018486 \t sm_val_loss=0.018669 \t time=0.79s\n",
      "Best: Epoch 3 \t loss=0.017551  val_loss=0.018160  sm_loss=0.017665 \t sm_val_loss=0.018200 \t time=0.80s\n",
      "Best: Epoch 4 \t loss=0.016878  val_loss=0.018006  sm_loss=0.017022 \t sm_val_loss=0.018019 \t time=0.71s\n",
      "Best: Epoch 6 \t loss=0.016397  val_loss=0.017983  sm_loss=0.016573 \t sm_val_loss=0.017990 \t time=0.94s\n",
      "Best: Epoch 7 \t loss=0.016311  val_loss=0.017877  sm_loss=0.016488 \t sm_val_loss=0.017931 \t time=0.83s\n",
      "Best: Epoch 8 \t loss=0.016286  val_loss=0.017828  sm_loss=0.016459 \t sm_val_loss=0.017908 \t time=0.85s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014464  val_loss=0.017197  sm_loss=0.014678 \t sm_val_loss=0.017263 \t time=0.71s\n",
      "Best: Epoch 14 \t loss=0.013175  val_loss=0.017167  sm_loss=0.013414 \t sm_val_loss=0.017225 \t time=0.96s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.01723593460261244\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.017394354631794198\n",
      "Fold 2 log loss: 0.017151502444623386\n",
      "Fold 3 log loss: 0.017588761516546227\n",
      "Fold 4 log loss: 0.017905397281693443\n",
      "Fold 5 log loss: 0.01742710636114265\n",
      "Fold 6 log loss: 0.017492217718010895\n",
      "Fold 7 log loss: 0.01723593460261244\n",
      "Std of log loss: 0.00022887184888842533\n",
      "Total log loss: 0.01745645820934168\n",
      "Total log loss in targets: 0.01727429288818289\n"
     ]
    }
   ],
   "source": [
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_lstm(fn_train, fn_targets, fn_test, seed_, fn_targets.shape[1])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:13:29.249895Z",
     "iopub.status.busy": "2020-11-09T23:13:29.249065Z",
     "iopub.status.idle": "2020-11-09T23:13:35.249977Z",
     "shell.execute_reply": "2020-11-09T23:13:35.249414Z"
    },
    "papermill": {
     "duration": 6.140628,
     "end_time": "2020-11-09T23:13:35.250092",
     "exception": false,
     "start_time": "2020-11-09T23:13:29.109464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.0159207264764357\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:13:35.472426Z",
     "iopub.status.busy": "2020-11-09T23:13:35.471409Z",
     "iopub.status.idle": "2020-11-09T23:13:47.419761Z",
     "shell.execute_reply": "2020-11-09T23:13:47.418378Z"
    },
    "papermill": {
     "duration": 12.064312,
     "end_time": "2020-11-09T23:13:47.419883",
     "exception": false,
     "start_time": "2020-11-09T23:13:35.355571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_checkscore.to_csv(\"lstm_newval_oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T23:13:47.631672Z",
     "iopub.status.busy": "2020-11-09T23:13:47.630856Z",
     "iopub.status.idle": "2020-11-09T23:13:49.961592Z",
     "shell.execute_reply": "2020-11-09T23:13:49.960364Z"
    },
    "papermill": {
     "duration": 2.437226,
     "end_time": "2020-11-09T23:13:49.961713",
     "exception": false,
     "start_time": "2020-11-09T23:13:47.524487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.100796,
     "end_time": "2020-11-09T23:13:50.163835",
     "exception": false,
     "start_time": "2020-11-09T23:13:50.063039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 615.743074,
   "end_time": "2020-11-09T23:13:50.777214",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-09T23:03:35.034140",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
