{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015386,
     "end_time": "2021-03-24T10:51:35.715897",
     "exception": false,
     "start_time": "2021-03-24T10:51:35.700511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 1st cnn try by deepinsight method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:35.752381Z",
     "iopub.status.busy": "2021-03-24T10:51:35.751713Z",
     "iopub.status.idle": "2021-03-24T10:51:43.424830Z",
     "shell.execute_reply": "2021-03-24T10:51:43.423690Z"
    },
    "papermill": {
     "duration": 7.69475,
     "end_time": "2021-03-24T10:51:43.425058",
     "exception": false,
     "start_time": "2021-03-24T10:51:35.730308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import jit\n",
    "import inspect\n",
    "import glob\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:43.462422Z",
     "iopub.status.busy": "2021-03-24T10:51:43.460728Z",
     "iopub.status.idle": "2021-03-24T10:51:43.463016Z",
     "shell.execute_reply": "2021-03-24T10:51:43.463397Z"
    },
    "papermill": {
     "duration": 0.023056,
     "end_time": "2021-03-24T10:51:43.463544",
     "exception": false,
     "start_time": "2021-03-24T10:51:43.440488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_mode = True\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "gpus = [0]\n",
    "\n",
    "batch_size = 128\n",
    "infer_batch_size = 256\n",
    "image_size = 224  # B0\n",
    "drop_rate = 0.2  # B0\n",
    "resolution = 100\n",
    "\n",
    "# model_type = \"b3\"\n",
    "# if model_type == \"b0\":\n",
    "#     batch_size = 128\n",
    "#     infer_batch_size = 256\n",
    "#     image_size = 224  # B0\n",
    "#     drop_rate = 0.2  # B0\n",
    "#     resolution = 224\n",
    "# elif model_type == \"b3\":\n",
    "#     batch_size = 48\n",
    "#     infer_batch_size = 512\n",
    "#     image_size = 300  # B3\n",
    "#     drop_rate = 0.3  # B3\n",
    "#     resolution = 300\n",
    "# elif model_type == \"b5\":\n",
    "#     batch_size = 8\n",
    "#     infer_batch_size = 16\n",
    "#     image_size = 456  # B5\n",
    "#     drop_rate = 0.4  # B5\n",
    "#     resolution = 456\n",
    "# elif model_type == \"b7\":\n",
    "#     batch_size = 2\n",
    "#     infer_batch_size = 4\n",
    "#     # image_size = 800  # B7\n",
    "#     image_size = 772  # B7\n",
    "#     drop_rate = 0.5  # B7\n",
    "#     resolution = 772\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "drop_connect_rate = 0.2\n",
    "fc_size = 512\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.15\n",
    "swap_portion = 0.1\n",
    "\n",
    "rand_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014383,
     "end_time": "2021-03-24T10:51:43.492373",
     "exception": false,
     "start_time": "2021-03-24T10:51:43.477990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:43.527222Z",
     "iopub.status.busy": "2021-03-24T10:51:43.526735Z",
     "iopub.status.idle": "2021-03-24T10:51:50.743452Z",
     "shell.execute_reply": "2021-03-24T10:51:50.742419Z"
    },
    "papermill": {
     "duration": 7.236878,
     "end_time": "2021-03-24T10:51:50.743623",
     "exception": false,
     "start_time": "2021-03-24T10:51:43.506745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:50.782561Z",
     "iopub.status.busy": "2021-03-24T10:51:50.781762Z",
     "iopub.status.idle": "2021-03-24T10:51:50.887623Z",
     "shell.execute_reply": "2021-03-24T10:51:50.887128Z"
    },
    "papermill": {
     "duration": 0.128643,
     "end_time": "2021-03-24T10:51:50.887764",
     "exception": false,
     "start_time": "2021-03-24T10:51:50.759121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:50.930535Z",
     "iopub.status.busy": "2021-03-24T10:51:50.929513Z",
     "iopub.status.idle": "2021-03-24T10:51:51.186844Z",
     "shell.execute_reply": "2021-03-24T10:51:51.186344Z"
    },
    "papermill": {
     "duration": 0.284014,
     "end_time": "2021-03-24T10:51:51.186968",
     "exception": false,
     "start_time": "2021-03-24T10:51:50.902954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "test = test[test.index.isin(cons_test_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:51.225007Z",
     "iopub.status.busy": "2021-03-24T10:51:51.223358Z",
     "iopub.status.idle": "2021-03-24T10:51:51.226143Z",
     "shell.execute_reply": "2021-03-24T10:51:51.226555Z"
    },
    "papermill": {
     "duration": 0.024756,
     "end_time": "2021-03-24T10:51:51.226683",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.201927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [\n",
    "    c for c in train.columns\n",
    "    if c != \"sig_id\" and c not in category_features\n",
    "]\n",
    "all_features = category_features + numeric_features\n",
    "\n",
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "extra_target_feats = [c for c in non_targets.columns if c != \"sig_id\"]\n",
    "\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:51.268125Z",
     "iopub.status.busy": "2021-03-24T10:51:51.264572Z",
     "iopub.status.idle": "2021-03-24T10:51:51.276328Z",
     "shell.execute_reply": "2021-03-24T10:51:51.275822Z"
    },
    "papermill": {
     "duration": 0.034671,
     "end_time": "2021-03-24T10:51:51.276430",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.241759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014765,
     "end_time": "2021-03-24T10:51:51.306147",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.291382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# deep insight transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:51.360114Z",
     "iopub.status.busy": "2021-03-24T10:51:51.344332Z",
     "iopub.status.idle": "2021-03-24T10:51:51.374108Z",
     "shell.execute_reply": "2021-03-24T10:51:51.373717Z"
    },
    "papermill": {
     "duration": 0.05321,
     "end_time": "2021-03-24T10:51:51.374215",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.321005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "        \n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "        \n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "    \n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "            \n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "        \n",
    "    @jit\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        img_matrix = blank_mat.copy()\n",
    "        img_matrix = img_matrix.reshape(1,img_matrix.shape[0], img_matrix.shape[1])\n",
    "        img_matrices = np.repeat(img_matrix, img_coords.shape[1]-2, axis=0)\n",
    "        for z in tqdm(range(2, img_coords.shape[1])):\n",
    "            img_matrices[z-2][img_coords[0].astype(int),\n",
    "                           img_coords[1].astype(int)] = img_coords[z]\n",
    "            \n",
    "        return img_matrices\n",
    "    \n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "    \n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "    \n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "        \n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "        \n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:51.414624Z",
     "iopub.status.busy": "2021-03-24T10:51:51.414042Z",
     "iopub.status.idle": "2021-03-24T10:51:51.417334Z",
     "shell.execute_reply": "2021-03-24T10:51:51.417718Z"
    },
    "papermill": {
     "duration": 0.028171,
     "end_time": "2021-03-24T10:51:51.417838",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.389667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(0, None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "        \n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "        1).clip(0, None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(self._min0[i], None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(0, None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01509,
     "end_time": "2021-03-24T10:51:51.448050",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.432960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:51:51.485869Z",
     "iopub.status.busy": "2021-03-24T10:51:51.484772Z",
     "iopub.status.idle": "2021-03-24T10:52:34.619558Z",
     "shell.execute_reply": "2021-03-24T10:52:34.619945Z"
    },
    "papermill": {
     "duration": 43.156803,
     "end_time": "2021-03-24T10:52:34.620094",
     "exception": false,
     "start_time": "2021-03-24T10:51:51.463291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21948/21948 [00:05<00:00, 3661.00it/s]\n",
      "100%|██████████| 3624/3624 [00:00<00:00, 4033.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LogScaler (Norm-2 Normalization)\n",
    "scaler = LogScaler()\n",
    "fn_train = scaler.fit_transform(train[all_features].values)\n",
    "fn_test = scaler.transform(test[all_features].values)\n",
    "\n",
    "# Load DeepInsight Feature Map\n",
    "transformer = DeepInsightTransformer(feature_extractor='tsne_exact',\n",
    "                                pixels=resolution,\n",
    "                                perplexity=5,\n",
    "                                random_state=rand_seed,\n",
    "                                n_jobs=-1)\n",
    "    \n",
    "fn_train = transformer.fit_transform(fn_train)\n",
    "fn_test = transformer.transform(fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:34.697921Z",
     "iopub.status.busy": "2021-03-24T10:52:34.697252Z",
     "iopub.status.idle": "2021-03-24T10:52:34.701083Z",
     "shell.execute_reply": "2021-03-24T10:52:34.701536Z"
    },
    "papermill": {
     "duration": 0.046597,
     "end_time": "2021-03-24T10:52:34.701679",
     "exception": false,
     "start_time": "2021-03-24T10:52:34.655082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 100, 100), (3624, 100, 100))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_train.shape, fn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:34.794140Z",
     "iopub.status.busy": "2021-03-24T10:52:34.792918Z",
     "iopub.status.idle": "2021-03-24T10:52:34.810018Z",
     "shell.execute_reply": "2021-03-24T10:52:34.809536Z"
    },
    "papermill": {
     "duration": 0.073804,
     "end_time": "2021-03-24T10:52:34.810153",
     "exception": false,
     "start_time": "2021-03-24T10:52:34.736349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:34.889855Z",
     "iopub.status.busy": "2021-03-24T10:52:34.889052Z",
     "iopub.status.idle": "2021-03-24T10:52:34.891464Z",
     "shell.execute_reply": "2021-03-24T10:52:34.891904Z"
    },
    "papermill": {
     "duration": 0.044078,
     "end_time": "2021-03-24T10:52:34.892033",
     "exception": false,
     "start_time": "2021-03-24T10:52:34.847955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034685,
     "end_time": "2021-03-24T10:52:34.960542",
     "exception": false,
     "start_time": "2021-03-24T10:52:34.925857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.041940Z",
     "iopub.status.busy": "2021-03-24T10:52:35.041164Z",
     "iopub.status.idle": "2021-03-24T10:52:35.044194Z",
     "shell.execute_reply": "2021-03-24T10:52:35.043390Z"
    },
    "papermill": {
     "duration": 0.050192,
     "end_time": "2021-03-24T10:52:35.044333",
     "exception": false,
     "start_time": "2021-03-24T10:52:34.994141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Net2D(nn.Module):\n",
    "    def __init__(self, init_num, last_num):\n",
    "        super(Net2D,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.bn2 = nn.BatchNorm2d(1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(10000)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dense = nn.utils.weight_norm(nn.Linear(10000, last_num))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        #x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        #x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        #x = self.maxpool3(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "                \n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034009,
     "end_time": "2021-03-24T10:52:35.113076",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.079067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.535268Z",
     "iopub.status.busy": "2021-03-24T10:52:35.534349Z",
     "iopub.status.idle": "2021-03-24T10:52:35.537357Z",
     "shell.execute_reply": "2021-03-24T10:52:35.536942Z"
    },
    "papermill": {
     "duration": 0.389656,
     "end_time": "2021-03-24T10:52:35.537499",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.147843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.611035Z",
     "iopub.status.busy": "2021-03-24T10:52:35.610313Z",
     "iopub.status.idle": "2021-03-24T10:52:35.612827Z",
     "shell.execute_reply": "2021-03-24T10:52:35.613193Z"
    },
    "papermill": {
     "duration": 0.041541,
     "end_time": "2021-03-24T10:52:35.613319",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.571778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.689489Z",
     "iopub.status.busy": "2021-03-24T10:52:35.688848Z",
     "iopub.status.idle": "2021-03-24T10:52:35.691807Z",
     "shell.execute_reply": "2021-03-24T10:52:35.691378Z"
    },
    "papermill": {
     "duration": 0.044232,
     "end_time": "2021-03-24T10:52:35.691912",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.647680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.791181Z",
     "iopub.status.busy": "2021-03-24T10:52:35.790325Z",
     "iopub.status.idle": "2021-03-24T10:52:35.793150Z",
     "shell.execute_reply": "2021-03-24T10:52:35.792737Z"
    },
    "papermill": {
     "duration": 0.067057,
     "end_time": "2021-03-24T10:52:35.793262",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.726205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "train_epochs = 20\n",
    "\n",
    "def modelling_cnn(tr, target, te, sample_seed, init_num, last_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=224)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test2 = X_test2.unsqueeze(axis=1)\n",
    "    test = torch.utils.data.TensorDataset(X_test2) \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Seed \"+str(sample_seed)+\"_Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        X_train2 = X_train2.unsqueeze(axis=1)\n",
    "        X_valid2 = X_valid2.unsqueeze(axis=1)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = Net2D(init_num, last_num)\n",
    "        loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "                    \n",
    "            if sm_avg_val_loss < best_val_loss:\n",
    "                best_val_loss = sm_avg_val_loss\n",
    "                print('Epoch {}  loss={:.5f}  val_loss={:.5f}  sm_loss={:.5f}  sm_val_loss={:.5f}  time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "        pred_model = Net2D(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))         \n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            oof_epoch[i * batch_size:(i+1) * batch_size,:] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "            target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "        del X_train2, X_valid2\n",
    "    \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:52:35.868511Z",
     "iopub.status.busy": "2021-03-24T10:52:35.867837Z",
     "iopub.status.idle": "2021-03-24T11:03:05.347228Z",
     "shell.execute_reply": "2021-03-24T11:03:05.347970Z"
    },
    "papermill": {
     "duration": 629.520866,
     "end_time": "2021-03-24T11:03:05.348135",
     "exception": false,
     "start_time": "2021-03-24T10:52:35.827269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0_Fold 1\n",
      "Epoch 1  loss=0.33636  val_loss=0.02071  sm_loss=0.33635  sm_val_loss=0.02070  time=4.60s\n",
      "Epoch 2  loss=0.02061  val_loss=0.02012  sm_loss=0.02062  sm_val_loss=0.02015  time=4.09s\n",
      "Epoch 3  loss=0.01984  val_loss=0.01986  sm_loss=0.01987  sm_val_loss=0.01991  time=4.06s\n",
      "Epoch 4  loss=0.01956  val_loss=0.01946  sm_loss=0.01960  sm_val_loss=0.01950  time=4.14s\n",
      "Epoch 6  loss=0.01943  val_loss=0.01946  sm_loss=0.01947  sm_val_loss=0.01950  time=3.95s\n",
      "Epoch 8  loss=0.01913  val_loss=0.01888  sm_loss=0.01918  sm_val_loss=0.01893  time=4.02s\n",
      "Epoch 9  loss=0.01894  val_loss=0.01887  sm_loss=0.01899  sm_val_loss=0.01892  time=4.06s\n",
      "Epoch 10  loss=0.01893  val_loss=0.01880  sm_loss=0.01897  sm_val_loss=0.01885  time=4.04s\n",
      "Epoch 11  loss=0.01873  val_loss=0.01876  sm_loss=0.01879  sm_val_loss=0.01882  time=4.03s\n",
      "Epoch 12  loss=0.01867  val_loss=0.01855  sm_loss=0.01872  sm_val_loss=0.01861  time=4.04s\n",
      "Epoch 14  loss=0.01848  val_loss=0.01844  sm_loss=0.01855  sm_val_loss=0.01850  time=4.08s\n",
      "Epoch 15  loss=0.01826  val_loss=0.01819  sm_loss=0.01834  sm_val_loss=0.01827  time=4.12s\n",
      "Epoch 16  loss=0.01812  val_loss=0.01815  sm_loss=0.01821  sm_val_loss=0.01822  time=4.10s\n",
      "Epoch 17  loss=0.01797  val_loss=0.01812  sm_loss=0.01806  sm_val_loss=0.01820  time=4.05s\n",
      "Epoch 18  loss=0.01783  val_loss=0.01796  sm_loss=0.01793  sm_val_loss=0.01804  time=3.96s\n",
      "Fold 1 log loss: 0.01813050659368899\n",
      "Seed 0_Fold 2\n",
      "Epoch 1  loss=0.33648  val_loss=0.02074  sm_loss=0.33646  sm_val_loss=0.02073  time=3.87s\n",
      "Epoch 2  loss=0.02083  val_loss=0.02048  sm_loss=0.02083  sm_val_loss=0.02052  time=3.92s\n",
      "Epoch 3  loss=0.02032  val_loss=0.02048  sm_loss=0.02035  sm_val_loss=0.02052  time=3.87s\n",
      "Epoch 4  loss=0.02025  val_loss=0.02024  sm_loss=0.02029  sm_val_loss=0.02028  time=3.85s\n",
      "Epoch 5  loss=0.02008  val_loss=0.02005  sm_loss=0.02011  sm_val_loss=0.02009  time=3.90s\n",
      "Epoch 6  loss=0.01991  val_loss=0.01964  sm_loss=0.01994  sm_val_loss=0.01966  time=3.84s\n",
      "Epoch 8  loss=0.01951  val_loss=0.01935  sm_loss=0.01955  sm_val_loss=0.01939  time=3.89s\n",
      "Epoch 9  loss=0.01934  val_loss=0.01930  sm_loss=0.01938  sm_val_loss=0.01935  time=3.86s\n",
      "Epoch 10  loss=0.01924  val_loss=0.01923  sm_loss=0.01928  sm_val_loss=0.01927  time=4.01s\n",
      "Epoch 11  loss=0.01904  val_loss=0.01913  sm_loss=0.01908  sm_val_loss=0.01917  time=3.92s\n",
      "Epoch 12  loss=0.01892  val_loss=0.01874  sm_loss=0.01897  sm_val_loss=0.01878  time=3.86s\n",
      "Epoch 13  loss=0.01880  val_loss=0.01862  sm_loss=0.01886  sm_val_loss=0.01868  time=3.89s\n",
      "Epoch 15  loss=0.01852  val_loss=0.01852  sm_loss=0.01858  sm_val_loss=0.01858  time=3.85s\n",
      "Epoch 17  loss=0.01822  val_loss=0.01833  sm_loss=0.01830  sm_val_loss=0.01838  time=3.85s\n",
      "Epoch 18  loss=0.01810  val_loss=0.01823  sm_loss=0.01819  sm_val_loss=0.01830  time=3.86s\n",
      "Epoch 19  loss=0.01797  val_loss=0.01814  sm_loss=0.01807  sm_val_loss=0.01821  time=4.03s\n",
      "Fold 2 log loss: 0.018197089831572536\n",
      "Seed 0_Fold 3\n",
      "Epoch 1  loss=0.33528  val_loss=0.02076  sm_loss=0.33527  sm_val_loss=0.02075  time=3.87s\n",
      "Epoch 2  loss=0.02117  val_loss=0.02060  sm_loss=0.02117  sm_val_loss=0.02062  time=3.90s\n",
      "Epoch 4  loss=0.02034  val_loss=0.02013  sm_loss=0.02037  sm_val_loss=0.02016  time=3.85s\n",
      "Epoch 5  loss=0.02033  val_loss=0.02001  sm_loss=0.02035  sm_val_loss=0.02003  time=3.94s\n",
      "Epoch 6  loss=0.02023  val_loss=0.01993  sm_loss=0.02025  sm_val_loss=0.01996  time=3.86s\n",
      "Epoch 7  loss=0.02005  val_loss=0.01982  sm_loss=0.02008  sm_val_loss=0.01985  time=3.87s\n",
      "Epoch 8  loss=0.01994  val_loss=0.01974  sm_loss=0.01997  sm_val_loss=0.01978  time=3.89s\n",
      "Epoch 11  loss=0.01959  val_loss=0.01944  sm_loss=0.01963  sm_val_loss=0.01947  time=3.89s\n",
      "Epoch 13  loss=0.01930  val_loss=0.01929  sm_loss=0.01935  sm_val_loss=0.01934  time=3.93s\n",
      "Epoch 14  loss=0.01912  val_loss=0.01908  sm_loss=0.01917  sm_val_loss=0.01913  time=3.85s\n",
      "Epoch 16  loss=0.01883  val_loss=0.01895  sm_loss=0.01889  sm_val_loss=0.01899  time=3.88s\n",
      "Epoch 17  loss=0.01863  val_loss=0.01870  sm_loss=0.01871  sm_val_loss=0.01875  time=3.84s\n",
      "Fold 3 log loss: 0.018824950483944287\n",
      "Seed 0_Fold 4\n",
      "Epoch 1  loss=0.33364  val_loss=0.02078  sm_loss=0.33364  sm_val_loss=0.02077  time=3.86s\n",
      "Epoch 2  loss=0.02058  val_loss=0.02029  sm_loss=0.02059  sm_val_loss=0.02032  time=3.88s\n",
      "Epoch 4  loss=0.02017  val_loss=0.02013  sm_loss=0.02020  sm_val_loss=0.02016  time=3.83s\n",
      "Epoch 6  loss=0.01985  val_loss=0.02003  sm_loss=0.01988  sm_val_loss=0.02006  time=3.83s\n",
      "Epoch 7  loss=0.01976  val_loss=0.01943  sm_loss=0.01980  sm_val_loss=0.01947  time=3.87s\n",
      "Epoch 10  loss=0.01932  val_loss=0.01932  sm_loss=0.01936  sm_val_loss=0.01936  time=3.83s\n",
      "Epoch 11  loss=0.01924  val_loss=0.01920  sm_loss=0.01929  sm_val_loss=0.01924  time=3.85s\n",
      "Epoch 13  loss=0.01891  val_loss=0.01874  sm_loss=0.01898  sm_val_loss=0.01880  time=3.82s\n",
      "Epoch 15  loss=0.01862  val_loss=0.01845  sm_loss=0.01869  sm_val_loss=0.01851  time=3.88s\n",
      "Epoch 16  loss=0.01846  val_loss=0.01830  sm_loss=0.01853  sm_val_loss=0.01836  time=3.85s\n",
      "Epoch 19  loss=0.01804  val_loss=0.01819  sm_loss=0.01814  sm_val_loss=0.01827  time=3.83s\n",
      "Fold 4 log loss: 0.01834104362830885\n",
      "Seed 0_Fold 5\n",
      "Epoch 1  loss=0.33557  val_loss=0.02150  sm_loss=0.33556  sm_val_loss=0.02151  time=3.93s\n",
      "Epoch 2  loss=0.02053  val_loss=0.02042  sm_loss=0.02054  sm_val_loss=0.02046  time=3.85s\n",
      "Epoch 3  loss=0.02015  val_loss=0.02024  sm_loss=0.02019  sm_val_loss=0.02028  time=3.88s\n",
      "Epoch 4  loss=0.02009  val_loss=0.02004  sm_loss=0.02013  sm_val_loss=0.02008  time=3.83s\n",
      "Epoch 8  loss=0.01969  val_loss=0.01969  sm_loss=0.01974  sm_val_loss=0.01974  time=3.83s\n",
      "Epoch 11  loss=0.01931  val_loss=0.01949  sm_loss=0.01936  sm_val_loss=0.01955  time=3.86s\n",
      "Epoch 12  loss=0.01909  val_loss=0.01926  sm_loss=0.01915  sm_val_loss=0.01931  time=3.82s\n",
      "Epoch 13  loss=0.01898  val_loss=0.01891  sm_loss=0.01903  sm_val_loss=0.01897  time=3.86s\n",
      "Epoch 15  loss=0.01867  val_loss=0.01858  sm_loss=0.01874  sm_val_loss=0.01867  time=3.85s\n",
      "Epoch 17  loss=0.01833  val_loss=0.01839  sm_loss=0.01841  sm_val_loss=0.01847  time=3.87s\n",
      "Epoch 19  loss=0.01804  val_loss=0.01830  sm_loss=0.01814  sm_val_loss=0.01839  time=3.83s\n",
      "Epoch 20  loss=0.01795  val_loss=0.01827  sm_loss=0.01805  sm_val_loss=0.01836  time=3.88s\n",
      "Fold 5 log loss: 0.01837322925413972\n",
      "Seed 0_Fold 6\n",
      "Epoch 1  loss=0.33527  val_loss=0.02094  sm_loss=0.33527  sm_val_loss=0.02093  time=3.87s\n",
      "Epoch 2  loss=0.02055  val_loss=0.02049  sm_loss=0.02056  sm_val_loss=0.02051  time=3.82s\n",
      "Epoch 3  loss=0.02030  val_loss=0.02017  sm_loss=0.02033  sm_val_loss=0.02020  time=3.87s\n",
      "Epoch 5  loss=0.02005  val_loss=0.02006  sm_loss=0.02008  sm_val_loss=0.02010  time=3.83s\n",
      "Epoch 6  loss=0.01986  val_loss=0.01950  sm_loss=0.01990  sm_val_loss=0.01953  time=3.88s\n",
      "Epoch 7  loss=0.01973  val_loss=0.01949  sm_loss=0.01977  sm_val_loss=0.01952  time=3.84s\n",
      "Epoch 9  loss=0.01946  val_loss=0.01884  sm_loss=0.01950  sm_val_loss=0.01889  time=3.87s\n",
      "Epoch 10  loss=0.01929  val_loss=0.01875  sm_loss=0.01933  sm_val_loss=0.01878  time=3.84s\n",
      "Epoch 12  loss=0.01905  val_loss=0.01872  sm_loss=0.01911  sm_val_loss=0.01877  time=3.93s\n",
      "Epoch 13  loss=0.01892  val_loss=0.01862  sm_loss=0.01898  sm_val_loss=0.01868  time=3.87s\n",
      "Epoch 14  loss=0.01878  val_loss=0.01845  sm_loss=0.01885  sm_val_loss=0.01851  time=3.85s\n",
      "Epoch 15  loss=0.01863  val_loss=0.01819  sm_loss=0.01870  sm_val_loss=0.01827  time=3.89s\n",
      "Epoch 16  loss=0.01845  val_loss=0.01816  sm_loss=0.01853  sm_val_loss=0.01823  time=3.86s\n",
      "Epoch 17  loss=0.01837  val_loss=0.01809  sm_loss=0.01846  sm_val_loss=0.01816  time=3.89s\n",
      "Epoch 18  loss=0.01822  val_loss=0.01800  sm_loss=0.01831  sm_val_loss=0.01808  time=3.86s\n",
      "Epoch 19  loss=0.01809  val_loss=0.01797  sm_loss=0.01819  sm_val_loss=0.01805  time=3.85s\n",
      "Epoch 20  loss=0.01800  val_loss=0.01794  sm_loss=0.01810  sm_val_loss=0.01801  time=4.10s\n",
      "Fold 6 log loss: 0.01800680175326104\n",
      "Seed 0_Fold 7\n",
      "Epoch 1  loss=0.33599  val_loss=0.02079  sm_loss=0.33599  sm_val_loss=0.02078  time=3.84s\n",
      "Epoch 2  loss=0.02054  val_loss=0.02039  sm_loss=0.02055  sm_val_loss=0.02042  time=3.83s\n",
      "Epoch 3  loss=0.02013  val_loss=0.02026  sm_loss=0.02017  sm_val_loss=0.02030  time=3.86s\n",
      "Epoch 4  loss=0.02005  val_loss=0.01983  sm_loss=0.02009  sm_val_loss=0.01987  time=3.82s\n",
      "Epoch 5  loss=0.01985  val_loss=0.01979  sm_loss=0.01988  sm_val_loss=0.01983  time=3.83s\n",
      "Epoch 6  loss=0.01971  val_loss=0.01930  sm_loss=0.01974  sm_val_loss=0.01935  time=4.05s\n",
      "Epoch 7  loss=0.01962  val_loss=0.01924  sm_loss=0.01966  sm_val_loss=0.01928  time=3.84s\n",
      "Epoch 8  loss=0.01939  val_loss=0.01901  sm_loss=0.01943  sm_val_loss=0.01906  time=3.88s\n",
      "Epoch 9  loss=0.01930  val_loss=0.01892  sm_loss=0.01934  sm_val_loss=0.01897  time=3.83s\n",
      "Epoch 11  loss=0.01907  val_loss=0.01860  sm_loss=0.01912  sm_val_loss=0.01866  time=3.86s\n",
      "Epoch 13  loss=0.01889  val_loss=0.01857  sm_loss=0.01894  sm_val_loss=0.01863  time=3.82s\n",
      "Epoch 15  loss=0.01856  val_loss=0.01831  sm_loss=0.01863  sm_val_loss=0.01839  time=3.82s\n",
      "Epoch 16  loss=0.01839  val_loss=0.01829  sm_loss=0.01847  sm_val_loss=0.01835  time=3.82s\n",
      "Epoch 17  loss=0.01827  val_loss=0.01813  sm_loss=0.01835  sm_val_loss=0.01821  time=3.87s\n",
      "Epoch 18  loss=0.01813  val_loss=0.01806  sm_loss=0.01822  sm_val_loss=0.01814  time=3.82s\n",
      "Epoch 19  loss=0.01802  val_loss=0.01804  sm_loss=0.01812  sm_val_loss=0.01811  time=3.84s\n",
      "Epoch 20  loss=0.01795  val_loss=0.01800  sm_loss=0.01804  sm_val_loss=0.01808  time=3.86s\n",
      "Fold 7 log loss: 0.01804160527771344\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.01813050659368899\n",
      "Fold 2 log loss: 0.018197089831572536\n",
      "Fold 3 log loss: 0.018824950483944287\n",
      "Fold 4 log loss: 0.01834104362830885\n",
      "Fold 5 log loss: 0.01837322925413972\n",
      "Fold 6 log loss: 0.01800680175326104\n",
      "Fold 7 log loss: 0.01804160527771344\n",
      "Std of log loss: 0.0002590626983561556\n",
      "Total log loss: 0.018273594314438896\n"
     ]
    }
   ],
   "source": [
    "seeds = [0] #,1,2,3,4]\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_cnn(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.06549,
     "end_time": "2021-03-24T11:03:05.479633",
     "exception": false,
     "start_time": "2021-03-24T11:03:05.414143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T11:03:05.621675Z",
     "iopub.status.busy": "2021-03-24T11:03:05.620911Z",
     "iopub.status.idle": "2021-03-24T11:03:11.217637Z",
     "shell.execute_reply": "2021-03-24T11:03:11.218055Z"
    },
    "papermill": {
     "duration": 5.672904,
     "end_time": "2021-03-24T11:03:11.218214",
     "exception": false,
     "start_time": "2021-03-24T11:03:05.545310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016841725372188938\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[cons_train_index,target_feats] = target_oof\n",
    "train_checkscore.loc[noncons_train_index,target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T11:03:11.355268Z",
     "iopub.status.busy": "2021-03-24T11:03:11.354329Z",
     "iopub.status.idle": "2021-03-24T11:03:13.599184Z",
     "shell.execute_reply": "2021-03-24T11:03:13.598438Z"
    },
    "papermill": {
     "duration": 2.315586,
     "end_time": "2021-03-24T11:03:13.599322",
     "exception": false,
     "start_time": "2021-03-24T11:03:11.283736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.loc[cons_test_index,target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 704.646469,
   "end_time": "2021-03-24T11:03:15.606089",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-24T10:51:30.959620",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
