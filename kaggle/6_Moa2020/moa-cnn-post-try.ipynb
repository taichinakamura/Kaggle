{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01419,
     "end_time": "2021-03-24T10:24:59.295697",
     "exception": false,
     "start_time": "2021-03-24T10:24:59.281507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 1st cnn try by deepinsight method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-24T10:24:59.329918Z",
     "iopub.status.busy": "2021-03-24T10:24:59.329249Z",
     "iopub.status.idle": "2021-03-24T10:25:07.179056Z",
     "shell.execute_reply": "2021-03-24T10:25:07.178018Z"
    },
    "papermill": {
     "duration": 7.870308,
     "end_time": "2021-03-24T10:25:07.179243",
     "exception": false,
     "start_time": "2021-03-24T10:24:59.308935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import jit\n",
    "import inspect\n",
    "import glob\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:07.212712Z",
     "iopub.status.busy": "2021-03-24T10:25:07.212164Z",
     "iopub.status.idle": "2021-03-24T10:25:07.216009Z",
     "shell.execute_reply": "2021-03-24T10:25:07.215475Z"
    },
    "papermill": {
     "duration": 0.022326,
     "end_time": "2021-03-24T10:25:07.216114",
     "exception": false,
     "start_time": "2021-03-24T10:25:07.193788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_mode = True\n",
    "\n",
    "num_workers = 2 if kernel_mode else 6\n",
    "gpus = [0]\n",
    "\n",
    "batch_size = 128\n",
    "infer_batch_size = 256\n",
    "image_size = 224  # B0\n",
    "drop_rate = 0.2  # B0\n",
    "resolution = 100\n",
    "\n",
    "# model_type = \"b3\"\n",
    "# if model_type == \"b0\":\n",
    "#     batch_size = 128\n",
    "#     infer_batch_size = 256\n",
    "#     image_size = 224  # B0\n",
    "#     drop_rate = 0.2  # B0\n",
    "#     resolution = 224\n",
    "# elif model_type == \"b3\":\n",
    "#     batch_size = 48\n",
    "#     infer_batch_size = 512\n",
    "#     image_size = 300  # B3\n",
    "#     drop_rate = 0.3  # B3\n",
    "#     resolution = 300\n",
    "# elif model_type == \"b5\":\n",
    "#     batch_size = 8\n",
    "#     infer_batch_size = 16\n",
    "#     image_size = 456  # B5\n",
    "#     drop_rate = 0.4  # B5\n",
    "#     resolution = 456\n",
    "# elif model_type == \"b7\":\n",
    "#     batch_size = 2\n",
    "#     infer_batch_size = 4\n",
    "#     # image_size = 800  # B7\n",
    "#     image_size = 772  # B7\n",
    "#     drop_rate = 0.5  # B7\n",
    "#     resolution = 772\n",
    "\n",
    "# DeepInsight Transform\n",
    "perplexity = 5\n",
    "\n",
    "drop_connect_rate = 0.2\n",
    "fc_size = 512\n",
    "\n",
    "# Swap Noise\n",
    "swap_prob = 0.15\n",
    "swap_portion = 0.1\n",
    "\n",
    "rand_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013115,
     "end_time": "2021-03-24T10:25:07.242442",
     "exception": false,
     "start_time": "2021-03-24T10:25:07.229327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:07.276285Z",
     "iopub.status.busy": "2021-03-24T10:25:07.275772Z",
     "iopub.status.idle": "2021-03-24T10:25:14.354108Z",
     "shell.execute_reply": "2021-03-24T10:25:14.353060Z"
    },
    "papermill": {
     "duration": 7.098374,
     "end_time": "2021-03-24T10:25:14.354250",
     "exception": false,
     "start_time": "2021-03-24T10:25:07.255876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:14.388120Z",
     "iopub.status.busy": "2021-03-24T10:25:14.387436Z",
     "iopub.status.idle": "2021-03-24T10:25:14.390958Z",
     "shell.execute_reply": "2021-03-24T10:25:14.390558Z"
    },
    "papermill": {
     "duration": 0.022955,
     "end_time": "2021-03-24T10:25:14.391068",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.368113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_features = [\"cp_type\", \"cp_dose\"]\n",
    "numeric_features = [\n",
    "    c for c in train.columns\n",
    "    if c != \"sig_id\" and c not in category_features\n",
    "]\n",
    "all_features = category_features + numeric_features\n",
    "\n",
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "extra_target_feats = [c for c in non_targets.columns if c != \"sig_id\"]\n",
    "\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:14.426960Z",
     "iopub.status.busy": "2021-03-24T10:25:14.426419Z",
     "iopub.status.idle": "2021-03-24T10:25:14.444136Z",
     "shell.execute_reply": "2021-03-24T10:25:14.444518Z"
    },
    "papermill": {
     "duration": 0.039956,
     "end_time": "2021-03-24T10:25:14.444671",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.404715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013448,
     "end_time": "2021-03-24T10:25:14.471813",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.458365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# deep insight transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:14.535735Z",
     "iopub.status.busy": "2021-03-24T10:25:14.524316Z",
     "iopub.status.idle": "2021-03-24T10:25:14.538152Z",
     "shell.execute_reply": "2021-03-24T10:25:14.537751Z"
    },
    "papermill": {
     "duration": 0.052798,
     "end_time": "2021-03-24T10:25:14.538262",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.485464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepInsightTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_extractor='tsne',\n",
    "                 perplexity=30,\n",
    "                 pixels=100,\n",
    "                 random_state=None,\n",
    "                 n_jobs=None):\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "            random_state: int or RandomState. Determines the random number\n",
    "                generator, if present, of a string defined feature_extractor.\n",
    "            n_jobs: The number of parallel jobs to run for a string defined\n",
    "                feature_extractor.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne_exact'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='exact',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'tsne'.casefold():\n",
    "                fe = TSNE(n_components=2,\n",
    "                          metric='cosine',\n",
    "                          perplexity=perplexity,\n",
    "                          n_iter=1000,\n",
    "                          method='barnes_hut',\n",
    "                          random_state=self.random_state,\n",
    "                          n_jobs=self.n_jobs)\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe = PCA(n_components=2, random_state=self.random_state)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe = KernelPCA(n_components=2,\n",
    "                               kernel='rbf',\n",
    "                               random_state=self.random_state,\n",
    "                               n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError((\"Feature extraction method '{}' not accepted\"\n",
    "                                  ).format(feature_extractor))\n",
    "            self._fe = fe\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            self._fe = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "\n",
    "        # The resolution of transformed image\n",
    "        self._pixels = pixels\n",
    "        self._xrot = None\n",
    "        \n",
    "    def fit(self, X, y=None, plot=False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # Transpose to get (n_features, n_samples)\n",
    "        X = X.T\n",
    "\n",
    "        # Perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X)\n",
    "\n",
    "        # Get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "        \n",
    "        # Determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "\n",
    "        # Rotate the matrix\n",
    "        # Save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "\n",
    "        # Determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n",
    "            ax[0, 0].scatter(x_new[:, 0],\n",
    "                             x_new[:, 1],\n",
    "                             cmap=plt.cm.get_cmap(\"jet\", 10),\n",
    "                             marker=\"x\",\n",
    "                             alpha=1.0)\n",
    "            ax[0, 0].fill(x_new[chvertices, 0],\n",
    "                          x_new[chvertices, 1],\n",
    "                          edgecolor='r',\n",
    "                          fill=False)\n",
    "            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def pixels(self):\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "    \n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels):\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "            \n",
    "    def _calculate_coords(self):\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        ax0_coord = np.digitize(self._xrot[:, 0],\n",
    "                                bins=np.linspace(min(self._xrot[:, 0]),\n",
    "                                                 max(self._xrot[:, 0]),\n",
    "                                                 self._pixels[0])) - 1\n",
    "        ax1_coord = np.digitize(self._xrot[:, 1],\n",
    "                                bins=np.linspace(min(self._xrot[:, 1]),\n",
    "                                                 max(self._xrot[:, 1]),\n",
    "                                                 self._pixels[1])) - 1\n",
    "        self._coords = np.stack((ax0_coord, ax1_coord))\n",
    "        \n",
    "    @jit\n",
    "    def transform(self, X, empty_value=0):\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        img_coords = pd.DataFrame(np.vstack(\n",
    "            (self._coords, X.clip(0, 1))).T).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).mean()\n",
    "\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        img_matrix = blank_mat.copy()\n",
    "        img_matrix = img_matrix.reshape(1,img_matrix.shape[0], img_matrix.shape[1])\n",
    "        img_matrices = np.repeat(img_matrix, img_coords.shape[1]-2, axis=0)\n",
    "        for z in tqdm(range(2, img_coords.shape[1])):\n",
    "            img_matrices[z-2][img_coords[0].astype(int),\n",
    "                           img_coords[1].astype(int)] = img_coords[z]\n",
    "            \n",
    "        return img_matrices\n",
    "    \n",
    "    def fit_transform(self, X, empty_value=0):\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0 (although it was 1 in the paper).\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, empty_value=empty_value)\n",
    "    \n",
    "    def feature_density_matrix(self):\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        # Group by location (x1, y1) of each feature\n",
    "        # Tranpose to get (n_features, n_samples)\n",
    "        coord_cnt = (\n",
    "            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n",
    "                [0, 1],  # (x1, y1)\n",
    "                as_index=False).count())\n",
    "        fdmat[coord_cnt[0].astype(int),\n",
    "              coord_cnt[1].astype(int)] = coord_cnt['count']\n",
    "        return fdmat\n",
    "    \n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points):\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2.\n",
    "\n",
    "        # Calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "        \n",
    "        # Find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            np.cos(angles - pi2),\n",
    "            np.cos(angles + pi2),\n",
    "            np.cos(angles)\n",
    "        ]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "\n",
    "        # Apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "\n",
    "        # Find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "\n",
    "        # Find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "\n",
    "        # Return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "        \n",
    "        # Generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:14.575489Z",
     "iopub.status.busy": "2021-03-24T10:25:14.574991Z",
     "iopub.status.idle": "2021-03-24T10:25:14.578923Z",
     "shell.execute_reply": "2021-03-24T10:25:14.578477Z"
    },
    "papermill": {
     "duration": 0.027048,
     "end_time": "2021-03-24T10:25:14.579028",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.551980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogScaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \n",
    "    Note: The dimensions of input matrix is (N samples, d features)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._min0 = None\n",
    "        self._max = None\n",
    "\n",
    "    \"\"\"\n",
    "    Use this as a preprocessing step in inference mode.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(0, None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "        \n",
    "    \"\"\"\n",
    "    For training set only.\n",
    "    \"\"\"\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Min. of training set per feature\n",
    "        self._min0 = X.min(axis=0)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "        1).clip(0, None)\n",
    "\n",
    "        # Global max. of training set from X_norm\n",
    "        self._max = X_norm.max()\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    For validation and test set only.\n",
    "    \"\"\"\n",
    "    def transform(self, X, y=None):\n",
    "        # Adjust min. of each feature of X by _min0\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = X[:, i].clip(self._min0[i], None)\n",
    "\n",
    "        # Log normalized X by log(X + _min0 + 1)\n",
    "        X_norm = np.log(\n",
    "            X +\n",
    "            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n",
    "            1).clip(0, None)\n",
    "\n",
    "        # Normalized again by global max. of training set\n",
    "        return (X_norm / self._max).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013932,
     "end_time": "2021-03-24T10:25:14.606924",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.592992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:25:14.642662Z",
     "iopub.status.busy": "2021-03-24T10:25:14.641486Z",
     "iopub.status.idle": "2021-03-24T10:26:00.247585Z",
     "shell.execute_reply": "2021-03-24T10:26:00.246753Z"
    },
    "papermill": {
     "duration": 45.626876,
     "end_time": "2021-03-24T10:26:00.247801",
     "exception": false,
     "start_time": "2021-03-24T10:25:14.620925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23814/23814 [00:05<00:00, 4207.81it/s]\n",
      "100%|██████████| 3982/3982 [00:01<00:00, 2688.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load LogScaler (Norm-2 Normalization)\n",
    "scaler = LogScaler()\n",
    "fn_train = scaler.fit_transform(train[all_features].values)\n",
    "fn_test = scaler.transform(test[all_features].values)\n",
    "\n",
    "# Load DeepInsight Feature Map\n",
    "transformer = DeepInsightTransformer(feature_extractor='tsne_exact',\n",
    "                                pixels=resolution,\n",
    "                                perplexity=5,\n",
    "                                random_state=rand_seed,\n",
    "                                n_jobs=-1)\n",
    "    \n",
    "fn_train = transformer.fit_transform(fn_train)\n",
    "fn_test = transformer.transform(fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:00.376082Z",
     "iopub.status.busy": "2021-03-24T10:26:00.375318Z",
     "iopub.status.idle": "2021-03-24T10:26:00.380417Z",
     "shell.execute_reply": "2021-03-24T10:26:00.381484Z"
    },
    "papermill": {
     "duration": 0.067389,
     "end_time": "2021-03-24T10:26:00.381692",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.314303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 100, 100), (3982, 100, 100))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_train.shape, fn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:00.510806Z",
     "iopub.status.busy": "2021-03-24T10:26:00.509870Z",
     "iopub.status.idle": "2021-03-24T10:26:00.534117Z",
     "shell.execute_reply": "2021-03-24T10:26:00.535109Z"
    },
    "papermill": {
     "duration": 0.093971,
     "end_time": "2021-03-24T10:26:00.535319",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.441348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:00.663279Z",
     "iopub.status.busy": "2021-03-24T10:26:00.660008Z",
     "iopub.status.idle": "2021-03-24T10:26:00.670806Z",
     "shell.execute_reply": "2021-03-24T10:26:00.671242Z"
    },
    "papermill": {
     "duration": 0.078923,
     "end_time": "2021-03-24T10:26:00.671406",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.592483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051197,
     "end_time": "2021-03-24T10:26:00.787712",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.736515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:00.896471Z",
     "iopub.status.busy": "2021-03-24T10:26:00.895806Z",
     "iopub.status.idle": "2021-03-24T10:26:00.913905Z",
     "shell.execute_reply": "2021-03-24T10:26:00.914935Z"
    },
    "papermill": {
     "duration": 0.078974,
     "end_time": "2021-03-24T10:26:00.915134",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.836160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Net2D(nn.Module):\n",
    "    def __init__(self, init_num, last_num):\n",
    "        super(Net2D,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.bn2 = nn.BatchNorm2d(1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1,1,kernel_size=7,stride=1,padding=3)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=7,stride=1,padding=3)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(10000)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dense = nn.utils.weight_norm(nn.Linear(10000, last_num))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        #x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        #x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        #x = self.maxpool3(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "                \n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033094,
     "end_time": "2021-03-24T10:26:00.989845",
     "exception": false,
     "start_time": "2021-03-24T10:26:00.956751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:01.605906Z",
     "iopub.status.busy": "2021-03-24T10:26:01.604670Z",
     "iopub.status.idle": "2021-03-24T10:26:01.609259Z",
     "shell.execute_reply": "2021-03-24T10:26:01.608843Z"
    },
    "papermill": {
     "duration": 0.572401,
     "end_time": "2021-03-24T10:26:01.609384",
     "exception": false,
     "start_time": "2021-03-24T10:26:01.036983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:01.684757Z",
     "iopub.status.busy": "2021-03-24T10:26:01.684112Z",
     "iopub.status.idle": "2021-03-24T10:26:01.688161Z",
     "shell.execute_reply": "2021-03-24T10:26:01.687742Z"
    },
    "papermill": {
     "duration": 0.043837,
     "end_time": "2021-03-24T10:26:01.688283",
     "exception": false,
     "start_time": "2021-03-24T10:26:01.644446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:01.761344Z",
     "iopub.status.busy": "2021-03-24T10:26:01.760740Z",
     "iopub.status.idle": "2021-03-24T10:26:01.764207Z",
     "shell.execute_reply": "2021-03-24T10:26:01.763786Z"
    },
    "papermill": {
     "duration": 0.043431,
     "end_time": "2021-03-24T10:26:01.764320",
     "exception": false,
     "start_time": "2021-03-24T10:26:01.720889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:01.858803Z",
     "iopub.status.busy": "2021-03-24T10:26:01.849267Z",
     "iopub.status.idle": "2021-03-24T10:26:01.860986Z",
     "shell.execute_reply": "2021-03-24T10:26:01.861425Z"
    },
    "papermill": {
     "duration": 0.064575,
     "end_time": "2021-03-24T10:26:01.861584",
     "exception": false,
     "start_time": "2021-03-24T10:26:01.797009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "train_epochs = 20\n",
    "\n",
    "def modelling_cnn(tr, target, te, sample_seed, init_num, last_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=224)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test2 = X_test2.unsqueeze(axis=1)\n",
    "    test = torch.utils.data.TensorDataset(X_test2) \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    \n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Seed \"+str(sample_seed)+\"_Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        X_train2 = X_train2.unsqueeze(axis=1)\n",
    "        X_valid2 = X_valid2.unsqueeze(axis=1)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = Net2D(init_num, last_num)\n",
    "        loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "                    \n",
    "            if sm_avg_val_loss < best_val_loss:\n",
    "                best_val_loss = sm_avg_val_loss\n",
    "                print('Epoch {}  loss={:.5f}  val_loss={:.5f}  sm_loss={:.5f}  sm_val_loss={:.5f}  time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "        pred_model = Net2D(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))         \n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            oof_epoch[i * batch_size:(i+1) * batch_size,:] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "            target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "        del X_train2, X_valid2\n",
    "    \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:26:01.938540Z",
     "iopub.status.busy": "2021-03-24T10:26:01.937093Z",
     "iopub.status.idle": "2021-03-24T10:37:36.263136Z",
     "shell.execute_reply": "2021-03-24T10:37:36.263711Z"
    },
    "papermill": {
     "duration": 694.36919,
     "end_time": "2021-03-24T10:37:36.263931",
     "exception": false,
     "start_time": "2021-03-24T10:26:01.894741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 0_Fold 1\n",
      "Epoch 1  loss=0.32495  val_loss=0.01930  sm_loss=0.32494  sm_val_loss=0.01928  time=5.17s\n",
      "Epoch 2  loss=0.01945  val_loss=0.01880  sm_loss=0.01946  sm_val_loss=0.01884  time=4.33s\n",
      "Epoch 3  loss=0.01872  val_loss=0.01858  sm_loss=0.01875  sm_val_loss=0.01862  time=4.27s\n",
      "Epoch 4  loss=0.01852  val_loss=0.01848  sm_loss=0.01856  sm_val_loss=0.01852  time=4.35s\n",
      "Epoch 6  loss=0.01824  val_loss=0.01805  sm_loss=0.01828  sm_val_loss=0.01809  time=4.27s\n",
      "Epoch 8  loss=0.01799  val_loss=0.01796  sm_loss=0.01803  sm_val_loss=0.01800  time=4.31s\n",
      "Epoch 10  loss=0.01776  val_loss=0.01788  sm_loss=0.01781  sm_val_loss=0.01794  time=4.29s\n",
      "Epoch 11  loss=0.01770  val_loss=0.01751  sm_loss=0.01776  sm_val_loss=0.01756  time=4.26s\n",
      "Epoch 12  loss=0.01750  val_loss=0.01722  sm_loss=0.01756  sm_val_loss=0.01729  time=4.27s\n",
      "Epoch 16  loss=0.01704  val_loss=0.01705  sm_loss=0.01712  sm_val_loss=0.01713  time=4.28s\n",
      "Epoch 17  loss=0.01682  val_loss=0.01691  sm_loss=0.01691  sm_val_loss=0.01700  time=4.29s\n",
      "Epoch 18  loss=0.01672  val_loss=0.01681  sm_loss=0.01683  sm_val_loss=0.01690  time=4.36s\n",
      "Epoch 19  loss=0.01656  val_loss=0.01680  sm_loss=0.01666  sm_val_loss=0.01689  time=4.31s\n",
      "Fold 1 log loss: 0.01690426347884273\n",
      "Seed 0_Fold 2\n",
      "Epoch 1  loss=0.32509  val_loss=0.01971  sm_loss=0.32508  sm_val_loss=0.01970  time=4.37s\n",
      "Epoch 2  loss=0.01930  val_loss=0.01855  sm_loss=0.01930  sm_val_loss=0.01860  time=4.29s\n",
      "Epoch 3  loss=0.01875  val_loss=0.01852  sm_loss=0.01878  sm_val_loss=0.01856  time=4.30s\n",
      "Epoch 6  loss=0.01833  val_loss=0.01836  sm_loss=0.01837  sm_val_loss=0.01840  time=4.26s\n",
      "Epoch 7  loss=0.01818  val_loss=0.01783  sm_loss=0.01822  sm_val_loss=0.01788  time=4.29s\n",
      "Epoch 11  loss=0.01763  val_loss=0.01769  sm_loss=0.01769  sm_val_loss=0.01774  time=4.29s\n",
      "Epoch 13  loss=0.01737  val_loss=0.01736  sm_loss=0.01743  sm_val_loss=0.01742  time=4.24s\n",
      "Epoch 16  loss=0.01705  val_loss=0.01693  sm_loss=0.01714  sm_val_loss=0.01701  time=4.25s\n",
      "Epoch 17  loss=0.01683  val_loss=0.01673  sm_loss=0.01693  sm_val_loss=0.01682  time=4.29s\n",
      "Epoch 19  loss=0.01655  val_loss=0.01673  sm_loss=0.01666  sm_val_loss=0.01681  time=4.31s\n",
      "Fold 2 log loss: 0.016839217491665084\n",
      "Seed 0_Fold 3\n",
      "Epoch 1  loss=0.32467  val_loss=0.01943  sm_loss=0.32465  sm_val_loss=0.01941  time=4.26s\n",
      "Epoch 2  loss=0.01955  val_loss=0.01897  sm_loss=0.01955  sm_val_loss=0.01900  time=4.25s\n",
      "Epoch 3  loss=0.01887  val_loss=0.01887  sm_loss=0.01890  sm_val_loss=0.01891  time=4.27s\n",
      "Epoch 4  loss=0.01877  val_loss=0.01838  sm_loss=0.01880  sm_val_loss=0.01843  time=4.30s\n",
      "Epoch 6  loss=0.01840  val_loss=0.01819  sm_loss=0.01844  sm_val_loss=0.01825  time=4.24s\n",
      "Epoch 7  loss=0.01813  val_loss=0.01795  sm_loss=0.01817  sm_val_loss=0.01800  time=4.24s\n",
      "Epoch 10  loss=0.01792  val_loss=0.01789  sm_loss=0.01797  sm_val_loss=0.01795  time=4.29s\n",
      "Epoch 11  loss=0.01778  val_loss=0.01770  sm_loss=0.01784  sm_val_loss=0.01776  time=4.32s\n",
      "Epoch 12  loss=0.01764  val_loss=0.01750  sm_loss=0.01770  sm_val_loss=0.01755  time=4.26s\n",
      "Epoch 15  loss=0.01727  val_loss=0.01727  sm_loss=0.01735  sm_val_loss=0.01732  time=4.27s\n",
      "Epoch 17  loss=0.01694  val_loss=0.01703  sm_loss=0.01703  sm_val_loss=0.01710  time=4.25s\n",
      "Epoch 18  loss=0.01679  val_loss=0.01690  sm_loss=0.01689  sm_val_loss=0.01698  time=4.27s\n",
      "Epoch 20  loss=0.01661  val_loss=0.01689  sm_loss=0.01672  sm_val_loss=0.01697  time=4.26s\n",
      "Fold 3 log loss: 0.01695004125423045\n",
      "Seed 0_Fold 4\n",
      "Epoch 1  loss=0.32397  val_loss=0.01904  sm_loss=0.32396  sm_val_loss=0.01903  time=4.26s\n",
      "Epoch 3  loss=0.01839  val_loss=0.01832  sm_loss=0.01843  sm_val_loss=0.01837  time=4.22s\n",
      "Epoch 7  loss=0.01809  val_loss=0.01782  sm_loss=0.01813  sm_val_loss=0.01788  time=4.26s\n",
      "Epoch 9  loss=0.01794  val_loss=0.01746  sm_loss=0.01800  sm_val_loss=0.01752  time=4.31s\n",
      "Epoch 12  loss=0.01763  val_loss=0.01730  sm_loss=0.01770  sm_val_loss=0.01736  time=4.33s\n",
      "Epoch 14  loss=0.01739  val_loss=0.01717  sm_loss=0.01746  sm_val_loss=0.01724  time=4.30s\n",
      "Epoch 15  loss=0.01725  val_loss=0.01699  sm_loss=0.01734  sm_val_loss=0.01707  time=4.27s\n",
      "Epoch 16  loss=0.01709  val_loss=0.01694  sm_loss=0.01718  sm_val_loss=0.01703  time=4.31s\n",
      "Epoch 17  loss=0.01693  val_loss=0.01680  sm_loss=0.01703  sm_val_loss=0.01688  time=4.26s\n",
      "Epoch 18  loss=0.01679  val_loss=0.01671  sm_loss=0.01689  sm_val_loss=0.01680  time=4.30s\n",
      "Epoch 19  loss=0.01664  val_loss=0.01666  sm_loss=0.01676  sm_val_loss=0.01675  time=4.42s\n",
      "Epoch 20  loss=0.01657  val_loss=0.01666  sm_loss=0.01669  sm_val_loss=0.01675  time=4.28s\n",
      "Fold 4 log loss: 0.01676677911048283\n",
      "Seed 0_Fold 5\n",
      "Epoch 1  loss=0.32398  val_loss=0.01939  sm_loss=0.32397  sm_val_loss=0.01937  time=4.28s\n",
      "Epoch 2  loss=0.01930  val_loss=0.01906  sm_loss=0.01931  sm_val_loss=0.01906  time=4.30s\n",
      "Epoch 3  loss=0.01853  val_loss=0.01862  sm_loss=0.01857  sm_val_loss=0.01866  time=4.28s\n",
      "Epoch 6  loss=0.01828  val_loss=0.01838  sm_loss=0.01832  sm_val_loss=0.01842  time=4.26s\n",
      "Epoch 8  loss=0.01818  val_loss=0.01804  sm_loss=0.01822  sm_val_loss=0.01810  time=4.26s\n",
      "Epoch 9  loss=0.01804  val_loss=0.01777  sm_loss=0.01808  sm_val_loss=0.01783  time=4.30s\n",
      "Epoch 10  loss=0.01792  val_loss=0.01772  sm_loss=0.01798  sm_val_loss=0.01778  time=4.25s\n",
      "Epoch 12  loss=0.01774  val_loss=0.01742  sm_loss=0.01780  sm_val_loss=0.01748  time=4.42s\n",
      "Epoch 13  loss=0.01760  val_loss=0.01737  sm_loss=0.01768  sm_val_loss=0.01744  time=4.26s\n",
      "Epoch 15  loss=0.01728  val_loss=0.01727  sm_loss=0.01736  sm_val_loss=0.01734  time=4.27s\n",
      "Epoch 16  loss=0.01717  val_loss=0.01702  sm_loss=0.01726  sm_val_loss=0.01710  time=4.29s\n",
      "Epoch 18  loss=0.01686  val_loss=0.01694  sm_loss=0.01696  sm_val_loss=0.01702  time=4.26s\n",
      "Epoch 19  loss=0.01671  val_loss=0.01683  sm_loss=0.01682  sm_val_loss=0.01692  time=4.33s\n",
      "Epoch 20  loss=0.01664  val_loss=0.01680  sm_loss=0.01675  sm_val_loss=0.01689  time=4.29s\n",
      "Fold 5 log loss: 0.01691498885598585\n",
      "Seed 0_Fold 6\n",
      "Epoch 1  loss=0.32433  val_loss=0.01915  sm_loss=0.32432  sm_val_loss=0.01914  time=4.47s\n",
      "Epoch 2  loss=0.01942  val_loss=0.01875  sm_loss=0.01943  sm_val_loss=0.01879  time=4.31s\n",
      "Epoch 7  loss=0.01815  val_loss=0.01823  sm_loss=0.01820  sm_val_loss=0.01827  time=4.26s\n",
      "Epoch 8  loss=0.01802  val_loss=0.01814  sm_loss=0.01807  sm_val_loss=0.01820  time=4.34s\n",
      "Epoch 9  loss=0.01793  val_loss=0.01752  sm_loss=0.01798  sm_val_loss=0.01757  time=4.28s\n",
      "Epoch 13  loss=0.01755  val_loss=0.01727  sm_loss=0.01761  sm_val_loss=0.01735  time=4.26s\n",
      "Epoch 14  loss=0.01735  val_loss=0.01719  sm_loss=0.01742  sm_val_loss=0.01726  time=4.24s\n",
      "Epoch 16  loss=0.01706  val_loss=0.01689  sm_loss=0.01715  sm_val_loss=0.01697  time=4.22s\n",
      "Epoch 18  loss=0.01679  val_loss=0.01689  sm_loss=0.01689  sm_val_loss=0.01697  time=4.29s\n",
      "Epoch 19  loss=0.01666  val_loss=0.01676  sm_loss=0.01677  sm_val_loss=0.01685  time=4.31s\n",
      "Epoch 20  loss=0.01659  val_loss=0.01673  sm_loss=0.01671  sm_val_loss=0.01682  time=4.35s\n",
      "Fold 6 log loss: 0.016801338240618004\n",
      "Seed 0_Fold 7\n",
      "Epoch 1  loss=0.32560  val_loss=0.01921  sm_loss=0.32559  sm_val_loss=0.01920  time=4.31s\n",
      "Epoch 2  loss=0.01951  val_loss=0.01885  sm_loss=0.01952  sm_val_loss=0.01888  time=4.28s\n",
      "Epoch 3  loss=0.01899  val_loss=0.01875  sm_loss=0.01902  sm_val_loss=0.01879  time=4.36s\n",
      "Epoch 4  loss=0.01883  val_loss=0.01859  sm_loss=0.01886  sm_val_loss=0.01863  time=4.27s\n",
      "Epoch 6  loss=0.01875  val_loss=0.01855  sm_loss=0.01878  sm_val_loss=0.01859  time=4.30s\n",
      "Epoch 8  loss=0.01835  val_loss=0.01816  sm_loss=0.01839  sm_val_loss=0.01820  time=4.33s\n",
      "Epoch 11  loss=0.01784  val_loss=0.01762  sm_loss=0.01789  sm_val_loss=0.01768  time=4.29s\n",
      "Epoch 12  loss=0.01771  val_loss=0.01760  sm_loss=0.01777  sm_val_loss=0.01766  time=4.33s\n",
      "Epoch 13  loss=0.01753  val_loss=0.01745  sm_loss=0.01759  sm_val_loss=0.01752  time=4.36s\n",
      "Epoch 14  loss=0.01746  val_loss=0.01739  sm_loss=0.01752  sm_val_loss=0.01746  time=4.28s\n",
      "Epoch 15  loss=0.01729  val_loss=0.01728  sm_loss=0.01737  sm_val_loss=0.01736  time=4.26s\n",
      "Epoch 17  loss=0.01702  val_loss=0.01718  sm_loss=0.01711  sm_val_loss=0.01726  time=4.27s\n",
      "Epoch 18  loss=0.01686  val_loss=0.01703  sm_loss=0.01696  sm_val_loss=0.01711  time=4.26s\n",
      "Epoch 19  loss=0.01672  val_loss=0.01698  sm_loss=0.01683  sm_val_loss=0.01706  time=4.30s\n",
      "Epoch 20  loss=0.01667  val_loss=0.01695  sm_loss=0.01678  sm_val_loss=0.01703  time=4.35s\n",
      "Fold 7 log loss: 0.01706507761152661\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.01690426347884273\n",
      "Fold 2 log loss: 0.016839217491665084\n",
      "Fold 3 log loss: 0.01695004125423045\n",
      "Fold 4 log loss: 0.01676677911048283\n",
      "Fold 5 log loss: 0.01691498885598585\n",
      "Fold 6 log loss: 0.016801338240618004\n",
      "Fold 7 log loss: 0.01706507761152661\n",
      "Std of log loss: 9.31117565393802e-05\n",
      "Total log loss: 0.016891672291907363\n"
     ]
    }
   ],
   "source": [
    "seeds = [0] #,1,2,3,4]\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_cnn(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058842,
     "end_time": "2021-03-24T10:37:36.382949",
     "exception": false,
     "start_time": "2021-03-24T10:37:36.324107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:37:36.513224Z",
     "iopub.status.busy": "2021-03-24T10:37:36.512417Z",
     "iopub.status.idle": "2021-03-24T10:37:37.040371Z",
     "shell.execute_reply": "2021-03-24T10:37:37.039923Z"
    },
    "papermill": {
     "duration": 0.597733,
     "end_time": "2021-03-24T10:37:37.040503",
     "exception": false,
     "start_time": "2021-03-24T10:37:36.442770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cons_train_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0a6c3589736f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_targets_scored.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_checkscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_checkscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_checkscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons_train_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_oof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_checkscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_checkscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoncons_train_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sig_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cons_train_index' is not defined"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-24T10:37:37.168655Z",
     "iopub.status.busy": "2021-03-24T10:37:37.163643Z",
     "iopub.status.idle": "2021-03-24T10:37:37.185223Z",
     "shell.execute_reply": "2021-03-24T10:37:37.184758Z"
    },
    "papermill": {
     "duration": 0.085332,
     "end_time": "2021-03-24T10:37:37.185346",
     "exception": false,
     "start_time": "2021-03-24T10:37:37.100014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cons_test_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ae44d2f02278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcons_test_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoncons_test_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_feats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cons_test_index' is not defined"
     ]
    }
   ],
   "source": [
    "sub.loc[cons_test_index,target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 764.624371,
   "end_time": "2021-03-24T10:37:39.059687",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-24T10:24:54.435316",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
