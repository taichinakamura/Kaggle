{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022644,
     "end_time": "2020-11-17T14:35:44.135116",
     "exception": false,
     "start_time": "2020-11-17T14:35:44.112472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- change model archi\n",
    "- clipping 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-17T14:35:44.189407Z",
     "iopub.status.busy": "2020-11-17T14:35:44.188608Z",
     "iopub.status.idle": "2020-11-17T14:35:54.047902Z",
     "shell.execute_reply": "2020-11-17T14:35:54.047233Z"
    },
    "papermill": {
     "duration": 9.89138,
     "end_time": "2020-11-17T14:35:54.048036",
     "exception": false,
     "start_time": "2020-11-17T14:35:44.156656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from umap import UMAP\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss,roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import make_column_transformer,ColumnTransformer\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline,make_union\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021689,
     "end_time": "2020-11-17T14:35:54.091158",
     "exception": false,
     "start_time": "2020-11-17T14:35:54.069469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# final engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:35:54.141886Z",
     "iopub.status.busy": "2020-11-17T14:35:54.140653Z",
     "iopub.status.idle": "2020-11-17T14:36:01.095896Z",
     "shell.execute_reply": "2020-11-17T14:36:01.094470Z"
    },
    "papermill": {
     "duration": 6.984051,
     "end_time": "2020-11-17T14:36:01.096029",
     "exception": false,
     "start_time": "2020-11-17T14:35:54.111978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:01.146233Z",
     "iopub.status.busy": "2020-11-17T14:36:01.144205Z",
     "iopub.status.idle": "2020-11-17T14:36:01.147003Z",
     "shell.execute_reply": "2020-11-17T14:36:01.147559Z"
    },
    "papermill": {
     "duration": 0.030543,
     "end_time": "2020-11-17T14:36:01.147688",
     "exception": false,
     "start_time": "2020-11-17T14:36:01.117145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:01.198343Z",
     "iopub.status.busy": "2020-11-17T14:36:01.197392Z",
     "iopub.status.idle": "2020-11-17T14:36:01.302013Z",
     "shell.execute_reply": "2020-11-17T14:36:01.301359Z"
    },
    "papermill": {
     "duration": 0.133519,
     "end_time": "2020-11-17T14:36:01.302131",
     "exception": false,
     "start_time": "2020-11-17T14:36:01.168612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021355,
     "end_time": "2020-11-17T14:36:01.345247",
     "exception": false,
     "start_time": "2020-11-17T14:36:01.323892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# target group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:01.412652Z",
     "iopub.status.busy": "2020-11-17T14:36:01.411863Z",
     "iopub.status.idle": "2020-11-17T14:36:01.415195Z",
     "shell.execute_reply": "2020-11-17T14:36:01.415753Z"
    },
    "papermill": {
     "duration": 0.043867,
     "end_time": "2020-11-17T14:36:01.415897",
     "exception": false,
     "start_time": "2020-11-17T14:36:01.372030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = targets.drop(\"sig_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:01.464982Z",
     "iopub.status.busy": "2020-11-17T14:36:01.463949Z",
     "iopub.status.idle": "2020-11-17T14:36:02.448401Z",
     "shell.execute_reply": "2020-11-17T14:36:02.447853Z"
    },
    "papermill": {
     "duration": 1.011572,
     "end_time": "2020-11-17T14:36:02.448583",
     "exception": false,
     "start_time": "2020-11-17T14:36:01.437011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "commun = y.T@y\n",
    "import networkx as nx\n",
    "G = nx.from_pandas_adjacency(commun,create_using=nx.DiGraph)\n",
    "multilabel_targets = list(nx.algorithms.community.modularity_max.greedy_modularity_communities(G)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021455,
     "end_time": "2020-11-17T14:36:02.491860",
     "exception": false,
     "start_time": "2020-11-17T14:36:02.470405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:02.548931Z",
     "iopub.status.busy": "2020-11-17T14:36:02.543063Z",
     "iopub.status.idle": "2020-11-17T14:36:02.799429Z",
     "shell.execute_reply": "2020-11-17T14:36:02.798796Z"
    },
    "papermill": {
     "duration": 0.287124,
     "end_time": "2020-11-17T14:36:02.799584",
     "exception": false,
     "start_time": "2020-11-17T14:36:02.512460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "test = test[test.index.isin(cons_test_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:02.878551Z",
     "iopub.status.busy": "2020-11-17T14:36:02.864625Z",
     "iopub.status.idle": "2020-11-17T14:36:03.403919Z",
     "shell.execute_reply": "2020-11-17T14:36:03.403097Z"
    },
    "papermill": {
     "duration": 0.581344,
     "end_time": "2020-11-17T14:36:03.404034",
     "exception": false,
     "start_time": "2020-11-17T14:36:02.822690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 6, ..., 3, 4, 0]], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/lish-moa/discussion/195195\n",
    "NB_SPLITS = 7\n",
    "seed = 34\n",
    "\n",
    "folds = []\n",
    "    \n",
    "# LOAD FILES\n",
    "train_score = targets.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_score.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 18].index.sort_values()\n",
    "vc2 = vc.loc[vc > 18].index.sort_values()\n",
    "    \n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.groupby('drug_id')[target_feats].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.loc[train_score.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "train_score['fold'] = train_score.drug_id.map(dct1)\n",
    "train_score.loc[train_score.fold.isna(),'fold'] = train_score.loc[train_score.fold.isna(),'sig_id'].map(dct2)\n",
    "train_score.fold = train_score.fold.astype('int8')\n",
    "folds.append(train_score.fold.values)\n",
    "    \n",
    "np.array(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:03.469131Z",
     "iopub.status.busy": "2020-11-17T14:36:03.461343Z",
     "iopub.status.idle": "2020-11-17T14:36:03.805467Z",
     "shell.execute_reply": "2020-11-17T14:36:03.806013Z"
    },
    "papermill": {
     "duration": 0.379779,
     "end_time": "2020-11-17T14:36:03.806154",
     "exception": false,
     "start_time": "2020-11-17T14:36:03.426375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 6, ..., 4, 4, 2]], dtype=int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/lish-moa/discussion/195195\n",
    "NB_SPLITS = 7\n",
    "seed = 34\n",
    "\n",
    "m_folds = []\n",
    "    \n",
    "# LOAD FILES\n",
    "train_score = targets[multilabel_targets+[\"sig_id\"]].merge(drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_score.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 18].index.sort_values()\n",
    "vc2 = vc.loc[vc > 18].index.sort_values()\n",
    "    \n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.groupby('drug_id')[multilabel_targets].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[multilabel_targets])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "tmp = train_score.loc[train_score.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[multilabel_targets])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "train_score['fold'] = train_score.drug_id.map(dct1)\n",
    "train_score.loc[train_score.fold.isna(),'fold'] = train_score.loc[train_score.fold.isna(),'sig_id'].map(dct2)\n",
    "train_score.fold = train_score.fold.astype('int8')\n",
    "m_folds.append(train_score.fold.values)\n",
    "    \n",
    "np.array(m_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0221,
     "end_time": "2020-11-17T14:36:03.849698",
     "exception": false,
     "start_time": "2020-11-17T14:36:03.827598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:03.956629Z",
     "iopub.status.busy": "2020-11-17T14:36:03.955187Z",
     "iopub.status.idle": "2020-11-17T14:36:04.334025Z",
     "shell.execute_reply": "2020-11-17T14:36:04.334549Z"
    },
    "papermill": {
     "duration": 0.461028,
     "end_time": "2020-11-17T14:36:04.334724",
     "exception": false,
     "start_time": "2020-11-17T14:36:03.873696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#importance = pd.read_csv('../input/moa-feat-importance-rapids/output_source.csv')\n",
    "#importance = importance.set_index(\"Feature\")\n",
    "\n",
    "#imp_scaled = importance.copy(deep=True)\n",
    "#for c in imp_scaled.columns:\n",
    "#    imp_scaled[c] = (1 - preprocessing.MinMaxScaler().fit_transform(imp_scaled[[c]])).round(14)\n",
    "\n",
    "#print(imp_scaled.min().min(), imp_scaled.max().max(), imp_scaled.std().mean())\n",
    "\n",
    "#imp_scaled[\"MeanImp\"] = imp_scaled[target_feats].mean(axis=1)\n",
    "#imp_scaled[\"MaxImp\"]  = imp_scaled[target_feats].max(axis=1)\n",
    "\n",
    "#thresh_mean, thresh_max = 0.3, 0.95\n",
    "#fs_both = imp_scaled.loc[(imp_scaled.MeanImp >= thresh_mean) & (imp_scaled.MaxImp >= thresh_max), Targets]\n",
    "#fs_any = list(imp_scaled.loc[(imp_scaled.MeanImp >= thresh_mean) | (imp_scaled.MaxImp >= thresh_max), target_feats].index)\n",
    "#print(len(fs_any))\n",
    "\n",
    "#least_contribution = list(imp_scaled.sort_values(\"MeanImp\", ascending=True).index)\n",
    "#least_contribution2 = list(imp_scaled.sort_values(\"MaxImp\", ascending=True).index)\n",
    "#drop_feats = [i for i in c_feats+g_feats if i not in fs_any]\n",
    "\n",
    "X = train.iloc[:,4:].copy().values\n",
    "select = VarianceThreshold(threshold=0.7)\n",
    "X_new = select.fit_transform(X)\n",
    "drop_feats = list(np.array(train.iloc[:,4:].columns)[select.get_support()==False])\n",
    "print(len(drop_feats))\n",
    "\n",
    "#drop_feats = least_contribution[:10]\n",
    "#drop_feats += least_contribution2[:10]\n",
    "\n",
    "train.drop(drop_feats, axis=1, inplace=True)\n",
    "test.drop(drop_feats, axis=1, inplace=True)\n",
    "\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:04.388724Z",
     "iopub.status.busy": "2020-11-17T14:36:04.387142Z",
     "iopub.status.idle": "2020-11-17T14:36:15.815344Z",
     "shell.execute_reply": "2020-11-17T14:36:15.814368Z"
    },
    "papermill": {
     "duration": 11.457898,
     "end_time": "2020-11-17T14:36:15.815474",
     "exception": false,
     "start_time": "2020-11-17T14:36:04.357576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rank gauss\n",
    "for i in c_feats + g_feats:\n",
    "    ss = preprocessing.QuantileTransformer(n_quantiles=1000, random_state=0, output_distribution=\"normal\")\n",
    "    #    ss.fit(pd.concat([train[i], test[i]]).values.reshape(-1,1))\n",
    "    ss.fit(train[i].values.reshape(-1,1))\n",
    "    train[i] = ss.transform(train[i].values.reshape(-1,1))\n",
    "    test[i] = ss.transform(test[i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:15.885705Z",
     "iopub.status.busy": "2020-11-17T14:36:15.883910Z",
     "iopub.status.idle": "2020-11-17T14:36:18.055532Z",
     "shell.execute_reply": "2020-11-17T14:36:18.054936Z"
    },
    "papermill": {
     "duration": 2.216429,
     "end_time": "2020-11-17T14:36:18.055647",
     "exception": false,
     "start_time": "2020-11-17T14:36:15.839218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_num = 10\n",
    "pca_c_cols = [\"pca-c\"+str(i+1) for i in range(c_num)]\n",
    "pca = PCA(n_components=c_num,random_state=42)\n",
    "c_train = pca.fit_transform(train[c_feats])\n",
    "c_test = pca.transform(test[c_feats])\n",
    "c_train = pd.DataFrame(c_train, columns=pca_c_cols)\n",
    "c_test = pd.DataFrame(c_test, columns=pca_c_cols)\n",
    "\n",
    "g_num = 60\n",
    "pca_g_cols = [\"pca-g\"+str(i+1) for i in range(g_num)]\n",
    "pca = PCA(n_components=g_num, random_state=42)\n",
    "g_train = pca.fit_transform(train[g_feats])\n",
    "g_test = pca.transform(test[g_feats])\n",
    "g_train = pd.DataFrame(g_train, columns=pca_g_cols)\n",
    "g_test = pd.DataFrame(g_test, columns=pca_g_cols)\n",
    "\n",
    "train = pd.concat([train, c_train],axis=1)\n",
    "test = pd.concat([test, c_test],axis=1)\n",
    "train = pd.concat([train, g_train],axis=1)\n",
    "test = pd.concat([test, g_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:18.114306Z",
     "iopub.status.busy": "2020-11-17T14:36:18.113012Z",
     "iopub.status.idle": "2020-11-17T14:36:20.029217Z",
     "shell.execute_reply": "2020-11-17T14:36:20.030568Z"
    },
    "papermill": {
     "duration": 1.951188,
     "end_time": "2020-11-17T14:36:20.030786",
     "exception": false,
     "start_time": "2020-11-17T14:36:18.079598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 918) (3624, 918)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    tmp['g_kurt'] = tmp[g_feats].kurtosis(axis = 1)\n",
    "    tmp['g_skew'] = tmp[g_feats].skew(axis = 1)\n",
    "    tmp['c_kurt'] = tmp[c_feats].kurtosis(axis = 1)\n",
    "    tmp['c_skew'] = tmp[c_feats].skew(axis = 1)\n",
    "    tmp = pd.get_dummies(tmp, columns=['cp_time','cp_dose'])\n",
    "    tmp.drop([\"cp_type\", \"sig_id\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "train = fe(train)\n",
    "test = fe(test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:20.118728Z",
     "iopub.status.busy": "2020-11-17T14:36:20.117688Z",
     "iopub.status.idle": "2020-11-17T14:36:20.122348Z",
     "shell.execute_reply": "2020-11-17T14:36:20.123143Z"
    },
    "papermill": {
     "duration": 0.051354,
     "end_time": "2020-11-17T14:36:20.123346",
     "exception": false,
     "start_time": "2020-11-17T14:36:20.071992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[\"fold\"] = np.array(folds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:20.211754Z",
     "iopub.status.busy": "2020-11-17T14:36:20.210760Z",
     "iopub.status.idle": "2020-11-17T14:36:20.276865Z",
     "shell.execute_reply": "2020-11-17T14:36:20.278194Z"
    },
    "papermill": {
     "duration": 0.118532,
     "end_time": "2020-11-17T14:36:20.278419",
     "exception": false,
     "start_time": "2020-11-17T14:36:20.159887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_train = train.copy()\n",
    "g_train[\"fold\"] = np.array(m_folds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:20.363580Z",
     "iopub.status.busy": "2020-11-17T14:36:20.362684Z",
     "iopub.status.idle": "2020-11-17T14:36:20.678078Z",
     "shell.execute_reply": "2020-11-17T14:36:20.677400Z"
    },
    "papermill": {
     "duration": 0.364544,
     "end_time": "2020-11-17T14:36:20.678215",
     "exception": false,
     "start_time": "2020-11-17T14:36:20.313671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_train = train.copy().to_numpy()\n",
    "fn_g_train = g_train.copy().to_numpy()\n",
    "\n",
    "fn_test = test.copy().to_numpy()\n",
    "\n",
    "fn_group_targets = targets[multilabel_targets].to_numpy()\n",
    "\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02334,
     "end_time": "2020-11-17T14:36:20.727311",
     "exception": false,
     "start_time": "2020-11-17T14:36:20.703971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:21.196474Z",
     "iopub.status.busy": "2020-11-17T14:36:21.195453Z",
     "iopub.status.idle": "2020-11-17T14:36:21.198761Z",
     "shell.execute_reply": "2020-11-17T14:36:21.198206Z"
    },
    "papermill": {
     "duration": 0.448241,
     "end_time": "2020-11-17T14:36:21.198882",
     "exception": false,
     "start_time": "2020-11-17T14:36:20.750641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:21.257268Z",
     "iopub.status.busy": "2020-11-17T14:36:21.256341Z",
     "iopub.status.idle": "2020-11-17T14:36:21.262780Z",
     "shell.execute_reply": "2020-11-17T14:36:21.262203Z"
    },
    "papermill": {
     "duration": 0.038971,
     "end_time": "2020-11-17T14:36:21.262893",
     "exception": false,
     "start_time": "2020-11-17T14:36:21.223922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:21.329084Z",
     "iopub.status.busy": "2020-11-17T14:36:21.323560Z",
     "iopub.status.idle": "2020-11-17T14:36:21.331341Z",
     "shell.execute_reply": "2020-11-17T14:36:21.331826Z"
    },
    "papermill": {
     "duration": 0.045579,
     "end_time": "2020-11-17T14:36:21.331936",
     "exception": false,
     "start_time": "2020-11-17T14:36:21.286357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "def seed_everything(seed=42): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1024))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, last_num))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024685,
     "end_time": "2020-11-17T14:36:21.381466",
     "exception": false,
     "start_time": "2020-11-17T14:36:21.356781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:21.454456Z",
     "iopub.status.busy": "2020-11-17T14:36:21.443315Z",
     "iopub.status.idle": "2020-11-17T14:36:21.485671Z",
     "shell.execute_reply": "2020-11-17T14:36:21.485153Z"
    },
    "papermill": {
     "duration": 0.080018,
     "end_time": "2020-11-17T14:36:21.485770",
     "exception": false,
     "start_time": "2020-11-17T14:36:21.405752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "train_epochs = 20\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=224)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test = torch.utils.data.TensorDataset(X_test2) \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold in range(n_folds):\n",
    "        valid_index = X_train[:,-1] == fold\n",
    "        train_index = X_train[:,-1] != fold\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        X_train2 = X_train2[:,:-1]\n",
    "        X_valid2 = X_valid2[:,:-1]\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "                    \n",
    "            if sm_avg_val_loss < best_val_loss:\n",
    "                best_val_loss = sm_avg_val_loss\n",
    "                print('Epoch {}  loss={:.5f}  val_loss={:.5f}  sm_loss={:.5f}  sm_val_loss={:.5f}  time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))         \n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            oof_epoch[i * batch_size:(i+1) * batch_size,:] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "            target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "    \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:36:21.555907Z",
     "iopub.status.busy": "2020-11-17T14:36:21.555232Z",
     "iopub.status.idle": "2020-11-17T14:37:53.695803Z",
     "shell.execute_reply": "2020-11-17T14:37:53.696395Z"
    },
    "papermill": {
     "duration": 92.179399,
     "end_time": "2020-11-17T14:37:53.696599",
     "exception": false,
     "start_time": "2020-11-17T14:36:21.517200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1  loss=0.41245  val_loss=0.02275  sm_loss=0.41239  sm_val_loss=0.02272  time=1.67s\n",
      "Epoch 2  loss=0.02013  val_loss=0.02023  sm_loss=0.02006  sm_val_loss=0.02016  time=0.94s\n",
      "Epoch 3  loss=0.01832  val_loss=0.01816  sm_loss=0.01835  sm_val_loss=0.01818  time=0.95s\n",
      "Epoch 4  loss=0.01760  val_loss=0.01792  sm_loss=0.01769  sm_val_loss=0.01794  time=0.93s\n",
      "Epoch 5  loss=0.01749  val_loss=0.01794  sm_loss=0.01760  sm_val_loss=0.01794  time=1.11s\n",
      "Epoch 8  loss=0.01739  val_loss=0.01778  sm_loss=0.01751  sm_val_loss=0.01782  time=0.92s\n",
      "Epoch 11  loss=0.01708  val_loss=0.01762  sm_loss=0.01721  sm_val_loss=0.01766  time=0.89s\n",
      "Epoch 12  loss=0.01680  val_loss=0.01753  sm_loss=0.01694  sm_val_loss=0.01755  time=0.90s\n",
      "Epoch 13  loss=0.01664  val_loss=0.01740  sm_loss=0.01678  sm_val_loss=0.01744  time=0.95s\n",
      "Epoch 15  loss=0.01602  val_loss=0.01727  sm_loss=0.01619  sm_val_loss=0.01728  time=1.25s\n",
      "Epoch 16  loss=0.01561  val_loss=0.01705  sm_loss=0.01579  sm_val_loss=0.01710  time=1.18s\n",
      "Epoch 17  loss=0.01518  val_loss=0.01706  sm_loss=0.01538  sm_val_loss=0.01709  time=0.98s\n",
      "Epoch 18  loss=0.01480  val_loss=0.01702  sm_loss=0.01500  sm_val_loss=0.01706  time=0.92s\n",
      "Epoch 19  loss=0.01440  val_loss=0.01700  sm_loss=0.01461  sm_val_loss=0.01704  time=0.95s\n",
      "Epoch 20  loss=0.01422  val_loss=0.01698  sm_loss=0.01444  sm_val_loss=0.01701  time=0.93s\n",
      "Fold 1 log loss: 0.01709653331552041\n",
      "Fold 2\n",
      "Epoch 1  loss=0.41387  val_loss=0.02214  sm_loss=0.41380  sm_val_loss=0.02210  time=0.93s\n",
      "Epoch 2  loss=0.02007  val_loss=0.01889  sm_loss=0.02003  sm_val_loss=0.01885  time=0.93s\n",
      "Epoch 3  loss=0.01835  val_loss=0.01792  sm_loss=0.01831  sm_val_loss=0.01794  time=0.96s\n",
      "Epoch 5  loss=0.01758  val_loss=0.01764  sm_loss=0.01769  sm_val_loss=0.01765  time=1.26s\n",
      "Epoch 6  loss=0.01753  val_loss=0.01759  sm_loss=0.01765  sm_val_loss=0.01761  time=0.98s\n",
      "Epoch 9  loss=0.01741  val_loss=0.01756  sm_loss=0.01753  sm_val_loss=0.01759  time=0.97s\n",
      "Epoch 11  loss=0.01710  val_loss=0.01745  sm_loss=0.01723  sm_val_loss=0.01747  time=0.90s\n",
      "Epoch 12  loss=0.01691  val_loss=0.01724  sm_loss=0.01705  sm_val_loss=0.01726  time=0.91s\n",
      "Epoch 13  loss=0.01666  val_loss=0.01711  sm_loss=0.01681  sm_val_loss=0.01711  time=0.90s\n",
      "Epoch 14  loss=0.01643  val_loss=0.01698  sm_loss=0.01660  sm_val_loss=0.01699  time=0.91s\n",
      "Epoch 15  loss=0.01604  val_loss=0.01681  sm_loss=0.01621  sm_val_loss=0.01687  time=0.97s\n",
      "Epoch 16  loss=0.01565  val_loss=0.01683  sm_loss=0.01583  sm_val_loss=0.01685  time=1.18s\n",
      "Epoch 17  loss=0.01523  val_loss=0.01679  sm_loss=0.01543  sm_val_loss=0.01682  time=0.93s\n",
      "Epoch 18  loss=0.01477  val_loss=0.01675  sm_loss=0.01498  sm_val_loss=0.01677  time=0.91s\n",
      "Epoch 19  loss=0.01447  val_loss=0.01673  sm_loss=0.01468  sm_val_loss=0.01675  time=0.96s\n",
      "Fold 2 log loss: 0.01673138123321087\n",
      "Fold 3\n",
      "Epoch 1  loss=0.41302  val_loss=0.02325  sm_loss=0.41293  sm_val_loss=0.02322  time=0.99s\n",
      "Epoch 2  loss=0.02002  val_loss=0.02041  sm_loss=0.01998  sm_val_loss=0.02037  time=1.22s\n",
      "Epoch 3  loss=0.01831  val_loss=0.01928  sm_loss=0.01830  sm_val_loss=0.01924  time=0.94s\n",
      "Epoch 4  loss=0.01763  val_loss=0.01913  sm_loss=0.01771  sm_val_loss=0.01911  time=1.21s\n",
      "Epoch 5  loss=0.01743  val_loss=0.01847  sm_loss=0.01755  sm_val_loss=0.01851  time=0.94s\n",
      "Epoch 7  loss=0.01733  val_loss=0.01842  sm_loss=0.01744  sm_val_loss=0.01843  time=1.34s\n",
      "Epoch 8  loss=0.01731  val_loss=0.01834  sm_loss=0.01743  sm_val_loss=0.01836  time=1.12s\n",
      "Epoch 10  loss=0.01707  val_loss=0.01828  sm_loss=0.01719  sm_val_loss=0.01829  time=0.92s\n",
      "Epoch 12  loss=0.01675  val_loss=0.01804  sm_loss=0.01690  sm_val_loss=0.01808  time=0.93s\n",
      "Epoch 13  loss=0.01653  val_loss=0.01799  sm_loss=0.01668  sm_val_loss=0.01802  time=1.03s\n",
      "Epoch 14  loss=0.01624  val_loss=0.01798  sm_loss=0.01640  sm_val_loss=0.01798  time=1.10s\n",
      "Epoch 15  loss=0.01596  val_loss=0.01776  sm_loss=0.01614  sm_val_loss=0.01778  time=1.02s\n",
      "Epoch 16  loss=0.01557  val_loss=0.01769  sm_loss=0.01575  sm_val_loss=0.01771  time=0.94s\n",
      "Epoch 17  loss=0.01514  val_loss=0.01764  sm_loss=0.01534  sm_val_loss=0.01764  time=0.93s\n",
      "Epoch 18  loss=0.01471  val_loss=0.01760  sm_loss=0.01491  sm_val_loss=0.01760  time=0.99s\n",
      "Fold 3 log loss: 0.017599709471007\n",
      "Fold 4\n",
      "Epoch 1  loss=0.41305  val_loss=0.02203  sm_loss=0.41297  sm_val_loss=0.02202  time=0.98s\n",
      "Epoch 3  loss=0.01812  val_loss=0.01817  sm_loss=0.01813  sm_val_loss=0.01821  time=1.12s\n",
      "Epoch 4  loss=0.01757  val_loss=0.01783  sm_loss=0.01766  sm_val_loss=0.01786  time=0.90s\n",
      "Epoch 11  loss=0.01700  val_loss=0.01756  sm_loss=0.01713  sm_val_loss=0.01762  time=1.13s\n",
      "Epoch 14  loss=0.01632  val_loss=0.01748  sm_loss=0.01648  sm_val_loss=0.01754  time=1.20s\n",
      "Epoch 15  loss=0.01598  val_loss=0.01739  sm_loss=0.01615  sm_val_loss=0.01744  time=0.99s\n",
      "Epoch 16  loss=0.01558  val_loss=0.01726  sm_loss=0.01576  sm_val_loss=0.01732  time=0.95s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-77cbdf71d557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelling_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtarget_oof\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moof\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpytorch_pred\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7fe01ed80674>\u001b[0m in \u001b[0;36mmodelling_torch\u001b[0;34m(tr, target, te, sample_seed, init_num, last_num)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0msm_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4,5,6,7,8,9]\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1]-1, fn_targets.shape[1])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:37:53.806275Z",
     "iopub.status.busy": "2020-11-17T14:37:53.805364Z",
     "iopub.status.idle": "2020-11-17T14:37:59.982939Z",
     "shell.execute_reply": "2020-11-17T14:37:59.982324Z"
    },
    "papermill": {
     "duration": 6.235858,
     "end_time": "2020-11-17T14:37:59.983051",
     "exception": false,
     "start_time": "2020-11-17T14:37:53.747193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.11859124020134244\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-17T14:38:00.082473Z",
     "iopub.status.busy": "2020-11-17T14:38:00.081874Z",
     "iopub.status.idle": "2020-11-17T14:38:01.786040Z",
     "shell.execute_reply": "2020-11-17T14:38:01.785012Z"
    },
    "papermill": {
     "duration": 1.756351,
     "end_time": "2020-11-17T14:38:01.786163",
     "exception": false,
     "start_time": "2020-11-17T14:38:00.029812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.loc[cons_test_index,target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 142.436967,
   "end_time": "2020-11-17T14:38:02.343098",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-17T14:35:39.906131",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
