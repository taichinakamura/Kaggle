{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02044,
     "end_time": "2020-10-13T07:26:26.476618",
     "exception": false,
     "start_time": "2020-10-13T07:26:26.456178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- cancel sum of moa\n",
    "- incorporate new features (modify mistake)\n",
    "- try robust scaler standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:26.525198Z",
     "iopub.status.busy": "2020-10-13T07:26:26.524273Z",
     "iopub.status.idle": "2020-10-13T07:26:33.829539Z",
     "shell.execute_reply": "2020-10-13T07:26:33.827874Z"
    },
    "papermill": {
     "duration": 7.333862,
     "end_time": "2020-10-13T07:26:33.829690",
     "exception": false,
     "start_time": "2020-10-13T07:26:26.495828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:33.880496Z",
     "iopub.status.busy": "2020-10-13T07:26:33.879479Z",
     "iopub.status.idle": "2020-10-13T07:26:40.363755Z",
     "shell.execute_reply": "2020-10-13T07:26:40.362667Z"
    },
    "papermill": {
     "duration": 6.514301,
     "end_time": "2020-10-13T07:26:40.363882",
     "exception": false,
     "start_time": "2020-10-13T07:26:33.849581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:40.412805Z",
     "iopub.status.busy": "2020-10-13T07:26:40.411853Z",
     "iopub.status.idle": "2020-10-13T07:26:40.415672Z",
     "shell.execute_reply": "2020-10-13T07:26:40.416196Z"
    },
    "papermill": {
     "duration": 0.032269,
     "end_time": "2020-10-13T07:26:40.416362",
     "exception": false,
     "start_time": "2020-10-13T07:26:40.384093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:40.466168Z",
     "iopub.status.busy": "2020-10-13T07:26:40.465134Z",
     "iopub.status.idle": "2020-10-13T07:26:40.566853Z",
     "shell.execute_reply": "2020-10-13T07:26:40.566184Z"
    },
    "papermill": {
     "duration": 0.130675,
     "end_time": "2020-10-13T07:26:40.566994",
     "exception": false,
     "start_time": "2020-10-13T07:26:40.436319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019137,
     "end_time": "2020-10-13T07:26:40.605827",
     "exception": false,
     "start_time": "2020-10-13T07:26:40.586690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:40.661645Z",
     "iopub.status.busy": "2020-10-13T07:26:40.660495Z",
     "iopub.status.idle": "2020-10-13T07:26:40.978941Z",
     "shell.execute_reply": "2020-10-13T07:26:40.979536Z"
    },
    "papermill": {
     "duration": 0.354149,
     "end_time": "2020-10-13T07:26:40.979711",
     "exception": false,
     "start_time": "2020-10-13T07:26:40.625562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:41.056310Z",
     "iopub.status.busy": "2020-10-13T07:26:41.055200Z",
     "iopub.status.idle": "2020-10-13T07:26:41.070075Z",
     "shell.execute_reply": "2020-10-13T07:26:41.070630Z"
    },
    "papermill": {
     "duration": 0.071052,
     "end_time": "2020-10-13T07:26:41.070800",
     "exception": false,
     "start_time": "2020-10-13T07:26:40.999748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:41.120346Z",
     "iopub.status.busy": "2020-10-13T07:26:41.119370Z",
     "iopub.status.idle": "2020-10-13T07:26:41.160218Z",
     "shell.execute_reply": "2020-10-13T07:26:41.160992Z"
    },
    "papermill": {
     "duration": 0.067784,
     "end_time": "2020-10-13T07:26:41.161175",
     "exception": false,
     "start_time": "2020-10-13T07:26:41.093391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n"
     ]
    }
   ],
   "source": [
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "#drop_list2 = list(nontarget_dists[(nontarget_dists.number>0) & (nontarget_dists.number<=6)][\"target\"].values)[:-1]\n",
    "#print(\"second drop\", len(drop_list2))\n",
    "#non_targets.drop(drop_list2, axis=1, inplace=True)\n",
    "#print(\"shape after 2nd drop:\", non_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:41.205724Z",
     "iopub.status.busy": "2020-10-13T07:26:41.204977Z",
     "iopub.status.idle": "2020-10-13T07:26:41.209506Z",
     "shell.execute_reply": "2020-10-13T07:26:41.210150Z"
    },
    "papermill": {
     "duration": 0.029108,
     "end_time": "2020-10-13T07:26:41.210298",
     "exception": false,
     "start_time": "2020-10-13T07:26:41.181190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X = train[g_feats].copy().values\n",
    "#select = VarianceThreshold(threshold=1)\n",
    "#X_new = select.fit_transform(X)\n",
    "#drop_g_feats = list(np.array(g_feats)[select.get_support()==False])\n",
    "#len(drop_g_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019806,
     "end_time": "2020-10-13T07:26:41.250884",
     "exception": false,
     "start_time": "2020-10-13T07:26:41.231078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:41.297345Z",
     "iopub.status.busy": "2020-10-13T07:26:41.296503Z",
     "iopub.status.idle": "2020-10-13T07:26:41.300766Z",
     "shell.execute_reply": "2020-10-13T07:26:41.301430Z"
    },
    "papermill": {
     "duration": 0.030286,
     "end_time": "2020-10-13T07:26:41.301583",
     "exception": false,
     "start_time": "2020-10-13T07:26:41.271297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#num = 10\n",
    "#pca_c_cols = [\"pca-c\"+str(i+1) for i in range(num)]\n",
    "#pca = PCA(n_components=num)\n",
    "#tmp_train = pca.fit_transform(train[c_feats])\n",
    "#tmp_test = pca.transform(test[c_feats])\n",
    "#tmp_train = pd.DataFrame(tmp_train, columns=pca_c_cols)\n",
    "#tmp_test = pd.DataFrame(tmp_test, columns=pca_c_cols)\n",
    "\n",
    "#train = pd.concat([train, tmp_train],axis=1)\n",
    "#test = pd.concat([test, tmp_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:41.364711Z",
     "iopub.status.busy": "2020-10-13T07:26:41.363204Z",
     "iopub.status.idle": "2020-10-13T07:26:47.127261Z",
     "shell.execute_reply": "2020-10-13T07:26:47.126561Z"
    },
    "papermill": {
     "duration": 5.804491,
     "end_time": "2020-10-13T07:26:47.127402",
     "exception": false,
     "start_time": "2020-10-13T07:26:41.322911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 889) (3982, 889)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    tmp['g_sum'] = tmp[g_feats].sum(axis = 1)\n",
    "    tmp['g_mean'] = tmp[g_feats].mean(axis = 1)\n",
    "    tmp['g_std'] = tmp[g_feats].std(axis = 1)\n",
    "    tmp['g_kurt'] = tmp[g_feats].kurtosis(axis = 1)\n",
    "    tmp['g_skew'] = tmp[g_feats].skew(axis = 1)\n",
    "    tmp['c_sum'] = tmp[c_feats].sum(axis = 1)\n",
    "    tmp['c_mean'] = tmp[c_feats].mean(axis = 1)\n",
    "    tmp['c_std'] = tmp[c_feats].std(axis = 1)\n",
    "    tmp['c_kurt'] = tmp[c_feats].kurtosis(axis = 1)\n",
    "    tmp['c_skew'] = tmp[c_feats].skew(axis = 1)\n",
    "    tmp['gc_sum'] = tmp[c_feats + g_feats].sum(axis = 1)\n",
    "    tmp['gc_mean'] = tmp[c_feats + g_feats].mean(axis = 1)\n",
    "    tmp['gc_std'] = tmp[c_feats + g_feats].std(axis = 1)\n",
    "    tmp['gc_kurt'] = tmp[c_feats + g_feats].kurtosis(axis = 1)\n",
    "    tmp['gc_skew'] = tmp[c_feats + g_feats].skew(axis = 1)\n",
    "    tmp.loc[:, 'cp_type'] = tmp.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021296,
     "end_time": "2020-10-13T07:26:47.172811",
     "exception": false,
     "start_time": "2020-10-13T07:26:47.151515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:47.595169Z",
     "iopub.status.busy": "2020-10-13T07:26:47.594345Z",
     "iopub.status.idle": "2020-10-13T07:26:47.598093Z",
     "shell.execute_reply": "2020-10-13T07:26:47.598606Z"
    },
    "papermill": {
     "duration": 0.404741,
     "end_time": "2020-10-13T07:26:47.598735",
     "exception": false,
     "start_time": "2020-10-13T07:26:47.193994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048,1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_num))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021923,
     "end_time": "2020-10-13T07:26:47.642110",
     "exception": false,
     "start_time": "2020-10-13T07:26:47.620187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# predict non-targets, targets separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:47.709646Z",
     "iopub.status.busy": "2020-10-13T07:26:47.688813Z",
     "iopub.status.idle": "2020-10-13T07:26:47.728512Z",
     "shell.execute_reply": "2020-10-13T07:26:47.728969Z"
    },
    "papermill": {
     "duration": 0.065443,
     "end_time": "2020-10-13T07:26:47.729108",
     "exception": false,
     "start_time": "2020-10-13T07:26:47.663665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 5\n",
    "n_folds=5\n",
    "\n",
    "def first_learning(tr, target, sample_seed, init_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    files = []\n",
    "        \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'parameters'+str(fold+1)+'.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        files.append('parameters'+str(fold+1)+'.pt')\n",
    "        pred_model = MoaModel(init_num)\n",
    "        pred_model.load_state_dict(torch.load('parameters'+str(fold+1)+'.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:47.780938Z",
     "iopub.status.busy": "2020-10-13T07:26:47.779540Z",
     "iopub.status.idle": "2020-10-13T07:26:49.087494Z",
     "shell.execute_reply": "2020-10-13T07:26:49.088194Z"
    },
    "papermill": {
     "duration": 1.337888,
     "end_time": "2020-10-13T07:26:49.088364",
     "exception": false,
     "start_time": "2020-10-13T07:26:47.750476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "ss = preprocessing.RobustScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "\n",
    "#seeds = [0]\n",
    "#for seed_ in seeds:\n",
    "#    files = first_learning(fn_train, fn_nontargets, seed_, fn_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02186,
     "end_time": "2020-10-13T07:26:49.132548",
     "exception": false,
     "start_time": "2020-10-13T07:26:49.110688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train by targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:49.181533Z",
     "iopub.status.busy": "2020-10-13T07:26:49.180431Z",
     "iopub.status.idle": "2020-10-13T07:26:49.228836Z",
     "shell.execute_reply": "2020-10-13T07:26:49.228299Z"
    },
    "papermill": {
     "duration": 0.074197,
     "end_time": "2020-10-13T07:26:49.228968",
     "exception": false,
     "start_time": "2020-10-13T07:26:49.154771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 40\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, files):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        if files != []:\n",
    "            clf.load_state_dict(torch.load(files[fold]))\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:26:49.287296Z",
     "iopub.status.busy": "2020-10-13T07:26:49.285818Z",
     "iopub.status.idle": "2020-10-13T07:33:58.563960Z",
     "shell.execute_reply": "2020-10-13T07:33:58.564585Z"
    },
    "papermill": {
     "duration": 429.312781,
     "end_time": "2020-10-13T07:33:58.564759",
     "exception": false,
     "start_time": "2020-10-13T07:26:49.251978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.413235 \t val_loss=0.077589 \t time=1.60s\n",
      "Best model: Epoch 2 \t loss=0.048799 \t val_loss=0.029939 \t time=1.13s\n",
      "Best model: Epoch 3 \t loss=0.027001 \t val_loss=0.022568 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023461 \t val_loss=0.021381 \t time=0.92s\n",
      "Best model: Epoch 5 \t loss=0.021641 \t val_loss=0.020312 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020482 \t val_loss=0.019438 \t time=0.93s\n",
      "Best model: Epoch 7 \t loss=0.019787 \t val_loss=0.018774 \t time=1.07s\n",
      "Best model: Epoch 8 \t loss=0.019211 \t val_loss=0.018476 \t time=0.94s\n",
      "Best model: Epoch 9 \t loss=0.018748 \t val_loss=0.018042 \t time=0.89s\n",
      "Best model: Epoch 10 \t loss=0.018529 \t val_loss=0.018035 \t time=0.84s\n",
      "Best model: Epoch 11 \t loss=0.018127 \t val_loss=0.017859 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.018116 \t val_loss=0.017517 \t time=0.86s\n",
      "Best model: Epoch 13 \t loss=0.017843 \t val_loss=0.017366 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017441 \t val_loss=0.017175 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017319 \t val_loss=0.017101 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.016975 \t val_loss=0.016952 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.016782 \t val_loss=0.016830 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016465 \t val_loss=0.016669 \t time=1.10s\n",
      "Best model: Epoch 21 \t loss=0.016204 \t val_loss=0.016607 \t time=1.12s\n",
      "Best model: Epoch 22 \t loss=0.016037 \t val_loss=0.016539 \t time=0.97s\n",
      "Best model: Epoch 23 \t loss=0.015857 \t val_loss=0.016525 \t time=0.99s\n",
      "Best model: Epoch 24 \t loss=0.015734 \t val_loss=0.016479 \t time=0.86s\n",
      "Best model: Epoch 26 \t loss=0.015688 \t val_loss=0.016370 \t time=0.85s\n",
      "Best model: Epoch 30 \t loss=0.015342 \t val_loss=0.016324 \t time=1.22s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014535 \t val_loss=0.016181 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.014225 \t val_loss=0.016120 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.013998 \t val_loss=0.016059 \t time=0.84s\n",
      "Best model: Epoch 38 \t loss=0.013847 \t val_loss=0.016040 \t time=0.84s\n",
      "Fold 1 log loss: 0.016141570942390368\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414150 \t val_loss=0.076450 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.049671 \t val_loss=0.030401 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027260 \t val_loss=0.022775 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.023183 \t val_loss=0.020913 \t time=0.84s\n",
      "Best model: Epoch 5 \t loss=0.021429 \t val_loss=0.020423 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020893 \t val_loss=0.019235 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.019850 \t val_loss=0.018726 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.019277 \t val_loss=0.018315 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.019042 \t val_loss=0.018147 \t time=0.84s\n",
      "Best model: Epoch 10 \t loss=0.018548 \t val_loss=0.017833 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.018297 \t val_loss=0.017562 \t time=1.10s\n",
      "Best model: Epoch 12 \t loss=0.017937 \t val_loss=0.017365 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017581 \t val_loss=0.017285 \t time=1.08s\n",
      "Best model: Epoch 14 \t loss=0.017322 \t val_loss=0.016976 \t time=0.99s\n",
      "Best model: Epoch 16 \t loss=0.017026 \t val_loss=0.016915 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016721 \t val_loss=0.016742 \t time=1.26s\n",
      "Best model: Epoch 18 \t loss=0.016476 \t val_loss=0.016618 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016397 \t val_loss=0.016617 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016339 \t val_loss=0.016587 \t time=1.18s\n",
      "Best model: Epoch 21 \t loss=0.016139 \t val_loss=0.016555 \t time=1.18s\n",
      "Best model: Epoch 22 \t loss=0.015915 \t val_loss=0.016368 \t time=1.00s\n",
      "Best model: Epoch 24 \t loss=0.015786 \t val_loss=0.016354 \t time=0.94s\n",
      "Best model: Epoch 25 \t loss=0.015721 \t val_loss=0.016320 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015580 \t val_loss=0.016285 \t time=0.88s\n",
      "Best model: Epoch 28 \t loss=0.015401 \t val_loss=0.016264 \t time=0.88s\n",
      "Best model: Epoch 31 \t loss=0.015187 \t val_loss=0.016241 \t time=0.85s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014414 \t val_loss=0.016103 \t time=0.84s\n",
      "Best model: Epoch 37 \t loss=0.014087 \t val_loss=0.016039 \t time=0.84s\n",
      "Best model: Epoch 39 \t loss=0.013731 \t val_loss=0.016008 \t time=0.93s\n",
      "Best model: Epoch 40 \t loss=0.013546 \t val_loss=0.015978 \t time=0.90s\n",
      "Fold 2 log loss: 0.01603932333765505\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.416551 \t val_loss=0.078894 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.049165 \t val_loss=0.029577 \t time=1.21s\n",
      "Best model: Epoch 3 \t loss=0.027611 \t val_loss=0.022721 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.023658 \t val_loss=0.020894 \t time=1.14s\n",
      "Best model: Epoch 5 \t loss=0.021446 \t val_loss=0.019832 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020382 \t val_loss=0.019031 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.019826 \t val_loss=0.018727 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019603 \t val_loss=0.018379 \t time=0.96s\n",
      "Best model: Epoch 9 \t loss=0.019083 \t val_loss=0.018217 \t time=1.00s\n",
      "Best model: Epoch 10 \t loss=0.018542 \t val_loss=0.017710 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018171 \t val_loss=0.017671 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.017858 \t val_loss=0.017430 \t time=0.94s\n",
      "Best model: Epoch 13 \t loss=0.017636 \t val_loss=0.017267 \t time=1.04s\n",
      "Best model: Epoch 15 \t loss=0.017510 \t val_loss=0.017078 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.017065 \t val_loss=0.017032 \t time=1.01s\n",
      "Best model: Epoch 17 \t loss=0.016875 \t val_loss=0.016764 \t time=0.96s\n",
      "Best model: Epoch 18 \t loss=0.016678 \t val_loss=0.016740 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016505 \t val_loss=0.016655 \t time=0.86s\n",
      "Best model: Epoch 20 \t loss=0.016307 \t val_loss=0.016537 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016219 \t val_loss=0.016479 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016114 \t val_loss=0.016444 \t time=0.97s\n",
      "Best model: Epoch 25 \t loss=0.015811 \t val_loss=0.016346 \t time=0.94s\n",
      "Best model: Epoch 26 \t loss=0.015677 \t val_loss=0.016310 \t time=0.84s\n",
      "Best model: Epoch 30 \t loss=0.015338 \t val_loss=0.016264 \t time=1.02s\n",
      "Best model: Epoch 31 \t loss=0.015265 \t val_loss=0.016241 \t time=0.86s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014443 \t val_loss=0.016132 \t time=1.78s\n",
      "Best model: Epoch 37 \t loss=0.014129 \t val_loss=0.016004 \t time=1.27s\n",
      "Best model: Epoch 38 \t loss=0.013983 \t val_loss=0.015983 \t time=1.21s\n",
      "Best model: Epoch 39 \t loss=0.013797 \t val_loss=0.015965 \t time=0.89s\n",
      "Fold 3 log loss: 0.015960235832428213\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413155 \t val_loss=0.077030 \t time=0.84s\n",
      "Best model: Epoch 2 \t loss=0.048766 \t val_loss=0.028281 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027334 \t val_loss=0.023151 \t time=1.10s\n",
      "Best model: Epoch 4 \t loss=0.023306 \t val_loss=0.021017 \t time=0.96s\n",
      "Best model: Epoch 5 \t loss=0.021705 \t val_loss=0.019710 \t time=1.01s\n",
      "Best model: Epoch 6 \t loss=0.020596 \t val_loss=0.018917 \t time=1.01s\n",
      "Best model: Epoch 7 \t loss=0.019787 \t val_loss=0.018402 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019434 \t val_loss=0.018186 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018987 \t val_loss=0.017734 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.018381 \t val_loss=0.017414 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.017974 \t val_loss=0.017221 \t time=0.84s\n",
      "Best model: Epoch 13 \t loss=0.017584 \t val_loss=0.017078 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017368 \t val_loss=0.016915 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017218 \t val_loss=0.016817 \t time=1.10s\n",
      "Best model: Epoch 16 \t loss=0.017067 \t val_loss=0.016784 \t time=0.92s\n",
      "Best model: Epoch 17 \t loss=0.016824 \t val_loss=0.016663 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016804 \t val_loss=0.016587 \t time=0.92s\n",
      "Best model: Epoch 19 \t loss=0.016534 \t val_loss=0.016486 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016408 \t val_loss=0.016402 \t time=0.95s\n",
      "Best model: Epoch 21 \t loss=0.016242 \t val_loss=0.016329 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015940 \t val_loss=0.016305 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015832 \t val_loss=0.016269 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015716 \t val_loss=0.016267 \t time=1.02s\n",
      "Best model: Epoch 28 \t loss=0.015552 \t val_loss=0.016189 \t time=0.85s\n",
      "Best model: Epoch 30 \t loss=0.015313 \t val_loss=0.016186 \t time=0.93s\n",
      "Best model: Epoch 32 \t loss=0.015266 \t val_loss=0.016159 \t time=0.88s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014381 \t val_loss=0.015965 \t time=0.85s\n",
      "Best model: Epoch 38 \t loss=0.014108 \t val_loss=0.015929 \t time=1.08s\n",
      "Best model: Epoch 39 \t loss=0.013879 \t val_loss=0.015863 \t time=0.85s\n",
      "Best model: Epoch 40 \t loss=0.013707 \t val_loss=0.015843 \t time=1.05s\n",
      "Fold 4 log loss: 0.015902543921904353\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412864 \t val_loss=0.087957 \t time=1.05s\n",
      "Best model: Epoch 2 \t loss=0.048772 \t val_loss=0.027856 \t time=0.84s\n",
      "Best model: Epoch 3 \t loss=0.027113 \t val_loss=0.022899 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023015 \t val_loss=0.021126 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021396 \t val_loss=0.019662 \t time=0.85s\n",
      "Best model: Epoch 6 \t loss=0.020321 \t val_loss=0.019166 \t time=0.90s\n",
      "Best model: Epoch 7 \t loss=0.019645 \t val_loss=0.019059 \t time=1.03s\n",
      "Best model: Epoch 8 \t loss=0.019096 \t val_loss=0.018319 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.019205 \t val_loss=0.018115 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.018861 \t val_loss=0.018052 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018410 \t val_loss=0.017678 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.017853 \t val_loss=0.017470 \t time=1.41s\n",
      "Best model: Epoch 13 \t loss=0.017603 \t val_loss=0.017411 \t time=0.96s\n",
      "Best model: Epoch 14 \t loss=0.017363 \t val_loss=0.017164 \t time=1.02s\n",
      "Best model: Epoch 15 \t loss=0.017074 \t val_loss=0.017045 \t time=0.94s\n",
      "Best model: Epoch 16 \t loss=0.016916 \t val_loss=0.016974 \t time=1.19s\n",
      "Best model: Epoch 17 \t loss=0.016790 \t val_loss=0.016898 \t time=1.10s\n",
      "Best model: Epoch 19 \t loss=0.016481 \t val_loss=0.016703 \t time=0.95s\n",
      "Best model: Epoch 21 \t loss=0.016170 \t val_loss=0.016590 \t time=0.90s\n",
      "Best model: Epoch 22 \t loss=0.016051 \t val_loss=0.016585 \t time=0.95s\n",
      "Best model: Epoch 23 \t loss=0.015904 \t val_loss=0.016564 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015775 \t val_loss=0.016477 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015681 \t val_loss=0.016451 \t time=0.90s\n",
      "Best model: Epoch 27 \t loss=0.015522 \t val_loss=0.016442 \t time=0.85s\n",
      "Best model: Epoch 29 \t loss=0.015390 \t val_loss=0.016417 \t time=0.96s\n",
      "Best model: Epoch 32 \t loss=0.015276 \t val_loss=0.016414 \t time=0.84s\n",
      "Best model: Epoch 35 \t loss=0.015031 \t val_loss=0.016361 \t time=0.85s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014354 \t val_loss=0.016234 \t time=1.09s\n",
      "Fold 5 log loss: 0.016241952399532023\n",
      "Seed 10\n",
      "Fold 1 log loss: 0.016141570942390368\n",
      "Fold 2 log loss: 0.01603932333765505\n",
      "Fold 3 log loss: 0.015960235832428213\n",
      "Fold 4 log loss: 0.015902543921904353\n",
      "Fold 5 log loss: 0.016241952399532023\n",
      "Std of log loss: 0.00012238627740377263\n",
      "Total log loss: 0.01605711767674174\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.413920 \t val_loss=0.079477 \t time=1.02s\n",
      "Best model: Epoch 2 \t loss=0.049634 \t val_loss=0.027734 \t time=0.97s\n",
      "Best model: Epoch 3 \t loss=0.028061 \t val_loss=0.023670 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.023152 \t val_loss=0.020986 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021452 \t val_loss=0.019898 \t time=1.03s\n",
      "Best model: Epoch 6 \t loss=0.020336 \t val_loss=0.019212 \t time=1.06s\n",
      "Best model: Epoch 7 \t loss=0.020097 \t val_loss=0.019036 \t time=1.29s\n",
      "Best model: Epoch 8 \t loss=0.019327 \t val_loss=0.018670 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019245 \t val_loss=0.018547 \t time=0.94s\n",
      "Best model: Epoch 10 \t loss=0.018712 \t val_loss=0.017913 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018083 \t val_loss=0.017483 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017780 \t val_loss=0.017430 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017454 \t val_loss=0.017045 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017252 \t val_loss=0.017028 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.016963 \t val_loss=0.016861 \t time=1.10s\n",
      "Best model: Epoch 17 \t loss=0.016672 \t val_loss=0.016786 \t time=0.84s\n",
      "Best model: Epoch 18 \t loss=0.016583 \t val_loss=0.016650 \t time=0.86s\n",
      "Best model: Epoch 21 \t loss=0.016054 \t val_loss=0.016557 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.015948 \t val_loss=0.016449 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015596 \t val_loss=0.016420 \t time=0.84s\n",
      "Best model: Epoch 28 \t loss=0.015357 \t val_loss=0.016351 \t time=1.11s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014403 \t val_loss=0.016215 \t time=0.94s\n",
      "Best model: Epoch 34 \t loss=0.014136 \t val_loss=0.016120 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.013879 \t val_loss=0.016098 \t time=0.88s\n",
      "Best model: Epoch 37 \t loss=0.013592 \t val_loss=0.016094 \t time=0.86s\n",
      "Best model: Epoch 38 \t loss=0.013435 \t val_loss=0.016093 \t time=1.00s\n",
      "Best model: Epoch 39 \t loss=0.013362 \t val_loss=0.016078 \t time=1.11s\n",
      "Best model: Epoch 40 \t loss=0.013172 \t val_loss=0.016052 \t time=0.86s\n",
      "Fold 1 log loss: 0.01614261035670331\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.409370 \t val_loss=0.081099 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.049288 \t val_loss=0.028235 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027326 \t val_loss=0.022247 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023620 \t val_loss=0.021324 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021389 \t val_loss=0.019787 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020357 \t val_loss=0.019135 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.020074 \t val_loss=0.018860 \t time=1.22s\n",
      "Best model: Epoch 8 \t loss=0.019576 \t val_loss=0.018777 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019116 \t val_loss=0.018289 \t time=0.84s\n",
      "Best model: Epoch 10 \t loss=0.018792 \t val_loss=0.018015 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018205 \t val_loss=0.017641 \t time=0.98s\n",
      "Best model: Epoch 12 \t loss=0.018107 \t val_loss=0.017435 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.017692 \t val_loss=0.017306 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017477 \t val_loss=0.017077 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017124 \t val_loss=0.016972 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.016984 \t val_loss=0.016939 \t time=0.86s\n",
      "Best model: Epoch 17 \t loss=0.016682 \t val_loss=0.016869 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016655 \t val_loss=0.016739 \t time=0.84s\n",
      "Best model: Epoch 19 \t loss=0.016579 \t val_loss=0.016685 \t time=1.13s\n",
      "Best model: Epoch 20 \t loss=0.016349 \t val_loss=0.016609 \t time=0.92s\n",
      "Best model: Epoch 21 \t loss=0.016171 \t val_loss=0.016579 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016052 \t val_loss=0.016439 \t time=0.92s\n",
      "Best model: Epoch 23 \t loss=0.015901 \t val_loss=0.016388 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015682 \t val_loss=0.016334 \t time=0.86s\n",
      "Best model: Epoch 28 \t loss=0.015480 \t val_loss=0.016309 \t time=0.84s\n",
      "Best model: Epoch 29 \t loss=0.015435 \t val_loss=0.016305 \t time=0.89s\n",
      "Best model: Epoch 30 \t loss=0.015357 \t val_loss=0.016277 \t time=1.00s\n",
      "Best model: Epoch 32 \t loss=0.015297 \t val_loss=0.016266 \t time=0.85s\n",
      "Best model: Epoch 36 \t loss=0.015070 \t val_loss=0.016245 \t time=0.82s\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.01631063487577899\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413884 \t val_loss=0.070913 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.048988 \t val_loss=0.029495 \t time=0.84s\n",
      "Best model: Epoch 3 \t loss=0.027590 \t val_loss=0.022484 \t time=1.12s\n",
      "Best model: Epoch 4 \t loss=0.023234 \t val_loss=0.020634 \t time=1.19s\n",
      "Best model: Epoch 5 \t loss=0.021657 \t val_loss=0.020416 \t time=0.91s\n",
      "Best model: Epoch 6 \t loss=0.020444 \t val_loss=0.019397 \t time=0.93s\n",
      "Best model: Epoch 7 \t loss=0.020049 \t val_loss=0.018992 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019304 \t val_loss=0.018322 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019057 \t val_loss=0.018188 \t time=1.10s\n",
      "Best model: Epoch 10 \t loss=0.018827 \t val_loss=0.017887 \t time=1.17s\n",
      "Best model: Epoch 11 \t loss=0.018296 \t val_loss=0.017657 \t time=0.92s\n",
      "Best model: Epoch 12 \t loss=0.018080 \t val_loss=0.017641 \t time=0.92s\n",
      "Best model: Epoch 13 \t loss=0.017928 \t val_loss=0.017470 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017765 \t val_loss=0.017199 \t time=0.90s\n",
      "Best model: Epoch 15 \t loss=0.017354 \t val_loss=0.017031 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017125 \t val_loss=0.016839 \t time=0.85s\n",
      "Best model: Epoch 17 \t loss=0.016888 \t val_loss=0.016751 \t time=0.85s\n",
      "Best model: Epoch 18 \t loss=0.016704 \t val_loss=0.016697 \t time=0.88s\n",
      "Best model: Epoch 20 \t loss=0.016422 \t val_loss=0.016483 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016227 \t val_loss=0.016467 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016103 \t val_loss=0.016438 \t time=1.08s\n",
      "Best model: Epoch 23 \t loss=0.016074 \t val_loss=0.016404 \t time=0.84s\n",
      "Best model: Epoch 25 \t loss=0.015771 \t val_loss=0.016350 \t time=0.89s\n",
      "Best model: Epoch 26 \t loss=0.015717 \t val_loss=0.016332 \t time=0.85s\n",
      "Best model: Epoch 27 \t loss=0.015609 \t val_loss=0.016332 \t time=0.85s\n",
      "Best model: Epoch 28 \t loss=0.015547 \t val_loss=0.016254 \t time=0.92s\n",
      "Best model: Epoch 29 \t loss=0.015493 \t val_loss=0.016229 \t time=0.86s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014676 \t val_loss=0.016053 \t time=0.85s\n",
      "Best model: Epoch 35 \t loss=0.014345 \t val_loss=0.016014 \t time=0.94s\n",
      "Best model: Epoch 36 \t loss=0.014175 \t val_loss=0.015938 \t time=0.94s\n",
      "Best model: Epoch 37 \t loss=0.014029 \t val_loss=0.015934 \t time=0.85s\n",
      "Best model: Epoch 38 \t loss=0.013880 \t val_loss=0.015933 \t time=0.85s\n",
      "Best model: Epoch 39 \t loss=0.013741 \t val_loss=0.015913 \t time=0.84s\n",
      "Fold 3 log loss: 0.015913579228712407\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413014 \t val_loss=0.084329 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.049405 \t val_loss=0.030326 \t time=1.04s\n",
      "Best model: Epoch 3 \t loss=0.027348 \t val_loss=0.022542 \t time=0.95s\n",
      "Best model: Epoch 4 \t loss=0.023328 \t val_loss=0.020785 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021820 \t val_loss=0.019584 \t time=0.84s\n",
      "Best model: Epoch 6 \t loss=0.020560 \t val_loss=0.019177 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.019902 \t val_loss=0.018625 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019668 \t val_loss=0.018135 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018982 \t val_loss=0.017875 \t time=0.87s\n",
      "Best model: Epoch 10 \t loss=0.018587 \t val_loss=0.017646 \t time=1.02s\n",
      "Best model: Epoch 12 \t loss=0.018282 \t val_loss=0.017330 \t time=0.89s\n",
      "Best model: Epoch 13 \t loss=0.017830 \t val_loss=0.017166 \t time=0.96s\n",
      "Best model: Epoch 14 \t loss=0.017516 \t val_loss=0.016927 \t time=1.01s\n",
      "Best model: Epoch 16 \t loss=0.017208 \t val_loss=0.016898 \t time=1.35s\n",
      "Best model: Epoch 17 \t loss=0.016986 \t val_loss=0.016638 \t time=0.91s\n",
      "Best model: Epoch 18 \t loss=0.016777 \t val_loss=0.016570 \t time=0.91s\n",
      "Best model: Epoch 19 \t loss=0.016579 \t val_loss=0.016486 \t time=0.93s\n",
      "Best model: Epoch 21 \t loss=0.016237 \t val_loss=0.016353 \t time=0.84s\n",
      "Best model: Epoch 23 \t loss=0.016096 \t val_loss=0.016291 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.016001 \t val_loss=0.016288 \t time=0.99s\n",
      "Best model: Epoch 25 \t loss=0.015885 \t val_loss=0.016244 \t time=0.98s\n",
      "Best model: Epoch 29 \t loss=0.015437 \t val_loss=0.016149 \t time=0.90s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014637 \t val_loss=0.016012 \t time=1.01s\n",
      "Best model: Epoch 35 \t loss=0.014330 \t val_loss=0.015986 \t time=0.86s\n",
      "Best model: Epoch 36 \t loss=0.014058 \t val_loss=0.015934 \t time=1.10s\n",
      "Best model: Epoch 37 \t loss=0.014010 \t val_loss=0.015925 \t time=0.85s\n",
      "Best model: Epoch 39 \t loss=0.013704 \t val_loss=0.015902 \t time=0.84s\n",
      "Best model: Epoch 40 \t loss=0.013628 \t val_loss=0.015879 \t time=0.86s\n",
      "Fold 4 log loss: 0.015931359204532123\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412001 \t val_loss=0.075188 \t time=0.84s\n",
      "Best model: Epoch 2 \t loss=0.048280 \t val_loss=0.028979 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.027315 \t val_loss=0.023299 \t time=0.97s\n",
      "Best model: Epoch 4 \t loss=0.022898 \t val_loss=0.020631 \t time=0.87s\n",
      "Best model: Epoch 5 \t loss=0.022182 \t val_loss=0.019981 \t time=1.07s\n",
      "Best model: Epoch 6 \t loss=0.020717 \t val_loss=0.019386 \t time=1.12s\n",
      "Best model: Epoch 7 \t loss=0.019955 \t val_loss=0.018870 \t time=1.02s\n",
      "Best model: Epoch 8 \t loss=0.019295 \t val_loss=0.018439 \t time=0.86s\n",
      "Best model: Epoch 9 \t loss=0.019075 \t val_loss=0.018254 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.018743 \t val_loss=0.017978 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018283 \t val_loss=0.017763 \t time=0.90s\n",
      "Best model: Epoch 13 \t loss=0.017654 \t val_loss=0.017433 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017393 \t val_loss=0.017142 \t time=0.96s\n",
      "Best model: Epoch 16 \t loss=0.017061 \t val_loss=0.017059 \t time=1.11s\n",
      "Best model: Epoch 17 \t loss=0.016857 \t val_loss=0.016948 \t time=0.85s\n",
      "Best model: Epoch 18 \t loss=0.016629 \t val_loss=0.016836 \t time=0.84s\n",
      "Best model: Epoch 19 \t loss=0.016587 \t val_loss=0.016786 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016382 \t val_loss=0.016715 \t time=0.85s\n",
      "Best model: Epoch 22 \t loss=0.016050 \t val_loss=0.016531 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.015940 \t val_loss=0.016529 \t time=0.92s\n",
      "Best model: Epoch 26 \t loss=0.015693 \t val_loss=0.016443 \t time=0.91s\n",
      "Best model: Epoch 29 \t loss=0.015379 \t val_loss=0.016330 \t time=0.90s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014537 \t val_loss=0.016250 \t time=0.92s\n",
      "Best model: Epoch 35 \t loss=0.014197 \t val_loss=0.016198 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.014037 \t val_loss=0.016164 \t time=0.85s\n",
      "Best model: Epoch 37 \t loss=0.013867 \t val_loss=0.016148 \t time=0.89s\n",
      "Best model: Epoch 38 \t loss=0.013675 \t val_loss=0.016146 \t time=0.87s\n",
      "Best model: Epoch 39 \t loss=0.013553 \t val_loss=0.016100 \t time=0.92s\n",
      "Fold 5 log loss: 0.016104923651747562\n",
      "Seed 40\n",
      "Fold 1 log loss: 0.01614261035670331\n",
      "Fold 2 log loss: 0.01631063487577899\n",
      "Fold 3 log loss: 0.015913579228712407\n",
      "Fold 4 log loss: 0.015931359204532123\n",
      "Fold 5 log loss: 0.016104923651747562\n",
      "Std of log loss: 0.00014664359462141007\n",
      "Total log loss: 0.016080609876306953\n",
      "Total log loss in targets: 0.0159686812923064\n"
     ]
    }
   ],
   "source": [
    "seeds = [10,40]\n",
    "#seeds = [0,1,2,3]\n",
    "\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "nontarget_oof = np.zeros([len(fn_train),fn_nontargets.shape[1]])\n",
    "nontarget_pred = np.zeros([len(fn_test),fn_nontargets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],[])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))\n",
    "\n",
    "#for seed_ in seeds:\n",
    "#    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_nontargets, fn_test, seed_, fn_train.shape[1], fn_nontargets.shape[1],[])\n",
    "#    nontarget_oof += oof / len(seeds)\n",
    "#    nontarget_pred += pytorch_pred / len(seeds)\n",
    "#print(\"Total log loss in Non targets: {}\".format(mean_log_loss(oof_targets, nontarget_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:33:58.869039Z",
     "iopub.status.busy": "2020-10-13T07:33:58.868012Z",
     "iopub.status.idle": "2020-10-13T07:33:58.870285Z",
     "shell.execute_reply": "2020-10-13T07:33:58.869708Z"
    },
    "papermill": {
     "duration": 0.158766,
     "end_time": "2020-10-13T07:33:58.870394",
     "exception": false,
     "start_time": "2020-10-13T07:33:58.711628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#n_train = f_train.copy()\n",
    "#n_test = f_test.copy()\n",
    "\n",
    "#n_train[\"target_sum\"] = target_oof.sum(axis=1)\n",
    "#n_train[\"nontarget_sum\"] = nontarget_oof.sum(axis=1)\n",
    "#n_test[\"target_sum\"] = target_pred.sum(axis=1)\n",
    "#n_test.loc[noncons_test_index, \"target_sum\"] = 0\n",
    "#n_test[\"nontarget_sum\"] = nontarget_pred.sum(axis=1)\n",
    "#n_test.loc[noncons_test_index, \"nontarget_sum\"] = 0\n",
    "\n",
    "#print(np.sqrt(mean_squared_error(n_train[\"target_sum\"],targets[target_feats].sum(axis=1))))\n",
    "\n",
    "#n_train = n_train.to_numpy()\n",
    "#n_test = n_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:33:59.263037Z",
     "iopub.status.busy": "2020-10-13T07:33:59.261941Z",
     "iopub.status.idle": "2020-10-13T07:33:59.266790Z",
     "shell.execute_reply": "2020-10-13T07:33:59.267598Z"
    },
    "papermill": {
     "duration": 0.220304,
     "end_time": "2020-10-13T07:33:59.267807",
     "exception": false,
     "start_time": "2020-10-13T07:33:59.047503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#oof_final = np.zeros([len(n_train),fn_targets.shape[1]])\n",
    "#pred_final = np.zeros([len(n_test),fn_targets.shape[1]])\n",
    "\n",
    "#seeds = [10,40]\n",
    "#for seed_ in seeds:\n",
    "#    oof, oof_targets, pytorch_pred = modelling_torch(n_train, fn_targets, n_test, seed_, n_train.shape[1], fn_targets.shape[1], [])\n",
    "#    oof_final += oof / len(seeds)\n",
    "#    pred_final += pytorch_pred / len(seeds)\n",
    "#print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:33:59.712363Z",
     "iopub.status.busy": "2020-10-13T07:33:59.711287Z",
     "iopub.status.idle": "2020-10-13T07:34:06.562876Z",
     "shell.execute_reply": "2020-10-13T07:34:06.563467Z"
    },
    "papermill": {
     "duration": 7.079766,
     "end_time": "2020-10-13T07:34:06.563618",
     "exception": false,
     "start_time": "2020-10-13T07:33:59.483852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014717419039369392\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-13T07:34:06.879699Z",
     "iopub.status.busy": "2020-10-13T07:34:06.878689Z",
     "iopub.status.idle": "2020-10-13T07:34:09.582469Z",
     "shell.execute_reply": "2020-10-13T07:34:09.581458Z"
    },
    "papermill": {
     "duration": 2.860037,
     "end_time": "2020-10-13T07:34:09.582588",
     "exception": false,
     "start_time": "2020-10-13T07:34:06.722551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.147453,
     "end_time": "2020-10-13T07:34:09.876393",
     "exception": false,
     "start_time": "2020-10-13T07:34:09.728940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 468.92225,
   "end_time": "2020-10-13T07:34:11.194509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-13T07:26:22.272259",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
