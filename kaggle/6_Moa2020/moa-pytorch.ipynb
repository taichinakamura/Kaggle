{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018754,
     "end_time": "2020-10-14T12:50:39.674666",
     "exception": false,
     "start_time": "2020-10-14T12:50:39.655912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- cancel new features and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:39.719430Z",
     "iopub.status.busy": "2020-10-14T12:50:39.717823Z",
     "iopub.status.idle": "2020-10-14T12:50:48.214115Z",
     "shell.execute_reply": "2020-10-14T12:50:48.212971Z"
    },
    "papermill": {
     "duration": 8.522674,
     "end_time": "2020-10-14T12:50:48.214270",
     "exception": false,
     "start_time": "2020-10-14T12:50:39.691596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:48.259793Z",
     "iopub.status.busy": "2020-10-14T12:50:48.258609Z",
     "iopub.status.idle": "2020-10-14T12:50:54.704820Z",
     "shell.execute_reply": "2020-10-14T12:50:54.705439Z"
    },
    "papermill": {
     "duration": 6.473565,
     "end_time": "2020-10-14T12:50:54.705612",
     "exception": false,
     "start_time": "2020-10-14T12:50:48.232047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:54.768923Z",
     "iopub.status.busy": "2020-10-14T12:50:54.767432Z",
     "iopub.status.idle": "2020-10-14T12:50:54.774094Z",
     "shell.execute_reply": "2020-10-14T12:50:54.774809Z"
    },
    "papermill": {
     "duration": 0.043096,
     "end_time": "2020-10-14T12:50:54.775042",
     "exception": false,
     "start_time": "2020-10-14T12:50:54.731946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:54.850203Z",
     "iopub.status.busy": "2020-10-14T12:50:54.847628Z",
     "iopub.status.idle": "2020-10-14T12:50:54.998470Z",
     "shell.execute_reply": "2020-10-14T12:50:54.997499Z"
    },
    "papermill": {
     "duration": 0.194551,
     "end_time": "2020-10-14T12:50:54.998640",
     "exception": false,
     "start_time": "2020-10-14T12:50:54.804089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030917,
     "end_time": "2020-10-14T12:50:55.061731",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.030814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.147087Z",
     "iopub.status.busy": "2020-10-14T12:50:55.138294Z",
     "iopub.status.idle": "2020-10-14T12:50:55.543230Z",
     "shell.execute_reply": "2020-10-14T12:50:55.544494Z"
    },
    "papermill": {
     "duration": 0.452002,
     "end_time": "2020-10-14T12:50:55.544718",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.092716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.610046Z",
     "iopub.status.busy": "2020-10-14T12:50:55.609318Z",
     "iopub.status.idle": "2020-10-14T12:50:55.662258Z",
     "shell.execute_reply": "2020-10-14T12:50:55.663509Z"
    },
    "papermill": {
     "duration": 0.089059,
     "end_time": "2020-10-14T12:50:55.663743",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.574684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.711531Z",
     "iopub.status.busy": "2020-10-14T12:50:55.710609Z",
     "iopub.status.idle": "2020-10-14T12:50:55.758792Z",
     "shell.execute_reply": "2020-10-14T12:50:55.758104Z"
    },
    "papermill": {
     "duration": 0.072582,
     "end_time": "2020-10-14T12:50:55.758915",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.686333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n"
     ]
    }
   ],
   "source": [
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "#drop_list2 = list(nontarget_dists[(nontarget_dists.number>0) & (nontarget_dists.number<=6)][\"target\"].values)[:-1]\n",
    "#print(\"second drop\", len(drop_list2))\n",
    "#non_targets.drop(drop_list2, axis=1, inplace=True)\n",
    "#print(\"shape after 2nd drop:\", non_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.818128Z",
     "iopub.status.busy": "2020-10-14T12:50:55.817236Z",
     "iopub.status.idle": "2020-10-14T12:50:55.819684Z",
     "shell.execute_reply": "2020-10-14T12:50:55.818866Z"
    },
    "papermill": {
     "duration": 0.034921,
     "end_time": "2020-10-14T12:50:55.819823",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.784902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X = train[g_feats].copy().values\n",
    "#select = VarianceThreshold(threshold=1)\n",
    "#X_new = select.fit_transform(X)\n",
    "#drop_g_feats = list(np.array(g_feats)[select.get_support()==False])\n",
    "#len(drop_g_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02521,
     "end_time": "2020-10-14T12:50:55.870277",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.845067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.928954Z",
     "iopub.status.busy": "2020-10-14T12:50:55.928105Z",
     "iopub.status.idle": "2020-10-14T12:50:55.930277Z",
     "shell.execute_reply": "2020-10-14T12:50:55.929644Z"
    },
    "papermill": {
     "duration": 0.03428,
     "end_time": "2020-10-14T12:50:55.930388",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.896108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#num = 10\n",
    "#pca_c_cols = [\"pca-c\"+str(i+1) for i in range(num)]\n",
    "#pca = PCA(n_components=num)\n",
    "#tmp_train = pca.fit_transform(train[c_feats])\n",
    "#tmp_test = pca.transform(test[c_feats])\n",
    "#tmp_train = pd.DataFrame(tmp_train, columns=pca_c_cols)\n",
    "#tmp_test = pd.DataFrame(tmp_test, columns=pca_c_cols)\n",
    "\n",
    "#train = pd.concat([train, tmp_train],axis=1)\n",
    "#test = pd.concat([test, tmp_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:55.991394Z",
     "iopub.status.busy": "2020-10-14T12:50:55.990563Z",
     "iopub.status.idle": "2020-10-14T12:50:56.169537Z",
     "shell.execute_reply": "2020-10-14T12:50:56.168782Z"
    },
    "papermill": {
     "duration": 0.21322,
     "end_time": "2020-10-14T12:50:56.169705",
     "exception": false,
     "start_time": "2020-10-14T12:50:55.956485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 872) (3982, 872)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    #tmp['g_sum'] = tmp[g_feats].sum(axis = 1)\n",
    "    #tmp['g_mean'] = tmp[g_feats].mean(axis = 1)\n",
    "    #tmp['g_std'] = tmp[g_feats].std(axis = 1)\n",
    "    #tmp['g_kurt'] = tmp[g_feats].kurtosis(axis = 1)\n",
    "    #tmp['g_skew'] = tmp[g_feats].skew(axis = 1)\n",
    "    #tmp['c_sum'] = tmp[c_feats].sum(axis = 1)\n",
    "    #tmp['c_mean'] = tmp[c_feats].mean(axis = 1)\n",
    "    #tmp['c_std'] = tmp[c_feats].std(axis = 1)\n",
    "    #tmp['c_kurt'] = tmp[c_feats].kurtosis(axis = 1)\n",
    "    #tmp['c_skew'] = tmp[c_feats].skew(axis = 1)\n",
    "    #tmp['gc_sum'] = tmp[c_feats + g_feats].sum(axis = 1)\n",
    "    #tmp['gc_mean'] = tmp[c_feats + g_feats].mean(axis = 1)\n",
    "    #tmp['gc_std'] = tmp[c_feats + g_feats].std(axis = 1)\n",
    "    #tmp['gc_kurt'] = tmp[c_feats + g_feats].kurtosis(axis = 1)\n",
    "    #tmp['gc_skew'] = tmp[c_feats + g_feats].skew(axis = 1)\n",
    "    tmp.loc[:, 'cp_type'] = tmp.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\", \"cp_time\", \"cp_dose\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019491,
     "end_time": "2020-10-14T12:50:56.210378",
     "exception": false,
     "start_time": "2020-10-14T12:50:56.190887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:56.668975Z",
     "iopub.status.busy": "2020-10-14T12:50:56.268493Z",
     "iopub.status.idle": "2020-10-14T12:50:56.674286Z",
     "shell.execute_reply": "2020-10-14T12:50:56.673126Z"
    },
    "papermill": {
     "duration": 0.444732,
     "end_time": "2020-10-14T12:50:56.674411",
     "exception": false,
     "start_time": "2020-10-14T12:50:56.229679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048,1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_num))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03013,
     "end_time": "2020-10-14T12:50:56.728052",
     "exception": false,
     "start_time": "2020-10-14T12:50:56.697922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# predict non-targets, targets separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:56.816961Z",
     "iopub.status.busy": "2020-10-14T12:50:56.815103Z",
     "iopub.status.idle": "2020-10-14T12:50:56.882647Z",
     "shell.execute_reply": "2020-10-14T12:50:56.883425Z"
    },
    "papermill": {
     "duration": 0.115771,
     "end_time": "2020-10-14T12:50:56.883658",
     "exception": false,
     "start_time": "2020-10-14T12:50:56.767887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 5\n",
    "n_folds=5\n",
    "\n",
    "def first_learning(tr, target, sample_seed, init_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    files = []\n",
    "        \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'parameters'+str(fold+1)+'.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        files.append('parameters'+str(fold+1)+'.pt')\n",
    "        pred_model = MoaModel(init_num)\n",
    "        pred_model.load_state_dict(torch.load('parameters'+str(fold+1)+'.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:57.022504Z",
     "iopub.status.busy": "2020-10-14T12:50:57.021571Z",
     "iopub.status.idle": "2020-10-14T12:50:58.424118Z",
     "shell.execute_reply": "2020-10-14T12:50:58.422978Z"
    },
    "papermill": {
     "duration": 1.47451,
     "end_time": "2020-10-14T12:50:58.424261",
     "exception": false,
     "start_time": "2020-10-14T12:50:56.949751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "ss = preprocessing.RobustScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "\n",
    "#seeds = [0]\n",
    "#for seed_ in seeds:\n",
    "#    files = first_learning(fn_train, fn_nontargets, seed_, fn_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021193,
     "end_time": "2020-10-14T12:50:58.466834",
     "exception": false,
     "start_time": "2020-10-14T12:50:58.445641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train by targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:58.548044Z",
     "iopub.status.busy": "2020-10-14T12:50:58.537657Z",
     "iopub.status.idle": "2020-10-14T12:50:58.557559Z",
     "shell.execute_reply": "2020-10-14T12:50:58.556975Z"
    },
    "papermill": {
     "duration": 0.070166,
     "end_time": "2020-10-14T12:50:58.557679",
     "exception": false,
     "start_time": "2020-10-14T12:50:58.487513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 50\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, files):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        if files != []:\n",
    "            clf.load_state_dict(torch.load(files[fold]))\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:50:58.612483Z",
     "iopub.status.busy": "2020-10-14T12:50:58.611089Z",
     "iopub.status.idle": "2020-10-14T12:58:54.432605Z",
     "shell.execute_reply": "2020-10-14T12:58:54.432008Z"
    },
    "papermill": {
     "duration": 475.853338,
     "end_time": "2020-10-14T12:58:54.432722",
     "exception": false,
     "start_time": "2020-10-14T12:50:58.579384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.410860 \t val_loss=0.081175 \t time=1.57s\n",
      "Best model: Epoch 2 \t loss=0.048698 \t val_loss=0.028190 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.027393 \t val_loss=0.023001 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.023080 \t val_loss=0.021055 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021607 \t val_loss=0.019910 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.020432 \t val_loss=0.019416 \t time=1.05s\n",
      "Best model: Epoch 7 \t loss=0.019751 \t val_loss=0.018891 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019508 \t val_loss=0.018673 \t time=1.11s\n",
      "Best model: Epoch 9 \t loss=0.019088 \t val_loss=0.018163 \t time=1.33s\n",
      "Best model: Epoch 10 \t loss=0.018533 \t val_loss=0.018048 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018360 \t val_loss=0.017672 \t time=1.15s\n",
      "Best model: Epoch 12 \t loss=0.017916 \t val_loss=0.017481 \t time=0.81s\n",
      "Best model: Epoch 14 \t loss=0.017453 \t val_loss=0.017182 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017216 \t val_loss=0.016971 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.016933 \t val_loss=0.016862 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.016736 \t val_loss=0.016833 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.016330 \t val_loss=0.016699 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.016160 \t val_loss=0.016545 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.015707 \t val_loss=0.016469 \t time=1.13s\n",
      "Best model: Epoch 26 \t loss=0.015588 \t val_loss=0.016444 \t time=0.88s\n",
      "Best model: Epoch 28 \t loss=0.015417 \t val_loss=0.016418 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015297 \t val_loss=0.016374 \t time=0.78s\n",
      "Best model: Epoch 33 \t loss=0.015075 \t val_loss=0.016335 \t time=0.80s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.014311 \t val_loss=0.016216 \t time=0.82s\n",
      "Best model: Epoch 39 \t loss=0.014007 \t val_loss=0.016196 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.013810 \t val_loss=0.016161 \t time=0.80s\n",
      "Best model: Epoch 41 \t loss=0.013650 \t val_loss=0.016110 \t time=0.78s\n",
      "Best model: Epoch 44 \t loss=0.013272 \t val_loss=0.016080 \t time=0.77s\n",
      "Fold 1 log loss: 0.016169478836795013\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.415448 \t val_loss=0.078916 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.049873 \t val_loss=0.031528 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.027313 \t val_loss=0.022268 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.023018 \t val_loss=0.020669 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021932 \t val_loss=0.020024 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020490 \t val_loss=0.019276 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019965 \t val_loss=0.018809 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019450 \t val_loss=0.018341 \t time=1.16s\n",
      "Best model: Epoch 9 \t loss=0.019017 \t val_loss=0.018056 \t time=1.14s\n",
      "Best model: Epoch 11 \t loss=0.018325 \t val_loss=0.017686 \t time=0.77s\n",
      "Best model: Epoch 12 \t loss=0.017950 \t val_loss=0.017345 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.017665 \t val_loss=0.017293 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.017349 \t val_loss=0.017012 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.017110 \t val_loss=0.016990 \t time=0.78s\n",
      "Best model: Epoch 16 \t loss=0.016941 \t val_loss=0.016853 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016740 \t val_loss=0.016735 \t time=0.77s\n",
      "Best model: Epoch 20 \t loss=0.016247 \t val_loss=0.016473 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.015997 \t val_loss=0.016401 \t time=0.79s\n",
      "Best model: Epoch 25 \t loss=0.015824 \t val_loss=0.016399 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015652 \t val_loss=0.016341 \t time=1.06s\n",
      "Best model: Epoch 28 \t loss=0.015518 \t val_loss=0.016317 \t time=0.80s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014603 \t val_loss=0.016123 \t time=0.85s\n",
      "Best model: Epoch 34 \t loss=0.014334 \t val_loss=0.016039 \t time=1.01s\n",
      "Best model: Epoch 35 \t loss=0.014058 \t val_loss=0.016031 \t time=0.79s\n",
      "Best model: Epoch 36 \t loss=0.014000 \t val_loss=0.016013 \t time=0.78s\n",
      "Best model: Epoch 38 \t loss=0.013667 \t val_loss=0.016001 \t time=0.80s\n",
      "Best model: Epoch 39 \t loss=0.013535 \t val_loss=0.015990 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.013346 \t val_loss=0.015987 \t time=0.80s\n",
      "Fold 2 log loss: 0.016045061265960503\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411601 \t val_loss=0.075153 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.049082 \t val_loss=0.031598 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.027593 \t val_loss=0.022326 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023113 \t val_loss=0.020635 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021903 \t val_loss=0.019966 \t time=1.04s\n",
      "Best model: Epoch 6 \t loss=0.020588 \t val_loss=0.019384 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.019956 \t val_loss=0.018927 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.019376 \t val_loss=0.018660 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.019264 \t val_loss=0.018136 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018615 \t val_loss=0.017979 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018313 \t val_loss=0.017712 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.018021 \t val_loss=0.017399 \t time=0.80s\n",
      "Best model: Epoch 13 \t loss=0.017667 \t val_loss=0.017242 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.017309 \t val_loss=0.016896 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.016982 \t val_loss=0.016807 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016691 \t val_loss=0.016685 \t time=1.06s\n",
      "Best model: Epoch 19 \t loss=0.016544 \t val_loss=0.016621 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.016256 \t val_loss=0.016504 \t time=0.77s\n",
      "Best model: Epoch 22 \t loss=0.015999 \t val_loss=0.016400 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.016022 \t val_loss=0.016295 \t time=0.78s\n",
      "Best model: Epoch 25 \t loss=0.015853 \t val_loss=0.016281 \t time=0.80s\n",
      "Best model: Epoch 28 \t loss=0.015577 \t val_loss=0.016281 \t time=0.78s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.015490 \t val_loss=0.016281 \t time=0.79s\n",
      "Best model: Epoch 30 \t loss=0.014771 \t val_loss=0.016121 \t time=1.25s\n",
      "Best model: Epoch 31 \t loss=0.014657 \t val_loss=0.016046 \t time=1.01s\n",
      "Best model: Epoch 32 \t loss=0.014421 \t val_loss=0.016018 \t time=0.78s\n",
      "Best model: Epoch 33 \t loss=0.014237 \t val_loss=0.015965 \t time=0.77s\n",
      "Best model: Epoch 34 \t loss=0.014123 \t val_loss=0.015954 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.013829 \t val_loss=0.015914 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.013790 \t val_loss=0.015906 \t time=0.81s\n",
      "Best model: Epoch 40 \t loss=0.013481 \t val_loss=0.015899 \t time=1.21s\n",
      "Fold 3 log loss: 0.015893959772506697\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413161 \t val_loss=0.073438 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.048870 \t val_loss=0.027741 \t time=0.84s\n",
      "Best model: Epoch 3 \t loss=0.027174 \t val_loss=0.023075 \t time=1.08s\n",
      "Best model: Epoch 4 \t loss=0.023005 \t val_loss=0.020925 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021418 \t val_loss=0.019678 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020624 \t val_loss=0.018818 \t time=0.85s\n",
      "Best model: Epoch 7 \t loss=0.020001 \t val_loss=0.018550 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.019976 \t val_loss=0.018242 \t time=0.77s\n",
      "Best model: Epoch 9 \t loss=0.019022 \t val_loss=0.017918 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.018464 \t val_loss=0.017685 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018102 \t val_loss=0.017325 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.018000 \t val_loss=0.017206 \t time=0.78s\n",
      "Best model: Epoch 14 \t loss=0.017322 \t val_loss=0.016879 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.017282 \t val_loss=0.016853 \t time=1.07s\n",
      "Best model: Epoch 16 \t loss=0.016996 \t val_loss=0.016703 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.016865 \t val_loss=0.016645 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.016639 \t val_loss=0.016563 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.016324 \t val_loss=0.016394 \t time=0.77s\n",
      "Best model: Epoch 21 \t loss=0.016215 \t val_loss=0.016392 \t time=0.78s\n",
      "Best model: Epoch 22 \t loss=0.016115 \t val_loss=0.016347 \t time=0.80s\n",
      "Best model: Epoch 23 \t loss=0.015944 \t val_loss=0.016237 \t time=0.84s\n",
      "Best model: Epoch 26 \t loss=0.015717 \t val_loss=0.016219 \t time=0.78s\n",
      "Best model: Epoch 27 \t loss=0.015635 \t val_loss=0.016189 \t time=0.78s\n",
      "Best model: Epoch 30 \t loss=0.015343 \t val_loss=0.016178 \t time=0.78s\n",
      "Best model: Epoch 32 \t loss=0.015304 \t val_loss=0.016178 \t time=0.77s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014631 \t val_loss=0.016030 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.014280 \t val_loss=0.015997 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.014175 \t val_loss=0.015941 \t time=0.78s\n",
      "Best model: Epoch 38 \t loss=0.013886 \t val_loss=0.015883 \t time=0.79s\n",
      "Fold 4 log loss: 0.01594277472662853\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.416550 \t val_loss=0.075950 \t time=0.94s\n",
      "Best model: Epoch 2 \t loss=0.049452 \t val_loss=0.029984 \t time=0.95s\n",
      "Best model: Epoch 3 \t loss=0.027384 \t val_loss=0.022589 \t time=1.27s\n",
      "Best model: Epoch 4 \t loss=0.023126 \t val_loss=0.021320 \t time=0.84s\n",
      "Best model: Epoch 5 \t loss=0.021771 \t val_loss=0.020214 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020643 \t val_loss=0.019674 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.020063 \t val_loss=0.019255 \t time=1.15s\n",
      "Best model: Epoch 8 \t loss=0.019602 \t val_loss=0.018552 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.019036 \t val_loss=0.018282 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018634 \t val_loss=0.017848 \t time=0.91s\n",
      "Best model: Epoch 11 \t loss=0.018247 \t val_loss=0.017764 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.018152 \t val_loss=0.017745 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017853 \t val_loss=0.017516 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017598 \t val_loss=0.017368 \t time=1.09s\n",
      "Best model: Epoch 15 \t loss=0.017300 \t val_loss=0.017068 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.016982 \t val_loss=0.016951 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016932 \t val_loss=0.016813 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016712 \t val_loss=0.016792 \t time=0.84s\n",
      "Best model: Epoch 20 \t loss=0.016214 \t val_loss=0.016608 \t time=0.78s\n",
      "Best model: Epoch 22 \t loss=0.015949 \t val_loss=0.016547 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.015801 \t val_loss=0.016474 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.015738 \t val_loss=0.016456 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015604 \t val_loss=0.016415 \t time=0.96s\n",
      "Best model: Epoch 28 \t loss=0.015453 \t val_loss=0.016413 \t time=0.78s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014566 \t val_loss=0.016260 \t time=0.78s\n",
      "Best model: Epoch 34 \t loss=0.014249 \t val_loss=0.016231 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.013885 \t val_loss=0.016159 \t time=0.84s\n",
      "Best model: Epoch 39 \t loss=0.013468 \t val_loss=0.016113 \t time=0.99s\n",
      "Fold 5 log loss: 0.016117812020693952\n",
      "Seed 10\n",
      "Fold 1 log loss: 0.016169478836795013\n",
      "Fold 2 log loss: 0.016045061265960503\n",
      "Fold 3 log loss: 0.015893959772506697\n",
      "Fold 4 log loss: 0.01594277472662853\n",
      "Fold 5 log loss: 0.016117812020693952\n",
      "Std of log loss: 0.00010337743525856783\n",
      "Total log loss: 0.016033812985231464\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.414008 \t val_loss=0.080886 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.048860 \t val_loss=0.027752 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.027668 \t val_loss=0.024784 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.023275 \t val_loss=0.021127 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021621 \t val_loss=0.020165 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.020490 \t val_loss=0.019480 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.019805 \t val_loss=0.019013 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019422 \t val_loss=0.018700 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.019255 \t val_loss=0.018436 \t time=1.00s\n",
      "Best model: Epoch 10 \t loss=0.018735 \t val_loss=0.018112 \t time=0.78s\n",
      "Best model: Epoch 11 \t loss=0.018275 \t val_loss=0.017599 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.017885 \t val_loss=0.017513 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.017708 \t val_loss=0.017380 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.017497 \t val_loss=0.017302 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017177 \t val_loss=0.016990 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.016969 \t val_loss=0.016930 \t time=1.74s\n",
      "Best model: Epoch 18 \t loss=0.016679 \t val_loss=0.016721 \t time=0.95s\n",
      "Best model: Epoch 19 \t loss=0.016424 \t val_loss=0.016680 \t time=0.86s\n",
      "Best model: Epoch 20 \t loss=0.016195 \t val_loss=0.016507 \t time=0.83s\n",
      "Best model: Epoch 22 \t loss=0.016152 \t val_loss=0.016503 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.015913 \t val_loss=0.016494 \t time=0.85s\n",
      "Best model: Epoch 24 \t loss=0.015716 \t val_loss=0.016449 \t time=0.82s\n",
      "Best model: Epoch 25 \t loss=0.015575 \t val_loss=0.016427 \t time=0.89s\n",
      "Best model: Epoch 26 \t loss=0.015555 \t val_loss=0.016417 \t time=0.79s\n",
      "Best model: Epoch 27 \t loss=0.015400 \t val_loss=0.016396 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015272 \t val_loss=0.016332 \t time=0.78s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014484 \t val_loss=0.016171 \t time=0.85s\n",
      "Best model: Epoch 35 \t loss=0.014129 \t val_loss=0.016153 \t time=0.84s\n",
      "Best model: Epoch 36 \t loss=0.013995 \t val_loss=0.016087 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.013772 \t val_loss=0.016059 \t time=0.80s\n",
      "Best model: Epoch 40 \t loss=0.013360 \t val_loss=0.016055 \t time=0.79s\n",
      "Best model: Epoch 42 \t loss=0.013099 \t val_loss=0.016046 \t time=0.79s\n",
      "Best model: Epoch 46 \t loss=0.012640 \t val_loss=0.016045 \t time=0.92s\n",
      "Fold 1 log loss: 0.01614321234983017\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.413675 \t val_loss=0.081676 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.048776 \t val_loss=0.031316 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.027764 \t val_loss=0.023243 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.022987 \t val_loss=0.020818 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021494 \t val_loss=0.019854 \t time=1.03s\n",
      "Best model: Epoch 6 \t loss=0.020525 \t val_loss=0.019545 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.019882 \t val_loss=0.018896 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019340 \t val_loss=0.018505 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019095 \t val_loss=0.018228 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018637 \t val_loss=0.018049 \t time=0.78s\n",
      "Best model: Epoch 11 \t loss=0.018315 \t val_loss=0.017714 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.017988 \t val_loss=0.017472 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017778 \t val_loss=0.017282 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017430 \t val_loss=0.017156 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.017297 \t val_loss=0.017044 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.017072 \t val_loss=0.016836 \t time=0.83s\n",
      "Best model: Epoch 17 \t loss=0.016825 \t val_loss=0.016730 \t time=0.78s\n",
      "Best model: Epoch 18 \t loss=0.016607 \t val_loss=0.016692 \t time=1.04s\n",
      "Best model: Epoch 20 \t loss=0.016326 \t val_loss=0.016553 \t time=0.77s\n",
      "Best model: Epoch 21 \t loss=0.016216 \t val_loss=0.016530 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016067 \t val_loss=0.016436 \t time=0.83s\n",
      "Best model: Epoch 24 \t loss=0.015833 \t val_loss=0.016329 \t time=0.83s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014881 \t val_loss=0.016155 \t time=0.80s\n",
      "Best model: Epoch 30 \t loss=0.014650 \t val_loss=0.016095 \t time=0.80s\n",
      "Best model: Epoch 31 \t loss=0.014375 \t val_loss=0.016056 \t time=1.33s\n",
      "Best model: Epoch 33 \t loss=0.014126 \t val_loss=0.016017 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.013260 \t val_loss=0.016016 \t time=0.81s\n",
      "Best model: Epoch 41 \t loss=0.013155 \t val_loss=0.016012 \t time=0.83s\n",
      "Best model: Epoch 43 \t loss=0.012970 \t val_loss=0.016003 \t time=0.86s\n",
      "Fold 2 log loss: 0.01606280617715384\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.412526 \t val_loss=0.078087 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.048938 \t val_loss=0.028640 \t time=1.04s\n",
      "Best model: Epoch 3 \t loss=0.027474 \t val_loss=0.023018 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023100 \t val_loss=0.020718 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021776 \t val_loss=0.019837 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020836 \t val_loss=0.019726 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.020046 \t val_loss=0.018811 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019427 \t val_loss=0.018586 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.019035 \t val_loss=0.018276 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018711 \t val_loss=0.017743 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.018433 \t val_loss=0.017618 \t time=0.80s\n",
      "Best model: Epoch 12 \t loss=0.018093 \t val_loss=0.017559 \t time=0.80s\n",
      "Best model: Epoch 13 \t loss=0.017761 \t val_loss=0.017369 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017556 \t val_loss=0.017136 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.017387 \t val_loss=0.016991 \t time=1.04s\n",
      "Best model: Epoch 16 \t loss=0.017152 \t val_loss=0.016827 \t time=0.84s\n",
      "Best model: Epoch 18 \t loss=0.016841 \t val_loss=0.016680 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016593 \t val_loss=0.016607 \t time=1.08s\n",
      "Best model: Epoch 20 \t loss=0.016469 \t val_loss=0.016461 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016113 \t val_loss=0.016358 \t time=0.81s\n",
      "Best model: Epoch 26 \t loss=0.015802 \t val_loss=0.016347 \t time=0.79s\n",
      "Best model: Epoch 27 \t loss=0.015618 \t val_loss=0.016287 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.015525 \t val_loss=0.016257 \t time=1.01s\n",
      "Best model: Epoch 29 \t loss=0.015507 \t val_loss=0.016228 \t time=0.80s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014608 \t val_loss=0.016036 \t time=0.80s\n",
      "Best model: Epoch 35 \t loss=0.014289 \t val_loss=0.015973 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.014172 \t val_loss=0.015970 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.013978 \t val_loss=0.015951 \t time=0.79s\n",
      "Best model: Epoch 40 \t loss=0.013549 \t val_loss=0.015931 \t time=0.82s\n",
      "Best model: Epoch 43 \t loss=0.013178 \t val_loss=0.015928 \t time=0.78s\n",
      "Best model: Epoch 44 \t loss=0.013096 \t val_loss=0.015910 \t time=0.78s\n",
      "Best model: Epoch 45 \t loss=0.013009 \t val_loss=0.015902 \t time=0.78s\n",
      "Fold 3 log loss: 0.01589315962637137\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.412761 \t val_loss=0.071816 \t time=1.36s\n",
      "Best model: Epoch 2 \t loss=0.050027 \t val_loss=0.029397 \t time=1.05s\n",
      "Best model: Epoch 3 \t loss=0.027728 \t val_loss=0.022791 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.023361 \t val_loss=0.021245 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.021502 \t val_loss=0.019722 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020643 \t val_loss=0.019165 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019890 \t val_loss=0.018621 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.019474 \t val_loss=0.018247 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.019128 \t val_loss=0.018147 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018689 \t val_loss=0.017670 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.018253 \t val_loss=0.017624 \t time=0.80s\n",
      "Best model: Epoch 12 \t loss=0.018089 \t val_loss=0.017230 \t time=1.05s\n",
      "Best model: Epoch 13 \t loss=0.017721 \t val_loss=0.017174 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.017431 \t val_loss=0.016913 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.017200 \t val_loss=0.016883 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.016957 \t val_loss=0.016707 \t time=0.79s\n",
      "Best model: Epoch 17 \t loss=0.016884 \t val_loss=0.016671 \t time=0.78s\n",
      "Best model: Epoch 18 \t loss=0.016650 \t val_loss=0.016595 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.016475 \t val_loss=0.016518 \t time=0.78s\n",
      "Best model: Epoch 20 \t loss=0.016360 \t val_loss=0.016352 \t time=0.80s\n",
      "Best model: Epoch 23 \t loss=0.015986 \t val_loss=0.016276 \t time=0.79s\n",
      "Best model: Epoch 25 \t loss=0.015708 \t val_loss=0.016215 \t time=1.01s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 30 \t loss=0.014806 \t val_loss=0.016027 \t time=0.77s\n",
      "Best model: Epoch 31 \t loss=0.014531 \t val_loss=0.015971 \t time=0.78s\n",
      "Best model: Epoch 32 \t loss=0.014415 \t val_loss=0.015916 \t time=0.78s\n",
      "Best model: Epoch 35 \t loss=0.013963 \t val_loss=0.015911 \t time=0.76s\n",
      "Best model: Epoch 36 \t loss=0.013870 \t val_loss=0.015904 \t time=0.85s\n",
      "Best model: Epoch 39 \t loss=0.013530 \t val_loss=0.015901 \t time=0.97s\n",
      "Fold 4 log loss: 0.01595994681311008\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413926 \t val_loss=0.081732 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.048790 \t val_loss=0.028003 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027321 \t val_loss=0.022917 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.023031 \t val_loss=0.021098 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.021741 \t val_loss=0.020612 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.020504 \t val_loss=0.019149 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.020016 \t val_loss=0.019024 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019410 \t val_loss=0.018558 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.019071 \t val_loss=0.018428 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018543 \t val_loss=0.017881 \t time=0.98s\n",
      "Best model: Epoch 11 \t loss=0.018197 \t val_loss=0.017644 \t time=1.44s\n",
      "Best model: Epoch 12 \t loss=0.017914 \t val_loss=0.017573 \t time=1.14s\n",
      "Best model: Epoch 13 \t loss=0.017814 \t val_loss=0.017277 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017460 \t val_loss=0.017205 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017245 \t val_loss=0.017097 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017092 \t val_loss=0.016966 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.016794 \t val_loss=0.016857 \t time=0.93s\n",
      "Best model: Epoch 18 \t loss=0.016619 \t val_loss=0.016778 \t time=0.78s\n",
      "Best model: Epoch 19 \t loss=0.016459 \t val_loss=0.016730 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.016304 \t val_loss=0.016623 \t time=0.84s\n",
      "Best model: Epoch 21 \t loss=0.016098 \t val_loss=0.016570 \t time=0.83s\n",
      "Best model: Epoch 23 \t loss=0.016052 \t val_loss=0.016541 \t time=0.99s\n",
      "Best model: Epoch 25 \t loss=0.015712 \t val_loss=0.016483 \t time=0.79s\n",
      "Best model: Epoch 26 \t loss=0.015542 \t val_loss=0.016463 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.015449 \t val_loss=0.016417 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015440 \t val_loss=0.016324 \t time=0.79s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014476 \t val_loss=0.016205 \t time=0.81s\n",
      "Best model: Epoch 35 \t loss=0.014239 \t val_loss=0.016177 \t time=1.02s\n",
      "Best model: Epoch 36 \t loss=0.013967 \t val_loss=0.016147 \t time=0.79s\n",
      "Best model: Epoch 38 \t loss=0.013717 \t val_loss=0.016120 \t time=0.78s\n",
      "Best model: Epoch 39 \t loss=0.013500 \t val_loss=0.016108 \t time=0.78s\n",
      "Best model: Epoch 40 \t loss=0.013433 \t val_loss=0.016105 \t time=0.79s\n",
      "Fold 5 log loss: 0.01611417481331195\n",
      "Seed 40\n",
      "Fold 1 log loss: 0.01614321234983017\n",
      "Fold 2 log loss: 0.01606280617715384\n",
      "Fold 3 log loss: 0.01589315962637137\n",
      "Fold 4 log loss: 0.01595994681311008\n",
      "Fold 5 log loss: 0.01611417481331195\n",
      "Std of log loss: 9.434282326742407e-05\n",
      "Total log loss: 0.016034655050675792\n",
      "Total log loss in targets: 0.01594675087988284\n"
     ]
    }
   ],
   "source": [
    "seeds = [10,40]\n",
    "#seeds = [0,1,2,3]\n",
    "\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "nontarget_oof = np.zeros([len(fn_train),fn_nontargets.shape[1]])\n",
    "nontarget_pred = np.zeros([len(fn_test),fn_nontargets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],[])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))\n",
    "\n",
    "#for seed_ in seeds:\n",
    "#    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_nontargets, fn_test, seed_, fn_train.shape[1], fn_nontargets.shape[1],[])\n",
    "#    nontarget_oof += oof / len(seeds)\n",
    "#    nontarget_pred += pytorch_pred / len(seeds)\n",
    "#print(\"Total log loss in Non targets: {}\".format(mean_log_loss(oof_targets, nontarget_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:58:54.715588Z",
     "iopub.status.busy": "2020-10-14T12:58:54.713675Z",
     "iopub.status.idle": "2020-10-14T12:58:54.716425Z",
     "shell.execute_reply": "2020-10-14T12:58:54.716931Z"
    },
    "papermill": {
     "duration": 0.144123,
     "end_time": "2020-10-14T12:58:54.717074",
     "exception": false,
     "start_time": "2020-10-14T12:58:54.572951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#n_train = f_train.copy()\n",
    "#n_test = f_test.copy()\n",
    "\n",
    "#n_train[\"target_sum\"] = target_oof.sum(axis=1)\n",
    "#n_train[\"nontarget_sum\"] = nontarget_oof.sum(axis=1)\n",
    "#n_test[\"target_sum\"] = target_pred.sum(axis=1)\n",
    "#n_test.loc[noncons_test_index, \"target_sum\"] = 0\n",
    "#n_test[\"nontarget_sum\"] = nontarget_pred.sum(axis=1)\n",
    "#n_test.loc[noncons_test_index, \"nontarget_sum\"] = 0\n",
    "\n",
    "#print(np.sqrt(mean_squared_error(n_train[\"target_sum\"],targets[target_feats].sum(axis=1))))\n",
    "\n",
    "#n_train = n_train.to_numpy()\n",
    "#n_test = n_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:58:54.994424Z",
     "iopub.status.busy": "2020-10-14T12:58:54.993355Z",
     "iopub.status.idle": "2020-10-14T12:58:54.995956Z",
     "shell.execute_reply": "2020-10-14T12:58:54.996565Z"
    },
    "papermill": {
     "duration": 0.144399,
     "end_time": "2020-10-14T12:58:54.996749",
     "exception": false,
     "start_time": "2020-10-14T12:58:54.852350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#oof_final = np.zeros([len(n_train),fn_targets.shape[1]])\n",
    "#pred_final = np.zeros([len(n_test),fn_targets.shape[1]])\n",
    "\n",
    "#seeds = [10,40]\n",
    "#for seed_ in seeds:\n",
    "#    oof, oof_targets, pytorch_pred = modelling_torch(n_train, fn_targets, n_test, seed_, n_train.shape[1], fn_targets.shape[1], [])\n",
    "#    oof_final += oof / len(seeds)\n",
    "#    pred_final += pytorch_pred / len(seeds)\n",
    "#print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:58:55.287735Z",
     "iopub.status.busy": "2020-10-14T12:58:55.286854Z",
     "iopub.status.idle": "2020-10-14T12:59:01.985687Z",
     "shell.execute_reply": "2020-10-14T12:59:01.985065Z"
    },
    "papermill": {
     "duration": 6.848683,
     "end_time": "2020-10-14T12:59:01.985821",
     "exception": false,
     "start_time": "2020-10-14T12:58:55.137138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014697207034167738\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T12:59:02.273774Z",
     "iopub.status.busy": "2020-10-14T12:59:02.272804Z",
     "iopub.status.idle": "2020-10-14T12:59:04.895640Z",
     "shell.execute_reply": "2020-10-14T12:59:04.894461Z"
    },
    "papermill": {
     "duration": 2.770334,
     "end_time": "2020-10-14T12:59:04.895814",
     "exception": false,
     "start_time": "2020-10-14T12:59:02.125480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.136101,
     "end_time": "2020-10-14T12:59:05.168435",
     "exception": false,
     "start_time": "2020-10-14T12:59:05.032334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 510.958069,
   "end_time": "2020-10-14T12:59:06.346212",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-14T12:50:35.388143",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
