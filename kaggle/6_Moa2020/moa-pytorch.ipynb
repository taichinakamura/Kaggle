{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020433,
     "end_time": "2020-10-11T09:34:57.238706",
     "exception": false,
     "start_time": "2020-10-11T09:34:57.218273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- retry sum of moa\n",
    "- retry standardization\n",
    "- consider seed to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-11T09:34:57.287376Z",
     "iopub.status.busy": "2020-10-11T09:34:57.286467Z",
     "iopub.status.idle": "2020-10-11T09:35:05.213823Z",
     "shell.execute_reply": "2020-10-11T09:35:05.212527Z"
    },
    "papermill": {
     "duration": 7.956127,
     "end_time": "2020-10-11T09:35:05.213977",
     "exception": false,
     "start_time": "2020-10-11T09:34:57.257850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:05.260602Z",
     "iopub.status.busy": "2020-10-11T09:35:05.259931Z",
     "iopub.status.idle": "2020-10-11T09:35:11.308588Z",
     "shell.execute_reply": "2020-10-11T09:35:11.307335Z"
    },
    "papermill": {
     "duration": 6.075968,
     "end_time": "2020-10-11T09:35:11.308709",
     "exception": false,
     "start_time": "2020-10-11T09:35:05.232741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:11.356073Z",
     "iopub.status.busy": "2020-10-11T09:35:11.354154Z",
     "iopub.status.idle": "2020-10-11T09:35:11.356868Z",
     "shell.execute_reply": "2020-10-11T09:35:11.357363Z"
    },
    "papermill": {
     "duration": 0.030121,
     "end_time": "2020-10-11T09:35:11.357498",
     "exception": false,
     "start_time": "2020-10-11T09:35:11.327377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:11.403559Z",
     "iopub.status.busy": "2020-10-11T09:35:11.402883Z",
     "iopub.status.idle": "2020-10-11T09:35:11.508723Z",
     "shell.execute_reply": "2020-10-11T09:35:11.508045Z"
    },
    "papermill": {
     "duration": 0.132532,
     "end_time": "2020-10-11T09:35:11.508889",
     "exception": false,
     "start_time": "2020-10-11T09:35:11.376357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019007,
     "end_time": "2020-10-11T09:35:11.546799",
     "exception": false,
     "start_time": "2020-10-11T09:35:11.527792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:11.602345Z",
     "iopub.status.busy": "2020-10-11T09:35:11.596847Z",
     "iopub.status.idle": "2020-10-11T09:35:11.913668Z",
     "shell.execute_reply": "2020-10-11T09:35:11.913043Z"
    },
    "papermill": {
     "duration": 0.346908,
     "end_time": "2020-10-11T09:35:11.913789",
     "exception": false,
     "start_time": "2020-10-11T09:35:11.566881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:11.987106Z",
     "iopub.status.busy": "2020-10-11T09:35:11.986126Z",
     "iopub.status.idle": "2020-10-11T09:35:11.999367Z",
     "shell.execute_reply": "2020-10-11T09:35:11.999897Z"
    },
    "papermill": {
     "duration": 0.067543,
     "end_time": "2020-10-11T09:35:12.000046",
     "exception": false,
     "start_time": "2020-10-11T09:35:11.932503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:12.044296Z",
     "iopub.status.busy": "2020-10-11T09:35:12.043360Z",
     "iopub.status.idle": "2020-10-11T09:35:12.085696Z",
     "shell.execute_reply": "2020-10-11T09:35:12.086156Z"
    },
    "papermill": {
     "duration": 0.067503,
     "end_time": "2020-10-11T09:35:12.086291",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.018788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n"
     ]
    }
   ],
   "source": [
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "#drop_list2 = list(nontarget_dists[(nontarget_dists.number>0) & (nontarget_dists.number<=6)][\"target\"].values)[:-1]\n",
    "#print(\"second drop\", len(drop_list2))\n",
    "#non_targets.drop(drop_list2, axis=1, inplace=True)\n",
    "#print(\"shape after 2nd drop:\", non_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:12.134525Z",
     "iopub.status.busy": "2020-10-11T09:35:12.133632Z",
     "iopub.status.idle": "2020-10-11T09:35:12.137008Z",
     "shell.execute_reply": "2020-10-11T09:35:12.136455Z"
    },
    "papermill": {
     "duration": 0.028295,
     "end_time": "2020-10-11T09:35:12.137107",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.108812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X = train[g_feats].copy().values\n",
    "#select = VarianceThreshold(threshold=1)\n",
    "#X_new = select.fit_transform(X)\n",
    "#drop_g_feats = list(np.array(g_feats)[select.get_support()==False])\n",
    "#len(drop_g_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019884,
     "end_time": "2020-10-11T09:35:12.176134",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.156250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:12.220906Z",
     "iopub.status.busy": "2020-10-11T09:35:12.220035Z",
     "iopub.status.idle": "2020-10-11T09:35:12.222819Z",
     "shell.execute_reply": "2020-10-11T09:35:12.223409Z"
    },
    "papermill": {
     "duration": 0.027575,
     "end_time": "2020-10-11T09:35:12.223543",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.195968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#num = 10\n",
    "#pca_c_cols = [\"pca-c\"+str(i+1) for i in range(num)]\n",
    "#pca = PCA(n_components=num)\n",
    "#tmp_train = pca.fit_transform(train[c_feats])\n",
    "#tmp_test = pca.transform(test[c_feats])\n",
    "#tmp_train = pd.DataFrame(tmp_train, columns=pca_c_cols)\n",
    "#tmp_test = pd.DataFrame(tmp_test, columns=pca_c_cols)\n",
    "\n",
    "#train = pd.concat([train, tmp_train],axis=1)\n",
    "#test = pd.concat([test, tmp_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:12.273198Z",
     "iopub.status.busy": "2020-10-11T09:35:12.271928Z",
     "iopub.status.idle": "2020-10-11T09:35:12.418519Z",
     "shell.execute_reply": "2020-10-11T09:35:12.419091Z"
    },
    "papermill": {
     "duration": 0.175291,
     "end_time": "2020-10-11T09:35:12.419233",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.243942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 874) (3982, 874)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    tmp.loc[:, 'cp_type'] = tmp.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\"], axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020099,
     "end_time": "2020-10-11T09:35:12.460005",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.439906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:12.883183Z",
     "iopub.status.busy": "2020-10-11T09:35:12.521241Z",
     "iopub.status.idle": "2020-10-11T09:35:12.886990Z",
     "shell.execute_reply": "2020-10-11T09:35:12.887560Z"
    },
    "papermill": {
     "duration": 0.40731,
     "end_time": "2020-10-11T09:35:12.887712",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.480402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048,1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_num))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021306,
     "end_time": "2020-10-11T09:35:12.930616",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.909310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# predict non-targets, targets separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:13.011210Z",
     "iopub.status.busy": "2020-10-11T09:35:12.991785Z",
     "iopub.status.idle": "2020-10-11T09:35:13.014050Z",
     "shell.execute_reply": "2020-10-11T09:35:13.013580Z"
    },
    "papermill": {
     "duration": 0.062329,
     "end_time": "2020-10-11T09:35:13.014145",
     "exception": false,
     "start_time": "2020-10-11T09:35:12.951816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 5\n",
    "n_folds=5\n",
    "\n",
    "def first_learning(tr, target, sample_seed, init_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    files = []\n",
    "        \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'parameters'+str(fold+1)+'.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        files.append('parameters'+str(fold+1)+'.pt')\n",
    "        pred_model = MoaModel(init_num)\n",
    "        pred_model.load_state_dict(torch.load('parameters'+str(fold+1)+'.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:13.064603Z",
     "iopub.status.busy": "2020-10-11T09:35:13.063302Z",
     "iopub.status.idle": "2020-10-11T09:35:13.915318Z",
     "shell.execute_reply": "2020-10-11T09:35:13.914675Z"
    },
    "papermill": {
     "duration": 0.880754,
     "end_time": "2020-10-11T09:35:13.915431",
     "exception": false,
     "start_time": "2020-10-11T09:35:13.034677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "\n",
    "#seeds = [0]\n",
    "#for seed_ in seeds:\n",
    "#    files = first_learning(fn_train, fn_nontargets, seed_, fn_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021476,
     "end_time": "2020-10-11T09:35:13.959443",
     "exception": false,
     "start_time": "2020-10-11T09:35:13.937967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train by targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:14.042235Z",
     "iopub.status.busy": "2020-10-11T09:35:14.026508Z",
     "iopub.status.idle": "2020-10-11T09:35:14.053472Z",
     "shell.execute_reply": "2020-10-11T09:35:14.052956Z"
    },
    "papermill": {
     "duration": 0.07284,
     "end_time": "2020-10-11T09:35:14.053601",
     "exception": false,
     "start_time": "2020-10-11T09:35:13.980761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 40\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, files):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        if files != []:\n",
    "            clf.load_state_dict(torch.load(files[fold]))\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            #if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "            #    break\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:35:14.110010Z",
     "iopub.status.busy": "2020-10-11T09:35:14.108474Z",
     "iopub.status.idle": "2020-10-11T09:55:17.699849Z",
     "shell.execute_reply": "2020-10-11T09:55:17.700318Z"
    },
    "papermill": {
     "duration": 1203.6248,
     "end_time": "2020-10-11T09:55:17.700473",
     "exception": false,
     "start_time": "2020-10-11T09:35:14.075673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412591 \t val_loss=0.077221 \t time=1.61s\n",
      "Best model: Epoch 2 \t loss=0.048580 \t val_loss=0.028151 \t time=1.06s\n",
      "Best model: Epoch 3 \t loss=0.027003 \t val_loss=0.022851 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023460 \t val_loss=0.021035 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021372 \t val_loss=0.019930 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020710 \t val_loss=0.019312 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.020091 \t val_loss=0.019111 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.019413 \t val_loss=0.018614 \t time=1.17s\n",
      "Best model: Epoch 9 \t loss=0.018994 \t val_loss=0.018114 \t time=0.82s\n",
      "Best model: Epoch 10 \t loss=0.018733 \t val_loss=0.017949 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.018251 \t val_loss=0.017597 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017651 \t val_loss=0.017239 \t time=0.83s\n",
      "Best model: Epoch 14 \t loss=0.017350 \t val_loss=0.017176 \t time=1.04s\n",
      "Best model: Epoch 15 \t loss=0.017291 \t val_loss=0.017116 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.017107 \t val_loss=0.016993 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.016768 \t val_loss=0.016788 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016622 \t val_loss=0.016715 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.016251 \t val_loss=0.016576 \t time=0.82s\n",
      "Best model: Epoch 22 \t loss=0.016066 \t val_loss=0.016500 \t time=0.83s\n",
      "Best model: Epoch 24 \t loss=0.015943 \t val_loss=0.016470 \t time=1.23s\n",
      "Best model: Epoch 25 \t loss=0.015734 \t val_loss=0.016424 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.015489 \t val_loss=0.016377 \t time=0.91s\n",
      "Best model: Epoch 31 \t loss=0.015327 \t val_loss=0.016313 \t time=0.81s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014506 \t val_loss=0.016155 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.014171 \t val_loss=0.016128 \t time=0.82s\n",
      "Best model: Epoch 38 \t loss=0.014019 \t val_loss=0.016098 \t time=1.08s\n",
      "Best model: Epoch 39 \t loss=0.013838 \t val_loss=0.016042 \t time=0.84s\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414173 \t val_loss=0.076114 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.048296 \t val_loss=0.029272 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.027405 \t val_loss=0.022898 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023185 \t val_loss=0.021045 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.021348 \t val_loss=0.019655 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020497 \t val_loss=0.019208 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.019868 \t val_loss=0.018997 \t time=1.08s\n",
      "Best model: Epoch 8 \t loss=0.019279 \t val_loss=0.018520 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018965 \t val_loss=0.017982 \t time=0.83s\n",
      "Best model: Epoch 10 \t loss=0.018675 \t val_loss=0.017759 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.018200 \t val_loss=0.017652 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.017911 \t val_loss=0.017377 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017592 \t val_loss=0.017221 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017180 \t val_loss=0.017090 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.017055 \t val_loss=0.016951 \t time=0.85s\n",
      "Best model: Epoch 17 \t loss=0.016813 \t val_loss=0.016760 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016564 \t val_loss=0.016719 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.016340 \t val_loss=0.016593 \t time=1.09s\n",
      "Best model: Epoch 20 \t loss=0.016182 \t val_loss=0.016538 \t time=0.87s\n",
      "Best model: Epoch 21 \t loss=0.016051 \t val_loss=0.016499 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016487 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015813 \t val_loss=0.016389 \t time=0.83s\n",
      "Best model: Epoch 25 \t loss=0.015707 \t val_loss=0.016370 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015672 \t val_loss=0.016358 \t time=0.85s\n",
      "Best model: Epoch 27 \t loss=0.015543 \t val_loss=0.016294 \t time=0.84s\n",
      "Best model: Epoch 28 \t loss=0.015430 \t val_loss=0.016279 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.015341 \t val_loss=0.016245 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.015306 \t val_loss=0.016228 \t time=0.85s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014418 \t val_loss=0.016175 \t time=0.86s\n",
      "Best model: Epoch 36 \t loss=0.014187 \t val_loss=0.016067 \t time=1.11s\n",
      "Best model: Epoch 37 \t loss=0.013917 \t val_loss=0.016034 \t time=0.82s\n",
      "Best model: Epoch 38 \t loss=0.013762 \t val_loss=0.016030 \t time=0.81s\n",
      "Best model: Epoch 39 \t loss=0.013603 \t val_loss=0.015983 \t time=0.80s\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413986 \t val_loss=0.076896 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.049192 \t val_loss=0.028994 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.027106 \t val_loss=0.022515 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023059 \t val_loss=0.020760 \t time=1.26s\n",
      "Best model: Epoch 5 \t loss=0.021690 \t val_loss=0.020538 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020625 \t val_loss=0.019409 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.019750 \t val_loss=0.018819 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019329 \t val_loss=0.018315 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019057 \t val_loss=0.018125 \t time=1.20s\n",
      "Best model: Epoch 10 \t loss=0.018609 \t val_loss=0.017987 \t time=0.84s\n",
      "Best model: Epoch 11 \t loss=0.018311 \t val_loss=0.017812 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.018039 \t val_loss=0.017600 \t time=1.10s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017359 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017500 \t val_loss=0.017197 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017385 \t val_loss=0.017085 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.017066 \t val_loss=0.016969 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.016860 \t val_loss=0.016758 \t time=0.85s\n",
      "Best model: Epoch 18 \t loss=0.016605 \t val_loss=0.016644 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.016521 \t val_loss=0.016627 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016267 \t val_loss=0.016498 \t time=0.79s\n",
      "Best model: Epoch 23 \t loss=0.015995 \t val_loss=0.016465 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.015823 \t val_loss=0.016417 \t time=1.06s\n",
      "Best model: Epoch 25 \t loss=0.015703 \t val_loss=0.016356 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015659 \t val_loss=0.016288 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.015393 \t val_loss=0.016223 \t time=0.82s\n",
      "Best model: Epoch 33 \t loss=0.015215 \t val_loss=0.016222 \t time=0.80s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014525 \t val_loss=0.016054 \t time=0.81s\n",
      "Best model: Epoch 36 \t loss=0.014278 \t val_loss=0.015991 \t time=0.91s\n",
      "Best model: Epoch 37 \t loss=0.014126 \t val_loss=0.015968 \t time=0.92s\n",
      "Best model: Epoch 38 \t loss=0.013885 \t val_loss=0.015964 \t time=0.82s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015933 \t time=0.83s\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414550 \t val_loss=0.077203 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.050157 \t val_loss=0.030820 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.028654 \t val_loss=0.022643 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.022900 \t val_loss=0.020759 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021488 \t val_loss=0.019617 \t time=0.84s\n",
      "Best model: Epoch 6 \t loss=0.020599 \t val_loss=0.019256 \t time=1.08s\n",
      "Best model: Epoch 7 \t loss=0.020185 \t val_loss=0.018790 \t time=0.91s\n",
      "Best model: Epoch 8 \t loss=0.019538 \t val_loss=0.018186 \t time=0.92s\n",
      "Best model: Epoch 9 \t loss=0.019437 \t val_loss=0.017978 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018881 \t val_loss=0.017777 \t time=0.84s\n",
      "Best model: Epoch 11 \t loss=0.018440 \t val_loss=0.017634 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.018149 \t val_loss=0.017331 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017247 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017593 \t val_loss=0.017178 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017360 \t val_loss=0.017018 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017128 \t val_loss=0.016771 \t time=0.99s\n",
      "Best model: Epoch 18 \t loss=0.016888 \t val_loss=0.016649 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016572 \t val_loss=0.016569 \t time=0.95s\n",
      "Best model: Epoch 20 \t loss=0.016374 \t val_loss=0.016445 \t time=1.11s\n",
      "Best model: Epoch 22 \t loss=0.016243 \t val_loss=0.016368 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.016123 \t val_loss=0.016358 \t time=0.82s\n",
      "Best model: Epoch 25 \t loss=0.015849 \t val_loss=0.016256 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.015719 \t val_loss=0.016254 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.015601 \t val_loss=0.016216 \t time=1.10s\n",
      "Best model: Epoch 29 \t loss=0.015491 \t val_loss=0.016183 \t time=0.85s\n",
      "Best model: Epoch 30 \t loss=0.015505 \t val_loss=0.016177 \t time=0.92s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014550 \t val_loss=0.016025 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.014275 \t val_loss=0.015951 \t time=0.84s\n",
      "Best model: Epoch 37 \t loss=0.014051 \t val_loss=0.015898 \t time=0.85s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015897 \t time=0.80s\n",
      "Best model: Epoch 40 \t loss=0.013633 \t val_loss=0.015858 \t time=1.03s\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412187 \t val_loss=0.088095 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.049344 \t val_loss=0.028300 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.027639 \t val_loss=0.022942 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023355 \t val_loss=0.020933 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.021415 \t val_loss=0.019995 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020525 \t val_loss=0.019669 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.019796 \t val_loss=0.018820 \t time=0.83s\n",
      "Best model: Epoch 8 \t loss=0.019289 \t val_loss=0.018472 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.019185 \t val_loss=0.018166 \t time=1.03s\n",
      "Best model: Epoch 11 \t loss=0.018588 \t val_loss=0.017788 \t time=1.00s\n",
      "Best model: Epoch 12 \t loss=0.018107 \t val_loss=0.017631 \t time=1.02s\n",
      "Best model: Epoch 13 \t loss=0.017794 \t val_loss=0.017333 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017391 \t val_loss=0.017204 \t time=0.83s\n",
      "Best model: Epoch 15 \t loss=0.017268 \t val_loss=0.017087 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.017022 \t val_loss=0.016977 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.016798 \t val_loss=0.016941 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016580 \t val_loss=0.016778 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.016403 \t val_loss=0.016737 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016202 \t val_loss=0.016591 \t time=1.05s\n",
      "Best model: Epoch 22 \t loss=0.016031 \t val_loss=0.016581 \t time=0.90s\n",
      "Best model: Epoch 23 \t loss=0.015834 \t val_loss=0.016542 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015865 \t val_loss=0.016475 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.015609 \t val_loss=0.016414 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.015342 \t val_loss=0.016413 \t time=0.86s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014680 \t val_loss=0.016273 \t time=0.87s\n",
      "Best model: Epoch 33 \t loss=0.014344 \t val_loss=0.016219 \t time=0.82s\n",
      "Best model: Epoch 34 \t loss=0.014130 \t val_loss=0.016189 \t time=1.05s\n",
      "Best model: Epoch 35 \t loss=0.014023 \t val_loss=0.016185 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.013851 \t val_loss=0.016143 \t time=0.83s\n",
      "Best model: Epoch 37 \t loss=0.013768 \t val_loss=0.016134 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.013329 \t val_loss=0.016125 \t time=0.81s\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Std of log loss: 9.362313645021518e-05\n",
      "Total log loss: 0.016033178295101806\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412054 \t val_loss=0.076325 \t time=1.00s\n",
      "Best model: Epoch 2 \t loss=0.048526 \t val_loss=0.028851 \t time=1.05s\n",
      "Best model: Epoch 3 \t loss=0.026873 \t val_loss=0.023145 \t time=1.14s\n",
      "Best model: Epoch 4 \t loss=0.023518 \t val_loss=0.020954 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021069 \t val_loss=0.019802 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020366 \t val_loss=0.019167 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.019715 \t val_loss=0.018802 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019131 \t val_loss=0.018499 \t time=1.11s\n",
      "Best model: Epoch 9 \t loss=0.018863 \t val_loss=0.018232 \t time=0.92s\n",
      "Best model: Epoch 10 \t loss=0.018576 \t val_loss=0.017987 \t time=0.93s\n",
      "Best model: Epoch 11 \t loss=0.018388 \t val_loss=0.017902 \t time=1.02s\n",
      "Best model: Epoch 12 \t loss=0.018011 \t val_loss=0.017349 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017622 \t val_loss=0.017302 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017534 \t val_loss=0.017155 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.017059 \t val_loss=0.016986 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.016869 \t val_loss=0.016890 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016664 \t val_loss=0.016821 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016417 \t val_loss=0.016683 \t time=0.85s\n",
      "Best model: Epoch 20 \t loss=0.016225 \t val_loss=0.016591 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.015986 \t val_loss=0.016465 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.015784 \t val_loss=0.016374 \t time=0.83s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014782 \t val_loss=0.016199 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.014521 \t val_loss=0.016131 \t time=0.82s\n",
      "Best model: Epoch 31 \t loss=0.014358 \t val_loss=0.016063 \t time=0.83s\n",
      "Best model: Epoch 32 \t loss=0.014177 \t val_loss=0.016035 \t time=0.82s\n",
      "Best model: Epoch 34 \t loss=0.013939 \t val_loss=0.016024 \t time=0.82s\n",
      "Best model: Epoch 35 \t loss=0.013725 \t val_loss=0.016020 \t time=1.33s\n",
      "Best model: Epoch 36 \t loss=0.013646 \t val_loss=0.016016 \t time=1.00s\n",
      "Best model: Epoch 40 \t loss=0.013190 \t val_loss=0.015981 \t time=0.85s\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414855 \t val_loss=0.075799 \t time=0.84s\n",
      "Best model: Epoch 2 \t loss=0.048749 \t val_loss=0.029421 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.027499 \t val_loss=0.023112 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020927 \t time=1.03s\n",
      "Best model: Epoch 5 \t loss=0.021469 \t val_loss=0.019786 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020582 \t val_loss=0.019286 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.020079 \t val_loss=0.018804 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.019293 \t val_loss=0.018348 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.019088 \t val_loss=0.018044 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.018684 \t val_loss=0.018009 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018275 \t val_loss=0.017549 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.017908 \t val_loss=0.017357 \t time=0.84s\n",
      "Best model: Epoch 13 \t loss=0.017698 \t val_loss=0.017252 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.017397 \t val_loss=0.017156 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017207 \t val_loss=0.016991 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.016897 \t val_loss=0.016768 \t time=0.92s\n",
      "Best model: Epoch 18 \t loss=0.016568 \t val_loss=0.016672 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016429 \t val_loss=0.016570 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.016223 \t val_loss=0.016533 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.016095 \t val_loss=0.016416 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.015917 \t val_loss=0.016410 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.015726 \t val_loss=0.016333 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.015412 \t val_loss=0.016291 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015315 \t val_loss=0.016222 \t time=1.10s\n",
      "Best model: Epoch 32 \t loss=0.015167 \t val_loss=0.016173 \t time=0.84s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014376 \t val_loss=0.016120 \t time=0.83s\n",
      "Best model: Epoch 38 \t loss=0.014005 \t val_loss=0.016016 \t time=0.81s\n",
      "Best model: Epoch 40 \t loss=0.013573 \t val_loss=0.016004 \t time=1.02s\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.415098 \t val_loss=0.080733 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.048767 \t val_loss=0.028509 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.026784 \t val_loss=0.023150 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023161 \t val_loss=0.021295 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021450 \t val_loss=0.019731 \t time=0.84s\n",
      "Best model: Epoch 6 \t loss=0.020675 \t val_loss=0.019469 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.020227 \t val_loss=0.019188 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.019444 \t val_loss=0.018675 \t time=0.86s\n",
      "Best model: Epoch 9 \t loss=0.019434 \t val_loss=0.018288 \t time=0.87s\n",
      "Best model: Epoch 10 \t loss=0.018643 \t val_loss=0.018101 \t time=0.99s\n",
      "Best model: Epoch 11 \t loss=0.018215 \t val_loss=0.017674 \t time=0.84s\n",
      "Best model: Epoch 12 \t loss=0.017846 \t val_loss=0.017324 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017431 \t val_loss=0.017053 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.017095 \t val_loss=0.016751 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.016675 \t val_loss=0.016733 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.016288 \t val_loss=0.016438 \t time=1.12s\n",
      "Best model: Epoch 22 \t loss=0.016149 \t val_loss=0.016371 \t time=1.05s\n",
      "Best model: Epoch 24 \t loss=0.015809 \t val_loss=0.016315 \t time=0.82s\n",
      "Best model: Epoch 25 \t loss=0.015760 \t val_loss=0.016313 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015703 \t val_loss=0.016287 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.015558 \t val_loss=0.016223 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015499 \t val_loss=0.016220 \t time=0.82s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014648 \t val_loss=0.016064 \t time=0.90s\n",
      "Best model: Epoch 35 \t loss=0.014381 \t val_loss=0.016009 \t time=0.97s\n",
      "Best model: Epoch 36 \t loss=0.014148 \t val_loss=0.015964 \t time=0.83s\n",
      "Best model: Epoch 37 \t loss=0.014036 \t val_loss=0.015958 \t time=0.82s\n",
      "Best model: Epoch 38 \t loss=0.013845 \t val_loss=0.015945 \t time=0.82s\n",
      "Best model: Epoch 39 \t loss=0.013695 \t val_loss=0.015936 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.013596 \t val_loss=0.015928 \t time=0.82s\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413201 \t val_loss=0.077351 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.047845 \t val_loss=0.027291 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.027879 \t val_loss=0.022880 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023083 \t val_loss=0.021099 \t time=1.02s\n",
      "Best model: Epoch 5 \t loss=0.021983 \t val_loss=0.020159 \t time=0.96s\n",
      "Best model: Epoch 6 \t loss=0.020838 \t val_loss=0.019249 \t time=1.29s\n",
      "Best model: Epoch 7 \t loss=0.019902 \t val_loss=0.018613 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.019397 \t val_loss=0.018178 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.018991 \t val_loss=0.017956 \t time=0.83s\n",
      "Best model: Epoch 10 \t loss=0.018744 \t val_loss=0.017936 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018373 \t val_loss=0.017435 \t time=1.05s\n",
      "Best model: Epoch 12 \t loss=0.017988 \t val_loss=0.017253 \t time=1.06s\n",
      "Best model: Epoch 13 \t loss=0.017706 \t val_loss=0.017092 \t time=0.95s\n",
      "Best model: Epoch 14 \t loss=0.017475 \t val_loss=0.016959 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017304 \t val_loss=0.016845 \t time=1.11s\n",
      "Best model: Epoch 16 \t loss=0.016985 \t val_loss=0.016721 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.016821 \t val_loss=0.016602 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016629 \t val_loss=0.016580 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.016474 \t val_loss=0.016482 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.016306 \t val_loss=0.016427 \t time=0.84s\n",
      "Best model: Epoch 21 \t loss=0.016230 \t val_loss=0.016412 \t time=0.83s\n",
      "Best model: Epoch 22 \t loss=0.016012 \t val_loss=0.016284 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.015916 \t val_loss=0.016250 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.015605 \t val_loss=0.016187 \t time=0.81s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016160 \t time=0.81s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014523 \t val_loss=0.016007 \t time=0.83s\n",
      "Best model: Epoch 35 \t loss=0.014326 \t val_loss=0.015953 \t time=0.82s\n",
      "Best model: Epoch 36 \t loss=0.014128 \t val_loss=0.015927 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.013961 \t val_loss=0.015913 \t time=0.82s\n",
      "Best model: Epoch 38 \t loss=0.013800 \t val_loss=0.015902 \t time=0.81s\n",
      "Best model: Epoch 39 \t loss=0.013707 \t val_loss=0.015887 \t time=0.85s\n",
      "Best model: Epoch 40 \t loss=0.013494 \t val_loss=0.015874 \t time=1.06s\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.414129 \t val_loss=0.077634 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.048715 \t val_loss=0.028425 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.027408 \t val_loss=0.022374 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023334 \t val_loss=0.020878 \t time=0.84s\n",
      "Best model: Epoch 5 \t loss=0.021927 \t val_loss=0.020042 \t time=1.04s\n",
      "Best model: Epoch 6 \t loss=0.020496 \t val_loss=0.019306 \t time=0.96s\n",
      "Best model: Epoch 7 \t loss=0.019769 \t val_loss=0.018887 \t time=0.93s\n",
      "Best model: Epoch 8 \t loss=0.019723 \t val_loss=0.018739 \t time=0.93s\n",
      "Best model: Epoch 9 \t loss=0.018921 \t val_loss=0.018396 \t time=1.02s\n",
      "Best model: Epoch 10 \t loss=0.018596 \t val_loss=0.017923 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018246 \t val_loss=0.017664 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.018002 \t val_loss=0.017398 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.017603 \t val_loss=0.017307 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.017409 \t val_loss=0.017181 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.017084 \t val_loss=0.017008 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016757 \t val_loss=0.016826 \t time=0.80s\n",
      "Best model: Epoch 19 \t loss=0.016483 \t val_loss=0.016648 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016282 \t val_loss=0.016632 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016561 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.015977 \t val_loss=0.016516 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.015867 \t val_loss=0.016485 \t time=0.83s\n",
      "Best model: Epoch 25 \t loss=0.015713 \t val_loss=0.016440 \t time=0.80s\n",
      "Best model: Epoch 28 \t loss=0.015600 \t val_loss=0.016433 \t time=1.03s\n",
      "Best model: Epoch 29 \t loss=0.015441 \t val_loss=0.016363 \t time=0.93s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014609 \t val_loss=0.016205 \t time=0.84s\n",
      "Best model: Epoch 35 \t loss=0.014249 \t val_loss=0.016198 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.014074 \t val_loss=0.016103 \t time=0.84s\n",
      "Best model: Epoch 37 \t loss=0.013971 \t val_loss=0.016069 \t time=0.83s\n",
      "Best model: Epoch 38 \t loss=0.013754 \t val_loss=0.016059 \t time=0.91s\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Std of log loss: 7.303465552417108e-05\n",
      "Total log loss: 0.01600996956097101\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.414243 \t val_loss=0.075401 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.049196 \t val_loss=0.028064 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.026940 \t val_loss=0.022525 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023264 \t val_loss=0.021226 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.021407 \t val_loss=0.019807 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020540 \t val_loss=0.019256 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019968 \t val_loss=0.018993 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.019501 \t val_loss=0.018829 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.019202 \t val_loss=0.018372 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.019079 \t val_loss=0.018125 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.018344 \t val_loss=0.017942 \t time=0.95s\n",
      "Best model: Epoch 12 \t loss=0.018138 \t val_loss=0.017556 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017707 \t val_loss=0.017321 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.017455 \t val_loss=0.017192 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017177 \t val_loss=0.017051 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017017 \t val_loss=0.016919 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.016662 \t val_loss=0.016903 \t time=0.80s\n",
      "Best model: Epoch 19 \t loss=0.016553 \t val_loss=0.016804 \t time=0.82s\n",
      "Best model: Epoch 20 \t loss=0.016457 \t val_loss=0.016643 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.016250 \t val_loss=0.016637 \t time=0.89s\n",
      "Best model: Epoch 22 \t loss=0.016172 \t val_loss=0.016621 \t time=0.83s\n",
      "Best model: Epoch 23 \t loss=0.016010 \t val_loss=0.016544 \t time=0.83s\n",
      "Best model: Epoch 25 \t loss=0.015790 \t val_loss=0.016434 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015725 \t val_loss=0.016409 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.015650 \t val_loss=0.016400 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015497 \t val_loss=0.016379 \t time=0.83s\n",
      "Best model: Epoch 33 \t loss=0.015218 \t val_loss=0.016377 \t time=0.82s\n",
      "Best model: Epoch 35 \t loss=0.015280 \t val_loss=0.016351 \t time=0.81s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014390 \t val_loss=0.016189 \t time=0.81s\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.412084 \t val_loss=0.076539 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.049085 \t val_loss=0.028798 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.026930 \t val_loss=0.022483 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023605 \t val_loss=0.021048 \t time=0.84s\n",
      "Best model: Epoch 5 \t loss=0.021499 \t val_loss=0.019946 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020550 \t val_loss=0.019083 \t time=1.42s\n",
      "Best model: Epoch 7 \t loss=0.019963 \t val_loss=0.018862 \t time=0.87s\n",
      "Best model: Epoch 8 \t loss=0.019379 \t val_loss=0.018356 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.018966 \t val_loss=0.018103 \t time=0.90s\n",
      "Best model: Epoch 10 \t loss=0.018577 \t val_loss=0.017829 \t time=1.11s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017426 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017755 \t val_loss=0.017169 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017197 \t val_loss=0.016995 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.016993 \t val_loss=0.016966 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016980 \t val_loss=0.016896 \t time=1.02s\n",
      "Best model: Epoch 18 \t loss=0.016810 \t val_loss=0.016787 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016601 \t val_loss=0.016749 \t time=0.82s\n",
      "Best model: Epoch 20 \t loss=0.016273 \t val_loss=0.016588 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.016209 \t val_loss=0.016517 \t time=0.83s\n",
      "Best model: Epoch 23 \t loss=0.015865 \t val_loss=0.016396 \t time=1.14s\n",
      "Best model: Epoch 25 \t loss=0.015726 \t val_loss=0.016323 \t time=0.79s\n",
      "Best model: Epoch 29 \t loss=0.015473 \t val_loss=0.016273 \t time=0.96s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014607 \t val_loss=0.016130 \t time=0.80s\n",
      "Best model: Epoch 35 \t loss=0.014302 \t val_loss=0.016075 \t time=0.81s\n",
      "Best model: Epoch 36 \t loss=0.014098 \t val_loss=0.016072 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.014008 \t val_loss=0.016029 \t time=0.81s\n",
      "Best model: Epoch 38 \t loss=0.013877 \t val_loss=0.016023 \t time=0.80s\n",
      "Best model: Epoch 40 \t loss=0.013495 \t val_loss=0.016019 \t time=0.80s\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411402 \t val_loss=0.077279 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.049547 \t val_loss=0.029069 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.027701 \t val_loss=0.022526 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.022974 \t val_loss=0.020914 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.021929 \t val_loss=0.020299 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020641 \t val_loss=0.019507 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.020131 \t val_loss=0.019061 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.019336 \t val_loss=0.018609 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018623 \t val_loss=0.017850 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017799 \t time=1.01s\n",
      "Best model: Epoch 13 \t loss=0.017806 \t val_loss=0.017264 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017459 \t val_loss=0.017123 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.017064 \t val_loss=0.016860 \t time=1.09s\n",
      "Best model: Epoch 18 \t loss=0.016741 \t val_loss=0.016715 \t time=0.88s\n",
      "Best model: Epoch 19 \t loss=0.016492 \t val_loss=0.016638 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.016265 \t val_loss=0.016467 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.016243 \t val_loss=0.016437 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.016021 \t val_loss=0.016410 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.015786 \t val_loss=0.016352 \t time=1.11s\n",
      "Best model: Epoch 26 \t loss=0.015692 \t val_loss=0.016319 \t time=0.88s\n",
      "Best model: Epoch 27 \t loss=0.015599 \t val_loss=0.016292 \t time=0.83s\n",
      "Best model: Epoch 29 \t loss=0.015427 \t val_loss=0.016235 \t time=0.92s\n",
      "Best model: Epoch 30 \t loss=0.015397 \t val_loss=0.016205 \t time=1.07s\n",
      "Best model: Epoch 31 \t loss=0.015269 \t val_loss=0.016160 \t time=0.84s\n",
      "Best model: Epoch 33 \t loss=0.015170 \t val_loss=0.016138 \t time=1.02s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.014412 \t val_loss=0.016074 \t time=0.85s\n",
      "Best model: Epoch 39 \t loss=0.014103 \t val_loss=0.016024 \t time=0.86s\n",
      "Best model: Epoch 40 \t loss=0.013940 \t val_loss=0.015989 \t time=0.81s\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414447 \t val_loss=0.083103 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.048190 \t val_loss=0.029241 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.028207 \t val_loss=0.022736 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.023313 \t val_loss=0.020537 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021788 \t val_loss=0.019921 \t time=1.07s\n",
      "Best model: Epoch 6 \t loss=0.020538 \t val_loss=0.019117 \t time=0.94s\n",
      "Best model: Epoch 7 \t loss=0.020011 \t val_loss=0.018677 \t time=0.83s\n",
      "Best model: Epoch 8 \t loss=0.019755 \t val_loss=0.018538 \t time=1.03s\n",
      "Best model: Epoch 9 \t loss=0.019270 \t val_loss=0.017984 \t time=0.96s\n",
      "Best model: Epoch 10 \t loss=0.018757 \t val_loss=0.017874 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.018413 \t val_loss=0.017491 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.018074 \t val_loss=0.017276 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017728 \t val_loss=0.017107 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.017003 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017251 \t val_loss=0.016845 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.017310 \t val_loss=0.016813 \t time=1.02s\n",
      "Best model: Epoch 17 \t loss=0.017063 \t val_loss=0.016771 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016799 \t val_loss=0.016573 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.016628 \t val_loss=0.016536 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016486 \t val_loss=0.016442 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.016320 \t val_loss=0.016359 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.016065 \t val_loss=0.016320 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.016071 \t val_loss=0.016278 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.015930 \t val_loss=0.016261 \t time=0.83s\n",
      "Best model: Epoch 26 \t loss=0.015693 \t val_loss=0.016257 \t time=0.80s\n",
      "Best model: Epoch 28 \t loss=0.015580 \t val_loss=0.016195 \t time=0.82s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014672 \t val_loss=0.016048 \t time=0.82s\n",
      "Best model: Epoch 34 \t loss=0.014376 \t val_loss=0.015966 \t time=0.80s\n",
      "Best model: Epoch 35 \t loss=0.014214 \t val_loss=0.015932 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.013900 \t val_loss=0.015920 \t time=0.81s\n",
      "Best model: Epoch 40 \t loss=0.013575 \t val_loss=0.015914 \t time=0.81s\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413087 \t val_loss=0.073699 \t time=1.09s\n",
      "Best model: Epoch 2 \t loss=0.049084 \t val_loss=0.029309 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.028082 \t val_loss=0.023504 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023135 \t val_loss=0.020912 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.021648 \t val_loss=0.020058 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020544 \t val_loss=0.019231 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.019869 \t val_loss=0.018961 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.019517 \t val_loss=0.018423 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.018956 \t val_loss=0.018169 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018656 \t val_loss=0.017995 \t time=1.42s\n",
      "Best model: Epoch 11 \t loss=0.018387 \t val_loss=0.017671 \t time=1.15s\n",
      "Best model: Epoch 12 \t loss=0.017993 \t val_loss=0.017505 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017741 \t val_loss=0.017458 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017536 \t val_loss=0.017309 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017324 \t val_loss=0.017063 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017025 \t val_loss=0.017025 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.016810 \t val_loss=0.016959 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.016821 \t val_loss=0.016803 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016505 \t val_loss=0.016753 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016673 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.016069 \t val_loss=0.016543 \t time=0.97s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016523 \t time=0.87s\n",
      "Best model: Epoch 25 \t loss=0.015810 \t val_loss=0.016492 \t time=0.81s\n",
      "Best model: Epoch 26 \t loss=0.015618 \t val_loss=0.016482 \t time=0.83s\n",
      "Best model: Epoch 27 \t loss=0.015527 \t val_loss=0.016386 \t time=0.83s\n",
      "Best model: Epoch 28 \t loss=0.015535 \t val_loss=0.016386 \t time=0.81s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014660 \t val_loss=0.016190 \t time=0.90s\n",
      "Best model: Epoch 34 \t loss=0.014195 \t val_loss=0.016131 \t time=0.83s\n",
      "Best model: Epoch 35 \t loss=0.014032 \t val_loss=0.016129 \t time=1.08s\n",
      "Best model: Epoch 36 \t loss=0.013952 \t val_loss=0.016119 \t time=0.84s\n",
      "Best model: Epoch 38 \t loss=0.013619 \t val_loss=0.016104 \t time=0.87s\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Std of log loss: 0.0001118079949946509\n",
      "Total log loss: 0.01608407334211503\n",
      "Total log loss in targets: 0.015928978274088006\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.408572 \t val_loss=0.066028 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.036649 \t val_loss=0.016669 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.014277 \t val_loss=0.009975 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.009188 \t val_loss=0.007213 \t time=0.84s\n",
      "Best model: Epoch 5 \t loss=0.007483 \t val_loss=0.006587 \t time=0.83s\n",
      "Best model: Epoch 6 \t loss=0.006642 \t val_loss=0.006208 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.006166 \t val_loss=0.005869 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.005950 \t val_loss=0.005680 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.005778 \t val_loss=0.005536 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.005627 \t val_loss=0.005398 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.005449 \t val_loss=0.005316 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.005404 \t val_loss=0.005250 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.005269 \t val_loss=0.005207 \t time=0.91s\n",
      "Best model: Epoch 21 \t loss=0.005158 \t val_loss=0.005108 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.005009 \t val_loss=0.005059 \t time=1.05s\n",
      "Best model: Epoch 25 \t loss=0.004965 \t val_loss=0.005026 \t time=1.15s\n",
      "Best model: Epoch 26 \t loss=0.004939 \t val_loss=0.005016 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.004905 \t val_loss=0.005013 \t time=1.08s\n",
      "Best model: Epoch 30 \t loss=0.004807 \t val_loss=0.004994 \t time=0.82s\n",
      "Best model: Epoch 31 \t loss=0.004834 \t val_loss=0.004989 \t time=0.94s\n",
      "Best model: Epoch 34 \t loss=0.004842 \t val_loss=0.004988 \t time=0.92s\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 39 \t loss=0.004646 \t val_loss=0.004907 \t time=0.83s\n",
      "Best model: Epoch 40 \t loss=0.004551 \t val_loss=0.004892 \t time=0.82s\n",
      "Fold 1 log loss: 0.004584021661370486\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.411267 \t val_loss=0.066460 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.036862 \t val_loss=0.016818 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.014079 \t val_loss=0.010783 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.009197 \t val_loss=0.007390 \t time=1.02s\n",
      "Best model: Epoch 5 \t loss=0.007404 \t val_loss=0.006697 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.006585 \t val_loss=0.006061 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.006254 \t val_loss=0.005947 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.005951 \t val_loss=0.005639 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.005925 \t val_loss=0.005638 \t time=0.93s\n",
      "Best model: Epoch 12 \t loss=0.005634 \t val_loss=0.005515 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.005519 \t val_loss=0.005479 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.005441 \t val_loss=0.005434 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.005345 \t val_loss=0.005378 \t time=1.05s\n",
      "Best model: Epoch 17 \t loss=0.005309 \t val_loss=0.005340 \t time=1.00s\n",
      "Best model: Epoch 18 \t loss=0.005309 \t val_loss=0.005273 \t time=0.95s\n",
      "Best model: Epoch 20 \t loss=0.005130 \t val_loss=0.005212 \t time=0.80s\n",
      "Best model: Epoch 22 \t loss=0.005074 \t val_loss=0.005205 \t time=0.84s\n",
      "Best model: Epoch 23 \t loss=0.005005 \t val_loss=0.005175 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.005016 \t val_loss=0.005161 \t time=0.81s\n",
      "Best model: Epoch 25 \t loss=0.004986 \t val_loss=0.005136 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.004924 \t val_loss=0.005118 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.004947 \t val_loss=0.005088 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.004840 \t val_loss=0.005085 \t time=0.83s\n",
      "Best model: Epoch 31 \t loss=0.004808 \t val_loss=0.005082 \t time=0.82s\n",
      "Best model: Epoch 33 \t loss=0.004809 \t val_loss=0.005073 \t time=0.83s\n",
      "Best model: Epoch 35 \t loss=0.004797 \t val_loss=0.005071 \t time=0.82s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.004611 \t val_loss=0.005010 \t time=0.80s\n",
      "Fold 2 log loss: 0.004678826863347746\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411747 \t val_loss=0.073359 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.038070 \t val_loss=0.019307 \t time=0.96s\n",
      "Best model: Epoch 3 \t loss=0.014028 \t val_loss=0.008955 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.009123 \t val_loss=0.007694 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.007441 \t val_loss=0.006444 \t time=0.83s\n",
      "Best model: Epoch 6 \t loss=0.006680 \t val_loss=0.006128 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.006494 \t val_loss=0.005904 \t time=0.83s\n",
      "Best model: Epoch 8 \t loss=0.006211 \t val_loss=0.005826 \t time=1.33s\n",
      "Best model: Epoch 9 \t loss=0.006021 \t val_loss=0.005686 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.005864 \t val_loss=0.005652 \t time=0.98s\n",
      "Best model: Epoch 11 \t loss=0.005655 \t val_loss=0.005464 \t time=0.98s\n",
      "Best model: Epoch 14 \t loss=0.005724 \t val_loss=0.005388 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.005443 \t val_loss=0.005385 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.005357 \t val_loss=0.005369 \t time=0.83s\n",
      "Best model: Epoch 17 \t loss=0.005347 \t val_loss=0.005276 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.005213 \t val_loss=0.005243 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.005163 \t val_loss=0.005203 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.005060 \t val_loss=0.005140 \t time=0.84s\n",
      "Best model: Epoch 24 \t loss=0.005005 \t val_loss=0.005110 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.004976 \t val_loss=0.005084 \t time=0.82s\n",
      "Best model: Epoch 26 \t loss=0.004935 \t val_loss=0.005071 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.004888 \t val_loss=0.005065 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.004868 \t val_loss=0.005034 \t time=0.81s\n",
      "Best model: Epoch 32 \t loss=0.004832 \t val_loss=0.005026 \t time=0.81s\n",
      "Best model: Epoch 34 \t loss=0.004814 \t val_loss=0.005009 \t time=0.97s\n",
      "Best model: Epoch 37 \t loss=0.004798 \t val_loss=0.005004 \t time=0.79s\n",
      "Fold 3 log loss: 0.004721389854379404\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408862 \t val_loss=0.061165 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.037493 \t val_loss=0.015900 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.013699 \t val_loss=0.010333 \t time=1.00s\n",
      "Best model: Epoch 4 \t loss=0.008977 \t val_loss=0.007489 \t time=0.98s\n",
      "Best model: Epoch 5 \t loss=0.007812 \t val_loss=0.006917 \t time=1.01s\n",
      "Best model: Epoch 6 \t loss=0.006728 \t val_loss=0.006059 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.006328 \t val_loss=0.005933 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.006117 \t val_loss=0.005664 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.005870 \t val_loss=0.005662 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.005743 \t val_loss=0.005540 \t time=0.81s\n",
      "Best model: Epoch 13 \t loss=0.005533 \t val_loss=0.005422 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.005470 \t val_loss=0.005379 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.005428 \t val_loss=0.005369 \t time=0.93s\n",
      "Best model: Epoch 17 \t loss=0.005447 \t val_loss=0.005336 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.005281 \t val_loss=0.005322 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.005182 \t val_loss=0.005253 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.005145 \t val_loss=0.005230 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.005006 \t val_loss=0.005145 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.004951 \t val_loss=0.005116 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.004942 \t val_loss=0.005093 \t time=1.23s\n",
      "Best model: Epoch 31 \t loss=0.004849 \t val_loss=0.005067 \t time=1.23s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.004628 \t val_loss=0.005032 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.004523 \t val_loss=0.005008 \t time=1.21s\n",
      "Best model: Epoch 38 \t loss=0.004463 \t val_loss=0.004985 \t time=1.09s\n",
      "Best model: Epoch 39 \t loss=0.004393 \t val_loss=0.004977 \t time=0.88s\n",
      "Best model: Epoch 40 \t loss=0.004338 \t val_loss=0.004965 \t time=0.86s\n",
      "Fold 4 log loss: 0.004635539761769005\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.407591 \t val_loss=0.067954 \t time=0.84s\n",
      "Best model: Epoch 2 \t loss=0.037969 \t val_loss=0.015452 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.013713 \t val_loss=0.009105 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.009382 \t val_loss=0.007125 \t time=0.85s\n",
      "Best model: Epoch 5 \t loss=0.007392 \t val_loss=0.006348 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.006592 \t val_loss=0.005954 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.006143 \t val_loss=0.005783 \t time=1.05s\n",
      "Best model: Epoch 9 \t loss=0.006107 \t val_loss=0.005693 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.005693 \t val_loss=0.005479 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.005518 \t val_loss=0.005426 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.005473 \t val_loss=0.005332 \t time=0.84s\n",
      "Best model: Epoch 18 \t loss=0.005407 \t val_loss=0.005219 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.005219 \t val_loss=0.005183 \t time=0.83s\n",
      "Best model: Epoch 21 \t loss=0.005171 \t val_loss=0.005153 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.005113 \t val_loss=0.005129 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.005021 \t val_loss=0.005075 \t time=0.81s\n",
      "Best model: Epoch 25 \t loss=0.005005 \t val_loss=0.005060 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.004968 \t val_loss=0.005057 \t time=0.82s\n",
      "Best model: Epoch 27 \t loss=0.004938 \t val_loss=0.005033 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.004940 \t val_loss=0.005023 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.004904 \t val_loss=0.005011 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.004899 \t val_loss=0.005006 \t time=0.84s\n",
      "Best model: Epoch 31 \t loss=0.004848 \t val_loss=0.004984 \t time=1.05s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.004638 \t val_loss=0.004923 \t time=0.82s\n",
      "Best model: Epoch 37 \t loss=0.004516 \t val_loss=0.004892 \t time=0.81s\n",
      "Best model: Epoch 38 \t loss=0.004431 \t val_loss=0.004885 \t time=0.83s\n",
      "Best model: Epoch 39 \t loss=0.004355 \t val_loss=0.004874 \t time=0.82s\n",
      "Best model: Epoch 40 \t loss=0.004328 \t val_loss=0.004857 \t time=0.82s\n",
      "Fold 5 log loss: 0.004565935712085979\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.004584021661370486\n",
      "Fold 2 log loss: 0.004678826863347746\n",
      "Fold 3 log loss: 0.004721389854379404\n",
      "Fold 4 log loss: 0.004635539761769005\n",
      "Fold 5 log loss: 0.004565935712085979\n",
      "Std of log loss: 5.7844622531125794e-05\n",
      "Total log loss: 0.00463714329168659\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.410240 \t val_loss=0.074631 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.037365 \t val_loss=0.017122 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.013988 \t val_loss=0.010081 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.009564 \t val_loss=0.007632 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.007518 \t val_loss=0.006520 \t time=0.83s\n",
      "Best model: Epoch 6 \t loss=0.007194 \t val_loss=0.006075 \t time=1.48s\n",
      "Best model: Epoch 7 \t loss=0.006135 \t val_loss=0.005737 \t time=1.22s\n",
      "Best model: Epoch 8 \t loss=0.005950 \t val_loss=0.005692 \t time=1.01s\n",
      "Best model: Epoch 10 \t loss=0.006127 \t val_loss=0.005583 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.005733 \t val_loss=0.005544 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.005619 \t val_loss=0.005487 \t time=0.91s\n",
      "Best model: Epoch 13 \t loss=0.005608 \t val_loss=0.005410 \t time=0.83s\n",
      "Best model: Epoch 14 \t loss=0.005469 \t val_loss=0.005402 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.005390 \t val_loss=0.005361 \t time=0.83s\n",
      "Best model: Epoch 17 \t loss=0.005324 \t val_loss=0.005302 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.005282 \t val_loss=0.005248 \t time=1.01s\n",
      "Best model: Epoch 20 \t loss=0.005170 \t val_loss=0.005155 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.005121 \t val_loss=0.005126 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.005028 \t val_loss=0.005106 \t time=0.86s\n",
      "Best model: Epoch 24 \t loss=0.004984 \t val_loss=0.005046 \t time=0.81s\n",
      "Best model: Epoch 28 \t loss=0.004918 \t val_loss=0.005040 \t time=0.80s\n",
      "Best model: Epoch 29 \t loss=0.004894 \t val_loss=0.005035 \t time=0.81s\n",
      "Best model: Epoch 31 \t loss=0.004870 \t val_loss=0.005012 \t time=0.87s\n",
      "Best model: Epoch 32 \t loss=0.004857 \t val_loss=0.004999 \t time=1.06s\n",
      "Best model: Epoch 35 \t loss=0.004844 \t val_loss=0.004983 \t time=0.85s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.004680 \t val_loss=0.004951 \t time=0.95s\n",
      "Fold 1 log loss: 0.004645941348972966\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.407218 \t val_loss=0.068475 \t time=1.05s\n",
      "Best model: Epoch 2 \t loss=0.037979 \t val_loss=0.016140 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.014340 \t val_loss=0.009121 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.009199 \t val_loss=0.007683 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.007449 \t val_loss=0.006462 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.006489 \t val_loss=0.006036 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.006265 \t val_loss=0.005940 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.005995 \t val_loss=0.005706 \t time=0.84s\n",
      "Best model: Epoch 10 \t loss=0.005815 \t val_loss=0.005689 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.005727 \t val_loss=0.005573 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.005735 \t val_loss=0.005532 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.005531 \t val_loss=0.005460 \t time=1.03s\n",
      "Best model: Epoch 14 \t loss=0.005479 \t val_loss=0.005439 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.005399 \t val_loss=0.005365 \t time=1.01s\n",
      "Best model: Epoch 18 \t loss=0.005277 \t val_loss=0.005320 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.005281 \t val_loss=0.005294 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.005240 \t val_loss=0.005259 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.005146 \t val_loss=0.005244 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.005165 \t val_loss=0.005199 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.005073 \t val_loss=0.005183 \t time=0.82s\n",
      "Best model: Epoch 25 \t loss=0.004992 \t val_loss=0.005148 \t time=0.91s\n",
      "Best model: Epoch 27 \t loss=0.004916 \t val_loss=0.005111 \t time=0.82s\n",
      "Best model: Epoch 28 \t loss=0.004868 \t val_loss=0.005103 \t time=1.06s\n",
      "Best model: Epoch 29 \t loss=0.004896 \t val_loss=0.005083 \t time=1.02s\n",
      "Best model: Epoch 31 \t loss=0.004833 \t val_loss=0.005081 \t time=0.82s\n",
      "Best model: Epoch 33 \t loss=0.004804 \t val_loss=0.005080 \t time=0.84s\n",
      "Best model: Epoch 34 \t loss=0.004819 \t val_loss=0.005047 \t time=1.06s\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 39 \t loss=0.004620 \t val_loss=0.005017 \t time=0.89s\n",
      "Best model: Epoch 40 \t loss=0.004498 \t val_loss=0.004987 \t time=0.82s\n",
      "Fold 2 log loss: 0.0046473394293845924\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.410134 \t val_loss=0.068467 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.037470 \t val_loss=0.016618 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.014102 \t val_loss=0.009521 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.009373 \t val_loss=0.008176 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.007759 \t val_loss=0.006610 \t time=0.83s\n",
      "Best model: Epoch 6 \t loss=0.006604 \t val_loss=0.006176 \t time=0.96s\n",
      "Best model: Epoch 7 \t loss=0.006364 \t val_loss=0.005767 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.006072 \t val_loss=0.005663 \t time=0.94s\n",
      "Best model: Epoch 10 \t loss=0.005986 \t val_loss=0.005613 \t time=0.98s\n",
      "Best model: Epoch 11 \t loss=0.005711 \t val_loss=0.005541 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.005634 \t val_loss=0.005531 \t time=0.84s\n",
      "Best model: Epoch 14 \t loss=0.005469 \t val_loss=0.005350 \t time=0.83s\n",
      "Best model: Epoch 15 \t loss=0.005405 \t val_loss=0.005307 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.005340 \t val_loss=0.005297 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.005242 \t val_loss=0.005248 \t time=0.98s\n",
      "Best model: Epoch 19 \t loss=0.005186 \t val_loss=0.005233 \t time=0.85s\n",
      "Best model: Epoch 20 \t loss=0.005136 \t val_loss=0.005189 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.005103 \t val_loss=0.005188 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.005083 \t val_loss=0.005126 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.005017 \t val_loss=0.005100 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.004937 \t val_loss=0.005070 \t time=0.79s\n",
      "Best model: Epoch 27 \t loss=0.004921 \t val_loss=0.005068 \t time=0.83s\n",
      "Best model: Epoch 28 \t loss=0.004949 \t val_loss=0.005048 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.004882 \t val_loss=0.005047 \t time=0.82s\n",
      "Best model: Epoch 30 \t loss=0.004855 \t val_loss=0.005019 \t time=0.82s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.004591 \t val_loss=0.004963 \t time=0.87s\n",
      "Best model: Epoch 36 \t loss=0.004489 \t val_loss=0.004937 \t time=0.85s\n",
      "Best model: Epoch 37 \t loss=0.004409 \t val_loss=0.004927 \t time=0.84s\n",
      "Best model: Epoch 38 \t loss=0.004331 \t val_loss=0.004912 \t time=0.84s\n",
      "Best model: Epoch 39 \t loss=0.004297 \t val_loss=0.004905 \t time=0.84s\n",
      "Best model: Epoch 40 \t loss=0.004262 \t val_loss=0.004890 \t time=0.81s\n",
      "Fold 3 log loss: 0.004604927845116698\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408760 \t val_loss=0.067429 \t time=0.84s\n",
      "Best model: Epoch 2 \t loss=0.038467 \t val_loss=0.015794 \t time=1.10s\n",
      "Best model: Epoch 3 \t loss=0.013610 \t val_loss=0.009636 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.009155 \t val_loss=0.007408 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.007525 \t val_loss=0.006654 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.006375 \t val_loss=0.005857 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.005977 \t val_loss=0.005712 \t time=1.28s\n",
      "Best model: Epoch 9 \t loss=0.005919 \t val_loss=0.005666 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.005792 \t val_loss=0.005575 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.005640 \t val_loss=0.005514 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.005596 \t val_loss=0.005431 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.005464 \t val_loss=0.005403 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.005342 \t val_loss=0.005366 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.005290 \t val_loss=0.005346 \t time=0.85s\n",
      "Best model: Epoch 18 \t loss=0.005250 \t val_loss=0.005240 \t time=0.89s\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 23 \t loss=0.004966 \t val_loss=0.005169 \t time=1.02s\n",
      "Best model: Epoch 24 \t loss=0.004842 \t val_loss=0.005133 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.004770 \t val_loss=0.005096 \t time=0.85s\n",
      "Best model: Epoch 27 \t loss=0.004686 \t val_loss=0.005080 \t time=0.80s\n",
      "Best model: Epoch 28 \t loss=0.004656 \t val_loss=0.005061 \t time=0.81s\n",
      "Best model: Epoch 29 \t loss=0.004640 \t val_loss=0.005057 \t time=0.80s\n",
      "Best model: Epoch 30 \t loss=0.004560 \t val_loss=0.005050 \t time=0.81s\n",
      "Best model: Epoch 31 \t loss=0.004542 \t val_loss=0.005039 \t time=0.81s\n",
      "Best model: Epoch 32 \t loss=0.004509 \t val_loss=0.005032 \t time=0.80s\n",
      "Best model: Epoch 34 \t loss=0.004434 \t val_loss=0.005019 \t time=0.82s\n",
      "Best model: Epoch 35 \t loss=0.004396 \t val_loss=0.005012 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.004323 \t val_loss=0.005010 \t time=0.86s\n",
      "Best model: Epoch 37 \t loss=0.004278 \t val_loss=0.005009 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.004268 \t val_loss=0.004991 \t time=1.08s\n",
      "Best model: Epoch 40 \t loss=0.004202 \t val_loss=0.004985 \t time=0.80s\n",
      "Fold 4 log loss: 0.00464935920817104\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.409975 \t val_loss=0.069771 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.038194 \t val_loss=0.016563 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.013839 \t val_loss=0.009287 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.008988 \t val_loss=0.007421 \t time=1.10s\n",
      "Best model: Epoch 5 \t loss=0.007356 \t val_loss=0.006568 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.006892 \t val_loss=0.006004 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.006317 \t val_loss=0.005855 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.006086 \t val_loss=0.005772 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.006306 \t val_loss=0.005615 \t time=0.82s\n",
      "Best model: Epoch 10 \t loss=0.005851 \t val_loss=0.005596 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.005681 \t val_loss=0.005449 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.005682 \t val_loss=0.005418 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.005470 \t val_loss=0.005339 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.005378 \t val_loss=0.005274 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.005277 \t val_loss=0.005193 \t time=1.00s\n",
      "Best model: Epoch 20 \t loss=0.005189 \t val_loss=0.005141 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.005177 \t val_loss=0.005122 \t time=0.80s\n",
      "Best model: Epoch 23 \t loss=0.005094 \t val_loss=0.005065 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.005008 \t val_loss=0.005025 \t time=0.80s\n",
      "Best model: Epoch 28 \t loss=0.004864 \t val_loss=0.005014 \t time=0.82s\n",
      "Best model: Epoch 29 \t loss=0.004876 \t val_loss=0.004988 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.004847 \t val_loss=0.004980 \t time=0.96s\n",
      "Best model: Epoch 32 \t loss=0.004823 \t val_loss=0.004978 \t time=1.31s\n",
      "Best model: Epoch 35 \t loss=0.004792 \t val_loss=0.004978 \t time=0.81s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.004586 \t val_loss=0.004922 \t time=0.86s\n",
      "Best model: Epoch 38 \t loss=0.004490 \t val_loss=0.004880 \t time=1.01s\n",
      "Best model: Epoch 39 \t loss=0.004431 \t val_loss=0.004863 \t time=0.85s\n",
      "Best model: Epoch 40 \t loss=0.004344 \t val_loss=0.004855 \t time=0.88s\n",
      "Fold 5 log loss: 0.004583211613077572\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.004645941348972966\n",
      "Fold 2 log loss: 0.0046473394293845924\n",
      "Fold 3 log loss: 0.004604927845116698\n",
      "Fold 4 log loss: 0.00464935920817104\n",
      "Fold 5 log loss: 0.004583211613077572\n",
      "Std of log loss: 2.710513424328941e-05\n",
      "Total log loss: 0.004626154022305223\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.410531 \t val_loss=0.072441 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.037545 \t val_loss=0.018593 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.014359 \t val_loss=0.010631 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.009510 \t val_loss=0.007788 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.007294 \t val_loss=0.006424 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.006961 \t val_loss=0.006059 \t time=1.03s\n",
      "Best model: Epoch 7 \t loss=0.006420 \t val_loss=0.005830 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.006182 \t val_loss=0.005585 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.005824 \t val_loss=0.005547 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.005822 \t val_loss=0.005541 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.005900 \t val_loss=0.005492 \t time=0.81s\n",
      "Best model: Epoch 13 \t loss=0.005629 \t val_loss=0.005403 \t time=0.81s\n",
      "Best model: Epoch 14 \t loss=0.005528 \t val_loss=0.005374 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.005401 \t val_loss=0.005309 \t time=0.79s\n",
      "Best model: Epoch 18 \t loss=0.005323 \t val_loss=0.005261 \t time=0.98s\n",
      "Best model: Epoch 19 \t loss=0.005312 \t val_loss=0.005166 \t time=0.86s\n",
      "Best model: Epoch 21 \t loss=0.005199 \t val_loss=0.005153 \t time=1.09s\n",
      "Best model: Epoch 22 \t loss=0.005116 \t val_loss=0.005135 \t time=0.85s\n",
      "Best model: Epoch 23 \t loss=0.005066 \t val_loss=0.005114 \t time=0.77s\n",
      "Best model: Epoch 24 \t loss=0.005009 \t val_loss=0.005062 \t time=0.77s\n",
      "Best model: Epoch 27 \t loss=0.004975 \t val_loss=0.005031 \t time=0.78s\n",
      "Best model: Epoch 28 \t loss=0.004901 \t val_loss=0.005006 \t time=0.78s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.004613 \t val_loss=0.004925 \t time=0.78s\n",
      "Best model: Epoch 34 \t loss=0.004493 \t val_loss=0.004894 \t time=0.78s\n",
      "Best model: Epoch 35 \t loss=0.004434 \t val_loss=0.004887 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.004365 \t val_loss=0.004865 \t time=0.78s\n",
      "Best model: Epoch 37 \t loss=0.004316 \t val_loss=0.004860 \t time=0.78s\n",
      "Best model: Epoch 38 \t loss=0.004286 \t val_loss=0.004849 \t time=0.80s\n",
      "Best model: Epoch 40 \t loss=0.004195 \t val_loss=0.004834 \t time=0.79s\n",
      "Fold 1 log loss: 0.00452115666100014\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.408895 \t val_loss=0.068768 \t time=0.95s\n",
      "Best model: Epoch 2 \t loss=0.037766 \t val_loss=0.017799 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.013389 \t val_loss=0.010457 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.009244 \t val_loss=0.008402 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.007471 \t val_loss=0.006501 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.006552 \t val_loss=0.006240 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.006222 \t val_loss=0.005894 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.006153 \t val_loss=0.005836 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.005770 \t val_loss=0.005532 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.005592 \t val_loss=0.005494 \t time=1.04s\n",
      "Best model: Epoch 14 \t loss=0.005478 \t val_loss=0.005437 \t time=0.98s\n",
      "Best model: Epoch 15 \t loss=0.005405 \t val_loss=0.005430 \t time=0.98s\n",
      "Best model: Epoch 16 \t loss=0.005354 \t val_loss=0.005395 \t time=0.93s\n",
      "Best model: Epoch 17 \t loss=0.005304 \t val_loss=0.005345 \t time=1.12s\n",
      "Best model: Epoch 18 \t loss=0.005249 \t val_loss=0.005297 \t time=0.84s\n",
      "Best model: Epoch 19 \t loss=0.005244 \t val_loss=0.005264 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.005189 \t val_loss=0.005203 \t time=0.83s\n",
      "Best model: Epoch 24 \t loss=0.005072 \t val_loss=0.005187 \t time=1.04s\n",
      "Best model: Epoch 25 \t loss=0.005016 \t val_loss=0.005170 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.004973 \t val_loss=0.005127 \t time=0.87s\n",
      "Best model: Epoch 29 \t loss=0.004904 \t val_loss=0.005127 \t time=0.84s\n",
      "Best model: Epoch 30 \t loss=0.004862 \t val_loss=0.005125 \t time=0.87s\n",
      "Best model: Epoch 32 \t loss=0.004863 \t val_loss=0.005103 \t time=0.84s\n",
      "Best model: Epoch 33 \t loss=0.004847 \t val_loss=0.005084 \t time=0.83s\n",
      "Best model: Epoch 35 \t loss=0.004841 \t val_loss=0.005083 \t time=0.85s\n",
      "Best model: Epoch 36 \t loss=0.004841 \t val_loss=0.005082 \t time=1.05s\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.004751353804878585\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.409649 \t val_loss=0.074777 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.038346 \t val_loss=0.016165 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.013545 \t val_loss=0.009875 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.009718 \t val_loss=0.007145 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.007804 \t val_loss=0.006742 \t time=0.92s\n",
      "Best model: Epoch 6 \t loss=0.006641 \t val_loss=0.006074 \t time=0.95s\n",
      "Best model: Epoch 7 \t loss=0.006373 \t val_loss=0.005801 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.006021 \t val_loss=0.005749 \t time=1.02s\n",
      "Best model: Epoch 9 \t loss=0.005838 \t val_loss=0.005662 \t time=0.83s\n",
      "Best model: Epoch 10 \t loss=0.005703 \t val_loss=0.005497 \t time=1.19s\n",
      "Best model: Epoch 12 \t loss=0.005684 \t val_loss=0.005463 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.005512 \t val_loss=0.005370 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.005406 \t val_loss=0.005370 \t time=0.84s\n",
      "Best model: Epoch 17 \t loss=0.005375 \t val_loss=0.005295 \t time=1.09s\n",
      "Best model: Epoch 19 \t loss=0.005243 \t val_loss=0.005206 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.005124 \t val_loss=0.005122 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.005039 \t val_loss=0.005100 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.004889 \t val_loss=0.005058 \t time=0.85s\n",
      "Best model: Epoch 28 \t loss=0.004830 \t val_loss=0.005051 \t time=0.84s\n",
      "Best model: Epoch 29 \t loss=0.004824 \t val_loss=0.005026 \t time=0.83s\n",
      "Best model: Epoch 33 \t loss=0.004753 \t val_loss=0.005005 \t time=0.89s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.004582 \t val_loss=0.004960 \t time=0.87s\n",
      "Best model: Epoch 39 \t loss=0.004485 \t val_loss=0.004934 \t time=0.86s\n",
      "Best model: Epoch 40 \t loss=0.004406 \t val_loss=0.004920 \t time=0.84s\n",
      "Fold 3 log loss: 0.004637686254643723\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408468 \t val_loss=0.071267 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.037301 \t val_loss=0.016402 \t time=1.11s\n",
      "Best model: Epoch 3 \t loss=0.014577 \t val_loss=0.010089 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.009475 \t val_loss=0.007715 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.008199 \t val_loss=0.007014 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.006612 \t val_loss=0.005984 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.006300 \t val_loss=0.005781 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.006039 \t val_loss=0.005733 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.005769 \t val_loss=0.005581 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.005705 \t val_loss=0.005530 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.005595 \t val_loss=0.005512 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.005575 \t val_loss=0.005505 \t time=0.84s\n",
      "Best model: Epoch 14 \t loss=0.005491 \t val_loss=0.005411 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.005392 \t val_loss=0.005400 \t time=0.82s\n",
      "Best model: Epoch 17 \t loss=0.005333 \t val_loss=0.005342 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.005340 \t val_loss=0.005314 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.005225 \t val_loss=0.005259 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.005167 \t val_loss=0.005238 \t time=0.80s\n",
      "Best model: Epoch 21 \t loss=0.005184 \t val_loss=0.005218 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.005041 \t val_loss=0.005181 \t time=0.83s\n",
      "Best model: Epoch 24 \t loss=0.005026 \t val_loss=0.005153 \t time=0.86s\n",
      "Best model: Epoch 26 \t loss=0.004990 \t val_loss=0.005148 \t time=0.81s\n",
      "Best model: Epoch 29 \t loss=0.004923 \t val_loss=0.005120 \t time=0.80s\n",
      "Best model: Epoch 30 \t loss=0.004884 \t val_loss=0.005105 \t time=0.82s\n",
      "Best model: Epoch 31 \t loss=0.004871 \t val_loss=0.005091 \t time=0.81s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.004648 \t val_loss=0.005044 \t time=0.84s\n",
      "Best model: Epoch 37 \t loss=0.004527 \t val_loss=0.005019 \t time=0.81s\n",
      "Best model: Epoch 38 \t loss=0.004492 \t val_loss=0.004990 \t time=0.83s\n",
      "Best model: Epoch 39 \t loss=0.004428 \t val_loss=0.004990 \t time=1.12s\n",
      "Best model: Epoch 40 \t loss=0.004371 \t val_loss=0.004977 \t time=0.90s\n",
      "Fold 4 log loss: 0.004644217038272317\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.408006 \t val_loss=0.069958 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.038563 \t val_loss=0.019963 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.014470 \t val_loss=0.009272 \t time=1.03s\n",
      "Best model: Epoch 4 \t loss=0.009151 \t val_loss=0.007178 \t time=0.89s\n",
      "Best model: Epoch 5 \t loss=0.007781 \t val_loss=0.006495 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.006602 \t val_loss=0.006350 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.006180 \t val_loss=0.005783 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.006125 \t val_loss=0.005668 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.006190 \t val_loss=0.005634 \t time=1.24s\n",
      "Best model: Epoch 11 \t loss=0.005828 \t val_loss=0.005493 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.005930 \t val_loss=0.005492 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.005579 \t val_loss=0.005428 \t time=1.04s\n",
      "Best model: Epoch 14 \t loss=0.005568 \t val_loss=0.005379 \t time=0.90s\n",
      "Best model: Epoch 15 \t loss=0.005501 \t val_loss=0.005308 \t time=1.10s\n",
      "Best model: Epoch 18 \t loss=0.005330 \t val_loss=0.005283 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.005236 \t val_loss=0.005197 \t time=0.93s\n",
      "Best model: Epoch 20 \t loss=0.005319 \t val_loss=0.005192 \t time=0.84s\n",
      "Best model: Epoch 21 \t loss=0.005190 \t val_loss=0.005137 \t time=0.85s\n",
      "Best model: Epoch 22 \t loss=0.005133 \t val_loss=0.005107 \t time=1.01s\n",
      "Best model: Epoch 24 \t loss=0.005026 \t val_loss=0.005076 \t time=0.80s\n",
      "Best model: Epoch 27 \t loss=0.004968 \t val_loss=0.005028 \t time=1.00s\n",
      "Best model: Epoch 29 \t loss=0.004911 \t val_loss=0.005008 \t time=0.81s\n",
      "Best model: Epoch 30 \t loss=0.004871 \t val_loss=0.004991 \t time=0.79s\n",
      "Best model: Epoch 34 \t loss=0.004853 \t val_loss=0.004986 \t time=0.81s\n",
      "Best model: Epoch 35 \t loss=0.004845 \t val_loss=0.004983 \t time=0.81s\n",
      "Best model: Epoch 37 \t loss=0.004841 \t val_loss=0.004966 \t time=0.82s\n",
      "Fold 5 log loss: 0.004690238151688292\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.00452115666100014\n",
      "Fold 2 log loss: 0.004751353804878585\n",
      "Fold 3 log loss: 0.004637686254643723\n",
      "Fold 4 log loss: 0.004644217038272317\n",
      "Fold 5 log loss: 0.004690238151688292\n",
      "Std of log loss: 7.572537114869211e-05\n",
      "Total log loss: 0.004648931537112937\n",
      "Total log loss in Non targets: 0.004603322453142659\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2]\n",
    "\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "nontarget_oof = np.zeros([len(fn_train),fn_nontargets.shape[1]])\n",
    "nontarget_pred = np.zeros([len(fn_test),fn_nontargets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],[])\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_nontargets, fn_test, seed_, fn_train.shape[1], fn_nontargets.shape[1],[])\n",
    "    nontarget_oof += oof / len(seeds)\n",
    "    nontarget_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in Non targets: {}\".format(mean_log_loss(oof_targets, nontarget_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:55:18.420924Z",
     "iopub.status.busy": "2020-10-11T09:55:18.419654Z",
     "iopub.status.idle": "2020-10-11T09:55:18.692654Z",
     "shell.execute_reply": "2020-10-11T09:55:18.692089Z"
    },
    "papermill": {
     "duration": 0.638171,
     "end_time": "2020-10-11T09:55:18.692776",
     "exception": false,
     "start_time": "2020-10-11T09:55:18.054605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = f_train.copy()\n",
    "n_test = f_test.copy()\n",
    "\n",
    "n_train[\"target_sum\"] = target_oof.sum(axis=1)\n",
    "n_train[\"nontarget_sum\"] = nontarget_oof.sum(axis=1)\n",
    "n_test[\"target_sum\"] = target_pred.sum(axis=1)\n",
    "n_test.loc[noncons_test_index, \"target_sum\"] = 0\n",
    "n_test[\"nontarget_sum\"] = nontarget_pred.sum(axis=1)\n",
    "n_test.loc[noncons_test_index, \"nontarget_sum\"] = 0\n",
    "\n",
    "n_train = n_train.to_numpy()\n",
    "n_test = n_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T09:55:19.440358Z",
     "iopub.status.busy": "2020-10-11T09:55:19.438659Z",
     "iopub.status.idle": "2020-10-11T10:01:50.777263Z",
     "shell.execute_reply": "2020-10-11T10:01:50.777840Z"
    },
    "papermill": {
     "duration": 391.728387,
     "end_time": "2020-10-11T10:01:50.777997",
     "exception": false,
     "start_time": "2020-10-11T09:55:19.049610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412570 \t val_loss=0.079392 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.048495 \t val_loss=0.028459 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.027157 \t val_loss=0.023130 \t time=1.02s\n",
      "Best model: Epoch 4 \t loss=0.023034 \t val_loss=0.021136 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021218 \t val_loss=0.019953 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.020353 \t val_loss=0.019359 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.019936 \t val_loss=0.019030 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019632 \t val_loss=0.018400 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.019037 \t val_loss=0.018215 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018509 \t val_loss=0.017978 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018233 \t val_loss=0.017576 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.017787 \t val_loss=0.017303 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017529 \t val_loss=0.017197 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.017372 \t val_loss=0.017128 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.017040 \t val_loss=0.016987 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.017005 \t val_loss=0.016956 \t time=1.04s\n",
      "Best model: Epoch 17 \t loss=0.016885 \t val_loss=0.016852 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.016547 \t val_loss=0.016724 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016268 \t val_loss=0.016610 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.016022 \t val_loss=0.016539 \t time=1.16s\n",
      "Best model: Epoch 23 \t loss=0.015902 \t val_loss=0.016504 \t time=0.78s\n",
      "Best model: Epoch 24 \t loss=0.015692 \t val_loss=0.016458 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.015659 \t val_loss=0.016427 \t time=0.79s\n",
      "Best model: Epoch 26 \t loss=0.015533 \t val_loss=0.016368 \t time=0.80s\n",
      "Best model: Epoch 27 \t loss=0.015397 \t val_loss=0.016348 \t time=1.14s\n",
      "Best model: Epoch 29 \t loss=0.015301 \t val_loss=0.016323 \t time=0.79s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014424 \t val_loss=0.016161 \t time=1.00s\n",
      "Best model: Epoch 35 \t loss=0.014175 \t val_loss=0.016119 \t time=0.83s\n",
      "Best model: Epoch 36 \t loss=0.013890 \t val_loss=0.016066 \t time=0.79s\n",
      "Best model: Epoch 38 \t loss=0.013644 \t val_loss=0.016046 \t time=0.83s\n",
      "Best model: Epoch 39 \t loss=0.013411 \t val_loss=0.016043 \t time=0.80s\n",
      "Fold 1 log loss: 0.016142502900119475\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.413642 \t val_loss=0.085341 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.048676 \t val_loss=0.027775 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.026977 \t val_loss=0.022568 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.023303 \t val_loss=0.020956 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.021624 \t val_loss=0.019811 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.020317 \t val_loss=0.018894 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019387 \t val_loss=0.018210 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.018844 \t val_loss=0.018054 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.018732 \t val_loss=0.017797 \t time=1.06s\n",
      "Best model: Epoch 11 \t loss=0.018218 \t val_loss=0.017564 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.017850 \t val_loss=0.017298 \t time=0.86s\n",
      "Best model: Epoch 13 \t loss=0.017570 \t val_loss=0.017212 \t time=0.96s\n",
      "Best model: Epoch 14 \t loss=0.017396 \t val_loss=0.017070 \t time=1.01s\n",
      "Best model: Epoch 15 \t loss=0.017195 \t val_loss=0.016972 \t time=0.84s\n",
      "Best model: Epoch 16 \t loss=0.017005 \t val_loss=0.016818 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.016896 \t val_loss=0.016755 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016671 \t val_loss=0.016685 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016253 \t val_loss=0.016541 \t time=0.80s\n",
      "Best model: Epoch 21 \t loss=0.016148 \t val_loss=0.016541 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016074 \t val_loss=0.016506 \t time=0.94s\n",
      "Best model: Epoch 23 \t loss=0.015860 \t val_loss=0.016434 \t time=0.83s\n",
      "Best model: Epoch 24 \t loss=0.015880 \t val_loss=0.016318 \t time=0.80s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014873 \t val_loss=0.016139 \t time=0.98s\n",
      "Best model: Epoch 30 \t loss=0.014596 \t val_loss=0.016068 \t time=0.81s\n",
      "Best model: Epoch 31 \t loss=0.014394 \t val_loss=0.016044 \t time=0.79s\n",
      "Best model: Epoch 32 \t loss=0.014261 \t val_loss=0.016026 \t time=0.84s\n",
      "Best model: Epoch 33 \t loss=0.014116 \t val_loss=0.016014 \t time=0.84s\n",
      "Best model: Epoch 34 \t loss=0.013939 \t val_loss=0.015970 \t time=1.07s\n",
      "Fold 2 log loss: 0.0160377863447536\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.414358 \t val_loss=0.077996 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.049218 \t val_loss=0.031222 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027631 \t val_loss=0.023587 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.023163 \t val_loss=0.020660 \t time=1.06s\n",
      "Best model: Epoch 5 \t loss=0.021581 \t val_loss=0.019780 \t time=0.84s\n",
      "Best model: Epoch 6 \t loss=0.020559 \t val_loss=0.019376 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.019835 \t val_loss=0.018695 \t time=0.94s\n",
      "Best model: Epoch 8 \t loss=0.019243 \t val_loss=0.018336 \t time=1.37s\n",
      "Best model: Epoch 9 \t loss=0.019060 \t val_loss=0.018131 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.018637 \t val_loss=0.017851 \t time=1.10s\n",
      "Best model: Epoch 11 \t loss=0.018469 \t val_loss=0.017602 \t time=0.80s\n",
      "Best model: Epoch 12 \t loss=0.018120 \t val_loss=0.017509 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.017646 \t val_loss=0.017303 \t time=0.84s\n",
      "Best model: Epoch 14 \t loss=0.017540 \t val_loss=0.017302 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017249 \t val_loss=0.016853 \t time=1.16s\n",
      "Best model: Epoch 17 \t loss=0.016865 \t val_loss=0.016774 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016744 \t val_loss=0.016649 \t time=0.84s\n",
      "Best model: Epoch 19 \t loss=0.016486 \t val_loss=0.016570 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.016277 \t val_loss=0.016493 \t time=1.11s\n",
      "Best model: Epoch 21 \t loss=0.016212 \t val_loss=0.016478 \t time=0.82s\n",
      "Best model: Epoch 22 \t loss=0.016081 \t val_loss=0.016417 \t time=0.80s\n",
      "Best model: Epoch 24 \t loss=0.015897 \t val_loss=0.016323 \t time=0.80s\n",
      "Best model: Epoch 26 \t loss=0.015633 \t val_loss=0.016322 \t time=0.83s\n",
      "Best model: Epoch 27 \t loss=0.015610 \t val_loss=0.016295 \t time=0.90s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016212 \t time=0.82s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014601 \t val_loss=0.016083 \t time=0.81s\n",
      "Best model: Epoch 35 \t loss=0.014238 \t val_loss=0.016035 \t time=0.80s\n",
      "Best model: Epoch 36 \t loss=0.014060 \t val_loss=0.016023 \t time=0.81s\n",
      "Best model: Epoch 37 \t loss=0.013968 \t val_loss=0.015973 \t time=0.79s\n",
      "Best model: Epoch 38 \t loss=0.013756 \t val_loss=0.015944 \t time=0.79s\n",
      "Fold 3 log loss: 0.015934020921781738\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413763 \t val_loss=0.087454 \t time=0.93s\n",
      "Best model: Epoch 2 \t loss=0.048704 \t val_loss=0.027694 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.027260 \t val_loss=0.022759 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023344 \t val_loss=0.021063 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021433 \t val_loss=0.019379 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.020359 \t val_loss=0.018881 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019655 \t val_loss=0.018304 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019308 \t val_loss=0.017993 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.018979 \t val_loss=0.017849 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018633 \t val_loss=0.017480 \t time=1.02s\n",
      "Best model: Epoch 11 \t loss=0.018213 \t val_loss=0.017313 \t time=0.81s\n",
      "Best model: Epoch 13 \t loss=0.017759 \t val_loss=0.017084 \t time=0.81s\n",
      "Best model: Epoch 14 \t loss=0.017507 \t val_loss=0.016880 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017051 \t val_loss=0.016644 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.016755 \t val_loss=0.016558 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016686 \t val_loss=0.016461 \t time=0.81s\n",
      "Best model: Epoch 19 \t loss=0.016464 \t val_loss=0.016457 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016256 \t val_loss=0.016395 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016157 \t val_loss=0.016327 \t time=0.80s\n",
      "Best model: Epoch 22 \t loss=0.016075 \t val_loss=0.016246 \t time=1.05s\n",
      "Best model: Epoch 23 \t loss=0.015952 \t val_loss=0.016212 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.015661 \t val_loss=0.016198 \t time=0.81s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.015432 \t val_loss=0.016197 \t time=0.81s\n",
      "Best model: Epoch 30 \t loss=0.014813 \t val_loss=0.015997 \t time=0.81s\n",
      "Best model: Epoch 31 \t loss=0.014554 \t val_loss=0.015963 \t time=0.80s\n",
      "Best model: Epoch 32 \t loss=0.014323 \t val_loss=0.015899 \t time=0.96s\n",
      "Best model: Epoch 33 \t loss=0.014157 \t val_loss=0.015898 \t time=1.06s\n",
      "Best model: Epoch 35 \t loss=0.013953 \t val_loss=0.015889 \t time=0.90s\n",
      "Best model: Epoch 36 \t loss=0.013790 \t val_loss=0.015857 \t time=1.52s\n",
      "Best model: Epoch 38 \t loss=0.013516 \t val_loss=0.015835 \t time=0.91s\n",
      "Fold 4 log loss: 0.015895623376585934\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.417393 \t val_loss=0.081148 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.050896 \t val_loss=0.028826 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027548 \t val_loss=0.022883 \t time=1.33s\n",
      "Best model: Epoch 4 \t loss=0.023109 \t val_loss=0.021051 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021366 \t val_loss=0.019945 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020782 \t val_loss=0.019356 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.019952 \t val_loss=0.018840 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.019294 \t val_loss=0.018441 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.019089 \t val_loss=0.018176 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018720 \t val_loss=0.017938 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.018408 \t val_loss=0.017667 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.017754 \t val_loss=0.017330 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017428 \t val_loss=0.017174 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.017019 \t val_loss=0.016982 \t time=0.85s\n",
      "Best model: Epoch 17 \t loss=0.016905 \t val_loss=0.016981 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016733 \t val_loss=0.016837 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016389 \t val_loss=0.016672 \t time=0.84s\n",
      "Best model: Epoch 21 \t loss=0.016189 \t val_loss=0.016631 \t time=0.83s\n",
      "Best model: Epoch 22 \t loss=0.016034 \t val_loss=0.016552 \t time=0.83s\n",
      "Best model: Epoch 23 \t loss=0.015936 \t val_loss=0.016539 \t time=0.83s\n",
      "Best model: Epoch 26 \t loss=0.015631 \t val_loss=0.016474 \t time=0.83s\n",
      "Best model: Epoch 27 \t loss=0.015565 \t val_loss=0.016440 \t time=1.01s\n",
      "Best model: Epoch 28 \t loss=0.015532 \t val_loss=0.016396 \t time=0.97s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014577 \t val_loss=0.016246 \t time=0.78s\n",
      "Best model: Epoch 34 \t loss=0.014346 \t val_loss=0.016132 \t time=0.78s\n",
      "Best model: Epoch 35 \t loss=0.014076 \t val_loss=0.016111 \t time=0.79s\n",
      "Best model: Epoch 36 \t loss=0.013947 \t val_loss=0.016081 \t time=0.79s\n",
      "Best model: Epoch 37 \t loss=0.013819 \t val_loss=0.016078 \t time=0.79s\n",
      "Best model: Epoch 40 \t loss=0.013443 \t val_loss=0.016075 \t time=0.80s\n",
      "Fold 5 log loss: 0.016082964565937823\n",
      "Seed 10\n",
      "Fold 1 log loss: 0.016142502900119475\n",
      "Fold 2 log loss: 0.0160377863447536\n",
      "Fold 3 log loss: 0.015934020921781738\n",
      "Fold 4 log loss: 0.015895623376585934\n",
      "Fold 5 log loss: 0.016082964565937823\n",
      "Std of log loss: 9.180382359533066e-05\n",
      "Total log loss: 0.016018575813212286\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412069 \t val_loss=0.079535 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.048858 \t val_loss=0.030787 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027057 \t val_loss=0.022749 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.023244 \t val_loss=0.021140 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021305 \t val_loss=0.019774 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020204 \t val_loss=0.019063 \t time=1.00s\n",
      "Best model: Epoch 8 \t loss=0.019187 \t val_loss=0.018357 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.018775 \t val_loss=0.017934 \t time=0.83s\n",
      "Best model: Epoch 10 \t loss=0.018460 \t val_loss=0.017686 \t time=0.99s\n",
      "Best model: Epoch 11 \t loss=0.018052 \t val_loss=0.017559 \t time=1.00s\n",
      "Best model: Epoch 13 \t loss=0.017634 \t val_loss=0.017195 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017190 \t val_loss=0.017007 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.016988 \t val_loss=0.016926 \t time=1.18s\n",
      "Best model: Epoch 17 \t loss=0.016782 \t val_loss=0.016853 \t time=0.83s\n",
      "Best model: Epoch 19 \t loss=0.016383 \t val_loss=0.016654 \t time=1.24s\n",
      "Best model: Epoch 20 \t loss=0.016180 \t val_loss=0.016612 \t time=0.87s\n",
      "Best model: Epoch 21 \t loss=0.016086 \t val_loss=0.016527 \t time=0.84s\n",
      "Best model: Epoch 22 \t loss=0.015915 \t val_loss=0.016480 \t time=0.83s\n",
      "Best model: Epoch 23 \t loss=0.015824 \t val_loss=0.016458 \t time=0.80s\n",
      "Best model: Epoch 24 \t loss=0.015773 \t val_loss=0.016349 \t time=0.82s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014706 \t val_loss=0.016199 \t time=1.01s\n",
      "Best model: Epoch 30 \t loss=0.014508 \t val_loss=0.016156 \t time=1.07s\n",
      "Best model: Epoch 31 \t loss=0.014364 \t val_loss=0.016142 \t time=0.85s\n",
      "Best model: Epoch 32 \t loss=0.014205 \t val_loss=0.016118 \t time=0.86s\n",
      "Best model: Epoch 34 \t loss=0.013908 \t val_loss=0.016090 \t time=0.78s\n",
      "Best model: Epoch 36 \t loss=0.013697 \t val_loss=0.016079 \t time=0.83s\n",
      "Best model: Epoch 37 \t loss=0.013576 \t val_loss=0.016062 \t time=0.80s\n",
      "Fold 1 log loss: 0.01615493274552058\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.415704 \t val_loss=0.075853 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.048697 \t val_loss=0.028997 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.027652 \t val_loss=0.023092 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.023242 \t val_loss=0.021373 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021371 \t val_loss=0.019539 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.020415 \t val_loss=0.019146 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.019942 \t val_loss=0.018597 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019294 \t val_loss=0.018472 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018909 \t val_loss=0.018013 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.018365 \t val_loss=0.017697 \t time=0.84s\n",
      "Best model: Epoch 11 \t loss=0.018057 \t val_loss=0.017483 \t time=0.83s\n",
      "Best model: Epoch 13 \t loss=0.017756 \t val_loss=0.017203 \t time=1.17s\n",
      "Best model: Epoch 14 \t loss=0.017371 \t val_loss=0.017164 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.017161 \t val_loss=0.017033 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.017189 \t val_loss=0.016881 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.016754 \t val_loss=0.016706 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.016593 \t val_loss=0.016651 \t time=0.80s\n",
      "Best model: Epoch 19 \t loss=0.016397 \t val_loss=0.016597 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.016218 \t val_loss=0.016596 \t time=0.83s\n",
      "Best model: Epoch 21 \t loss=0.016121 \t val_loss=0.016539 \t time=1.02s\n",
      "Best model: Epoch 22 \t loss=0.015953 \t val_loss=0.016427 \t time=0.80s\n",
      "Best model: Epoch 24 \t loss=0.015780 \t val_loss=0.016344 \t time=1.00s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014823 \t val_loss=0.016147 \t time=0.80s\n",
      "Best model: Epoch 30 \t loss=0.014559 \t val_loss=0.016079 \t time=0.80s\n",
      "Best model: Epoch 31 \t loss=0.014336 \t val_loss=0.016065 \t time=0.79s\n",
      "Best model: Epoch 33 \t loss=0.014003 \t val_loss=0.016040 \t time=0.79s\n",
      "Best model: Epoch 34 \t loss=0.013938 \t val_loss=0.016035 \t time=0.95s\n",
      "Best model: Epoch 38 \t loss=0.013469 \t val_loss=0.016032 \t time=0.79s\n",
      "Best model: Epoch 39 \t loss=0.013339 \t val_loss=0.016009 \t time=0.81s\n",
      "Fold 2 log loss: 0.016075429269813892\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413196 \t val_loss=0.075675 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.049461 \t val_loss=0.028360 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.026829 \t val_loss=0.022723 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.023408 \t val_loss=0.020974 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.021254 \t val_loss=0.019569 \t time=1.21s\n",
      "Best model: Epoch 6 \t loss=0.020424 \t val_loss=0.019130 \t time=1.11s\n",
      "Best model: Epoch 7 \t loss=0.020016 \t val_loss=0.019076 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019359 \t val_loss=0.018494 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.018826 \t val_loss=0.018011 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.018864 \t val_loss=0.017852 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.018372 \t val_loss=0.017573 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.018104 \t val_loss=0.017544 \t time=1.05s\n",
      "Best model: Epoch 13 \t loss=0.017747 \t val_loss=0.017235 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.017431 \t val_loss=0.017053 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.017122 \t val_loss=0.016797 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.016831 \t val_loss=0.016764 \t time=0.96s\n",
      "Best model: Epoch 18 \t loss=0.016624 \t val_loss=0.016622 \t time=1.00s\n",
      "Best model: Epoch 19 \t loss=0.016396 \t val_loss=0.016522 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016429 \t val_loss=0.016513 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016253 \t val_loss=0.016384 \t time=0.82s\n",
      "Best model: Epoch 24 \t loss=0.015858 \t val_loss=0.016335 \t time=0.81s\n",
      "Best model: Epoch 25 \t loss=0.015760 \t val_loss=0.016311 \t time=0.80s\n",
      "Best model: Epoch 27 \t loss=0.015515 \t val_loss=0.016287 \t time=0.80s\n",
      "Best model: Epoch 29 \t loss=0.015394 \t val_loss=0.016234 \t time=0.81s\n",
      "Best model: Epoch 30 \t loss=0.015363 \t val_loss=0.016224 \t time=1.04s\n",
      "Best model: Epoch 32 \t loss=0.015191 \t val_loss=0.016216 \t time=0.80s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014407 \t val_loss=0.016055 \t time=0.80s\n",
      "Best model: Epoch 38 \t loss=0.014106 \t val_loss=0.015999 \t time=0.81s\n",
      "Best model: Epoch 39 \t loss=0.013912 \t val_loss=0.015942 \t time=0.81s\n",
      "Fold 3 log loss: 0.015941438654431324\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.412061 \t val_loss=0.077500 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.049361 \t val_loss=0.027828 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.026833 \t val_loss=0.022177 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023168 \t val_loss=0.020657 \t time=1.02s\n",
      "Best model: Epoch 5 \t loss=0.021375 \t val_loss=0.019345 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.020578 \t val_loss=0.018904 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019770 \t val_loss=0.018503 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.019489 \t val_loss=0.018260 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018929 \t val_loss=0.017622 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.018467 \t val_loss=0.017303 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.017916 \t val_loss=0.017105 \t time=1.01s\n",
      "Best model: Epoch 13 \t loss=0.017784 \t val_loss=0.017062 \t time=0.82s\n",
      "Best model: Epoch 14 \t loss=0.017451 \t val_loss=0.016900 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017223 \t val_loss=0.016781 \t time=1.23s\n",
      "Best model: Epoch 16 \t loss=0.016977 \t val_loss=0.016622 \t time=0.86s\n",
      "Best model: Epoch 17 \t loss=0.016766 \t val_loss=0.016595 \t time=1.05s\n",
      "Best model: Epoch 18 \t loss=0.016521 \t val_loss=0.016487 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.016418 \t val_loss=0.016337 \t time=0.83s\n",
      "Best model: Epoch 22 \t loss=0.015980 \t val_loss=0.016288 \t time=0.82s\n",
      "Best model: Epoch 23 \t loss=0.015787 \t val_loss=0.016279 \t time=0.81s\n",
      "Best model: Epoch 24 \t loss=0.015790 \t val_loss=0.016177 \t time=1.06s\n",
      "Best model: Epoch 26 \t loss=0.015664 \t val_loss=0.016161 \t time=0.81s\n",
      "Best model: Epoch 27 \t loss=0.015507 \t val_loss=0.016133 \t time=0.80s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016123 \t time=0.79s\n",
      "Best model: Epoch 30 \t loss=0.015304 \t val_loss=0.016104 \t time=0.79s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014414 \t val_loss=0.015969 \t time=0.84s\n",
      "Best model: Epoch 36 \t loss=0.014174 \t val_loss=0.015925 \t time=1.06s\n",
      "Best model: Epoch 37 \t loss=0.013965 \t val_loss=0.015891 \t time=0.86s\n",
      "Best model: Epoch 38 \t loss=0.013792 \t val_loss=0.015878 \t time=1.14s\n",
      "Best model: Epoch 39 \t loss=0.013702 \t val_loss=0.015844 \t time=0.85s\n",
      "Best model: Epoch 40 \t loss=0.013524 \t val_loss=0.015838 \t time=0.82s\n",
      "Fold 4 log loss: 0.01590136364112784\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413803 \t val_loss=0.084744 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.049558 \t val_loss=0.028844 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.027513 \t val_loss=0.022710 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.023101 \t val_loss=0.021033 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.021639 \t val_loss=0.020074 \t time=0.98s\n",
      "Best model: Epoch 6 \t loss=0.020228 \t val_loss=0.019115 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.019759 \t val_loss=0.018750 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.019294 \t val_loss=0.018271 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.019048 \t val_loss=0.018161 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.018587 \t val_loss=0.017823 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.018295 \t val_loss=0.017749 \t time=0.80s\n",
      "Best model: Epoch 12 \t loss=0.017939 \t val_loss=0.017443 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.017462 \t val_loss=0.017316 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.017221 \t val_loss=0.017033 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.016908 \t val_loss=0.016896 \t time=0.78s\n",
      "Best model: Epoch 18 \t loss=0.016691 \t val_loss=0.016894 \t time=1.00s\n",
      "Best model: Epoch 19 \t loss=0.016465 \t val_loss=0.016690 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.016222 \t val_loss=0.016636 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.015990 \t val_loss=0.016529 \t time=0.81s\n",
      "Best model: Epoch 23 \t loss=0.015844 \t val_loss=0.016526 \t time=0.80s\n",
      "Best model: Epoch 24 \t loss=0.015719 \t val_loss=0.016494 \t time=0.80s\n",
      "Best model: Epoch 25 \t loss=0.015622 \t val_loss=0.016483 \t time=0.83s\n",
      "Best model: Epoch 26 \t loss=0.015530 \t val_loss=0.016430 \t time=0.79s\n",
      "Best model: Epoch 27 \t loss=0.015495 \t val_loss=0.016375 \t time=0.95s\n",
      "Best model: Epoch 31 \t loss=0.015145 \t val_loss=0.016373 \t time=0.91s\n",
      "Best model: Epoch 34 \t loss=0.015028 \t val_loss=0.016321 \t time=0.79s\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 39 \t loss=0.014158 \t val_loss=0.016217 \t time=1.10s\n",
      "Best model: Epoch 40 \t loss=0.013933 \t val_loss=0.016166 \t time=0.97s\n",
      "Fold 5 log loss: 0.01617105449218427\n",
      "Seed 30\n",
      "Fold 1 log loss: 0.01615493274552058\n",
      "Fold 2 log loss: 0.016075429269813892\n",
      "Fold 3 log loss: 0.015941438654431324\n",
      "Fold 4 log loss: 0.01590136364112784\n",
      "Fold 5 log loss: 0.01617105449218427\n",
      "Std of log loss: 0.0001097117745792406\n",
      "Total log loss: 0.01604883698112584\n",
      "Total log loss: 0.01594933817953661\n"
     ]
    }
   ],
   "source": [
    "oof_final = np.zeros([len(n_train),fn_targets.shape[1]])\n",
    "pred_final = np.zeros([len(n_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [10, 30]\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(n_train, fn_targets, n_test, seed_, n_train.shape[1], fn_targets.shape[1], [])\n",
    "    oof_final += oof / len(seeds)\n",
    "    pred_final += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T10:01:51.704992Z",
     "iopub.status.busy": "2020-10-11T10:01:51.704099Z",
     "iopub.status.idle": "2020-10-11T10:01:57.508288Z",
     "shell.execute_reply": "2020-10-11T10:01:57.507752Z"
    },
    "papermill": {
     "duration": 6.272127,
     "end_time": "2020-10-11T10:01:57.508403",
     "exception": false,
     "start_time": "2020-10-11T10:01:51.236276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014699591600086982\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = oof_final\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T10:01:58.499373Z",
     "iopub.status.busy": "2020-10-11T10:01:58.498477Z",
     "iopub.status.idle": "2020-10-11T10:02:01.151276Z",
     "shell.execute_reply": "2020-10-11T10:02:01.150100Z"
    },
    "papermill": {
     "duration": 3.124376,
     "end_time": "2020-10-11T10:02:01.151398",
     "exception": false,
     "start_time": "2020-10-11T10:01:58.027022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = pred_final\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.453535,
     "end_time": "2020-10-11T10:02:02.068460",
     "exception": false,
     "start_time": "2020-10-11T10:02:01.614925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1630.076907,
   "end_time": "2020-10-11T10:02:03.036885",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-11T09:34:52.959978",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
