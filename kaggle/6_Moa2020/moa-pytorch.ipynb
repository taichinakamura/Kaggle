{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019137,
     "end_time": "2020-10-09T09:51:25.221885",
     "exception": false,
     "start_time": "2020-10-09T09:51:25.202748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- add pca and remove original c vars to delete correlation\n",
    "- remove g vars by variance threshold\n",
    "- change model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:25.267956Z",
     "iopub.status.busy": "2020-10-09T09:51:25.267151Z",
     "iopub.status.idle": "2020-10-09T09:51:33.553806Z",
     "shell.execute_reply": "2020-10-09T09:51:33.552207Z"
    },
    "papermill": {
     "duration": 8.31384,
     "end_time": "2020-10-09T09:51:33.553948",
     "exception": false,
     "start_time": "2020-10-09T09:51:25.240108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "sys.path.append('../input/lookahead/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lookahead import Lookahead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:33.604305Z",
     "iopub.status.busy": "2020-10-09T09:51:33.602142Z",
     "iopub.status.idle": "2020-10-09T09:51:39.782325Z",
     "shell.execute_reply": "2020-10-09T09:51:39.781107Z"
    },
    "papermill": {
     "duration": 6.208512,
     "end_time": "2020-10-09T09:51:39.782452",
     "exception": false,
     "start_time": "2020-10-09T09:51:33.573940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:39.826293Z",
     "iopub.status.busy": "2020-10-09T09:51:39.825551Z",
     "iopub.status.idle": "2020-10-09T09:51:39.830465Z",
     "shell.execute_reply": "2020-10-09T09:51:39.829686Z"
    },
    "papermill": {
     "duration": 0.030577,
     "end_time": "2020-10-09T09:51:39.830585",
     "exception": false,
     "start_time": "2020-10-09T09:51:39.800008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:39.877459Z",
     "iopub.status.busy": "2020-10-09T09:51:39.876556Z",
     "iopub.status.idle": "2020-10-09T09:51:39.979973Z",
     "shell.execute_reply": "2020-10-09T09:51:39.979369Z"
    },
    "papermill": {
     "duration": 0.130378,
     "end_time": "2020-10-09T09:51:39.980103",
     "exception": false,
     "start_time": "2020-10-09T09:51:39.849725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018477,
     "end_time": "2020-10-09T09:51:40.016452",
     "exception": false,
     "start_time": "2020-10-09T09:51:39.997975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:40.069454Z",
     "iopub.status.busy": "2020-10-09T09:51:40.068304Z",
     "iopub.status.idle": "2020-10-09T09:51:40.387787Z",
     "shell.execute_reply": "2020-10-09T09:51:40.387220Z"
    },
    "papermill": {
     "duration": 0.353204,
     "end_time": "2020-10-09T09:51:40.387918",
     "exception": false,
     "start_time": "2020-10-09T09:51:40.034714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[train.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "targets = targets[targets.index.isin(cons_train_index)].copy().reset_index(drop=True)\n",
    "non_targets = non_targets[non_targets.index.isin(cons_train_index)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:40.463128Z",
     "iopub.status.busy": "2020-10-09T09:51:40.462033Z",
     "iopub.status.idle": "2020-10-09T09:51:40.476040Z",
     "shell.execute_reply": "2020-10-09T09:51:40.476598Z"
    },
    "papermill": {
     "duration": 0.069615,
     "end_time": "2020-10-09T09:51:40.476760",
     "exception": false,
     "start_time": "2020-10-09T09:51:40.407145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_target_feats = [i for i in non_targets.columns if i != \"sig_id\"]\n",
    "nontarget_dists = pd.DataFrame(np.sum(non_targets[non_target_feats])).reset_index(drop=False)\n",
    "nontarget_dists.columns = [\"target\", \"number\"]\n",
    "nontarget_dists = nontarget_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:40.523643Z",
     "iopub.status.busy": "2020-10-09T09:51:40.522886Z",
     "iopub.status.idle": "2020-10-09T09:51:40.527442Z",
     "shell.execute_reply": "2020-10-09T09:51:40.527951Z"
    },
    "papermill": {
     "duration": 0.028621,
     "end_time": "2020-10-09T09:51:40.528116",
     "exception": false,
     "start_time": "2020-10-09T09:51:40.499495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#target_dists = pd.DataFrame(np.sum(targets[target_feats])).reset_index(drop=False)\n",
    "#target_dists.columns = [\"target\", \"number\"]\n",
    "#target_dists = target_dists.sort_values(\"number\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:40.577383Z",
     "iopub.status.busy": "2020-10-09T09:51:40.576522Z",
     "iopub.status.idle": "2020-10-09T09:51:40.640546Z",
     "shell.execute_reply": "2020-10-09T09:51:40.639684Z"
    },
    "papermill": {
     "duration": 0.092678,
     "end_time": "2020-10-09T09:51:40.640685",
     "exception": false,
     "start_time": "2020-10-09T09:51:40.548007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first drop 71\n",
      "shape after 1st drop: (21948, 332)\n",
      "second drop 125\n",
      "shape after 2nd drop: (21948, 207)\n"
     ]
    }
   ],
   "source": [
    "drop_list1 = list(nontarget_dists[nontarget_dists.number==0][\"target\"].values)\n",
    "print(\"first drop\", len(drop_list1))\n",
    "non_targets.drop(drop_list1, axis=1, inplace=True)\n",
    "print(\"shape after 1st drop:\", non_targets.shape)\n",
    "drop_list2 = list(nontarget_dists[(nontarget_dists.number>0) & (nontarget_dists.number<=6)][\"target\"].values)[:-1]\n",
    "print(\"second drop\", len(drop_list2))\n",
    "non_targets.drop(drop_list2, axis=1, inplace=True)\n",
    "print(\"shape after 2nd drop:\", non_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:40.691174Z",
     "iopub.status.busy": "2020-10-09T09:51:40.689796Z",
     "iopub.status.idle": "2020-10-09T09:51:40.982437Z",
     "shell.execute_reply": "2020-10-09T09:51:40.982940Z"
    },
    "papermill": {
     "duration": 0.321314,
     "end_time": "2020-10-09T09:51:40.983093",
     "exception": false,
     "start_time": "2020-10-09T09:51:40.661779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[g_feats].copy().values\n",
    "select = VarianceThreshold(threshold=1)\n",
    "X_new = select.fit_transform(X)\n",
    "drop_g_feats = list(np.array(g_feats)[select.get_support()==False])\n",
    "len(drop_g_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020072,
     "end_time": "2020-10-09T09:51:41.023900",
     "exception": false,
     "start_time": "2020-10-09T09:51:41.003828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:41.083215Z",
     "iopub.status.busy": "2020-10-09T09:51:41.081312Z",
     "iopub.status.idle": "2020-10-09T09:51:41.350584Z",
     "shell.execute_reply": "2020-10-09T09:51:41.349858Z"
    },
    "papermill": {
     "duration": 0.306298,
     "end_time": "2020-10-09T09:51:41.350713",
     "exception": false,
     "start_time": "2020-10-09T09:51:41.044415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = 10\n",
    "pca_c_cols = [\"pca-c\"+str(i+1) for i in range(num)]\n",
    "pca = PCA(n_components=num)\n",
    "tmp_train = pca.fit_transform(train[c_feats])\n",
    "tmp_test = pca.transform(test[c_feats])\n",
    "tmp_train = pd.DataFrame(tmp_train, columns=pca_c_cols)\n",
    "tmp_test = pd.DataFrame(tmp_test, columns=pca_c_cols)\n",
    "\n",
    "train = pd.concat([train, tmp_train],axis=1)\n",
    "test = pd.concat([test, tmp_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:41.402627Z",
     "iopub.status.busy": "2020-10-09T09:51:41.401243Z",
     "iopub.status.idle": "2020-10-09T09:51:41.727375Z",
     "shell.execute_reply": "2020-10-09T09:51:41.723735Z"
    },
    "papermill": {
     "duration": 0.356566,
     "end_time": "2020-10-09T09:51:41.727665",
     "exception": false,
     "start_time": "2020-10-09T09:51:41.371099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 603) (3982, 603)\n"
     ]
    }
   ],
   "source": [
    "def fe(df):\n",
    "    tmp = df.copy()\n",
    "    tmp.loc[:, 'cp_type'] = tmp.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "        \n",
    "    tmp.drop([\"cp_type\", \"sig_id\"] + c_feats + drop_g_feats, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "f_train = fe(train)\n",
    "f_test = fe(test)\n",
    "\n",
    "print(f_train.shape, f_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031832,
     "end_time": "2020-10-09T09:51:41.793968",
     "exception": false,
     "start_time": "2020-10-09T09:51:41.762136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:41.865668Z",
     "iopub.status.busy": "2020-10-09T09:51:41.864770Z",
     "iopub.status.idle": "2020-10-09T09:51:42.326185Z",
     "shell.execute_reply": "2020-10-09T09:51:42.327250Z"
    },
    "papermill": {
     "duration": 0.504239,
     "end_time": "2020-10-09T09:51:42.327458",
     "exception": false,
     "start_time": "2020-10-09T09:51:41.823219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048,1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, 206))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023183,
     "end_time": "2020-10-09T09:51:42.389943",
     "exception": false,
     "start_time": "2020-10-09T09:51:42.366760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train by non-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:42.463159Z",
     "iopub.status.busy": "2020-10-09T09:51:42.457930Z",
     "iopub.status.idle": "2020-10-09T09:51:42.479773Z",
     "shell.execute_reply": "2020-10-09T09:51:42.479148Z"
    },
    "papermill": {
     "duration": 0.068288,
     "end_time": "2020-10-09T09:51:42.479882",
     "exception": false,
     "start_time": "2020-10-09T09:51:42.411594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 20\n",
    "n_folds=5\n",
    "\n",
    "def first_learning(tr, target, sample_seed, init_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    files = []\n",
    "        \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'parameters'+str(fold+1)+'.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        files.append('parameters'+str(fold+1)+'.pt')\n",
    "        pred_model = MoaModel(init_num)\n",
    "        pred_model.load_state_dict(torch.load('parameters'+str(fold+1)+'.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:51:42.533521Z",
     "iopub.status.busy": "2020-10-09T09:51:42.532189Z",
     "iopub.status.idle": "2020-10-09T09:53:32.628556Z",
     "shell.execute_reply": "2020-10-09T09:53:32.627697Z"
    },
    "papermill": {
     "duration": 110.127029,
     "end_time": "2020-10-09T09:53:32.628729",
     "exception": false,
     "start_time": "2020-10-09T09:51:42.501700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.406228 \t val_loss=0.070501 \t time=1.72s\n",
      "Best model: Epoch 2 \t loss=0.040090 \t val_loss=0.018255 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.015327 \t val_loss=0.011185 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.010929 \t val_loss=0.009408 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.009393 \t val_loss=0.008351 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.008331 \t val_loss=0.007715 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.007899 \t val_loss=0.007590 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.007692 \t val_loss=0.007479 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.007526 \t val_loss=0.007326 \t time=1.13s\n",
      "Best model: Epoch 10 \t loss=0.007503 \t val_loss=0.007311 \t time=0.90s\n",
      "Best model: Epoch 11 \t loss=0.007374 \t val_loss=0.007234 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.007529 \t val_loss=0.007215 \t time=0.99s\n",
      "Best model: Epoch 16 \t loss=0.007147 \t val_loss=0.007064 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.007025 \t val_loss=0.007009 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.006884 \t val_loss=0.006863 \t time=0.81s\n",
      "Fold 1 log loss: 0.00690978185068056\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.412193 \t val_loss=0.072829 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.039901 \t val_loss=0.017800 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.014854 \t val_loss=0.010859 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.010571 \t val_loss=0.009318 \t time=1.06s\n",
      "Best model: Epoch 5 \t loss=0.009020 \t val_loss=0.008088 \t time=0.83s\n",
      "Best model: Epoch 7 \t loss=0.007894 \t val_loss=0.007583 \t time=0.86s\n",
      "Best model: Epoch 9 \t loss=0.007694 \t val_loss=0.007433 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.007395 \t val_loss=0.007397 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.007287 \t val_loss=0.007276 \t time=0.83s\n",
      "Best model: Epoch 15 \t loss=0.007121 \t val_loss=0.007183 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.007156 \t val_loss=0.007173 \t time=1.01s\n",
      "Best model: Epoch 17 \t loss=0.007063 \t val_loss=0.007165 \t time=1.24s\n",
      "Best model: Epoch 18 \t loss=0.006962 \t val_loss=0.007074 \t time=0.88s\n",
      "Best model: Epoch 20 \t loss=0.006774 \t val_loss=0.006981 \t time=0.86s\n",
      "Fold 2 log loss: 0.0069417110353691995\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.414150 \t val_loss=0.070606 \t time=1.12s\n",
      "Best model: Epoch 2 \t loss=0.038032 \t val_loss=0.018765 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.015526 \t val_loss=0.011286 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.010707 \t val_loss=0.009662 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.009377 \t val_loss=0.008297 \t time=0.93s\n",
      "Best model: Epoch 6 \t loss=0.008407 \t val_loss=0.007670 \t time=0.93s\n",
      "Best model: Epoch 7 \t loss=0.007976 \t val_loss=0.007487 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.007684 \t val_loss=0.007357 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.007468 \t val_loss=0.007217 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.007374 \t val_loss=0.007212 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.007354 \t val_loss=0.007154 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.007139 \t val_loss=0.007000 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.007045 \t val_loss=0.006973 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.006999 \t val_loss=0.006911 \t time=1.08s\n",
      "Best model: Epoch 19 \t loss=0.006896 \t val_loss=0.006889 \t time=0.85s\n",
      "Fold 3 log loss: 0.006951320409305341\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414067 \t val_loss=0.077970 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.038681 \t val_loss=0.017946 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.015888 \t val_loss=0.011038 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.010693 \t val_loss=0.009022 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.009172 \t val_loss=0.008467 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.008462 \t val_loss=0.007827 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.007906 \t val_loss=0.007811 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.007917 \t val_loss=0.007560 \t time=0.83s\n",
      "Best model: Epoch 10 \t loss=0.007627 \t val_loss=0.007386 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.007388 \t val_loss=0.007324 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.007331 \t val_loss=0.007203 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.007345 \t val_loss=0.007157 \t time=0.83s\n",
      "Best model: Epoch 16 \t loss=0.007178 \t val_loss=0.007155 \t time=0.91s\n",
      "Best model: Epoch 17 \t loss=0.007083 \t val_loss=0.007067 \t time=1.05s\n",
      "Best model: Epoch 18 \t loss=0.007002 \t val_loss=0.007026 \t time=0.87s\n",
      "Best model: Epoch 19 \t loss=0.006906 \t val_loss=0.006921 \t time=0.84s\n",
      "Best model: Epoch 20 \t loss=0.006847 \t val_loss=0.006882 \t time=0.83s\n",
      "Fold 4 log loss: 0.006826449816423469\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412742 \t val_loss=0.075728 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.039627 \t val_loss=0.016596 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.015594 \t val_loss=0.011201 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.010777 \t val_loss=0.009622 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.009173 \t val_loss=0.008362 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.008271 \t val_loss=0.007815 \t time=0.83s\n",
      "Best model: Epoch 8 \t loss=0.007670 \t val_loss=0.007473 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.007533 \t val_loss=0.007306 \t time=0.88s\n",
      "Best model: Epoch 11 \t loss=0.007396 \t val_loss=0.007257 \t time=1.02s\n",
      "Best model: Epoch 13 \t loss=0.007321 \t val_loss=0.007197 \t time=0.81s\n",
      "Best model: Epoch 14 \t loss=0.007372 \t val_loss=0.007163 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.007166 \t val_loss=0.007111 \t time=0.82s\n",
      "Best model: Epoch 16 \t loss=0.007097 \t val_loss=0.007087 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.007012 \t val_loss=0.007079 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.007038 \t val_loss=0.007043 \t time=1.25s\n",
      "Best model: Epoch 19 \t loss=0.006963 \t val_loss=0.006937 \t time=0.83s\n",
      "Best model: Epoch 20 \t loss=0.006968 \t val_loss=0.006927 \t time=0.85s\n",
      "Fold 5 log loss: 0.006896671346297835\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.00690978185068056\n",
      "Fold 2 log loss: 0.0069417110353691995\n",
      "Fold 3 log loss: 0.006951320409305341\n",
      "Fold 4 log loss: 0.006826449816423469\n",
      "Fold 5 log loss: 0.006896671346297835\n",
      "Std of log loss: 4.417118903095574e-05\n",
      "Total log loss: 0.006905188377106329\n"
     ]
    }
   ],
   "source": [
    "fn_train = f_train.copy().to_numpy()\n",
    "fn_test = f_test.copy().to_numpy()\n",
    "\n",
    "#ss = preprocessing.StandardScaler()\n",
    "#fn_train= ss.fit_transform(fn_train)\n",
    "#fn_test = ss.transform(fn_test)\n",
    "\n",
    "fn_nontargets = non_targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "nontarget_oof = np.zeros([len(fn_train),fn_nontargets.shape[1]])\n",
    "nontarget_pred = np.zeros([len(fn_test),fn_nontargets.shape[1]])\n",
    "\n",
    "seeds = [0]\n",
    "for seed_ in seeds:\n",
    "    files = first_learning(fn_train, fn_nontargets, seed_, fn_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056428,
     "end_time": "2020-10-09T09:53:32.738651",
     "exception": false,
     "start_time": "2020-10-09T09:53:32.682223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train by targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:53:32.872529Z",
     "iopub.status.busy": "2020-10-09T09:53:32.856760Z",
     "iopub.status.idle": "2020-10-09T09:53:32.896468Z",
     "shell.execute_reply": "2020-10-09T09:53:32.895919Z"
    },
    "papermill": {
     "duration": 0.103674,
     "end_time": "2020-10-09T09:53:32.896578",
     "exception": false,
     "start_time": "2020-10-09T09:53:32.792904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_epochs = 30\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 3\n",
    "\n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, files):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num)\n",
    "        clf.load_state_dict(torch.load(files[fold]))\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001) \n",
    "        #lookahead = Lookahead(optimizer, k=10, alpha=0.6) #lookahead\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)        \n",
    "            \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "            if stop_counts >= EARLY_STOPPING_STEPS: \n",
    "                break\n",
    "         \n",
    "        pred_model = MoaModel(init_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T09:53:33.023679Z",
     "iopub.status.busy": "2020-10-09T09:53:33.014828Z",
     "iopub.status.idle": "2020-10-09T10:04:00.575401Z",
     "shell.execute_reply": "2020-10-09T10:04:00.575914Z"
    },
    "papermill": {
     "duration": 627.625726,
     "end_time": "2020-10-09T10:04:00.576430",
     "exception": false,
     "start_time": "2020-10-09T09:53:32.950704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.020610 \t val_loss=0.018155 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.018478 \t val_loss=0.017479 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.017754 \t val_loss=0.017219 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.017370 \t val_loss=0.016934 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.016956 \t val_loss=0.016741 \t time=1.07s\n",
      "Best model: Epoch 6 \t loss=0.016665 \t val_loss=0.016602 \t time=0.83s\n",
      "Best model: Epoch 8 \t loss=0.016129 \t val_loss=0.016421 \t time=0.83s\n",
      "Best model: Epoch 9 \t loss=0.015972 \t val_loss=0.016383 \t time=0.91s\n",
      "Best model: Epoch 10 \t loss=0.015682 \t val_loss=0.016342 \t time=0.78s\n",
      "Best model: Epoch 11 \t loss=0.015445 \t val_loss=0.016278 \t time=0.81s\n",
      "Best model: Epoch 12 \t loss=0.015296 \t val_loss=0.016257 \t time=0.79s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 15 \t loss=0.014387 \t val_loss=0.016177 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.014268 \t val_loss=0.016139 \t time=0.81s\n",
      "Best model: Epoch 17 \t loss=0.014238 \t val_loss=0.016113 \t time=0.82s\n",
      "Best model: Epoch 18 \t loss=0.014144 \t val_loss=0.016110 \t time=0.80s\n",
      "Best model: Epoch 19 \t loss=0.014045 \t val_loss=0.016104 \t time=0.80s\n",
      "Best model: Epoch 22 \t loss=0.013898 \t val_loss=0.016100 \t time=0.95s\n",
      "Best model: Epoch 24 \t loss=0.013782 \t val_loss=0.016082 \t time=0.77s\n",
      "Best model: Epoch 25 \t loss=0.013670 \t val_loss=0.016079 \t time=0.78s\n",
      "Best model: Epoch 26 \t loss=0.013684 \t val_loss=0.016072 \t time=0.78s\n",
      "Fold 1 log loss: 0.016169821197948627\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.020863 \t val_loss=0.018230 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.018493 \t val_loss=0.017513 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017732 \t val_loss=0.017108 \t time=1.10s\n",
      "Best model: Epoch 4 \t loss=0.017419 \t val_loss=0.016868 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.017045 \t val_loss=0.016717 \t time=0.78s\n",
      "Best model: Epoch 6 \t loss=0.016719 \t val_loss=0.016593 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.016460 \t val_loss=0.016460 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.016201 \t val_loss=0.016340 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.015986 \t val_loss=0.016315 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.015795 \t val_loss=0.016296 \t time=0.77s\n",
      "Best model: Epoch 11 \t loss=0.015583 \t val_loss=0.016257 \t time=0.78s\n",
      "Best model: Epoch 13 \t loss=0.015253 \t val_loss=0.016213 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.015000 \t val_loss=0.016154 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.014937 \t val_loss=0.016142 \t time=0.97s\n",
      "Best model: Epoch 17 \t loss=0.014623 \t val_loss=0.016129 \t time=0.79s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 20 \t loss=0.013779 \t val_loss=0.016108 \t time=0.80s\n",
      "Best model: Epoch 21 \t loss=0.013678 \t val_loss=0.016071 \t time=0.79s\n",
      "Best model: Epoch 22 \t loss=0.013626 \t val_loss=0.016049 \t time=0.78s\n",
      "Best model: Epoch 24 \t loss=0.013510 \t val_loss=0.016025 \t time=0.78s\n",
      "Best model: Epoch 26 \t loss=0.013296 \t val_loss=0.016015 \t time=0.77s\n",
      "Fold 2 log loss: 0.0160723849865928\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.020898 \t val_loss=0.018364 \t time=0.97s\n",
      "Best model: Epoch 2 \t loss=0.018617 \t val_loss=0.017580 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.017871 \t val_loss=0.017102 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.017457 \t val_loss=0.016912 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.017036 \t val_loss=0.016732 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.016748 \t val_loss=0.016627 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.016524 \t val_loss=0.016516 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.016332 \t val_loss=0.016292 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.016184 \t val_loss=0.016276 \t time=1.03s\n",
      "Best model: Epoch 11 \t loss=0.015659 \t val_loss=0.016194 \t time=0.78s\n",
      "Best model: Epoch 12 \t loss=0.015485 \t val_loss=0.016150 \t time=0.81s\n",
      "Best model: Epoch 13 \t loss=0.015326 \t val_loss=0.016075 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.015104 \t val_loss=0.016040 \t time=0.81s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 17 \t loss=0.014288 \t val_loss=0.015966 \t time=0.80s\n",
      "Best model: Epoch 18 \t loss=0.014190 \t val_loss=0.015954 \t time=1.09s\n",
      "Best model: Epoch 19 \t loss=0.014100 \t val_loss=0.015910 \t time=0.84s\n",
      "Fold 3 log loss: 0.015904277879938995\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.020584 \t val_loss=0.017939 \t time=0.81s\n",
      "Best model: Epoch 2 \t loss=0.018443 \t val_loss=0.017361 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.017711 \t val_loss=0.017056 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.017282 \t val_loss=0.016778 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.016967 \t val_loss=0.016581 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.016667 \t val_loss=0.016519 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.016433 \t val_loss=0.016372 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.016162 \t val_loss=0.016338 \t time=0.81s\n",
      "Best model: Epoch 9 \t loss=0.016011 \t val_loss=0.016315 \t time=0.94s\n",
      "Best model: Epoch 11 \t loss=0.015543 \t val_loss=0.016074 \t time=0.79s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 14 \t loss=0.014753 \t val_loss=0.016000 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.014584 \t val_loss=0.015978 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.014505 \t val_loss=0.015940 \t time=0.80s\n",
      "Fold 4 log loss: 0.016004394553758697\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.020747 \t val_loss=0.018250 \t time=1.00s\n",
      "Best model: Epoch 2 \t loss=0.018413 \t val_loss=0.017627 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.017701 \t val_loss=0.017286 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.017274 \t val_loss=0.016999 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016927 \t val_loss=0.016800 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.016635 \t val_loss=0.016648 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.016115 \t val_loss=0.016486 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.015711 \t val_loss=0.016307 \t time=1.09s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 13 \t loss=0.014780 \t val_loss=0.016249 \t time=1.03s\n",
      "Best model: Epoch 14 \t loss=0.014609 \t val_loss=0.016184 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.014446 \t val_loss=0.016183 \t time=0.79s\n",
      "Best model: Epoch 17 \t loss=0.014354 \t val_loss=0.016158 \t time=0.81s\n",
      "Best model: Epoch 18 \t loss=0.014323 \t val_loss=0.016148 \t time=0.78s\n",
      "Best model: Epoch 19 \t loss=0.014225 \t val_loss=0.016132 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.014065 \t val_loss=0.016131 \t time=0.81s\n",
      "Best model: Epoch 22 \t loss=0.014076 \t val_loss=0.016126 \t time=0.86s\n",
      "Best model: Epoch 24 \t loss=0.013920 \t val_loss=0.016104 \t time=0.80s\n",
      "Fold 5 log loss: 0.016109225344728702\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016169821197948627\n",
      "Fold 2 log loss: 0.0160723849865928\n",
      "Fold 3 log loss: 0.015904277879938995\n",
      "Fold 4 log loss: 0.016004394553758697\n",
      "Fold 5 log loss: 0.016109225344728702\n",
      "Std of log loss: 9.128003456314958e-05\n",
      "Total log loss: 0.01605201725838789\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.020797 \t val_loss=0.018399 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.018444 \t val_loss=0.017643 \t time=0.81s\n",
      "Best model: Epoch 3 \t loss=0.017783 \t val_loss=0.017143 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.017395 \t val_loss=0.016903 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016984 \t val_loss=0.016755 \t time=0.98s\n",
      "Best model: Epoch 6 \t loss=0.016690 \t val_loss=0.016608 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.016442 \t val_loss=0.016551 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.016194 \t val_loss=0.016445 \t time=0.82s\n",
      "Best model: Epoch 10 \t loss=0.015759 \t val_loss=0.016403 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.015530 \t val_loss=0.016320 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.015339 \t val_loss=0.016210 \t time=0.78s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 15 \t loss=0.014529 \t val_loss=0.016199 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.014369 \t val_loss=0.016170 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.014238 \t val_loss=0.016151 \t time=0.93s\n",
      "Best model: Epoch 18 \t loss=0.014267 \t val_loss=0.016105 \t time=0.85s\n",
      "Best model: Epoch 19 \t loss=0.014132 \t val_loss=0.016099 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.014087 \t val_loss=0.016069 \t time=0.79s\n",
      "Fold 1 log loss: 0.016174094458844317\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.020621 \t val_loss=0.018090 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.018407 \t val_loss=0.017515 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017807 \t val_loss=0.017160 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.017374 \t val_loss=0.016932 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016966 \t val_loss=0.016699 \t time=0.99s\n",
      "Best model: Epoch 6 \t loss=0.016579 \t val_loss=0.016586 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.016424 \t val_loss=0.016499 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.016283 \t val_loss=0.016432 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.016000 \t val_loss=0.016337 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.015776 \t val_loss=0.016262 \t time=0.78s\n",
      "Best model: Epoch 11 \t loss=0.015619 \t val_loss=0.016165 \t time=0.82s\n",
      "Best model: Epoch 13 \t loss=0.015239 \t val_loss=0.016161 \t time=0.77s\n",
      "Best model: Epoch 14 \t loss=0.015009 \t val_loss=0.016086 \t time=0.79s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 17 \t loss=0.014224 \t val_loss=0.016069 \t time=0.79s\n",
      "Best model: Epoch 18 \t loss=0.014074 \t val_loss=0.016021 \t time=1.00s\n",
      "Fold 2 log loss: 0.016085001820501463\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.020873 \t val_loss=0.018441 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.018524 \t val_loss=0.017537 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017814 \t val_loss=0.017280 \t time=1.08s\n",
      "Best model: Epoch 4 \t loss=0.017407 \t val_loss=0.017030 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.016975 \t val_loss=0.016640 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.016786 \t val_loss=0.016494 \t time=1.55s\n",
      "Best model: Epoch 7 \t loss=0.016452 \t val_loss=0.016466 \t time=0.95s\n",
      "Best model: Epoch 8 \t loss=0.016246 \t val_loss=0.016357 \t time=0.93s\n",
      "Best model: Epoch 9 \t loss=0.016042 \t val_loss=0.016150 \t time=0.84s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 11 \t loss=0.015564 \t val_loss=0.016149 \t time=0.83s\n",
      "Best model: Epoch 12 \t loss=0.015058 \t val_loss=0.016087 \t time=0.80s\n",
      "Best model: Epoch 13 \t loss=0.015010 \t val_loss=0.016000 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.014866 \t val_loss=0.015961 \t time=0.83s\n",
      "Best model: Epoch 15 \t loss=0.014770 \t val_loss=0.015926 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.014585 \t val_loss=0.015909 \t time=1.06s\n",
      "Best model: Epoch 21 \t loss=0.014436 \t val_loss=0.015895 \t time=0.77s\n",
      "Best model: Epoch 22 \t loss=0.014345 \t val_loss=0.015867 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.014282 \t val_loss=0.015866 \t time=0.78s\n",
      "Fold 3 log loss: 0.015866971853996464\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.020654 \t val_loss=0.018007 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.018492 \t val_loss=0.017360 \t time=0.93s\n",
      "Best model: Epoch 3 \t loss=0.017837 \t val_loss=0.017000 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.017266 \t val_loss=0.016778 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.016999 \t val_loss=0.016679 \t time=0.84s\n",
      "Best model: Epoch 6 \t loss=0.016625 \t val_loss=0.016555 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.016375 \t val_loss=0.016352 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.016225 \t val_loss=0.016325 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.016000 \t val_loss=0.016253 \t time=0.80s\n",
      "Best model: Epoch 10 \t loss=0.015869 \t val_loss=0.016247 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.015626 \t val_loss=0.016129 \t time=1.04s\n",
      "Best model: Epoch 13 \t loss=0.015267 \t val_loss=0.016083 \t time=0.80s\n",
      "Best model: Epoch 14 \t loss=0.015083 \t val_loss=0.016053 \t time=0.82s\n",
      "Best model: Epoch 15 \t loss=0.014896 \t val_loss=0.016043 \t time=1.03s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 18 \t loss=0.014062 \t val_loss=0.015963 \t time=0.82s\n",
      "Best model: Epoch 19 \t loss=0.013892 \t val_loss=0.015914 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.013776 \t val_loss=0.015904 \t time=0.76s\n",
      "Best model: Epoch 22 \t loss=0.013732 \t val_loss=0.015902 \t time=0.77s\n",
      "Fold 4 log loss: 0.01595669901514425\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.020637 \t val_loss=0.018308 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.018530 \t val_loss=0.017647 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.017836 \t val_loss=0.017300 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.017331 \t val_loss=0.017028 \t time=0.77s\n",
      "Best model: Epoch 5 \t loss=0.017035 \t val_loss=0.016848 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.016673 \t val_loss=0.016691 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.016513 \t val_loss=0.016572 \t time=0.77s\n",
      "Best model: Epoch 8 \t loss=0.016235 \t val_loss=0.016509 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.015948 \t val_loss=0.016464 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.015726 \t val_loss=0.016333 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.015391 \t val_loss=0.016326 \t time=0.77s\n",
      "Best model: Epoch 13 \t loss=0.015252 \t val_loss=0.016257 \t time=0.91s\n",
      "Best model: Epoch 15 \t loss=0.014845 \t val_loss=0.016226 \t time=0.78s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 18 \t loss=0.014015 \t val_loss=0.016138 \t time=0.77s\n",
      "Best model: Epoch 20 \t loss=0.013859 \t val_loss=0.016136 \t time=0.82s\n",
      "Best model: Epoch 22 \t loss=0.013671 \t val_loss=0.016120 \t time=1.47s\n",
      "Best model: Epoch 23 \t loss=0.013647 \t val_loss=0.016104 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.013444 \t val_loss=0.016104 \t time=0.84s\n",
      "Best model: Epoch 28 \t loss=0.013402 \t val_loss=0.016094 \t time=0.82s\n",
      "Fold 5 log loss: 0.016099878975413106\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016174094458844317\n",
      "Fold 2 log loss: 0.016085001820501463\n",
      "Fold 3 log loss: 0.015866971853996464\n",
      "Fold 4 log loss: 0.01595669901514425\n",
      "Fold 5 log loss: 0.016099878975413106\n",
      "Std of log loss: 0.00010991773416690892\n",
      "Total log loss: 0.016036524129903564\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.020588 \t val_loss=0.018301 \t time=0.77s\n",
      "Best model: Epoch 2 \t loss=0.018425 \t val_loss=0.017487 \t time=0.97s\n",
      "Best model: Epoch 3 \t loss=0.017738 \t val_loss=0.017149 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.017322 \t val_loss=0.017041 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.016968 \t val_loss=0.016707 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.016676 \t val_loss=0.016641 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.016412 \t val_loss=0.016590 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.016183 \t val_loss=0.016510 \t time=0.78s\n",
      "Best model: Epoch 9 \t loss=0.015960 \t val_loss=0.016377 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.015740 \t val_loss=0.016287 \t time=0.79s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 13 \t loss=0.014830 \t val_loss=0.016197 \t time=0.78s\n",
      "Best model: Epoch 14 \t loss=0.014680 \t val_loss=0.016126 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.014463 \t val_loss=0.016115 \t time=0.77s\n",
      "Best model: Epoch 19 \t loss=0.014363 \t val_loss=0.016049 \t time=0.78s\n",
      "Fold 1 log loss: 0.01615041180560485\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.020768 \t val_loss=0.018173 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.018464 \t val_loss=0.017582 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017843 \t val_loss=0.017303 \t time=1.01s\n",
      "Best model: Epoch 4 \t loss=0.017375 \t val_loss=0.016944 \t time=0.78s\n",
      "Best model: Epoch 5 \t loss=0.017068 \t val_loss=0.016724 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.016676 \t val_loss=0.016665 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.016464 \t val_loss=0.016497 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.016239 \t val_loss=0.016453 \t time=0.80s\n",
      "Best model: Epoch 9 \t loss=0.016018 \t val_loss=0.016357 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.015805 \t val_loss=0.016287 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.015641 \t val_loss=0.016263 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.015460 \t val_loss=0.016213 \t time=0.78s\n",
      "Best model: Epoch 13 \t loss=0.015176 \t val_loss=0.016181 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.014945 \t val_loss=0.016167 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.014789 \t val_loss=0.016097 \t time=1.04s\n",
      "Best model: Epoch 17 \t loss=0.014646 \t val_loss=0.016073 \t time=0.79s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 20 \t loss=0.013847 \t val_loss=0.016057 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.013632 \t val_loss=0.016036 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.013507 \t val_loss=0.016022 \t time=0.78s\n",
      "Fold 2 log loss: 0.016092408945144115\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.020805 \t val_loss=0.018387 \t time=1.17s\n",
      "Best model: Epoch 2 \t loss=0.018532 \t val_loss=0.017507 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.017836 \t val_loss=0.017109 \t time=0.81s\n",
      "Best model: Epoch 4 \t loss=0.017424 \t val_loss=0.016858 \t time=0.83s\n",
      "Best model: Epoch 5 \t loss=0.017071 \t val_loss=0.016680 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.016774 \t val_loss=0.016598 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.016528 \t val_loss=0.016355 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.016265 \t val_loss=0.016297 \t time=1.04s\n",
      "Best model: Epoch 9 \t loss=0.016057 \t val_loss=0.016235 \t time=0.93s\n",
      "Best model: Epoch 10 \t loss=0.015779 \t val_loss=0.016188 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.015602 \t val_loss=0.016143 \t time=0.82s\n",
      "Best model: Epoch 12 \t loss=0.015501 \t val_loss=0.016083 \t time=1.00s\n",
      "Best model: Epoch 13 \t loss=0.015270 \t val_loss=0.016045 \t time=0.80s\n",
      "Best model: Epoch 15 \t loss=0.014910 \t val_loss=0.016043 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.014603 \t val_loss=0.015968 \t time=0.77s\n",
      "Best model: Epoch 18 \t loss=0.014437 \t val_loss=0.015911 \t time=0.79s\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.015911519913075184\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.020709 \t val_loss=0.018140 \t time=0.95s\n",
      "Best model: Epoch 2 \t loss=0.018441 \t val_loss=0.017269 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.017833 \t val_loss=0.016969 \t time=0.77s\n",
      "Best model: Epoch 4 \t loss=0.017419 \t val_loss=0.016745 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.017038 \t val_loss=0.016617 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.016629 \t val_loss=0.016460 \t time=0.78s\n",
      "Best model: Epoch 7 \t loss=0.016425 \t val_loss=0.016429 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.016220 \t val_loss=0.016422 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.016095 \t val_loss=0.016277 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.015785 \t val_loss=0.016133 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.015433 \t val_loss=0.016070 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.015208 \t val_loss=0.016057 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.015082 \t val_loss=0.016033 \t time=0.98s\n",
      "Best model: Epoch 16 \t loss=0.014826 \t val_loss=0.015996 \t time=0.78s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 19 \t loss=0.013915 \t val_loss=0.015954 \t time=0.78s\n",
      "Best model: Epoch 22 \t loss=0.013736 \t val_loss=0.015932 \t time=0.78s\n",
      "Best model: Epoch 23 \t loss=0.013630 \t val_loss=0.015893 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.013461 \t val_loss=0.015887 \t time=0.79s\n",
      "Fold 4 log loss: 0.015944675780771703\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.020723 \t val_loss=0.018383 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.018525 \t val_loss=0.017588 \t time=0.79s\n",
      "Best model: Epoch 3 \t loss=0.017743 \t val_loss=0.017209 \t time=0.79s\n",
      "Best model: Epoch 4 \t loss=0.017350 \t val_loss=0.017042 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016992 \t val_loss=0.016915 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.016658 \t val_loss=0.016697 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.016449 \t val_loss=0.016575 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.016213 \t val_loss=0.016511 \t time=1.07s\n",
      "Best model: Epoch 9 \t loss=0.015960 \t val_loss=0.016436 \t time=0.81s\n",
      "Best model: Epoch 10 \t loss=0.015807 \t val_loss=0.016389 \t time=0.82s\n",
      "Best model: Epoch 11 \t loss=0.015484 \t val_loss=0.016261 \t time=1.03s\n",
      "Best model: Epoch 13 \t loss=0.015133 \t val_loss=0.016241 \t time=0.84s\n",
      "Best model: Epoch 15 \t loss=0.014831 \t val_loss=0.016202 \t time=1.06s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 18 \t loss=0.014019 \t val_loss=0.016195 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.013908 \t val_loss=0.016139 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.013829 \t val_loss=0.016130 \t time=1.18s\n",
      "Best model: Epoch 21 \t loss=0.013696 \t val_loss=0.016120 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.013677 \t val_loss=0.016110 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.013557 \t val_loss=0.016102 \t time=0.83s\n",
      "Best model: Epoch 25 \t loss=0.013503 \t val_loss=0.016096 \t time=0.83s\n",
      "Fold 5 log loss: 0.016107774081218065\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.01615041180560485\n",
      "Fold 2 log loss: 0.016092408945144115\n",
      "Fold 3 log loss: 0.015911519913075184\n",
      "Fold 4 log loss: 0.015944675780771703\n",
      "Fold 5 log loss: 0.016107774081218065\n",
      "Std of log loss: 9.498990229758304e-05\n",
      "Total log loss: 0.016041352753111752\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.021012 \t val_loss=0.018467 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.018542 \t val_loss=0.017582 \t time=0.83s\n",
      "Best model: Epoch 3 \t loss=0.017800 \t val_loss=0.017354 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.017358 \t val_loss=0.016951 \t time=0.82s\n",
      "Best model: Epoch 5 \t loss=0.016989 \t val_loss=0.016816 \t time=0.82s\n",
      "Best model: Epoch 6 \t loss=0.016704 \t val_loss=0.016592 \t time=0.82s\n",
      "Best model: Epoch 7 \t loss=0.016432 \t val_loss=0.016532 \t time=0.82s\n",
      "Best model: Epoch 8 \t loss=0.016221 \t val_loss=0.016453 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.016024 \t val_loss=0.016432 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.015813 \t val_loss=0.016369 \t time=1.00s\n",
      "Best model: Epoch 11 \t loss=0.015624 \t val_loss=0.016314 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.015432 \t val_loss=0.016281 \t time=0.83s\n",
      "Best model: Epoch 14 \t loss=0.015080 \t val_loss=0.016230 \t time=0.82s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 17 \t loss=0.014214 \t val_loss=0.016181 \t time=0.83s\n",
      "Best model: Epoch 18 \t loss=0.014120 \t val_loss=0.016145 \t time=0.82s\n",
      "Best model: Epoch 21 \t loss=0.013892 \t val_loss=0.016144 \t time=0.82s\n",
      "Best model: Epoch 22 \t loss=0.013816 \t val_loss=0.016137 \t time=0.98s\n",
      "Best model: Epoch 23 \t loss=0.013713 \t val_loss=0.016137 \t time=0.91s\n",
      "Best model: Epoch 24 \t loss=0.013670 \t val_loss=0.016094 \t time=0.83s\n",
      "Best model: Epoch 27 \t loss=0.013525 \t val_loss=0.016093 \t time=0.82s\n",
      "Fold 1 log loss: 0.016193163757361417\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.020762 \t val_loss=0.018357 \t time=0.97s\n",
      "Best model: Epoch 2 \t loss=0.018431 \t val_loss=0.017610 \t time=1.06s\n",
      "Best model: Epoch 3 \t loss=0.017860 \t val_loss=0.017145 \t time=0.80s\n",
      "Best model: Epoch 4 \t loss=0.017308 \t val_loss=0.016920 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.016971 \t val_loss=0.016674 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.016645 \t val_loss=0.016559 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.016392 \t val_loss=0.016425 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.016144 \t val_loss=0.016407 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.015991 \t val_loss=0.016335 \t time=0.84s\n",
      "Best model: Epoch 10 \t loss=0.015796 \t val_loss=0.016288 \t time=0.83s\n",
      "Best model: Epoch 11 \t loss=0.015631 \t val_loss=0.016177 \t time=1.29s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 14 \t loss=0.014623 \t val_loss=0.016084 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.014532 \t val_loss=0.016082 \t time=0.81s\n",
      "Best model: Epoch 16 \t loss=0.014570 \t val_loss=0.016033 \t time=0.83s\n",
      "Best model: Epoch 17 \t loss=0.014425 \t val_loss=0.016016 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.014366 \t val_loss=0.016009 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.014132 \t val_loss=0.016004 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.014113 \t val_loss=0.015998 \t time=0.84s\n",
      "Best model: Epoch 23 \t loss=0.014066 \t val_loss=0.015993 \t time=0.83s\n",
      "Best model: Epoch 25 \t loss=0.013907 \t val_loss=0.015967 \t time=0.82s\n",
      "Fold 2 log loss: 0.016029652380135317\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.020778 \t val_loss=0.018409 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.018453 \t val_loss=0.017540 \t time=0.80s\n",
      "Best model: Epoch 3 \t loss=0.017831 \t val_loss=0.017130 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.017360 \t val_loss=0.016894 \t time=1.34s\n",
      "Best model: Epoch 5 \t loss=0.017016 \t val_loss=0.016697 \t time=0.91s\n",
      "Best model: Epoch 6 \t loss=0.016754 \t val_loss=0.016552 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.016290 \t val_loss=0.016369 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.016005 \t val_loss=0.016302 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.015736 \t val_loss=0.016211 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.015533 \t val_loss=0.016129 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.015416 \t val_loss=0.016081 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.015189 \t val_loss=0.016048 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.014925 \t val_loss=0.016040 \t time=0.78s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 18 \t loss=0.014170 \t val_loss=0.015956 \t time=0.79s\n",
      "Best model: Epoch 19 \t loss=0.013939 \t val_loss=0.015898 \t time=0.79s\n",
      "Best model: Epoch 22 \t loss=0.013789 \t val_loss=0.015881 \t time=0.79s\n",
      "Best model: Epoch 24 \t loss=0.013672 \t val_loss=0.015871 \t time=0.78s\n",
      "Fold 3 log loss: 0.015871676017586916\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.020619 \t val_loss=0.017953 \t time=0.79s\n",
      "Best model: Epoch 2 \t loss=0.018500 \t val_loss=0.017274 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017851 \t val_loss=0.017022 \t time=0.99s\n",
      "Best model: Epoch 4 \t loss=0.017386 \t val_loss=0.016763 \t time=0.79s\n",
      "Best model: Epoch 5 \t loss=0.017022 \t val_loss=0.016553 \t time=0.80s\n",
      "Best model: Epoch 6 \t loss=0.016638 \t val_loss=0.016436 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.016513 \t val_loss=0.016361 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.016192 \t val_loss=0.016253 \t time=0.88s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 11 \t loss=0.015286 \t val_loss=0.016074 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.015104 \t val_loss=0.016062 \t time=1.00s\n",
      "Best model: Epoch 13 \t loss=0.014993 \t val_loss=0.016045 \t time=0.93s\n",
      "Best model: Epoch 14 \t loss=0.014931 \t val_loss=0.016031 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.014947 \t val_loss=0.016011 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.014801 \t val_loss=0.015967 \t time=0.97s\n",
      "Best model: Epoch 19 \t loss=0.014645 \t val_loss=0.015958 \t time=0.80s\n",
      "Fold 4 log loss: 0.01601952668958326\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.020607 \t val_loss=0.018295 \t time=0.82s\n",
      "Best model: Epoch 2 \t loss=0.018375 \t val_loss=0.017618 \t time=0.82s\n",
      "Best model: Epoch 3 \t loss=0.017803 \t val_loss=0.017211 \t time=1.00s\n",
      "Best model: Epoch 4 \t loss=0.017309 \t val_loss=0.016934 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.016977 \t val_loss=0.016778 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.016677 \t val_loss=0.016639 \t time=0.80s\n",
      "Best model: Epoch 7 \t loss=0.016411 \t val_loss=0.016568 \t time=0.79s\n",
      "Best model: Epoch 8 \t loss=0.016108 \t val_loss=0.016490 \t time=0.84s\n",
      "Best model: Epoch 9 \t loss=0.015886 \t val_loss=0.016374 \t time=0.79s\n",
      "Best model: Epoch 10 \t loss=0.015680 \t val_loss=0.016358 \t time=0.79s\n",
      "Best model: Epoch 11 \t loss=0.015504 \t val_loss=0.016342 \t time=0.78s\n",
      "Best model: Epoch 13 \t loss=0.015178 \t val_loss=0.016271 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.014762 \t val_loss=0.016190 \t time=0.77s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 18 \t loss=0.013963 \t val_loss=0.016146 \t time=0.81s\n",
      "Best model: Epoch 20 \t loss=0.013803 \t val_loss=0.016143 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.013701 \t val_loss=0.016142 \t time=0.93s\n",
      "Best model: Epoch 22 \t loss=0.013631 \t val_loss=0.016119 \t time=0.79s\n",
      "Best model: Epoch 23 \t loss=0.013560 \t val_loss=0.016096 \t time=0.79s\n",
      "Fold 5 log loss: 0.016102409252149202\n",
      "Seed 3\n",
      "Fold 1 log loss: 0.016193163757361417\n",
      "Fold 2 log loss: 0.016029652380135317\n",
      "Fold 3 log loss: 0.015871676017586916\n",
      "Fold 4 log loss: 0.01601952668958326\n",
      "Fold 5 log loss: 0.016102409252149202\n",
      "Std of log loss: 0.00010598043930987842\n",
      "Total log loss: 0.01604328354671908\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.020754 \t val_loss=0.018302 \t time=0.83s\n",
      "Best model: Epoch 2 \t loss=0.018420 \t val_loss=0.017534 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.017745 \t val_loss=0.017346 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.017338 \t val_loss=0.016871 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016948 \t val_loss=0.016795 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.016599 \t val_loss=0.016647 \t time=0.81s\n",
      "Best model: Epoch 7 \t loss=0.016417 \t val_loss=0.016513 \t time=0.81s\n",
      "Best model: Epoch 8 \t loss=0.016158 \t val_loss=0.016419 \t time=0.86s\n",
      "Best model: Epoch 9 \t loss=0.015977 \t val_loss=0.016366 \t time=1.03s\n",
      "Best model: Epoch 10 \t loss=0.015767 \t val_loss=0.016320 \t time=0.81s\n",
      "Best model: Epoch 11 \t loss=0.015596 \t val_loss=0.016249 \t time=0.82s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 14 \t loss=0.014689 \t val_loss=0.016195 \t time=0.81s\n",
      "Best model: Epoch 15 \t loss=0.014601 \t val_loss=0.016155 \t time=0.79s\n",
      "Best model: Epoch 16 \t loss=0.014468 \t val_loss=0.016137 \t time=0.80s\n",
      "Best model: Epoch 17 \t loss=0.014394 \t val_loss=0.016133 \t time=0.79s\n",
      "Best model: Epoch 20 \t loss=0.014197 \t val_loss=0.016098 \t time=0.81s\n",
      "Best model: Epoch 21 \t loss=0.014133 \t val_loss=0.016077 \t time=0.80s\n",
      "Fold 1 log loss: 0.016175372861022025\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.020658 \t val_loss=0.018239 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.018438 \t val_loss=0.017472 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017808 \t val_loss=0.017155 \t time=1.58s\n",
      "Best model: Epoch 4 \t loss=0.017370 \t val_loss=0.016905 \t time=1.00s\n",
      "Best model: Epoch 5 \t loss=0.017052 \t val_loss=0.016837 \t time=1.04s\n",
      "Best model: Epoch 6 \t loss=0.016712 \t val_loss=0.016586 \t time=1.00s\n",
      "Best model: Epoch 7 \t loss=0.016418 \t val_loss=0.016509 \t time=0.84s\n",
      "Best model: Epoch 8 \t loss=0.016183 \t val_loss=0.016378 \t time=0.82s\n",
      "Best model: Epoch 9 \t loss=0.015998 \t val_loss=0.016330 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.015580 \t val_loss=0.016245 \t time=0.87s\n",
      "Best model: Epoch 13 \t loss=0.015200 \t val_loss=0.016218 \t time=0.79s\n",
      "Best model: Epoch 15 \t loss=0.014890 \t val_loss=0.016167 \t time=0.79s\n",
      "Best model: Epoch 17 \t loss=0.014578 \t val_loss=0.016116 \t time=0.80s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.016183193311222664\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.020849 \t val_loss=0.018319 \t time=0.80s\n",
      "Best model: Epoch 2 \t loss=0.018511 \t val_loss=0.017522 \t time=0.77s\n",
      "Best model: Epoch 3 \t loss=0.017807 \t val_loss=0.017070 \t time=0.82s\n",
      "Best model: Epoch 4 \t loss=0.017432 \t val_loss=0.016825 \t time=0.81s\n",
      "Best model: Epoch 5 \t loss=0.017095 \t val_loss=0.016767 \t time=0.81s\n",
      "Best model: Epoch 6 \t loss=0.016801 \t val_loss=0.016619 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.016529 \t val_loss=0.016463 \t time=0.78s\n",
      "Best model: Epoch 8 \t loss=0.016318 \t val_loss=0.016325 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.016063 \t val_loss=0.016214 \t time=0.95s\n",
      "Best model: Epoch 11 \t loss=0.015661 \t val_loss=0.016208 \t time=0.77s\n",
      "Best model: Epoch 12 \t loss=0.015484 \t val_loss=0.016126 \t time=0.95s\n",
      "Best model: Epoch 13 \t loss=0.015304 \t val_loss=0.016095 \t time=0.79s\n",
      "Best model: Epoch 14 \t loss=0.015103 \t val_loss=0.016071 \t time=0.82s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 17 \t loss=0.014241 \t val_loss=0.016020 \t time=0.78s\n",
      "Best model: Epoch 18 \t loss=0.014241 \t val_loss=0.015997 \t time=1.03s\n",
      "Best model: Epoch 19 \t loss=0.014105 \t val_loss=0.015926 \t time=0.90s\n",
      "Best model: Epoch 21 \t loss=0.013990 \t val_loss=0.015887 \t time=0.78s\n",
      "Fold 3 log loss: 0.015890273253953025\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.020766 \t val_loss=0.018104 \t time=0.78s\n",
      "Best model: Epoch 2 \t loss=0.018509 \t val_loss=0.017377 \t time=0.78s\n",
      "Best model: Epoch 3 \t loss=0.017813 \t val_loss=0.017004 \t time=0.78s\n",
      "Best model: Epoch 4 \t loss=0.017278 \t val_loss=0.016768 \t time=0.80s\n",
      "Best model: Epoch 5 \t loss=0.016994 \t val_loss=0.016629 \t time=0.79s\n",
      "Best model: Epoch 6 \t loss=0.016740 \t val_loss=0.016468 \t time=0.80s\n",
      "Best model: Epoch 8 \t loss=0.016211 \t val_loss=0.016284 \t time=1.01s\n",
      "Best model: Epoch 10 \t loss=0.015786 \t val_loss=0.016225 \t time=0.79s\n",
      "Best model: Epoch 12 \t loss=0.015432 \t val_loss=0.016114 \t time=0.79s\n",
      "Best model: Epoch 13 \t loss=0.015264 \t val_loss=0.016072 \t time=0.78s\n",
      "Best model: Epoch 15 \t loss=0.014887 \t val_loss=0.016068 \t time=0.78s\n",
      "Best model: Epoch 16 \t loss=0.014771 \t val_loss=0.016001 \t time=0.78s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 19 \t loss=0.013916 \t val_loss=0.016000 \t time=0.78s\n",
      "Best model: Epoch 20 \t loss=0.013812 \t val_loss=0.015980 \t time=0.78s\n",
      "Best model: Epoch 22 \t loss=0.013605 \t val_loss=0.015954 \t time=1.18s\n",
      "Fold 4 log loss: 0.01601296242150395\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.020684 \t val_loss=0.018360 \t time=0.97s\n",
      "Best model: Epoch 2 \t loss=0.018432 \t val_loss=0.017603 \t time=0.97s\n",
      "Best model: Epoch 3 \t loss=0.017813 \t val_loss=0.017262 \t time=0.83s\n",
      "Best model: Epoch 4 \t loss=0.017317 \t val_loss=0.016991 \t time=1.06s\n",
      "Best model: Epoch 5 \t loss=0.016909 \t val_loss=0.016890 \t time=0.77s\n",
      "Best model: Epoch 6 \t loss=0.016669 \t val_loss=0.016694 \t time=0.79s\n",
      "Best model: Epoch 7 \t loss=0.016386 \t val_loss=0.016544 \t time=0.76s\n",
      "Best model: Epoch 8 \t loss=0.016170 \t val_loss=0.016517 \t time=0.79s\n",
      "Best model: Epoch 9 \t loss=0.015953 \t val_loss=0.016446 \t time=0.78s\n",
      "Best model: Epoch 10 \t loss=0.015747 \t val_loss=0.016406 \t time=0.80s\n",
      "Best model: Epoch 11 \t loss=0.015554 \t val_loss=0.016344 \t time=0.77s\n",
      "Best model: Epoch 13 \t loss=0.015187 \t val_loss=0.016310 \t time=0.77s\n",
      "Best model: Epoch 14 \t loss=0.014959 \t val_loss=0.016271 \t time=0.80s\n",
      "Best model: Epoch 16 \t loss=0.014652 \t val_loss=0.016253 \t time=0.78s\n",
      "Best model: Epoch 17 \t loss=0.014489 \t val_loss=0.016224 \t time=1.01s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 20 \t loss=0.013669 \t val_loss=0.016213 \t time=0.79s\n",
      "Best model: Epoch 21 \t loss=0.013582 \t val_loss=0.016209 \t time=0.79s\n",
      "Best model: Epoch 22 \t loss=0.013520 \t val_loss=0.016168 \t time=0.79s\n",
      "Best model: Epoch 24 \t loss=0.013313 \t val_loss=0.016149 \t time=0.77s\n",
      "Fold 5 log loss: 0.0161595191904402\n",
      "Seed 4\n",
      "Fold 1 log loss: 0.016175372861022025\n",
      "Fold 2 log loss: 0.016183193311222664\n",
      "Fold 3 log loss: 0.015890273253953025\n",
      "Fold 4 log loss: 0.01601296242150395\n",
      "Fold 5 log loss: 0.0161595191904402\n",
      "Std of log loss: 0.00011529744273971296\n",
      "Total log loss: 0.016084256271411572\n",
      "Total log loss in targets: 0.015883434942866266\n"
     ]
    }
   ],
   "source": [
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy().to_numpy()\n",
    "target_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "target_pred = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [0,1,2,3,4]  \n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], files)\n",
    "    target_oof += oof / len(seeds)\n",
    "    target_pred += pytorch_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, target_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T10:04:01.065642Z",
     "iopub.status.busy": "2020-10-09T10:04:01.064011Z",
     "iopub.status.idle": "2020-10-09T10:04:07.606410Z",
     "shell.execute_reply": "2020-10-09T10:04:07.607144Z"
    },
    "papermill": {
     "duration": 6.792145,
     "end_time": "2020-10-09T10:04:07.607336",
     "exception": false,
     "start_time": "2020-10-09T10:04:00.815191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014638852361049403\n"
     ]
    }
   ],
   "source": [
    "t = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "train_checkscore = t.copy()\n",
    "train_checkscore.loc[train_checkscore.index.isin(cons_train_index),target_feats] = target_oof\n",
    "train_checkscore.loc[train_checkscore.index.isin(noncons_train_index),target_feats] = 0\n",
    "t.drop(\"sig_id\", axis=1, inplace=True)\n",
    "print('OOF log loss: ', log_loss(np.ravel(t), np.ravel(np.array(train_checkscore.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-09T10:04:08.118281Z",
     "iopub.status.busy": "2020-10-09T10:04:08.117271Z",
     "iopub.status.idle": "2020-10-09T10:04:10.926953Z",
     "shell.execute_reply": "2020-10-09T10:04:10.926201Z"
    },
    "papermill": {
     "duration": 3.06163,
     "end_time": "2020-10-09T10:04:10.927102",
     "exception": false,
     "start_time": "2020-10-09T10:04:07.865472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub[target_feats] = target_pred\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.243838,
     "end_time": "2020-10-09T10:04:11.414226",
     "exception": false,
     "start_time": "2020-10-09T10:04:11.170388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 772.13047,
   "end_time": "2020-10-09T10:04:13.087006",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-09T09:51:20.956536",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
