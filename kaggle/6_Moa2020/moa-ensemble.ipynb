{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023398,
     "end_time": "2020-11-02T07:43:02.542363",
     "exception": false,
     "start_time": "2020-11-02T07:43:02.518965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- only ensemble of tabnet and mlp\n",
    "- change seed for weight optimization\n",
    "- reduce tabent ensemble to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:43:02.594471Z",
     "iopub.status.busy": "2020-11-02T07:43:02.593549Z",
     "iopub.status.idle": "2020-11-02T07:43:12.974997Z",
     "shell.execute_reply": "2020-11-02T07:43:12.973916Z"
    },
    "papermill": {
     "duration": 10.410227,
     "end_time": "2020-11-02T07:43:12.975137",
     "exception": false,
     "start_time": "2020-11-02T07:43:02.564910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.5)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:43:13.059066Z",
     "iopub.status.busy": "2020-11-02T07:43:13.058253Z",
     "iopub.status.idle": "2020-11-02T07:44:49.219769Z",
     "shell.execute_reply": "2020-11-02T07:44:49.220862Z"
    },
    "papermill": {
     "duration": 96.209734,
     "end_time": "2020-11-02T07:44:49.221075",
     "exception": false,
     "start_time": "2020-11-02T07:43:13.011341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-02T07:44:49.371467Z",
     "iopub.status.busy": "2020-11-02T07:44:49.369439Z",
     "iopub.status.idle": "2020-11-02T07:45:03.025658Z",
     "shell.execute_reply": "2020-11-02T07:45:03.030437Z"
    },
    "papermill": {
     "duration": 13.722839,
     "end_time": "2020-11-02T07:45:03.032434",
     "exception": false,
     "start_time": "2020-11-02T07:44:49.309595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cuml.svm import SVC, SVR\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03758,
     "end_time": "2020-11-02T07:45:03.276973",
     "exception": false,
     "start_time": "2020-11-02T07:45:03.239393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:03.359877Z",
     "iopub.status.busy": "2020-11-02T07:45:03.358842Z",
     "iopub.status.idle": "2020-11-02T07:45:10.794162Z",
     "shell.execute_reply": "2020-11-02T07:45:10.792910Z"
    },
    "papermill": {
     "duration": 7.480262,
     "end_time": "2020-11-02T07:45:10.794328",
     "exception": false,
     "start_time": "2020-11-02T07:45:03.314066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:10.854692Z",
     "iopub.status.busy": "2020-11-02T07:45:10.852584Z",
     "iopub.status.idle": "2020-11-02T07:45:10.855484Z",
     "shell.execute_reply": "2020-11-02T07:45:10.856212Z"
    },
    "papermill": {
     "duration": 0.03629,
     "end_time": "2020-11-02T07:45:10.856376",
     "exception": false,
     "start_time": "2020-11-02T07:45:10.820086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:10.928335Z",
     "iopub.status.busy": "2020-11-02T07:45:10.922134Z",
     "iopub.status.idle": "2020-11-02T07:45:11.413639Z",
     "shell.execute_reply": "2020-11-02T07:45:11.412977Z"
    },
    "papermill": {
     "duration": 0.53197,
     "end_time": "2020-11-02T07:45:11.413775",
     "exception": false,
     "start_time": "2020-11-02T07:45:10.881805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index\n",
    "\n",
    "test = test[test.index.isin(cons_test_index)].reset_index(drop=True)\n",
    "train = train[train.index.isin(cons_train_index)].reset_index(drop=True)\n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy()\n",
    "fn_targets = fn_targets[fn_targets.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "y = targets.drop(\"sig_id\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025328,
     "end_time": "2020-11-02T07:45:11.465031",
     "exception": false,
     "start_time": "2020-11-02T07:45:11.439703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:11.572003Z",
     "iopub.status.busy": "2020-11-02T07:45:11.541382Z",
     "iopub.status.idle": "2020-11-02T07:45:11.574551Z",
     "shell.execute_reply": "2020-11-02T07:45:11.573910Z"
    },
    "papermill": {
     "duration": 0.083838,
     "end_time": "2020-11-02T07:45:11.574679",
     "exception": false,
     "start_time": "2020-11-02T07:45:11.490841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe_simple(df, remove_features):\n",
    "    tmp = df.copy()\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    tmp.drop(remove_features, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def fe_mlp(df_train, df_test):\n",
    "    tmp_train = df_train.copy()\n",
    "    tmp_test = df_test.copy()\n",
    "    X = tmp_train.iloc[:,4:].copy().values\n",
    "    select = VarianceThreshold(threshold=0.7)\n",
    "    X_new = select.fit_transform(X)\n",
    "    drop_feats = list(np.array(tmp_train.iloc[:,4:].columns)[select.get_support()==False])\n",
    "    \n",
    "    tmp_train.drop(drop_feats, axis=1, inplace=True)\n",
    "    tmp_test.drop(drop_feats, axis=1, inplace=True)\n",
    "\n",
    "    modg_feats = [i for i in tmp_train.columns if \"g-\" in i]\n",
    "    modc_feats = [i for i in tmp_train.columns if \"c-\" in i]\n",
    "    \n",
    "    for i in modc_feats + modg_feats:\n",
    "        ss = preprocessing.QuantileTransformer(n_quantiles=1000, random_state=0, output_distribution=\"normal\")\n",
    "        ss.fit(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_train[i] = ss.transform(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_test[i] = ss.transform(tmp_test[i].values.reshape(-1,1))\n",
    "    \n",
    "    c_num = 10\n",
    "    pca_c_cols = [\"pca-c\"+str(i+1) for i in range(c_num)]\n",
    "    pca = PCA(n_components=c_num,random_state=42)\n",
    "    c_train = pca.fit_transform(tmp_train[modc_feats])\n",
    "    c_test = pca.transform(tmp_test[modc_feats])\n",
    "    c_train = pd.DataFrame(c_train, columns=pca_c_cols)\n",
    "    c_test = pd.DataFrame(c_test, columns=pca_c_cols)\n",
    "\n",
    "    g_num = 60\n",
    "    pca_g_cols = [\"pca-g\"+str(i+1) for i in range(g_num)]\n",
    "    pca = PCA(n_components=g_num, random_state=42)\n",
    "    g_train = pca.fit_transform(tmp_train[modg_feats])\n",
    "    g_test = pca.transform(tmp_test[modg_feats])\n",
    "    g_train = pd.DataFrame(g_train, columns=pca_g_cols)\n",
    "    g_test = pd.DataFrame(g_test, columns=pca_g_cols)\n",
    "\n",
    "    tmp_train = pd.concat([tmp_train, c_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, c_test],axis=1)\n",
    "    tmp_train = pd.concat([tmp_train, g_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, g_test],axis=1)\n",
    "    \n",
    "    return tmp_train, tmp_test\n",
    "\n",
    "def fe_mlp2(df):\n",
    "    tmp = df.copy()\n",
    "    modg_feats = [i for i in tmp.columns if \"g-\" in i]\n",
    "    modc_feats = [i for i in tmp.columns if \"c-\" in i]\n",
    "    tmp['g_sum'] = tmp[modg_feats].sum(axis = 1)\n",
    "    tmp['g_mean'] = tmp[modg_feats].mean(axis = 1)\n",
    "    tmp['g_std'] = tmp[modg_feats].std(axis = 1)\n",
    "    tmp['g_kurt'] = tmp[modg_feats].kurtosis(axis = 1)\n",
    "    tmp['g_skew'] = tmp[modg_feats].skew(axis = 1)\n",
    "    tmp['c_sum'] = tmp[modc_feats].sum(axis = 1)\n",
    "    tmp['c_mean'] = tmp[modc_feats].mean(axis = 1)\n",
    "    tmp['c_std'] = tmp[modc_feats].std(axis = 1)\n",
    "    tmp['c_kurt'] = tmp[modc_feats].kurtosis(axis = 1)\n",
    "    tmp['c_skew'] = tmp[modc_feats].skew(axis = 1)\n",
    "    tmp['gc_sum'] = tmp[modc_feats + modg_feats].sum(axis = 1)\n",
    "    tmp['gc_mean'] = tmp[modc_feats + modg_feats].mean(axis = 1)\n",
    "    tmp['gc_std'] = tmp[modc_feats + modg_feats].std(axis = 1)\n",
    "    tmp['gc_kurt'] = tmp[modc_feats + modg_feats].kurtosis(axis = 1)\n",
    "    tmp['gc_skew'] = tmp[modc_feats + modg_feats].skew(axis = 1)\n",
    "    tmp = pd.get_dummies(tmp, columns=['cp_time','cp_dose'])\n",
    "    tmp.drop([\"cp_type\", \"sig_id\"], axis=1, inplace=True) \n",
    "    return tmp\n",
    "\n",
    "def fe_tabnet(df_train, df_test):\n",
    "    tmp_train = df_train.copy()\n",
    "    tmp_test = df_test.copy()\n",
    "    \n",
    "    for i in c_feats + g_feats:\n",
    "        ss = preprocessing.QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n",
    "        ss.fit(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_train[i] = ss.transform(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_test[i] = ss.transform(tmp_test[i].values.reshape(-1,1))\n",
    "        \n",
    "    c_num = 1\n",
    "    pca_c_cols = [\"pca-c\"+str(i+1) for i in range(c_num)]\n",
    "    pca = PCA(n_components=c_num,random_state=42)\n",
    "    c_train = pca.fit_transform(tmp_train[c_feats])\n",
    "    c_test = pca.transform(tmp_test[c_feats])\n",
    "    c_train = pd.DataFrame(c_train, columns=pca_c_cols)\n",
    "    c_test = pd.DataFrame(c_test, columns=pca_c_cols)\n",
    "\n",
    "    g_num = 10\n",
    "    pca_g_cols = [\"pca-g\"+str(i+1) for i in range(g_num)]\n",
    "    pca = PCA(n_components=g_num, random_state=42)\n",
    "    g_train = pca.fit_transform(tmp_train[g_feats])\n",
    "    g_test = pca.transform(tmp_test[g_feats])\n",
    "    g_train = pd.DataFrame(g_train, columns=pca_g_cols)\n",
    "    g_test = pd.DataFrame(g_test, columns=pca_g_cols)\n",
    "\n",
    "    tmp_train = pd.concat([tmp_train, c_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, c_test],axis=1)\n",
    "    tmp_train = pd.concat([tmp_train, g_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, g_test],axis=1)\n",
    "        \n",
    "    return tmp_train, tmp_test\n",
    "\n",
    "remove_features = [\"cp_type\" , \"sig_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:11.638428Z",
     "iopub.status.busy": "2020-11-02T07:45:11.636878Z",
     "iopub.status.idle": "2020-11-02T07:45:45.244884Z",
     "shell.execute_reply": "2020-11-02T07:45:45.245712Z"
    },
    "papermill": {
     "duration": 33.645084,
     "end_time": "2020-11-02T07:45:45.245873",
     "exception": false,
     "start_time": "2020-11-02T07:45:11.600789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 929) (3624, 929)\n",
      "(21948, 885) (3624, 885)\n",
      "(21948, 874) (3624, 874)\n"
     ]
    }
   ],
   "source": [
    "# pytorch mlp -----------------------------------\n",
    "mlp_train, mlp_test = fe_mlp(train, test)\n",
    "mlp_train = fe_mlp2(mlp_train).to_numpy()\n",
    "mlp_test = fe_mlp2(mlp_test).to_numpy()\n",
    "\n",
    "# pytorch tabnet ----------------------------------\n",
    "tab_train, tab_test = fe_tabnet(train, test)\n",
    "tab_train = fe_simple(tab_train, remove_features)\n",
    "tab_test = fe_simple(tab_test, remove_features)\n",
    "\n",
    "tab_train = tab_train.to_numpy()\n",
    "tab_test = tab_test.to_numpy()\n",
    "\n",
    "# svm-----------------------\n",
    "fn_train = fe_simple(train, remove_features)\n",
    "fn_test = fe_simple(test, remove_features)\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "print(mlp_train.shape, mlp_test.shape)\n",
    "print(tab_train.shape, tab_test.shape)\n",
    "print(fn_train.shape, fn_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025691,
     "end_time": "2020-11-02T07:45:45.297458",
     "exception": false,
     "start_time": "2020-11-02T07:45:45.271767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:45.363141Z",
     "iopub.status.busy": "2020-11-02T07:45:45.362048Z",
     "iopub.status.idle": "2020-11-02T07:45:45.365372Z",
     "shell.execute_reply": "2020-11-02T07:45:45.364793Z"
    },
    "papermill": {
     "duration": 0.041819,
     "end_time": "2020-11-02T07:45:45.365493",
     "exception": false,
     "start_time": "2020-11-02T07:45:45.323674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:45.484659Z",
     "iopub.status.busy": "2020-11-02T07:45:45.442750Z",
     "iopub.status.idle": "2020-11-02T07:45:45.487728Z",
     "shell.execute_reply": "2020-11-02T07:45:45.487088Z"
    },
    "papermill": {
     "duration": 0.096029,
     "end_time": "2020-11-02T07:45:45.487841",
     "exception": false,
     "start_time": "2020-11-02T07:45:45.391812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128\n",
    "n_folds=7\n",
    "train_epochs = 20\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 1024))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(1024, 1024))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, last_num))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def modelling_torch(X_train, y_train, X_test, sample_seed, last_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=224)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test = torch.utils.data.TensorDataset(X_test2) \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = MoaModel(mlp_train.shape[1], last_num)\n",
    "        loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "        \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        \n",
    "        for epoch in range(train_epochs):\n",
    "            clf.train()\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            sm_avg_val_loss = 0.\n",
    "            \n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                            \n",
    "            if sm_avg_val_loss < best_val_loss:\n",
    "                best_val_loss = sm_avg_val_loss\n",
    "                print('Epoch {} sm_loss={:.5f}  sm_val_loss={:.5f}'.format(epoch + 1, sm_avg_loss, sm_avg_val_loss))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "                \n",
    "        pred_model = MoaModel(mlp_train.shape[1], last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))         \n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            oof_epoch[i * batch_size:(i+1) * batch_size,:] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "            target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:45:45.554110Z",
     "iopub.status.busy": "2020-11-02T07:45:45.551981Z",
     "iopub.status.idle": "2020-11-02T07:58:53.050239Z",
     "shell.execute_reply": "2020-11-02T07:58:53.051047Z"
    },
    "papermill": {
     "duration": 787.536203,
     "end_time": "2020-11-02T07:58:53.051329",
     "exception": false,
     "start_time": "2020-11-02T07:45:45.515126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 sm_loss=0.41417  sm_val_loss=0.02222\n",
      "Epoch 2 sm_loss=0.02011  sm_val_loss=0.01953\n",
      "Epoch 3 sm_loss=0.01818  sm_val_loss=0.01902\n",
      "Epoch 4 sm_loss=0.01763  sm_val_loss=0.01781\n",
      "Epoch 5 sm_loss=0.01750  sm_val_loss=0.01770\n",
      "Epoch 6 sm_loss=0.01744  sm_val_loss=0.01759\n",
      "Epoch 7 sm_loss=0.01743  sm_val_loss=0.01740\n",
      "Epoch 8 sm_loss=0.01734  sm_val_loss=0.01735\n",
      "Epoch 11 sm_loss=0.01707  sm_val_loss=0.01709\n",
      "Epoch 12 sm_loss=0.01687  sm_val_loss=0.01680\n",
      "Epoch 13 sm_loss=0.01671  sm_val_loss=0.01674\n",
      "Epoch 14 sm_loss=0.01642  sm_val_loss=0.01673\n",
      "Epoch 15 sm_loss=0.01609  sm_val_loss=0.01636\n",
      "Epoch 16 sm_loss=0.01566  sm_val_loss=0.01632\n",
      "Epoch 17 sm_loss=0.01520  sm_val_loss=0.01608\n",
      "Epoch 18 sm_loss=0.01475  sm_val_loss=0.01598\n",
      "Epoch 19 sm_loss=0.01437  sm_val_loss=0.01595\n",
      "Epoch 20 sm_loss=0.01419  sm_val_loss=0.01592\n",
      "Fold 2\n",
      "Epoch 1 sm_loss=0.41399  sm_val_loss=0.02262\n",
      "Epoch 2 sm_loss=0.02004  sm_val_loss=0.02017\n",
      "Epoch 4 sm_loss=0.01781  sm_val_loss=0.01759\n",
      "Epoch 7 sm_loss=0.01742  sm_val_loss=0.01745\n",
      "Epoch 8 sm_loss=0.01737  sm_val_loss=0.01740\n",
      "Epoch 11 sm_loss=0.01709  sm_val_loss=0.01712\n",
      "Epoch 12 sm_loss=0.01686  sm_val_loss=0.01711\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01691\n",
      "Epoch 15 sm_loss=0.01607  sm_val_loss=0.01647\n",
      "Epoch 16 sm_loss=0.01561  sm_val_loss=0.01637\n",
      "Epoch 17 sm_loss=0.01516  sm_val_loss=0.01629\n",
      "Epoch 18 sm_loss=0.01472  sm_val_loss=0.01620\n",
      "Epoch 19 sm_loss=0.01431  sm_val_loss=0.01614\n",
      "Fold 3\n",
      "Epoch 1 sm_loss=0.41327  sm_val_loss=0.02234\n",
      "Epoch 2 sm_loss=0.02013  sm_val_loss=0.01860\n",
      "Epoch 3 sm_loss=0.01823  sm_val_loss=0.01806\n",
      "Epoch 4 sm_loss=0.01771  sm_val_loss=0.01723\n",
      "Epoch 6 sm_loss=0.01746  sm_val_loss=0.01707\n",
      "Epoch 10 sm_loss=0.01719  sm_val_loss=0.01691\n",
      "Epoch 11 sm_loss=0.01709  sm_val_loss=0.01670\n",
      "Epoch 13 sm_loss=0.01670  sm_val_loss=0.01643\n",
      "Epoch 14 sm_loss=0.01641  sm_val_loss=0.01634\n",
      "Epoch 15 sm_loss=0.01607  sm_val_loss=0.01615\n",
      "Epoch 16 sm_loss=0.01569  sm_val_loss=0.01598\n",
      "Epoch 17 sm_loss=0.01523  sm_val_loss=0.01591\n",
      "Epoch 18 sm_loss=0.01474  sm_val_loss=0.01582\n",
      "Epoch 19 sm_loss=0.01433  sm_val_loss=0.01572\n",
      "Fold 4\n",
      "Epoch 1 sm_loss=0.41474  sm_val_loss=0.02265\n",
      "Epoch 2 sm_loss=0.02026  sm_val_loss=0.01899\n",
      "Epoch 3 sm_loss=0.01819  sm_val_loss=0.01783\n",
      "Epoch 4 sm_loss=0.01763  sm_val_loss=0.01759\n",
      "Epoch 5 sm_loss=0.01744  sm_val_loss=0.01758\n",
      "Epoch 9 sm_loss=0.01728  sm_val_loss=0.01755\n",
      "Epoch 10 sm_loss=0.01717  sm_val_loss=0.01707\n",
      "Epoch 12 sm_loss=0.01680  sm_val_loss=0.01702\n",
      "Epoch 13 sm_loss=0.01665  sm_val_loss=0.01690\n",
      "Epoch 14 sm_loss=0.01628  sm_val_loss=0.01663\n",
      "Epoch 15 sm_loss=0.01605  sm_val_loss=0.01659\n",
      "Epoch 16 sm_loss=0.01558  sm_val_loss=0.01639\n",
      "Epoch 17 sm_loss=0.01509  sm_val_loss=0.01626\n",
      "Epoch 18 sm_loss=0.01463  sm_val_loss=0.01617\n",
      "Epoch 19 sm_loss=0.01422  sm_val_loss=0.01613\n",
      "Epoch 20 sm_loss=0.01398  sm_val_loss=0.01613\n",
      "Fold 5\n",
      "Epoch 1 sm_loss=0.41454  sm_val_loss=0.02291\n",
      "Epoch 2 sm_loss=0.02000  sm_val_loss=0.01854\n",
      "Epoch 3 sm_loss=0.01808  sm_val_loss=0.01815\n",
      "Epoch 4 sm_loss=0.01752  sm_val_loss=0.01771\n",
      "Epoch 8 sm_loss=0.01733  sm_val_loss=0.01769\n",
      "Epoch 9 sm_loss=0.01725  sm_val_loss=0.01764\n",
      "Epoch 10 sm_loss=0.01713  sm_val_loss=0.01755\n",
      "Epoch 11 sm_loss=0.01701  sm_val_loss=0.01733\n",
      "Epoch 13 sm_loss=0.01662  sm_val_loss=0.01710\n",
      "Epoch 14 sm_loss=0.01634  sm_val_loss=0.01686\n",
      "Epoch 15 sm_loss=0.01601  sm_val_loss=0.01664\n",
      "Epoch 16 sm_loss=0.01555  sm_val_loss=0.01654\n",
      "Epoch 17 sm_loss=0.01508  sm_val_loss=0.01645\n",
      "Epoch 18 sm_loss=0.01458  sm_val_loss=0.01637\n",
      "Epoch 19 sm_loss=0.01420  sm_val_loss=0.01631\n",
      "Fold 6\n",
      "Epoch 1 sm_loss=0.41390  sm_val_loss=0.02243\n",
      "Epoch 2 sm_loss=0.02009  sm_val_loss=0.01890\n",
      "Epoch 3 sm_loss=0.01825  sm_val_loss=0.01855\n",
      "Epoch 4 sm_loss=0.01764  sm_val_loss=0.01738\n",
      "Epoch 7 sm_loss=0.01742  sm_val_loss=0.01735\n",
      "Epoch 8 sm_loss=0.01740  sm_val_loss=0.01724\n",
      "Epoch 9 sm_loss=0.01738  sm_val_loss=0.01722\n",
      "Epoch 10 sm_loss=0.01727  sm_val_loss=0.01702\n",
      "Epoch 11 sm_loss=0.01706  sm_val_loss=0.01682\n",
      "Epoch 13 sm_loss=0.01675  sm_val_loss=0.01660\n",
      "Epoch 14 sm_loss=0.01646  sm_val_loss=0.01650\n",
      "Epoch 15 sm_loss=0.01611  sm_val_loss=0.01624\n",
      "Epoch 16 sm_loss=0.01578  sm_val_loss=0.01608\n",
      "Epoch 17 sm_loss=0.01530  sm_val_loss=0.01591\n",
      "Epoch 18 sm_loss=0.01483  sm_val_loss=0.01581\n",
      "Epoch 19 sm_loss=0.01448  sm_val_loss=0.01576\n",
      "Epoch 20 sm_loss=0.01428  sm_val_loss=0.01576\n",
      "Fold 7\n",
      "Epoch 1 sm_loss=0.41471  sm_val_loss=0.02247\n",
      "Epoch 2 sm_loss=0.02007  sm_val_loss=0.01854\n",
      "Epoch 3 sm_loss=0.01824  sm_val_loss=0.01780\n",
      "Epoch 4 sm_loss=0.01768  sm_val_loss=0.01753\n",
      "Epoch 5 sm_loss=0.01747  sm_val_loss=0.01750\n",
      "Epoch 8 sm_loss=0.01741  sm_val_loss=0.01728\n",
      "Epoch 10 sm_loss=0.01725  sm_val_loss=0.01708\n",
      "Epoch 11 sm_loss=0.01708  sm_val_loss=0.01707\n",
      "Epoch 12 sm_loss=0.01691  sm_val_loss=0.01689\n",
      "Epoch 13 sm_loss=0.01674  sm_val_loss=0.01682\n",
      "Epoch 14 sm_loss=0.01638  sm_val_loss=0.01651\n",
      "Epoch 15 sm_loss=0.01601  sm_val_loss=0.01632\n",
      "Epoch 16 sm_loss=0.01571  sm_val_loss=0.01622\n",
      "Epoch 17 sm_loss=0.01519  sm_val_loss=0.01611\n",
      "Epoch 18 sm_loss=0.01469  sm_val_loss=0.01602\n",
      "Epoch 19 sm_loss=0.01431  sm_val_loss=0.01597\n",
      "Epoch 20 sm_loss=0.01409  sm_val_loss=0.01596\n",
      "Seed 0\n",
      "Total log loss: 0.016019747647044533\n",
      "Fold 1\n",
      "Epoch 1 sm_loss=0.41406  sm_val_loss=0.02259\n",
      "Epoch 2 sm_loss=0.02008  sm_val_loss=0.01875\n",
      "Epoch 3 sm_loss=0.01814  sm_val_loss=0.01821\n",
      "Epoch 4 sm_loss=0.01764  sm_val_loss=0.01760\n",
      "Epoch 5 sm_loss=0.01741  sm_val_loss=0.01749\n",
      "Epoch 8 sm_loss=0.01739  sm_val_loss=0.01733\n",
      "Epoch 10 sm_loss=0.01723  sm_val_loss=0.01732\n",
      "Epoch 11 sm_loss=0.01704  sm_val_loss=0.01726\n",
      "Epoch 12 sm_loss=0.01695  sm_val_loss=0.01699\n",
      "Epoch 13 sm_loss=0.01667  sm_val_loss=0.01669\n",
      "Epoch 15 sm_loss=0.01604  sm_val_loss=0.01644\n",
      "Epoch 16 sm_loss=0.01573  sm_val_loss=0.01623\n",
      "Epoch 17 sm_loss=0.01526  sm_val_loss=0.01612\n",
      "Epoch 18 sm_loss=0.01478  sm_val_loss=0.01604\n",
      "Epoch 19 sm_loss=0.01440  sm_val_loss=0.01595\n",
      "Fold 2\n",
      "Epoch 1 sm_loss=0.41428  sm_val_loss=0.02257\n",
      "Epoch 2 sm_loss=0.02041  sm_val_loss=0.01965\n",
      "Epoch 3 sm_loss=0.01836  sm_val_loss=0.01791\n",
      "Epoch 5 sm_loss=0.01759  sm_val_loss=0.01762\n",
      "Epoch 6 sm_loss=0.01751  sm_val_loss=0.01755\n",
      "Epoch 9 sm_loss=0.01735  sm_val_loss=0.01721\n",
      "Epoch 12 sm_loss=0.01688  sm_val_loss=0.01697\n",
      "Epoch 14 sm_loss=0.01641  sm_val_loss=0.01671\n",
      "Epoch 15 sm_loss=0.01604  sm_val_loss=0.01651\n",
      "Epoch 16 sm_loss=0.01567  sm_val_loss=0.01632\n",
      "Epoch 17 sm_loss=0.01516  sm_val_loss=0.01629\n",
      "Epoch 18 sm_loss=0.01471  sm_val_loss=0.01613\n",
      "Epoch 19 sm_loss=0.01429  sm_val_loss=0.01610\n",
      "Epoch 20 sm_loss=0.01407  sm_val_loss=0.01608\n",
      "Fold 3\n",
      "Epoch 1 sm_loss=0.41425  sm_val_loss=0.02221\n",
      "Epoch 2 sm_loss=0.02001  sm_val_loss=0.01817\n",
      "Epoch 3 sm_loss=0.01825  sm_val_loss=0.01739\n",
      "Epoch 4 sm_loss=0.01763  sm_val_loss=0.01736\n",
      "Epoch 6 sm_loss=0.01754  sm_val_loss=0.01725\n",
      "Epoch 8 sm_loss=0.01742  sm_val_loss=0.01714\n",
      "Epoch 9 sm_loss=0.01735  sm_val_loss=0.01711\n",
      "Epoch 10 sm_loss=0.01724  sm_val_loss=0.01706\n",
      "Epoch 11 sm_loss=0.01718  sm_val_loss=0.01695\n",
      "Epoch 12 sm_loss=0.01690  sm_val_loss=0.01663\n",
      "Epoch 13 sm_loss=0.01673  sm_val_loss=0.01641\n",
      "Epoch 14 sm_loss=0.01644  sm_val_loss=0.01628\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01617\n",
      "Epoch 16 sm_loss=0.01564  sm_val_loss=0.01604\n",
      "Epoch 17 sm_loss=0.01521  sm_val_loss=0.01585\n",
      "Epoch 18 sm_loss=0.01473  sm_val_loss=0.01582\n",
      "Epoch 19 sm_loss=0.01434  sm_val_loss=0.01578\n",
      "Epoch 20 sm_loss=0.01411  sm_val_loss=0.01574\n",
      "Fold 4\n",
      "Epoch 1 sm_loss=0.41363  sm_val_loss=0.02255\n",
      "Epoch 2 sm_loss=0.02003  sm_val_loss=0.01884\n",
      "Epoch 3 sm_loss=0.01806  sm_val_loss=0.01802\n",
      "Epoch 4 sm_loss=0.01760  sm_val_loss=0.01797\n",
      "Epoch 5 sm_loss=0.01740  sm_val_loss=0.01777\n",
      "Epoch 6 sm_loss=0.01737  sm_val_loss=0.01750\n",
      "Epoch 8 sm_loss=0.01738  sm_val_loss=0.01711\n",
      "Epoch 13 sm_loss=0.01659  sm_val_loss=0.01687\n",
      "Epoch 14 sm_loss=0.01637  sm_val_loss=0.01667\n",
      "Epoch 15 sm_loss=0.01607  sm_val_loss=0.01661\n",
      "Epoch 16 sm_loss=0.01564  sm_val_loss=0.01643\n",
      "Epoch 17 sm_loss=0.01519  sm_val_loss=0.01633\n",
      "Epoch 18 sm_loss=0.01472  sm_val_loss=0.01618\n",
      "Epoch 19 sm_loss=0.01432  sm_val_loss=0.01612\n",
      "Fold 5\n",
      "Epoch 1 sm_loss=0.41480  sm_val_loss=0.02265\n",
      "Epoch 2 sm_loss=0.02000  sm_val_loss=0.01896\n",
      "Epoch 3 sm_loss=0.01819  sm_val_loss=0.01833\n",
      "Epoch 4 sm_loss=0.01755  sm_val_loss=0.01783\n",
      "Epoch 5 sm_loss=0.01743  sm_val_loss=0.01764\n",
      "Epoch 11 sm_loss=0.01708  sm_val_loss=0.01749\n",
      "Epoch 12 sm_loss=0.01685  sm_val_loss=0.01723\n",
      "Epoch 13 sm_loss=0.01662  sm_val_loss=0.01709\n",
      "Epoch 14 sm_loss=0.01637  sm_val_loss=0.01682\n",
      "Epoch 15 sm_loss=0.01602  sm_val_loss=0.01678\n",
      "Epoch 16 sm_loss=0.01564  sm_val_loss=0.01660\n",
      "Epoch 17 sm_loss=0.01522  sm_val_loss=0.01645\n",
      "Epoch 18 sm_loss=0.01477  sm_val_loss=0.01634\n",
      "Epoch 19 sm_loss=0.01439  sm_val_loss=0.01631\n",
      "Fold 6\n",
      "Epoch 1 sm_loss=0.41438  sm_val_loss=0.02245\n",
      "Epoch 2 sm_loss=0.02007  sm_val_loss=0.01876\n",
      "Epoch 3 sm_loss=0.01855  sm_val_loss=0.01770\n",
      "Epoch 4 sm_loss=0.01766  sm_val_loss=0.01750\n",
      "Epoch 6 sm_loss=0.01756  sm_val_loss=0.01739\n",
      "Epoch 7 sm_loss=0.01752  sm_val_loss=0.01730\n",
      "Epoch 9 sm_loss=0.01734  sm_val_loss=0.01716\n",
      "Epoch 10 sm_loss=0.01723  sm_val_loss=0.01714\n",
      "Epoch 11 sm_loss=0.01714  sm_val_loss=0.01688\n",
      "Epoch 12 sm_loss=0.01693  sm_val_loss=0.01683\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01669\n",
      "Epoch 14 sm_loss=0.01650  sm_val_loss=0.01649\n",
      "Epoch 15 sm_loss=0.01611  sm_val_loss=0.01637\n",
      "Epoch 16 sm_loss=0.01579  sm_val_loss=0.01617\n",
      "Epoch 17 sm_loss=0.01533  sm_val_loss=0.01599\n",
      "Epoch 18 sm_loss=0.01487  sm_val_loss=0.01588\n",
      "Epoch 19 sm_loss=0.01450  sm_val_loss=0.01580\n",
      "Fold 7\n",
      "Epoch 1 sm_loss=0.41382  sm_val_loss=0.02258\n",
      "Epoch 2 sm_loss=0.02019  sm_val_loss=0.01870\n",
      "Epoch 3 sm_loss=0.01818  sm_val_loss=0.01759\n",
      "Epoch 4 sm_loss=0.01759  sm_val_loss=0.01755\n",
      "Epoch 6 sm_loss=0.01750  sm_val_loss=0.01739\n",
      "Epoch 7 sm_loss=0.01743  sm_val_loss=0.01738\n",
      "Epoch 8 sm_loss=0.01739  sm_val_loss=0.01734\n",
      "Epoch 11 sm_loss=0.01712  sm_val_loss=0.01724\n",
      "Epoch 12 sm_loss=0.01690  sm_val_loss=0.01695\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01667\n",
      "Epoch 14 sm_loss=0.01641  sm_val_loss=0.01658\n",
      "Epoch 15 sm_loss=0.01609  sm_val_loss=0.01638\n",
      "Epoch 16 sm_loss=0.01566  sm_val_loss=0.01633\n",
      "Epoch 17 sm_loss=0.01523  sm_val_loss=0.01609\n",
      "Epoch 18 sm_loss=0.01475  sm_val_loss=0.01603\n",
      "Epoch 19 sm_loss=0.01437  sm_val_loss=0.01600\n",
      "Seed 1\n",
      "Total log loss: 0.01602759954171412\n",
      "Fold 1\n",
      "Epoch 1 sm_loss=0.41478  sm_val_loss=0.02309\n",
      "Epoch 2 sm_loss=0.02020  sm_val_loss=0.01892\n",
      "Epoch 3 sm_loss=0.01807  sm_val_loss=0.01786\n",
      "Epoch 5 sm_loss=0.01740  sm_val_loss=0.01766\n",
      "Epoch 6 sm_loss=0.01742  sm_val_loss=0.01750\n",
      "Epoch 8 sm_loss=0.01731  sm_val_loss=0.01745\n",
      "Epoch 9 sm_loss=0.01726  sm_val_loss=0.01733\n",
      "Epoch 12 sm_loss=0.01688  sm_val_loss=0.01695\n",
      "Epoch 13 sm_loss=0.01665  sm_val_loss=0.01682\n",
      "Epoch 14 sm_loss=0.01634  sm_val_loss=0.01664\n",
      "Epoch 15 sm_loss=0.01603  sm_val_loss=0.01645\n",
      "Epoch 16 sm_loss=0.01557  sm_val_loss=0.01626\n",
      "Epoch 17 sm_loss=0.01515  sm_val_loss=0.01611\n",
      "Epoch 18 sm_loss=0.01467  sm_val_loss=0.01602\n",
      "Epoch 19 sm_loss=0.01422  sm_val_loss=0.01595\n",
      "Fold 2\n",
      "Epoch 1 sm_loss=0.41456  sm_val_loss=0.02264\n",
      "Epoch 2 sm_loss=0.02006  sm_val_loss=0.01966\n",
      "Epoch 3 sm_loss=0.01812  sm_val_loss=0.01795\n",
      "Epoch 4 sm_loss=0.01755  sm_val_loss=0.01789\n",
      "Epoch 5 sm_loss=0.01748  sm_val_loss=0.01773\n",
      "Epoch 8 sm_loss=0.01744  sm_val_loss=0.01752\n",
      "Epoch 10 sm_loss=0.01722  sm_val_loss=0.01721\n",
      "Epoch 11 sm_loss=0.01709  sm_val_loss=0.01717\n",
      "Epoch 13 sm_loss=0.01664  sm_val_loss=0.01696\n",
      "Epoch 14 sm_loss=0.01640  sm_val_loss=0.01661\n",
      "Epoch 15 sm_loss=0.01609  sm_val_loss=0.01654\n",
      "Epoch 16 sm_loss=0.01571  sm_val_loss=0.01640\n",
      "Epoch 17 sm_loss=0.01521  sm_val_loss=0.01628\n",
      "Epoch 18 sm_loss=0.01476  sm_val_loss=0.01622\n",
      "Epoch 19 sm_loss=0.01438  sm_val_loss=0.01614\n",
      "Epoch 20 sm_loss=0.01419  sm_val_loss=0.01612\n",
      "Fold 3\n",
      "Epoch 1 sm_loss=0.41348  sm_val_loss=0.02245\n",
      "Epoch 2 sm_loss=0.02000  sm_val_loss=0.01825\n",
      "Epoch 3 sm_loss=0.01824  sm_val_loss=0.01755\n",
      "Epoch 4 sm_loss=0.01770  sm_val_loss=0.01739\n",
      "Epoch 5 sm_loss=0.01754  sm_val_loss=0.01733\n",
      "Epoch 6 sm_loss=0.01757  sm_val_loss=0.01721\n",
      "Epoch 7 sm_loss=0.01752  sm_val_loss=0.01719\n",
      "Epoch 8 sm_loss=0.01750  sm_val_loss=0.01714\n",
      "Epoch 9 sm_loss=0.01738  sm_val_loss=0.01703\n",
      "Epoch 10 sm_loss=0.01729  sm_val_loss=0.01693\n",
      "Epoch 11 sm_loss=0.01716  sm_val_loss=0.01681\n",
      "Epoch 12 sm_loss=0.01698  sm_val_loss=0.01657\n",
      "Epoch 14 sm_loss=0.01652  sm_val_loss=0.01634\n",
      "Epoch 15 sm_loss=0.01619  sm_val_loss=0.01623\n",
      "Epoch 16 sm_loss=0.01580  sm_val_loss=0.01601\n",
      "Epoch 17 sm_loss=0.01536  sm_val_loss=0.01586\n",
      "Epoch 18 sm_loss=0.01496  sm_val_loss=0.01575\n",
      "Epoch 19 sm_loss=0.01460  sm_val_loss=0.01574\n",
      "Epoch 20 sm_loss=0.01441  sm_val_loss=0.01573\n",
      "Fold 4\n",
      "Epoch 1 sm_loss=0.41455  sm_val_loss=0.02231\n",
      "Epoch 2 sm_loss=0.02003  sm_val_loss=0.01863\n",
      "Epoch 3 sm_loss=0.01818  sm_val_loss=0.01835\n",
      "Epoch 4 sm_loss=0.01752  sm_val_loss=0.01828\n",
      "Epoch 5 sm_loss=0.01745  sm_val_loss=0.01768\n",
      "Epoch 6 sm_loss=0.01744  sm_val_loss=0.01749\n",
      "Epoch 8 sm_loss=0.01740  sm_val_loss=0.01738\n",
      "Epoch 10 sm_loss=0.01718  sm_val_loss=0.01732\n",
      "Epoch 11 sm_loss=0.01701  sm_val_loss=0.01725\n",
      "Epoch 12 sm_loss=0.01681  sm_val_loss=0.01707\n",
      "Epoch 13 sm_loss=0.01664  sm_val_loss=0.01704\n",
      "Epoch 14 sm_loss=0.01633  sm_val_loss=0.01674\n",
      "Epoch 15 sm_loss=0.01600  sm_val_loss=0.01657\n",
      "Epoch 16 sm_loss=0.01564  sm_val_loss=0.01652\n",
      "Epoch 17 sm_loss=0.01516  sm_val_loss=0.01632\n",
      "Epoch 18 sm_loss=0.01470  sm_val_loss=0.01621\n",
      "Epoch 19 sm_loss=0.01432  sm_val_loss=0.01617\n",
      "Epoch 20 sm_loss=0.01414  sm_val_loss=0.01614\n",
      "Fold 5\n",
      "Epoch 1 sm_loss=0.41370  sm_val_loss=0.02295\n",
      "Epoch 2 sm_loss=0.02015  sm_val_loss=0.02016\n",
      "Epoch 3 sm_loss=0.01823  sm_val_loss=0.01784\n",
      "Epoch 5 sm_loss=0.01744  sm_val_loss=0.01762\n",
      "Epoch 9 sm_loss=0.01721  sm_val_loss=0.01756\n",
      "Epoch 10 sm_loss=0.01717  sm_val_loss=0.01743\n",
      "Epoch 11 sm_loss=0.01705  sm_val_loss=0.01725\n",
      "Epoch 12 sm_loss=0.01679  sm_val_loss=0.01721\n",
      "Epoch 13 sm_loss=0.01662  sm_val_loss=0.01699\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01667\n",
      "Epoch 16 sm_loss=0.01565  sm_val_loss=0.01656\n",
      "Epoch 17 sm_loss=0.01521  sm_val_loss=0.01639\n",
      "Epoch 18 sm_loss=0.01477  sm_val_loss=0.01639\n",
      "Epoch 19 sm_loss=0.01440  sm_val_loss=0.01630\n",
      "Epoch 20 sm_loss=0.01418  sm_val_loss=0.01630\n",
      "Fold 6\n",
      "Epoch 1 sm_loss=0.41409  sm_val_loss=0.02212\n",
      "Epoch 2 sm_loss=0.02030  sm_val_loss=0.01864\n",
      "Epoch 3 sm_loss=0.01817  sm_val_loss=0.01765\n",
      "Epoch 4 sm_loss=0.01761  sm_val_loss=0.01746\n",
      "Epoch 7 sm_loss=0.01745  sm_val_loss=0.01741\n",
      "Epoch 9 sm_loss=0.01743  sm_val_loss=0.01735\n",
      "Epoch 10 sm_loss=0.01731  sm_val_loss=0.01710\n",
      "Epoch 12 sm_loss=0.01697  sm_val_loss=0.01684\n",
      "Epoch 13 sm_loss=0.01676  sm_val_loss=0.01654\n",
      "Epoch 14 sm_loss=0.01645  sm_val_loss=0.01645\n",
      "Epoch 15 sm_loss=0.01622  sm_val_loss=0.01628\n",
      "Epoch 16 sm_loss=0.01578  sm_val_loss=0.01613\n",
      "Epoch 17 sm_loss=0.01540  sm_val_loss=0.01598\n",
      "Epoch 18 sm_loss=0.01492  sm_val_loss=0.01586\n",
      "Epoch 19 sm_loss=0.01454  sm_val_loss=0.01581\n",
      "Fold 7\n",
      "Epoch 1 sm_loss=0.41460  sm_val_loss=0.02264\n",
      "Epoch 2 sm_loss=0.02007  sm_val_loss=0.01872\n",
      "Epoch 3 sm_loss=0.01818  sm_val_loss=0.01775\n",
      "Epoch 4 sm_loss=0.01762  sm_val_loss=0.01770\n",
      "Epoch 5 sm_loss=0.01752  sm_val_loss=0.01741\n",
      "Epoch 8 sm_loss=0.01739  sm_val_loss=0.01740\n",
      "Epoch 9 sm_loss=0.01728  sm_val_loss=0.01726\n",
      "Epoch 10 sm_loss=0.01724  sm_val_loss=0.01724\n",
      "Epoch 11 sm_loss=0.01702  sm_val_loss=0.01697\n",
      "Epoch 12 sm_loss=0.01693  sm_val_loss=0.01693\n",
      "Epoch 13 sm_loss=0.01669  sm_val_loss=0.01669\n",
      "Epoch 14 sm_loss=0.01633  sm_val_loss=0.01654\n",
      "Epoch 15 sm_loss=0.01601  sm_val_loss=0.01645\n",
      "Epoch 16 sm_loss=0.01562  sm_val_loss=0.01623\n",
      "Epoch 17 sm_loss=0.01521  sm_val_loss=0.01616\n",
      "Epoch 18 sm_loss=0.01470  sm_val_loss=0.01603\n",
      "Epoch 19 sm_loss=0.01436  sm_val_loss=0.01597\n",
      "Epoch 20 sm_loss=0.01412  sm_val_loss=0.01596\n",
      "Seed 2\n",
      "Total log loss: 0.016031838518680914\n",
      "Fold 1\n",
      "Epoch 1 sm_loss=0.41534  sm_val_loss=0.02231\n",
      "Epoch 2 sm_loss=0.01989  sm_val_loss=0.01894\n",
      "Epoch 3 sm_loss=0.01799  sm_val_loss=0.01773\n",
      "Epoch 6 sm_loss=0.01745  sm_val_loss=0.01761\n",
      "Epoch 7 sm_loss=0.01747  sm_val_loss=0.01747\n",
      "Epoch 9 sm_loss=0.01736  sm_val_loss=0.01737\n",
      "Epoch 11 sm_loss=0.01708  sm_val_loss=0.01736\n",
      "Epoch 12 sm_loss=0.01689  sm_val_loss=0.01696\n",
      "Epoch 13 sm_loss=0.01669  sm_val_loss=0.01676\n",
      "Epoch 14 sm_loss=0.01645  sm_val_loss=0.01666\n",
      "Epoch 15 sm_loss=0.01608  sm_val_loss=0.01655\n",
      "Epoch 16 sm_loss=0.01572  sm_val_loss=0.01632\n",
      "Epoch 17 sm_loss=0.01524  sm_val_loss=0.01609\n",
      "Epoch 18 sm_loss=0.01480  sm_val_loss=0.01602\n",
      "Epoch 19 sm_loss=0.01443  sm_val_loss=0.01596\n",
      "Epoch 20 sm_loss=0.01422  sm_val_loss=0.01595\n",
      "Fold 2\n",
      "Epoch 1 sm_loss=0.41378  sm_val_loss=0.02261\n",
      "Epoch 2 sm_loss=0.02012  sm_val_loss=0.01925\n",
      "Epoch 3 sm_loss=0.01806  sm_val_loss=0.01794\n",
      "Epoch 4 sm_loss=0.01758  sm_val_loss=0.01769\n",
      "Epoch 5 sm_loss=0.01752  sm_val_loss=0.01764\n",
      "Epoch 8 sm_loss=0.01739  sm_val_loss=0.01743\n",
      "Epoch 11 sm_loss=0.01706  sm_val_loss=0.01723\n",
      "Epoch 12 sm_loss=0.01689  sm_val_loss=0.01703\n",
      "Epoch 13 sm_loss=0.01665  sm_val_loss=0.01699\n",
      "Epoch 14 sm_loss=0.01641  sm_val_loss=0.01675\n",
      "Epoch 15 sm_loss=0.01609  sm_val_loss=0.01668\n",
      "Epoch 16 sm_loss=0.01567  sm_val_loss=0.01636\n",
      "Epoch 17 sm_loss=0.01528  sm_val_loss=0.01630\n",
      "Epoch 18 sm_loss=0.01475  sm_val_loss=0.01619\n",
      "Epoch 19 sm_loss=0.01436  sm_val_loss=0.01617\n",
      "Epoch 20 sm_loss=0.01415  sm_val_loss=0.01616\n",
      "Fold 3\n",
      "Epoch 1 sm_loss=0.41496  sm_val_loss=0.02223\n",
      "Epoch 2 sm_loss=0.02010  sm_val_loss=0.02017\n",
      "Epoch 3 sm_loss=0.01825  sm_val_loss=0.01754\n",
      "Epoch 4 sm_loss=0.01756  sm_val_loss=0.01724\n",
      "Epoch 6 sm_loss=0.01747  sm_val_loss=0.01705\n",
      "Epoch 10 sm_loss=0.01720  sm_val_loss=0.01704\n",
      "Epoch 11 sm_loss=0.01711  sm_val_loss=0.01671\n",
      "Epoch 13 sm_loss=0.01665  sm_val_loss=0.01647\n",
      "Epoch 14 sm_loss=0.01649  sm_val_loss=0.01629\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01620\n",
      "Epoch 16 sm_loss=0.01570  sm_val_loss=0.01605\n",
      "Epoch 17 sm_loss=0.01525  sm_val_loss=0.01591\n",
      "Epoch 18 sm_loss=0.01477  sm_val_loss=0.01578\n",
      "Epoch 19 sm_loss=0.01440  sm_val_loss=0.01577\n",
      "Epoch 20 sm_loss=0.01419  sm_val_loss=0.01575\n",
      "Fold 4\n",
      "Epoch 1 sm_loss=0.41302  sm_val_loss=0.02299\n",
      "Epoch 2 sm_loss=0.02002  sm_val_loss=0.02096\n",
      "Epoch 3 sm_loss=0.01845  sm_val_loss=0.01824\n",
      "Epoch 4 sm_loss=0.01767  sm_val_loss=0.01764\n",
      "Epoch 6 sm_loss=0.01741  sm_val_loss=0.01759\n",
      "Epoch 8 sm_loss=0.01742  sm_val_loss=0.01747\n",
      "Epoch 9 sm_loss=0.01734  sm_val_loss=0.01740\n",
      "Epoch 10 sm_loss=0.01718  sm_val_loss=0.01733\n",
      "Epoch 11 sm_loss=0.01704  sm_val_loss=0.01721\n",
      "Epoch 12 sm_loss=0.01689  sm_val_loss=0.01716\n",
      "Epoch 13 sm_loss=0.01660  sm_val_loss=0.01678\n",
      "Epoch 15 sm_loss=0.01607  sm_val_loss=0.01666\n",
      "Epoch 16 sm_loss=0.01567  sm_val_loss=0.01648\n",
      "Epoch 17 sm_loss=0.01527  sm_val_loss=0.01623\n",
      "Epoch 18 sm_loss=0.01477  sm_val_loss=0.01618\n",
      "Epoch 19 sm_loss=0.01435  sm_val_loss=0.01615\n",
      "Fold 5\n",
      "Epoch 1 sm_loss=0.41413  sm_val_loss=0.02258\n",
      "Epoch 2 sm_loss=0.02008  sm_val_loss=0.01920\n",
      "Epoch 3 sm_loss=0.01822  sm_val_loss=0.01871\n",
      "Epoch 4 sm_loss=0.01749  sm_val_loss=0.01788\n",
      "Epoch 5 sm_loss=0.01749  sm_val_loss=0.01778\n",
      "Epoch 6 sm_loss=0.01743  sm_val_loss=0.01767\n",
      "Epoch 9 sm_loss=0.01730  sm_val_loss=0.01762\n",
      "Epoch 10 sm_loss=0.01716  sm_val_loss=0.01753\n",
      "Epoch 11 sm_loss=0.01706  sm_val_loss=0.01733\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01702\n",
      "Epoch 14 sm_loss=0.01636  sm_val_loss=0.01689\n",
      "Epoch 15 sm_loss=0.01600  sm_val_loss=0.01676\n",
      "Epoch 16 sm_loss=0.01563  sm_val_loss=0.01655\n",
      "Epoch 17 sm_loss=0.01520  sm_val_loss=0.01647\n",
      "Epoch 18 sm_loss=0.01475  sm_val_loss=0.01637\n",
      "Epoch 19 sm_loss=0.01438  sm_val_loss=0.01632\n",
      "Fold 6\n",
      "Epoch 1 sm_loss=0.41444  sm_val_loss=0.02202\n",
      "Epoch 2 sm_loss=0.02005  sm_val_loss=0.01875\n",
      "Epoch 3 sm_loss=0.01826  sm_val_loss=0.01735\n",
      "Epoch 9 sm_loss=0.01732  sm_val_loss=0.01705\n",
      "Epoch 11 sm_loss=0.01713  sm_val_loss=0.01700\n",
      "Epoch 12 sm_loss=0.01695  sm_val_loss=0.01697\n",
      "Epoch 13 sm_loss=0.01676  sm_val_loss=0.01656\n",
      "Epoch 15 sm_loss=0.01612  sm_val_loss=0.01636\n",
      "Epoch 16 sm_loss=0.01578  sm_val_loss=0.01609\n",
      "Epoch 17 sm_loss=0.01537  sm_val_loss=0.01595\n",
      "Epoch 18 sm_loss=0.01488  sm_val_loss=0.01586\n",
      "Epoch 19 sm_loss=0.01450  sm_val_loss=0.01581\n",
      "Fold 7\n",
      "Epoch 1 sm_loss=0.41483  sm_val_loss=0.02235\n",
      "Epoch 2 sm_loss=0.02015  sm_val_loss=0.01904\n",
      "Epoch 3 sm_loss=0.01819  sm_val_loss=0.01764\n",
      "Epoch 4 sm_loss=0.01755  sm_val_loss=0.01748\n",
      "Epoch 5 sm_loss=0.01748  sm_val_loss=0.01738\n",
      "Epoch 10 sm_loss=0.01715  sm_val_loss=0.01721\n",
      "Epoch 11 sm_loss=0.01707  sm_val_loss=0.01707\n",
      "Epoch 12 sm_loss=0.01690  sm_val_loss=0.01681\n",
      "Epoch 14 sm_loss=0.01640  sm_val_loss=0.01665\n",
      "Epoch 15 sm_loss=0.01610  sm_val_loss=0.01646\n",
      "Epoch 16 sm_loss=0.01568  sm_val_loss=0.01619\n",
      "Epoch 17 sm_loss=0.01524  sm_val_loss=0.01607\n",
      "Epoch 18 sm_loss=0.01477  sm_val_loss=0.01598\n",
      "Epoch 19 sm_loss=0.01441  sm_val_loss=0.01592\n",
      "Seed 3\n",
      "Total log loss: 0.016037907744538272\n",
      "Fold 1\n",
      "Epoch 1 sm_loss=0.41379  sm_val_loss=0.02282\n",
      "Epoch 2 sm_loss=0.02008  sm_val_loss=0.01954\n",
      "Epoch 3 sm_loss=0.01812  sm_val_loss=0.01798\n",
      "Epoch 4 sm_loss=0.01750  sm_val_loss=0.01761\n",
      "Epoch 5 sm_loss=0.01739  sm_val_loss=0.01747\n",
      "Epoch 10 sm_loss=0.01713  sm_val_loss=0.01707\n",
      "Epoch 12 sm_loss=0.01686  sm_val_loss=0.01704\n",
      "Epoch 13 sm_loss=0.01667  sm_val_loss=0.01674\n",
      "Epoch 14 sm_loss=0.01639  sm_val_loss=0.01661\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01645\n",
      "Epoch 16 sm_loss=0.01564  sm_val_loss=0.01626\n",
      "Epoch 17 sm_loss=0.01520  sm_val_loss=0.01611\n",
      "Epoch 18 sm_loss=0.01471  sm_val_loss=0.01598\n",
      "Epoch 19 sm_loss=0.01433  sm_val_loss=0.01597\n",
      "Fold 2\n",
      "Epoch 1 sm_loss=0.41524  sm_val_loss=0.02247\n",
      "Epoch 2 sm_loss=0.01994  sm_val_loss=0.01952\n",
      "Epoch 3 sm_loss=0.01807  sm_val_loss=0.01794\n",
      "Epoch 4 sm_loss=0.01757  sm_val_loss=0.01786\n",
      "Epoch 5 sm_loss=0.01748  sm_val_loss=0.01774\n",
      "Epoch 6 sm_loss=0.01745  sm_val_loss=0.01756\n",
      "Epoch 10 sm_loss=0.01724  sm_val_loss=0.01719\n",
      "Epoch 12 sm_loss=0.01688  sm_val_loss=0.01703\n",
      "Epoch 13 sm_loss=0.01670  sm_val_loss=0.01698\n",
      "Epoch 14 sm_loss=0.01639  sm_val_loss=0.01676\n",
      "Epoch 15 sm_loss=0.01604  sm_val_loss=0.01662\n",
      "Epoch 16 sm_loss=0.01569  sm_val_loss=0.01645\n",
      "Epoch 17 sm_loss=0.01522  sm_val_loss=0.01628\n",
      "Epoch 18 sm_loss=0.01475  sm_val_loss=0.01620\n",
      "Epoch 19 sm_loss=0.01434  sm_val_loss=0.01617\n",
      "Fold 3\n",
      "Epoch 1 sm_loss=0.41405  sm_val_loss=0.02261\n",
      "Epoch 2 sm_loss=0.02014  sm_val_loss=0.01858\n",
      "Epoch 3 sm_loss=0.01821  sm_val_loss=0.01788\n",
      "Epoch 4 sm_loss=0.01774  sm_val_loss=0.01737\n",
      "Epoch 6 sm_loss=0.01748  sm_val_loss=0.01731\n",
      "Epoch 8 sm_loss=0.01743  sm_val_loss=0.01707\n",
      "Epoch 9 sm_loss=0.01731  sm_val_loss=0.01704\n",
      "Epoch 10 sm_loss=0.01726  sm_val_loss=0.01676\n",
      "Epoch 11 sm_loss=0.01706  sm_val_loss=0.01672\n",
      "Epoch 12 sm_loss=0.01694  sm_val_loss=0.01671\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01649\n",
      "Epoch 14 sm_loss=0.01641  sm_val_loss=0.01637\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01616\n",
      "Epoch 16 sm_loss=0.01568  sm_val_loss=0.01601\n",
      "Epoch 17 sm_loss=0.01528  sm_val_loss=0.01580\n",
      "Epoch 18 sm_loss=0.01479  sm_val_loss=0.01579\n",
      "Epoch 19 sm_loss=0.01441  sm_val_loss=0.01570\n",
      "Fold 4\n",
      "Epoch 1 sm_loss=0.41399  sm_val_loss=0.02237\n",
      "Epoch 2 sm_loss=0.02001  sm_val_loss=0.01914\n",
      "Epoch 3 sm_loss=0.01812  sm_val_loss=0.01852\n",
      "Epoch 4 sm_loss=0.01770  sm_val_loss=0.01747\n",
      "Epoch 10 sm_loss=0.01717  sm_val_loss=0.01717\n",
      "Epoch 12 sm_loss=0.01685  sm_val_loss=0.01694\n",
      "Epoch 13 sm_loss=0.01661  sm_val_loss=0.01671\n",
      "Epoch 15 sm_loss=0.01602  sm_val_loss=0.01665\n",
      "Epoch 16 sm_loss=0.01563  sm_val_loss=0.01646\n",
      "Epoch 17 sm_loss=0.01513  sm_val_loss=0.01634\n",
      "Epoch 18 sm_loss=0.01468  sm_val_loss=0.01616\n",
      "Epoch 19 sm_loss=0.01430  sm_val_loss=0.01611\n",
      "Epoch 20 sm_loss=0.01408  sm_val_loss=0.01611\n",
      "Fold 5\n",
      "Epoch 1 sm_loss=0.41480  sm_val_loss=0.02310\n",
      "Epoch 2 sm_loss=0.02002  sm_val_loss=0.01839\n",
      "Epoch 3 sm_loss=0.01841  sm_val_loss=0.01785\n",
      "Epoch 6 sm_loss=0.01740  sm_val_loss=0.01752\n",
      "Epoch 8 sm_loss=0.01739  sm_val_loss=0.01747\n",
      "Epoch 10 sm_loss=0.01716  sm_val_loss=0.01744\n",
      "Epoch 11 sm_loss=0.01701  sm_val_loss=0.01735\n",
      "Epoch 12 sm_loss=0.01700  sm_val_loss=0.01729\n",
      "Epoch 13 sm_loss=0.01666  sm_val_loss=0.01721\n",
      "Epoch 14 sm_loss=0.01638  sm_val_loss=0.01695\n",
      "Epoch 15 sm_loss=0.01609  sm_val_loss=0.01676\n",
      "Epoch 16 sm_loss=0.01567  sm_val_loss=0.01666\n",
      "Epoch 17 sm_loss=0.01523  sm_val_loss=0.01648\n",
      "Epoch 18 sm_loss=0.01481  sm_val_loss=0.01640\n",
      "Epoch 19 sm_loss=0.01441  sm_val_loss=0.01636\n",
      "Epoch 20 sm_loss=0.01424  sm_val_loss=0.01630\n",
      "Fold 6\n",
      "Epoch 1 sm_loss=0.41412  sm_val_loss=0.02232\n",
      "Epoch 2 sm_loss=0.02019  sm_val_loss=0.01857\n",
      "Epoch 3 sm_loss=0.01807  sm_val_loss=0.01779\n",
      "Epoch 4 sm_loss=0.01771  sm_val_loss=0.01747\n",
      "Epoch 5 sm_loss=0.01759  sm_val_loss=0.01741\n",
      "Epoch 6 sm_loss=0.01751  sm_val_loss=0.01740\n",
      "Epoch 8 sm_loss=0.01746  sm_val_loss=0.01721\n",
      "Epoch 10 sm_loss=0.01726  sm_val_loss=0.01705\n",
      "Epoch 11 sm_loss=0.01712  sm_val_loss=0.01696\n",
      "Epoch 12 sm_loss=0.01699  sm_val_loss=0.01689\n",
      "Epoch 13 sm_loss=0.01677  sm_val_loss=0.01664\n",
      "Epoch 14 sm_loss=0.01648  sm_val_loss=0.01647\n",
      "Epoch 15 sm_loss=0.01613  sm_val_loss=0.01628\n",
      "Epoch 16 sm_loss=0.01573  sm_val_loss=0.01611\n",
      "Epoch 17 sm_loss=0.01529  sm_val_loss=0.01594\n",
      "Epoch 18 sm_loss=0.01483  sm_val_loss=0.01584\n",
      "Epoch 19 sm_loss=0.01445  sm_val_loss=0.01578\n",
      "Fold 7\n",
      "Epoch 1 sm_loss=0.41370  sm_val_loss=0.02277\n",
      "Epoch 2 sm_loss=0.02010  sm_val_loss=0.01848\n",
      "Epoch 3 sm_loss=0.01819  sm_val_loss=0.01783\n",
      "Epoch 4 sm_loss=0.01791  sm_val_loss=0.01782\n",
      "Epoch 5 sm_loss=0.01762  sm_val_loss=0.01743\n",
      "Epoch 7 sm_loss=0.01750  sm_val_loss=0.01734\n",
      "Epoch 9 sm_loss=0.01738  sm_val_loss=0.01729\n",
      "Epoch 10 sm_loss=0.01722  sm_val_loss=0.01726\n",
      "Epoch 11 sm_loss=0.01712  sm_val_loss=0.01711\n",
      "Epoch 12 sm_loss=0.01692  sm_val_loss=0.01699\n",
      "Epoch 13 sm_loss=0.01668  sm_val_loss=0.01686\n",
      "Epoch 14 sm_loss=0.01644  sm_val_loss=0.01654\n",
      "Epoch 15 sm_loss=0.01606  sm_val_loss=0.01637\n",
      "Epoch 16 sm_loss=0.01573  sm_val_loss=0.01625\n",
      "Epoch 17 sm_loss=0.01528  sm_val_loss=0.01608\n",
      "Epoch 18 sm_loss=0.01483  sm_val_loss=0.01596\n",
      "Epoch 19 sm_loss=0.01443  sm_val_loss=0.01592\n",
      "Seed 4\n",
      "Total log loss: 0.01601924208855057\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "mlp1_oof = np.zeros([len(mlp_train),fn_targets.shape[1]])\n",
    "mlp1_test = np.zeros([len(mlp_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, pytorch_pred = modelling_torch(mlp_train, fn_targets, mlp_test, seed_, fn_targets.shape[1])\n",
    "    mlp1_oof += oof / len(seeds)\n",
    "    mlp1_test += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:58:53.576076Z",
     "iopub.status.busy": "2020-11-02T07:58:53.574456Z",
     "iopub.status.idle": "2020-11-02T07:58:55.039699Z",
     "shell.execute_reply": "2020-11-02T07:58:55.040560Z"
    },
    "papermill": {
     "duration": 1.729102,
     "end_time": "2020-11-02T07:58:55.040779",
     "exception": false,
     "start_time": "2020-11-02T07:58:53.311677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014597014592739207\n"
     ]
    }
   ],
   "source": [
    "check_mlp = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_mlp[cons_train_index,:] = mlp1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:58:55.569202Z",
     "iopub.status.busy": "2020-11-02T07:58:55.568116Z",
     "iopub.status.idle": "2020-11-02T07:58:56.910585Z",
     "shell.execute_reply": "2020-11-02T07:58:56.911173Z"
    },
    "papermill": {
     "duration": 1.607688,
     "end_time": "2020-11-02T07:58:56.911344",
     "exception": false,
     "start_time": "2020-11-02T07:58:55.303656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC : 0.8085230460948042\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "for task_id in range(y.shape[1]):\n",
    "    aucs.append(roc_auc_score(y_true=y.iloc[:, task_id].values,\n",
    "                              y_score=check_mlp[:, task_id]))\n",
    "print(f\"Overall AUC : {np.mean(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.256412,
     "end_time": "2020-11-02T07:58:57.422103",
     "exception": false,
     "start_time": "2020-11-02T07:58:57.165691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:58:57.966124Z",
     "iopub.status.busy": "2020-11-02T07:58:57.961418Z",
     "iopub.status.idle": "2020-11-02T07:58:58.036776Z",
     "shell.execute_reply": "2020-11-02T07:58:58.038271Z"
    },
    "papermill": {
     "duration": 0.362318,
     "end_time": "2020-11-02T07:58:58.038514",
     "exception": false,
     "start_time": "2020-11-02T07:58:57.676196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "smoothing = 0.00005\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "class LogitsLogLoss(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        \n",
    "        #smoothing = 0.00001\n",
    "        #n_classes = 206\n",
    "        #y_true = y_true * (1 - smoothing) + np.ones_like(y_true) * smoothing / n_classes\n",
    "        \n",
    "        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:58:58.900381Z",
     "iopub.status.busy": "2020-11-02T07:58:58.885075Z",
     "iopub.status.idle": "2020-11-02T07:58:58.903450Z",
     "shell.execute_reply": "2020-11-02T07:58:58.902844Z"
    },
    "papermill": {
     "duration": 0.337704,
     "end_time": "2020-11-02T07:58:58.903570",
     "exception": false,
     "start_time": "2020-11-02T07:58:58.565866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH=200\n",
    "\n",
    "def seed_tabnet_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "def modelling_tabnet(tr, target, te, sample_seed):\n",
    "    seed_tabnet_everything(sample_seed) \n",
    "    tabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.5, seed = sample_seed,\n",
    "                     lambda_sparse=0, n_independent=1, n_shared=1,\n",
    "                     optimizer_fn=torch.optim.Adam,\n",
    "                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "                     mask_type='entmax',\n",
    "                     scheduler_params=dict(mode=\"min\",\n",
    "                                           patience=5,\n",
    "                                           min_lr=1e-5,\n",
    "                                           factor=0.9,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                     verbose=10,\n",
    "                     )\n",
    "    test_cv_preds = []\n",
    "    \n",
    "    NB_SPLITS = 5\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n",
    "    oof_preds = np.zeros([len(tr),target.shape[1]])\n",
    "    for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, target)):\n",
    "        print(\"FOLDS : \", fold_nb+1)\n",
    "\n",
    "        ## model\n",
    "        X_train, y_train = tr[train_idx, :], target[train_idx, :]\n",
    "        X_val, y_val = tr[val_idx, :], target[val_idx, :]\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "        model.fit(X_train=X_train,\n",
    "              y_train=y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_name = [\"val\"],\n",
    "              eval_metric = [\"logits_ll\"],\n",
    "              max_epochs=MAX_EPOCH,\n",
    "              patience=20, batch_size=1024, virtual_batch_size=128,\n",
    "              num_workers=1, drop_last=False,\n",
    "              # use binary cross entropy as this is not a regression problem\n",
    "              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "        \n",
    "        preds_val = model.predict(X_val)\n",
    "        preds =  1 / (1 + np.exp(-preds_val))\n",
    "        oof_preds[val_idx,:] = preds\n",
    "        #oof_preds[val_idx,:] = np.clip(preds, p_min, p_max)\n",
    "        \n",
    "        # preds on test\n",
    "        preds_test = model.predict(te)\n",
    "        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n",
    "        #test_cv_preds.append(np.clip(1 / (1 + np.exp(-preds_test)), p_min, p_max))\n",
    "\n",
    "\n",
    "    test_preds_all = np.stack(test_cv_preds)\n",
    "    return oof_preds, test_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T07:58:59.455305Z",
     "iopub.status.busy": "2020-11-02T07:58:59.453646Z",
     "iopub.status.idle": "2020-11-02T08:20:13.016837Z",
     "shell.execute_reply": "2020-11-02T08:20:13.016170Z"
    },
    "papermill": {
     "duration": 1273.827839,
     "end_time": "2020-11-02T08:20:13.016977",
     "exception": false,
     "start_time": "2020-11-02T07:58:59.189138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4388  | val_logits_ll: 0.15052 |  0:00:01s\n",
      "epoch 10 | loss: 0.01926 | val_logits_ll: 0.01938 |  0:00:11s\n",
      "epoch 20 | loss: 0.01741 | val_logits_ll: 0.01861 |  0:00:22s\n",
      "epoch 30 | loss: 0.01671 | val_logits_ll: 0.01751 |  0:00:32s\n",
      "epoch 40 | loss: 0.01646 | val_logits_ll: 0.01724 |  0:00:42s\n",
      "epoch 50 | loss: 0.01589 | val_logits_ll: 0.01728 |  0:00:53s\n",
      "epoch 60 | loss: 0.01548 | val_logits_ll: 0.01681 |  0:01:04s\n",
      "epoch 70 | loss: 0.01513 | val_logits_ll: 0.01676 |  0:01:15s\n",
      "\n",
      "Early stopping occured at epoch 79 with best_epoch = 59 and best_val_logits_ll = 0.01676\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44418 | val_logits_ll: 0.16504 |  0:00:00s\n",
      "epoch 10 | loss: 0.01906 | val_logits_ll: 0.01926 |  0:00:11s\n",
      "epoch 20 | loss: 0.01709 | val_logits_ll: 0.02044 |  0:00:21s\n",
      "epoch 30 | loss: 0.0164  | val_logits_ll: 0.01993 |  0:00:33s\n",
      "epoch 40 | loss: 0.01579 | val_logits_ll: 0.01746 |  0:00:44s\n",
      "epoch 50 | loss: 0.01526 | val_logits_ll: 0.01733 |  0:00:54s\n",
      "epoch 60 | loss: 0.01482 | val_logits_ll: 0.01743 |  0:01:05s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_logits_ll = 0.01703\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44406 | val_logits_ll: 0.14174 |  0:00:01s\n",
      "epoch 10 | loss: 0.01894 | val_logits_ll: 0.01922 |  0:00:11s\n",
      "epoch 20 | loss: 0.01722 | val_logits_ll: 0.01796 |  0:00:22s\n",
      "epoch 30 | loss: 0.0165  | val_logits_ll: 0.01779 |  0:00:33s\n",
      "epoch 40 | loss: 0.01616 | val_logits_ll: 0.01719 |  0:00:43s\n",
      "epoch 50 | loss: 0.01541 | val_logits_ll: 0.01729 |  0:00:55s\n",
      "epoch 60 | loss: 0.01491 | val_logits_ll: 0.0172  |  0:01:05s\n",
      "\n",
      "Early stopping occured at epoch 63 with best_epoch = 43 and best_val_logits_ll = 0.0171\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.43682 | val_logits_ll: 0.1486  |  0:00:01s\n",
      "epoch 10 | loss: 0.0191  | val_logits_ll: 0.01917 |  0:00:11s\n",
      "epoch 20 | loss: 0.01713 | val_logits_ll: 0.01787 |  0:00:23s\n",
      "epoch 30 | loss: 0.01649 | val_logits_ll: 0.01717 |  0:00:33s\n",
      "epoch 40 | loss: 0.01598 | val_logits_ll: 0.01763 |  0:00:43s\n",
      "epoch 50 | loss: 0.01582 | val_logits_ll: 0.01689 |  0:00:54s\n",
      "epoch 60 | loss: 0.01537 | val_logits_ll: 0.01693 |  0:01:05s\n",
      "epoch 70 | loss: 0.01564 | val_logits_ll: 0.01731 |  0:01:16s\n",
      "epoch 80 | loss: 0.01494 | val_logits_ll: 0.01908 |  0:01:27s\n",
      "epoch 90 | loss: 0.01458 | val_logits_ll: 0.01691 |  0:01:37s\n",
      "\n",
      "Early stopping occured at epoch 99 with best_epoch = 79 and best_val_logits_ll = 0.01663\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.43901 | val_logits_ll: 0.1596  |  0:00:01s\n",
      "epoch 10 | loss: 0.0189  | val_logits_ll: 0.01916 |  0:00:11s\n",
      "epoch 20 | loss: 0.01708 | val_logits_ll: 0.01848 |  0:00:22s\n",
      "epoch 30 | loss: 0.01653 | val_logits_ll: 0.01943 |  0:00:33s\n",
      "epoch 40 | loss: 0.01583 | val_logits_ll: 0.01746 |  0:00:44s\n",
      "epoch 50 | loss: 0.01552 | val_logits_ll: 0.01722 |  0:00:55s\n",
      "epoch 60 | loss: 0.01544 | val_logits_ll: 0.01718 |  0:01:05s\n",
      "epoch 70 | loss: 0.0149  | val_logits_ll: 0.01719 |  0:01:16s\n",
      "\n",
      "Early stopping occured at epoch 76 with best_epoch = 56 and best_val_logits_ll = 0.01694\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44344 | val_logits_ll: 0.15177 |  0:00:01s\n",
      "epoch 10 | loss: 0.01925 | val_logits_ll: 0.0193  |  0:00:11s\n",
      "epoch 20 | loss: 0.01733 | val_logits_ll: 0.01829 |  0:00:22s\n",
      "epoch 30 | loss: 0.01662 | val_logits_ll: 0.01747 |  0:00:33s\n",
      "epoch 40 | loss: 0.01602 | val_logits_ll: 0.01708 |  0:00:43s\n",
      "epoch 50 | loss: 0.01554 | val_logits_ll: 0.01734 |  0:00:54s\n",
      "epoch 60 | loss: 0.01514 | val_logits_ll: 0.01696 |  0:01:06s\n",
      "epoch 70 | loss: 0.01444 | val_logits_ll: 0.01747 |  0:01:16s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 54 and best_val_logits_ll = 0.01689\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44169 | val_logits_ll: 0.14659 |  0:00:00s\n",
      "epoch 10 | loss: 0.01892 | val_logits_ll: 0.01953 |  0:00:11s\n",
      "epoch 20 | loss: 0.0172  | val_logits_ll: 0.01833 |  0:00:21s\n",
      "epoch 30 | loss: 0.01637 | val_logits_ll: 0.01736 |  0:00:32s\n",
      "epoch 40 | loss: 0.0158  | val_logits_ll: 0.0178  |  0:00:44s\n",
      "epoch 50 | loss: 0.0152  | val_logits_ll: 0.01722 |  0:00:54s\n",
      "epoch 60 | loss: 0.015   | val_logits_ll: 0.01706 |  0:01:04s\n",
      "epoch 70 | loss: 0.01468 | val_logits_ll: 0.01722 |  0:01:15s\n",
      "\n",
      "Early stopping occured at epoch 72 with best_epoch = 52 and best_val_logits_ll = 0.01692\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44322 | val_logits_ll: 0.14496 |  0:00:01s\n",
      "epoch 10 | loss: 0.01921 | val_logits_ll: 0.0203  |  0:00:11s\n",
      "epoch 20 | loss: 0.01713 | val_logits_ll: 0.0184  |  0:00:22s\n",
      "epoch 30 | loss: 0.0163  | val_logits_ll: 0.01782 |  0:00:34s\n",
      "epoch 40 | loss: 0.01576 | val_logits_ll: 0.01808 |  0:00:44s\n",
      "epoch 50 | loss: 0.01539 | val_logits_ll: 0.01712 |  0:00:54s\n",
      "epoch 60 | loss: 0.01494 | val_logits_ll: 0.01742 |  0:01:05s\n",
      "\n",
      "Early stopping occured at epoch 64 with best_epoch = 44 and best_val_logits_ll = 0.01701\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.44208 | val_logits_ll: 0.14815 |  0:00:01s\n",
      "epoch 10 | loss: 0.0192  | val_logits_ll: 0.01927 |  0:00:12s\n",
      "epoch 20 | loss: 0.01746 | val_logits_ll: 0.01772 |  0:00:23s\n",
      "epoch 30 | loss: 0.01656 | val_logits_ll: 0.01727 |  0:00:33s\n",
      "epoch 40 | loss: 0.016   | val_logits_ll: 0.0171  |  0:00:44s\n",
      "epoch 50 | loss: 0.01574 | val_logits_ll: 0.0173  |  0:00:54s\n",
      "epoch 60 | loss: 0.01536 | val_logits_ll: 0.01714 |  0:01:06s\n",
      "epoch 70 | loss: 0.01477 | val_logits_ll: 0.01733 |  0:01:16s\n",
      "\n",
      "Early stopping occured at epoch 75 with best_epoch = 55 and best_val_logits_ll = 0.01687\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4367  | val_logits_ll: 0.13967 |  0:00:00s\n",
      "epoch 10 | loss: 0.01912 | val_logits_ll: 0.01928 |  0:00:11s\n",
      "epoch 20 | loss: 0.01723 | val_logits_ll: 0.01788 |  0:00:22s\n",
      "epoch 30 | loss: 0.01667 | val_logits_ll: 0.01746 |  0:00:33s\n",
      "epoch 40 | loss: 0.01617 | val_logits_ll: 0.01713 |  0:00:44s\n",
      "epoch 50 | loss: 0.01588 | val_logits_ll: 0.01761 |  0:00:55s\n",
      "epoch 60 | loss: 0.01577 | val_logits_ll: 0.01687 |  0:01:05s\n",
      "epoch 70 | loss: 0.01537 | val_logits_ll: 0.01767 |  0:01:16s\n",
      "epoch 80 | loss: 0.0153  | val_logits_ll: 0.01689 |  0:01:27s\n",
      "epoch 90 | loss: 0.01459 | val_logits_ll: 0.01682 |  0:01:37s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 75 and best_val_logits_ll = 0.01671\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47021 | val_logits_ll: 0.18315 |  0:00:01s\n",
      "epoch 10 | loss: 0.01918 | val_logits_ll: 0.01924 |  0:00:11s\n",
      "epoch 20 | loss: 0.01739 | val_logits_ll: 0.01801 |  0:00:21s\n",
      "epoch 30 | loss: 0.01679 | val_logits_ll: 0.01736 |  0:00:32s\n",
      "epoch 40 | loss: 0.01605 | val_logits_ll: 0.0183  |  0:00:43s\n",
      "epoch 50 | loss: 0.01581 | val_logits_ll: 0.01695 |  0:00:53s\n",
      "epoch 60 | loss: 0.01551 | val_logits_ll: 0.01689 |  0:01:05s\n",
      "epoch 70 | loss: 0.01525 | val_logits_ll: 0.01702 |  0:01:15s\n",
      "epoch 80 | loss: 0.01493 | val_logits_ll: 0.01711 |  0:01:25s\n",
      "\n",
      "Early stopping occured at epoch 83 with best_epoch = 63 and best_val_logits_ll = 0.01662\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.4721  | val_logits_ll: 0.18773 |  0:00:01s\n",
      "epoch 10 | loss: 0.01907 | val_logits_ll: 0.01938 |  0:00:12s\n",
      "epoch 20 | loss: 0.01727 | val_logits_ll: 0.02044 |  0:00:22s\n",
      "epoch 30 | loss: 0.01654 | val_logits_ll: 0.01827 |  0:00:33s\n",
      "epoch 40 | loss: 0.01596 | val_logits_ll: 0.01776 |  0:00:44s\n",
      "epoch 50 | loss: 0.0154  | val_logits_ll: 0.0174  |  0:00:55s\n",
      "epoch 60 | loss: 0.01491 | val_logits_ll: 0.01727 |  0:01:05s\n",
      "epoch 70 | loss: 0.01436 | val_logits_ll: 0.0174  |  0:01:15s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 54 and best_val_logits_ll = 0.01719\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47379 | val_logits_ll: 0.17655 |  0:00:01s\n",
      "epoch 10 | loss: 0.01927 | val_logits_ll: 0.01939 |  0:00:12s\n",
      "epoch 20 | loss: 0.0175  | val_logits_ll: 0.0195  |  0:00:23s\n",
      "epoch 30 | loss: 0.01668 | val_logits_ll: 0.01868 |  0:00:33s\n",
      "epoch 40 | loss: 0.01599 | val_logits_ll: 0.01737 |  0:00:44s\n",
      "epoch 50 | loss: 0.01562 | val_logits_ll: 0.0171  |  0:00:55s\n",
      "epoch 60 | loss: 0.01517 | val_logits_ll: 0.0174  |  0:01:05s\n",
      "epoch 70 | loss: 0.01452 | val_logits_ll: 0.01723 |  0:01:16s\n",
      "\n",
      "Early stopping occured at epoch 78 with best_epoch = 58 and best_val_logits_ll = 0.01706\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47449 | val_logits_ll: 0.18264 |  0:00:01s\n",
      "epoch 10 | loss: 0.01983 | val_logits_ll: 0.01979 |  0:00:12s\n",
      "epoch 20 | loss: 0.01734 | val_logits_ll: 0.01802 |  0:00:22s\n",
      "epoch 30 | loss: 0.01665 | val_logits_ll: 0.01753 |  0:00:32s\n",
      "epoch 40 | loss: 0.01607 | val_logits_ll: 0.01733 |  0:00:43s\n",
      "epoch 50 | loss: 0.01552 | val_logits_ll: 0.01764 |  0:00:54s\n",
      "epoch 60 | loss: 0.01504 | val_logits_ll: 0.01746 |  0:01:04s\n",
      "epoch 70 | loss: 0.01462 | val_logits_ll: 0.01726 |  0:01:15s\n",
      "epoch 80 | loss: 0.01391 | val_logits_ll: 0.01753 |  0:01:25s\n",
      "\n",
      "Early stopping occured at epoch 81 with best_epoch = 61 and best_val_logits_ll = 0.0169\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.47568 | val_logits_ll: 0.1844  |  0:00:01s\n",
      "epoch 10 | loss: 0.01929 | val_logits_ll: 0.01974 |  0:00:12s\n",
      "epoch 20 | loss: 0.01718 | val_logits_ll: 0.01788 |  0:00:23s\n",
      "epoch 30 | loss: 0.01636 | val_logits_ll: 0.01744 |  0:00:33s\n",
      "epoch 40 | loss: 0.01591 | val_logits_ll: 0.01741 |  0:00:44s\n",
      "epoch 50 | loss: 0.01576 | val_logits_ll: 0.01704 |  0:00:54s\n",
      "epoch 60 | loss: 0.01544 | val_logits_ll: 0.01728 |  0:01:05s\n",
      "epoch 70 | loss: 0.01495 | val_logits_ll: 0.0171  |  0:01:16s\n",
      "\n",
      "Early stopping occured at epoch 75 with best_epoch = 55 and best_val_logits_ll = 0.01695\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "tabnet1_oof = np.zeros([len(tab_train),fn_targets.shape[1]])\n",
    "tabnet1_test = np.zeros([len(tab_test),fn_targets.shape[1]])\n",
    "seeds = [0,1,2]\n",
    "for seed_ in seeds:\n",
    "    oof_preds, test_preds_all = modelling_tabnet(tab_train, fn_targets, tab_test, seed_)\n",
    "    tabnet1_oof += oof_preds / len(seeds)\n",
    "    tabnet1_test += test_preds_all.mean(axis=0) / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:13.704969Z",
     "iopub.status.busy": "2020-11-02T08:20:13.703808Z",
     "iopub.status.idle": "2020-11-02T08:20:15.167497Z",
     "shell.execute_reply": "2020-11-02T08:20:15.166857Z"
    },
    "papermill": {
     "duration": 1.829725,
     "end_time": "2020-11-02T08:20:15.167650",
     "exception": false,
     "start_time": "2020-11-02T08:20:13.337925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.015058652807535171\n"
     ]
    }
   ],
   "source": [
    "check_tabnet = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "check_tabnet[cons_train_index,:] = tabnet1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_tabnet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:15.820978Z",
     "iopub.status.busy": "2020-11-02T08:20:15.819948Z",
     "iopub.status.idle": "2020-11-02T08:20:17.310322Z",
     "shell.execute_reply": "2020-11-02T08:20:17.309677Z"
    },
    "papermill": {
     "duration": 1.823149,
     "end_time": "2020-11-02T08:20:17.310457",
     "exception": false,
     "start_time": "2020-11-02T08:20:15.487308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC : 0.7887125709129963\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "for task_id in range(y.shape[1]):\n",
    "    aucs.append(roc_auc_score(y_true=y.iloc[:, task_id].values,\n",
    "                              y_score=check_tabnet[:, task_id]))\n",
    "print(f\"Overall AUC : {np.mean(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.330711,
     "end_time": "2020-11-02T08:20:17.959979",
     "exception": false,
     "start_time": "2020-11-02T08:20:17.629268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:18.861557Z",
     "iopub.status.busy": "2020-11-02T08:20:18.860194Z",
     "iopub.status.idle": "2020-11-02T08:20:18.862413Z",
     "shell.execute_reply": "2020-11-02T08:20:18.862957Z"
    },
    "papermill": {
     "duration": 0.437343,
     "end_time": "2020-11-02T08:20:18.863101",
     "exception": false,
     "start_time": "2020-11-02T08:20:18.425758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#N_STARTS = 1\n",
    "#N_SPLITS = 5\n",
    "\n",
    "#svm0_oof = np.zeros([len(fn_train), fn_targets.shape[1]])\n",
    "#svm0_test = np.zeros([len(fn_test), fn_targets.shape[1]])\n",
    "\n",
    "#svm1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "#svm1_oof = np.zeros([fn_targets.shape[0],fn_targets.shape[1]]) \n",
    "\n",
    "#for ind in tqdm(range(fn_targets.shape[1])):\n",
    "#    ind_target_sum = fn_targets[:, ind].sum()\n",
    "#    if ind_target_sum >= N_SPLITS:\n",
    "#        for seed in range(N_STARTS):\n",
    "#            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "\n",
    "#            for n, (train_index, val_index) in enumerate(skf.split(fn_train, fn_targets[:,ind])):\n",
    "                \n",
    "#                x_tr, x_val = fn_train[train_index], fn_train[val_index]\n",
    "#                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "#                model = SVC(C = 40, cache_size = 2000)\n",
    "#                model.fit(x_tr, y_tr)\n",
    "#                svm0_test[:, ind] += model.decision_function(fn_test) / (N_SPLITS * N_STARTS)\n",
    "#                svm0_oof[val_index, ind] += model.decision_function(x_val) / N_STARTS\n",
    "\n",
    "#        for seed in range(N_STARTS):\n",
    "#            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "            \n",
    "#            for n, (train_index, val_index) in enumerate(skf.split(svm0_oof, fn_targets[:,ind])):\n",
    "\n",
    "#                x_tr, x_val = svm0_oof[train_index, ind].reshape(-1, 1), svm0_oof[val_index, ind].reshape(-1, 1)\n",
    "#                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "#                model = LogisticRegression(C = 35, max_iter = 1000)\n",
    "#                model.fit(x_tr, y_tr)\n",
    "#                svm1_test[:, ind] += model.predict_proba(svm0_test[:, ind].reshape(-1, 1))[:, 1] / (N_SPLITS * N_STARTS)\n",
    "#                svm1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1] / N_STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:19.512386Z",
     "iopub.status.busy": "2020-11-02T08:20:19.510179Z",
     "iopub.status.idle": "2020-11-02T08:20:19.513183Z",
     "shell.execute_reply": "2020-11-02T08:20:19.513761Z"
    },
    "papermill": {
     "duration": 0.328062,
     "end_time": "2020-11-02T08:20:19.513905",
     "exception": false,
     "start_time": "2020-11-02T08:20:19.185843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check_svm = np.zeros([targets.shape[0], targets.shape[1]-1])\n",
    "#check_svm[cons_train_index,:] = svm1_oof\n",
    "#print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:20.159281Z",
     "iopub.status.busy": "2020-11-02T08:20:20.158172Z",
     "iopub.status.idle": "2020-11-02T08:20:20.161481Z",
     "shell.execute_reply": "2020-11-02T08:20:20.160948Z"
    },
    "papermill": {
     "duration": 0.327103,
     "end_time": "2020-11-02T08:20:20.161614",
     "exception": false,
     "start_time": "2020-11-02T08:20:19.834511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aucs = []\n",
    "#for task_id in range(y.shape[1]):\n",
    "#    aucs.append(roc_auc_score(y_true=y.iloc[:, task_id].values,\n",
    "#                              y_score=check_svm[:, task_id]))\n",
    "#print(f\"Overall AUC : {np.mean(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.318126,
     "end_time": "2020-11-02T08:20:20.799092",
     "exception": false,
     "start_time": "2020-11-02T08:20:20.480966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.317548,
     "end_time": "2020-11-02T08:20:21.433414",
     "exception": false,
     "start_time": "2020-11-02T08:20:21.115866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- https://www.kaggle.com/ipythonx/optimizing-metrics-out-of-fold-weights-ensemble\n",
    "- https://www.kaggle.com/hsperr/finding-ensamble-weights\n",
    "- https://stackoverflow.com/questions/18767657/how-do-i-use-a-minimization-function-in-scipy-with-constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T08:20:22.088729Z",
     "iopub.status.busy": "2020-11-02T08:20:22.087092Z",
     "iopub.status.idle": "2020-11-02T08:25:56.369702Z",
     "shell.execute_reply": "2020-11-02T08:25:56.369086Z"
    },
    "papermill": {
     "duration": 334.619078,
     "end_time": "2020-11-02T08:25:56.369830",
     "exception": false,
     "start_time": "2020-11-02T08:20:21.750752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.00586967, Weights: [0.7618586 0.2381414]\n",
      "Score: 0.00633394, Weights: [0.71974959 0.28025041]\n",
      "Score: 0.00833199, Weights: [0.48347697 0.51652303]\n",
      "Score: 0.04728654, Weights: [0.99046205 0.00953795]\n",
      "Score: 0.06808368, Weights: [0.52172604 0.47827396]\n",
      "Score: 0.02149528, Weights: [0.35168062 0.64831938]\n",
      "Score: 0.01599884, Weights: [0.83261429 0.16738571]\n",
      "Score: 0.02655482, Weights: [1.62314407e-04 9.99837686e-01]\n",
      "Score: 0.00183226, Weights: [0.77876796 0.22123204]\n",
      "Score: 0.05405401, Weights: [0.26291256 0.73708744]\n",
      "Score: 0.07899107, Weights: [0.8701621 0.1298379]\n",
      "Score: 0.01052213, Weights: [2.77555756e-17 1.00000000e+00]\n",
      "Score: 0.00235013, Weights: [0.88941143 0.11058857]\n",
      "Score: 0.01006002, Weights: [0.19970531 0.80029469]\n",
      "Score: 0.0047048, Weights: [0.35965553 0.64034447]\n",
      "Score: 0.00465385, Weights: [0.57680463 0.42319537]\n",
      "Score: 0.01467425, Weights: [0.56222771 0.43777229]\n",
      "Score: 0.02517772, Weights: [0.55994893 0.44005107]\n",
      "Score: 0.02305404, Weights: [0.41355475 0.58644525]\n",
      "Score: 0.01186568, Weights: [0.71351316 0.28648684]\n",
      "Score: 0.01191927, Weights: [0.09212973 0.90787027]\n",
      "Score: 0.02022289, Weights: [0.40896458 0.59103542]\n",
      "Score: 0.00262001, Weights: [0.71694481 0.28305519]\n",
      "Score: 0.01282992, Weights: [0.58747609 0.41252391]\n",
      "Score: 0.00473001, Weights: [0.50229396 0.49770604]\n",
      "Score: 0.00493197, Weights: [0.12301071 0.87698929]\n",
      "Score: 0.00472478, Weights: [0.34052314 0.65947686]\n",
      "Score: 0.00629719, Weights: [0.21992312 0.78007688]\n",
      "Score: 0.02140459, Weights: [0.43785558 0.56214442]\n",
      "Score: 0.01175575, Weights: [0.68715195 0.31284805]\n",
      "Score: 0.00808056, Weights: [0.7767077 0.2232923]\n",
      "Score: 0.01483542, Weights: [0.44327673 0.55672327]\n",
      "Score: 0.01463801, Weights: [0.04530926 0.95469074]\n",
      "Score: 0.00241905, Weights: [0.64734704 0.35265296]\n",
      "Score: 0.00115874, Weights: [0.29306362 0.70693638]\n",
      "Score: 0.00254424, Weights: [0.8104779 0.1895221]\n",
      "Score: 0.02055438, Weights: [1.2490009e-16 1.0000000e+00]\n",
      "Score: 0.00470467, Weights: [0.73079224 0.26920776]\n",
      "Score: 0.009386, Weights: [0.00891002 0.99108998]\n",
      "Score: 0.00268425, Weights: [0.52822516 0.47177484]\n",
      "Score: 0.01809121, Weights: [0.07601286 0.92398714]\n",
      "Score: 0.0223103, Weights: [0.51652124 0.48347876]\n",
      "Score: 0.01129228, Weights: [0.16715022 0.83284978]\n",
      "Score: 0.04812303, Weights: [0.5720452 0.4279548]\n",
      "Score: 0.02475042, Weights: [0.38288784 0.61711216]\n",
      "Score: 0.03071995, Weights: [0.33985291 0.66014709]\n",
      "Score: 0.00204521, Weights: [0.57376233 0.42623767]\n",
      "Score: 0.00886112, Weights: [0.15187945 0.84812055]\n",
      "Score: 0.00856389, Weights: [0.42780808 0.57219192]\n",
      "Score: 0.01981951, Weights: [0.15098487 0.84901513]\n",
      "Score: 0.00845925, Weights: [0.253858 0.746142]\n",
      "Score: 0.01267889, Weights: [0.26274259 0.73725741]\n",
      "Score: 0.00739754, Weights: [0.22477496 0.77522504]\n",
      "Score: 0.0028031, Weights: [0.30117115 0.69882885]\n",
      "Score: 0.06347648, Weights: [0.14352724 0.85647276]\n",
      "Score: 0.01317007, Weights: [0.62298277 0.37701723]\n",
      "Score: 0.01689068, Weights: [0.75971986 0.24028014]\n",
      "Score: 0.01164884, Weights: [0.45190799 0.54809201]\n",
      "Score: 0.01128224, Weights: [0.1899653 0.8100347]\n",
      "Score: 0.00650921, Weights: [0.57964095 0.42035905]\n",
      "Score: 0.00479724, Weights: [0.00901254 0.99098746]\n",
      "Score: 0.02855364, Weights: [0.76136925 0.23863075]\n",
      "Score: 0.0065074, Weights: [0.63007912 0.36992088]\n",
      "Score: 0.0173046, Weights: [0.13604298 0.86395702]\n",
      "Score: 0.01648506, Weights: [0.50236764 0.49763236]\n",
      "Score: 0.00306839, Weights: [0.533388 0.466612]\n",
      "Score: 0.01304856, Weights: [0.73262973 0.26737027]\n",
      "Score: 0.01503396, Weights: [0.02175762 0.97824238]\n",
      "Score: 0.01643862, Weights: [0.75403184 0.24596816]\n",
      "Score: 0.00286397, Weights: [0.09328756 0.90671244]\n",
      "Score: 0.00829737, Weights: [0.51977456 0.48022544]\n",
      "Score: 0.08885496, Weights: [0.30668092 0.69331908]\n",
      "Score: 0.02832904, Weights: [0.31673879 0.68326121]\n",
      "Score: 0.00894319, Weights: [0.61087548 0.38912452]\n",
      "Score: 0.00852066, Weights: [0.58017413 0.41982587]\n",
      "Score: 0.0028072, Weights: [0.2628793 0.7371207]\n",
      "Score: 0.01502932, Weights: [0.20776633 0.79223367]\n",
      "Score: 0.07906509, Weights: [0.17054156 0.82945844]\n",
      "Score: 0.03280847, Weights: [0.74836678 0.25163322]\n",
      "Score: 0.08497758, Weights: [0.31935661 0.68064339]\n",
      "Score: 0.01402527, Weights: [0. 1.]\n",
      "Score: 0.00256513, Weights: [0.86171913 0.13828087]\n",
      "Score: 0.00119995, Weights: [0.2218371 0.7781629]\n",
      "Score: 0.03898006, Weights: [0.42757971 0.57242029]\n",
      "Score: 0.01416708, Weights: [0.35672174 0.64327826]\n",
      "Score: 0.01059945, Weights: [0.59286001 0.40713999]\n",
      "Score: 0.00304843, Weights: [0. 1.]\n",
      "Score: 0.00828311, Weights: [0.50846441 0.49153559]\n",
      "Score: 0.00920429, Weights: [0.19296791 0.80703209]\n",
      "Score: 0.01769435, Weights: [0.06311555 0.93688445]\n",
      "Score: 0.00226409, Weights: [1.31838984e-16 1.00000000e+00]\n",
      "Score: 0.00659616, Weights: [0.13898879 0.86101121]\n",
      "Score: 0.0074047, Weights: [0.45518443 0.54481557]\n",
      "Score: 0.02897665, Weights: [0.30814801 0.69185199]\n",
      "Score: 0.04253995, Weights: [0.78490607 0.21509393]\n",
      "Score: 0.00971289, Weights: [0.15425813 0.84574187]\n",
      "Score: 0.01438555, Weights: [0.5610363 0.4389637]\n",
      "Score: 0.00482717, Weights: [0.51154198 0.48845802]\n",
      "Score: 0.02142255, Weights: [0.26229056 0.73770944]\n",
      "Score: 0.08083613, Weights: [0.69504026 0.30495974]\n",
      "Score: 0.00646903, Weights: [0.66396335 0.33603665]\n",
      "Score: 0.00975956, Weights: [0.11938564 0.88061436]\n",
      "Score: 0.02123519, Weights: [0.13241106 0.86758894]\n",
      "Score: 0.01008791, Weights: [0.51081692 0.48918308]\n",
      "Score: 0.01786986, Weights: [0.34673476 0.65326524]\n",
      "Score: 0.05736406, Weights: [0.45141836 0.54858164]\n",
      "Score: 0.00579352, Weights: [0.2676719 0.7323281]\n",
      "Score: 0.00986075, Weights: [0.71645718 0.28354282]\n",
      "Score: 0.02074357, Weights: [0.10658305 0.89341695]\n",
      "Score: 0.00948112, Weights: [0.2860692 0.7139308]\n",
      "Score: 0.00722483, Weights: [0.59683039 0.40316961]\n",
      "Score: 0.00902781, Weights: [0.30098352 0.69901648]\n",
      "Score: 0.00729772, Weights: [0.29084265 0.70915735]\n",
      "Score: 0.01040492, Weights: [0.77658962 0.22341038]\n",
      "Score: 0.01880307, Weights: [0.98314022 0.01685978]\n",
      "Score: 0.00988993, Weights: [0.59313642 0.40686358]\n",
      "Score: 0.01133582, Weights: [0.54171859 0.45828141]\n",
      "Score: 0.01323991, Weights: [0.50651209 0.49348791]\n",
      "Score: 0.01091089, Weights: [0.14234235 0.85765765]\n",
      "Score: 0.01543317, Weights: [0.1295709 0.8704291]\n",
      "Score: 0.00274299, Weights: [0.46738251 0.53261749]\n",
      "Score: 0.00286611, Weights: [0.07138194 0.92861806]\n",
      "Score: 0.01885089, Weights: [0.69283682 0.30716318]\n",
      "Score: 0.00471842, Weights: [0.44255902 0.55744098]\n",
      "Score: 0.01756547, Weights: [0.36017711 0.63982289]\n",
      "Score: 0.00242971, Weights: [0.58890165 0.41109835]\n",
      "Score: 0.00443091, Weights: [0. 1.]\n",
      "Score: 0.00715056, Weights: [0.28560012 0.71439988]\n",
      "Score: 0.02169327, Weights: [0.80078832 0.19921168]\n",
      "Score: 0.00844617, Weights: [0.15804918 0.84195082]\n",
      "Score: 0.00478627, Weights: [0.14296036 0.85703964]\n",
      "Score: 0.02439752, Weights: [0.00652667 0.99347333]\n",
      "Score: 0.00567935, Weights: [0.61232979 0.38767021]\n",
      "Score: 0.01485874, Weights: [0. 1.]\n",
      "Score: 0.01500529, Weights: [0.08760523 0.91239477]\n",
      "Score: 0.01153293, Weights: [0.80935041 0.19064959]\n",
      "Score: 0.03125264, Weights: [0.28882461 0.71117539]\n",
      "Score: 0.00260151, Weights: [0.94761812 0.05238188]\n",
      "Score: 0.0088637, Weights: [0.38051938 0.61948062]\n",
      "Score: 0.00397482, Weights: [0.37332774 0.62667226]\n",
      "Score: 0.00879452, Weights: [0.80514304 0.19485696]\n",
      "Score: 0.00308126, Weights: [0.48190333 0.51809667]\n",
      "Score: 0.0045308, Weights: [0.4343562 0.5656438]\n",
      "Score: 0.01836189, Weights: [0.14263436 0.85736564]\n",
      "Score: 0.0270714, Weights: [0.22004166 0.77995834]\n",
      "Score: 0.01200038, Weights: [0.93409751 0.06590249]\n",
      "Score: 0.00897999, Weights: [0.51671848 0.48328152]\n",
      "Score: 0.00789682, Weights: [0.73546344 0.26453656]\n",
      "Score: 0.01614201, Weights: [0.28592548 0.71407452]\n",
      "Score: 0.01735632, Weights: [0.17604413 0.82395587]\n",
      "Score: 0.00594767, Weights: [0.5816019 0.4183981]\n",
      "Score: 0.06010335, Weights: [0.15197113 0.84802887]\n",
      "Score: 0.00852441, Weights: [0.68597121 0.31402879]\n",
      "Score: 0.02329591, Weights: [1.11022302e-16 1.00000000e+00]\n",
      "Score: 0.0094136, Weights: [0.58522912 0.41477088]\n",
      "Score: 0.01670743, Weights: [0.55716427 0.44283573]\n",
      "Score: 0.02732117, Weights: [0.48085378 0.51914622]\n",
      "Score: 0.02234558, Weights: [0.12522083 0.87477917]\n",
      "Score: 0.00916433, Weights: [0.27769699 0.72230301]\n",
      "Score: 0.02774401, Weights: [0. 1.]\n",
      "Score: 0.00538779, Weights: [0.8764795 0.1235205]\n",
      "Score: 0.01164047, Weights: [0.14844941 0.85155059]\n",
      "Score: 0.02392641, Weights: [0.25990624 0.74009376]\n",
      "Score: 0.00255499, Weights: [0.83957692 0.16042308]\n",
      "Score: 0.01446534, Weights: [0.70722177 0.29277823]\n",
      "Score: 0.00247494, Weights: [0.49699547 0.50300453]\n",
      "Score: 0.02257342, Weights: [0.33337862 0.66662138]\n",
      "Score: 0.00660594, Weights: [0.26095935 0.73904065]\n",
      "Score: 0.01682524, Weights: [0.44412288 0.55587712]\n",
      "Score: 0.00622875, Weights: [0.4791233 0.5208767]\n",
      "Score: 0.00450016, Weights: [0.1531961 0.8468039]\n",
      "Score: 0.0092185, Weights: [0.13609627 0.86390373]\n",
      "Score: 0.00256635, Weights: [0.88264531 0.11735469]\n",
      "Score: 0.00827774, Weights: [0. 1.]\n",
      "Score: 0.00894576, Weights: [0.78750852 0.21249148]\n",
      "Score: 0.00817573, Weights: [0.58219106 0.41780894]\n",
      "Score: 0.05690533, Weights: [0.16427117 0.83572883]\n",
      "Score: 0.08466405, Weights: [0.37824678 0.62175322]\n",
      "Score: 0.01388717, Weights: [0.7049926 0.2950074]\n",
      "Score: 0.01182578, Weights: [0.18232719 0.81767281]\n",
      "Score: 0.01173613, Weights: [0.52262996 0.47737004]\n",
      "Score: 0.00858842, Weights: [0.79973663 0.20026337]\n",
      "Score: 0.06309773, Weights: [0.49554077 0.50445923]\n",
      "Score: 0.00833341, Weights: [0.59056662 0.40943338]\n",
      "Score: 0.01380881, Weights: [5.55111512e-17 1.00000000e+00]\n",
      "Score: 0.00274317, Weights: [0.49385348 0.50614652]\n",
      "Score: 0.0057067, Weights: [0.1946509 0.8053491]\n",
      "Score: 0.01826012, Weights: [0.70927263 0.29072737]\n",
      "Score: 0.00308792, Weights: [0.14294272 0.85705728]\n",
      "Score: 0.00677596, Weights: [0.34305568 0.65694432]\n",
      "Score: 0.01022463, Weights: [0.4509925 0.5490075]\n",
      "Score: 0.01015119, Weights: [0.74494989 0.25505011]\n",
      "Score: 0.00309065, Weights: [0.10801933 0.89198067]\n",
      "Score: 0.01126492, Weights: [0.62306953 0.37693047]\n",
      "Score: 0.0127478, Weights: [0.30329629 0.69670371]\n",
      "Score: 0.00655187, Weights: [0.69100471 0.30899529]\n",
      "Score: 0.00279652, Weights: [0.30576289 0.69423711]\n",
      "Score: 0.00706831, Weights: [0.36988944 0.63011056]\n",
      "Score: 0.01498283, Weights: [0.57182364 0.42817636]\n",
      "Score: 0.01884968, Weights: [0.02460082 0.97539918]\n",
      "Score: 0.01907125, Weights: [0.71131864 0.28868136]\n",
      "Score: 0.00273765, Weights: [0.3168777 0.6831223]\n",
      "Score: 0.02325661, Weights: [0.30251461 0.69748539]\n",
      "Score: 0.00896781, Weights: [0.09027973 0.90972027]\n",
      "Score: 0.00502216, Weights: [0.02122475 0.97877525]\n",
      "Score: 0.01008187, Weights: [0.95597833 0.04402167]\n",
      "final ensemble oof score: 0.015802008219964983\n",
      "OOF log loss:  0.01450387502626172\n",
      "CPU times: user 5min 25s, sys: 1.93 s, total: 5min 27s\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "check_score = pd.read_csv(DATA_DIR + 'train_targets_scored.csv').drop(\"sig_id\", axis=1)\n",
    "\n",
    "blend_train = []\n",
    "blend_test = []\n",
    "\n",
    "# out of fold prediction\n",
    "#blend_train.append(svm1_oof)\n",
    "blend_train.append(tabnet1_oof)\n",
    "blend_train.append(mlp1_oof)\n",
    "blend_train = np.array(blend_train)\n",
    "\n",
    "# submission scores\n",
    "#blend_test.append(svm1_test)\n",
    "blend_test.append(tabnet1_test)\n",
    "blend_test.append(mlp1_test)\n",
    "blend_test = np.array(blend_test)\n",
    "\n",
    "svm_ratio = 0\n",
    "\n",
    "np.random.seed(224)\n",
    "total_scores = []\n",
    "for i in range(len(target_feats)):\n",
    "    def log_loss_func(weights):\n",
    "        final_prediction = 0 #svm_ratio * svm1_oof[:,i]\n",
    "        for weight, prediction in zip(weights, blend_train):\n",
    "            final_prediction += weight * prediction[:,i]\n",
    "        return log_loss(np.ravel(fn_targets[:,i]), np.ravel(final_prediction))\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_weights = [0] * len(blend_train)\n",
    "    for k in range(50):\n",
    "        starting_values = np.random.rand(len(blend_train))\n",
    "        starting_values /= sum(starting_values)\n",
    "        bounds = [(0, 1)] * len(blend_train)\n",
    "        cons = ({'type': 'eq', 'fun': lambda x:  1 - svm_ratio - sum(x)}) \n",
    "            \n",
    "        res = minimize(log_loss_func,\n",
    "                   starting_values,\n",
    "                   method='SLSQP',\n",
    "                   bounds=bounds,\n",
    "                   constraints = cons) \n",
    "    \n",
    "        if best_score > res[\"fun\"]:\n",
    "            best_score = res[\"fun\"]\n",
    "            best_weights = res[\"x\"]\n",
    "        \n",
    "    valid_prediction = 0 #svm_ratio * svm1_oof[:,i]\n",
    "    for weight, prediction in zip(best_weights, blend_train):\n",
    "        valid_prediction += weight * prediction[:,i]\n",
    "    print('Score: {}, Weights: {}'.format(round(res['fun'],8), res['x']))\n",
    "    total_scores.append(res['fun'])\n",
    "    check_score.loc[cons_train_index,target_feats[i]] = valid_prediction\n",
    "    \n",
    "    oof_test = 0 #svm_ratio * svm1_test[:,i]\n",
    "    for weight, prediction in zip(best_weights, blend_test):\n",
    "        oof_test += weight * prediction[:,i]\n",
    "        \n",
    "    sub.loc[cons_test_index,target_feats[i]] = oof_test\n",
    "\n",
    "print(\"final ensemble oof score:\", np.mean(total_scores))\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_score)))\n",
    "\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2580.521325,
   "end_time": "2020-11-02T08:25:57.900238",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-02T07:42:57.378913",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
