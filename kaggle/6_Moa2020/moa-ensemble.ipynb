{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030439,
     "end_time": "2020-11-10T04:36:01.786976",
     "exception": false,
     "start_time": "2020-11-10T04:36:01.756537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- incorporate drug_id validation in pytorch mlp (mlp version 37)\n",
    "- tabnet version 45\n",
    "- svm version 7\n",
    "- all models are based on new validation scheme\n",
    "- decide weight ratio by optimization\n",
    "- also predict unbalanced feats in svm\n",
    "- add lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:36:01.877734Z",
     "iopub.status.busy": "2020-11-10T04:36:01.876495Z",
     "iopub.status.idle": "2020-11-10T04:36:11.360869Z",
     "shell.execute_reply": "2020-11-10T04:36:11.360196Z"
    },
    "papermill": {
     "duration": 9.530409,
     "end_time": "2020-11-10T04:36:11.361006",
     "exception": false,
     "start_time": "2020-11-10T04:36:01.830597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.5)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:36:11.438629Z",
     "iopub.status.busy": "2020-11-10T04:36:11.437637Z",
     "iopub.status.idle": "2020-11-10T04:37:50.287290Z",
     "shell.execute_reply": "2020-11-10T04:37:50.285972Z"
    },
    "papermill": {
     "duration": 98.895782,
     "end_time": "2020-11-10T04:37:50.287479",
     "exception": false,
     "start_time": "2020-11-10T04:36:11.391697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!cp ../input/rapids/rapids.0.15.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:03.306872Z",
     "iopub.status.busy": "2020-11-10T04:38:03.305898Z",
     "iopub.status.idle": "2020-11-10T04:38:12.561492Z",
     "shell.execute_reply": "2020-11-10T04:38:12.560355Z"
    },
    "papermill": {
     "duration": 9.330344,
     "end_time": "2020-11-10T04:38:12.561617",
     "exception": false,
     "start_time": "2020-11-10T04:38:03.231273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from category_encoders import CountEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cuml.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030857,
     "end_time": "2020-11-10T04:38:12.622846",
     "exception": false,
     "start_time": "2020-11-10T04:38:12.591989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:12.701732Z",
     "iopub.status.busy": "2020-11-10T04:38:12.697267Z",
     "iopub.status.idle": "2020-11-10T04:38:18.552961Z",
     "shell.execute_reply": "2020-11-10T04:38:18.551679Z"
    },
    "papermill": {
     "duration": 5.899507,
     "end_time": "2020-11-10T04:38:18.553098",
     "exception": false,
     "start_time": "2020-11-10T04:38:12.653591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')\n",
    "drug = pd.read_csv(DATA_DIR + 'train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:18.642066Z",
     "iopub.status.busy": "2020-11-10T04:38:18.641015Z",
     "iopub.status.idle": "2020-11-10T04:38:18.656760Z",
     "shell.execute_reply": "2020-11-10T04:38:18.655923Z"
    },
    "papermill": {
     "duration": 0.071872,
     "end_time": "2020-11-10T04:38:18.656921",
     "exception": false,
     "start_time": "2020-11-10T04:38:18.585049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:18.787415Z",
     "iopub.status.busy": "2020-11-10T04:38:18.777354Z",
     "iopub.status.idle": "2020-11-10T04:38:19.361528Z",
     "shell.execute_reply": "2020-11-10T04:38:19.362167Z"
    },
    "papermill": {
     "duration": 0.653547,
     "end_time": "2020-11-10T04:38:19.362372",
     "exception": false,
     "start_time": "2020-11-10T04:38:18.708825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index\n",
    "\n",
    "test = test[test.index.isin(cons_test_index)].reset_index(drop=True)\n",
    "train = train[train.index.isin(cons_train_index)].reset_index(drop=True)\n",
    "y = targets.drop(\"sig_id\", axis=1).copy()\n",
    "targets = targets[targets.index.isin(cons_train_index)].reset_index(drop=True)\n",
    "fn_targets = targets.copy().drop(\"sig_id\", axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:19.466330Z",
     "iopub.status.busy": "2020-11-10T04:38:19.465572Z",
     "iopub.status.idle": "2020-11-10T04:38:19.469008Z",
     "shell.execute_reply": "2020-11-10T04:38:19.469534Z"
    },
    "papermill": {
     "duration": 0.059747,
     "end_time": "2020-11-10T04:38:19.469698",
     "exception": false,
     "start_time": "2020-11-10T04:38:19.409951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_fold(NB_SPLITS, seed):   \n",
    "    folds = []\n",
    "    # LOAD FILES\n",
    "    train_score = targets.merge(drug, on='sig_id', how='left') \n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = train_score.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= 19].index.sort_values()\n",
    "    vc2 = vc.loc[vc > 19].index.sort_values()\n",
    "    \n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "    tmp = train_score.groupby('drug_id')[target_feats].mean().loc[vc1]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, shuffle = True, random_state = seed)\n",
    "    tmp = train_score.loc[train_score.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_feats])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN FOLDS\n",
    "    train_score['fold'] = train_score.drug_id.map(dct1)\n",
    "    train_score.loc[train_score.fold.isna(),'fold'] = train_score.loc[train_score.fold.isna(),'sig_id'].map(dct2)\n",
    "    train_score.fold = train_score.fold.astype('int8')\n",
    "    folds.append(train_score.fold.values)\n",
    "    \n",
    "    return np.array(folds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:19.551134Z",
     "iopub.status.busy": "2020-11-10T04:38:19.535619Z",
     "iopub.status.idle": "2020-11-10T04:38:20.553581Z",
     "shell.execute_reply": "2020-11-10T04:38:20.553011Z"
    },
    "papermill": {
     "duration": 1.054263,
     "end_time": "2020-11-10T04:38:20.553711",
     "exception": false,
     "start_time": "2020-11-10T04:38:19.499448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_fold = make_fold(7,34)\n",
    "tab_fold = make_fold(5,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:20.627345Z",
     "iopub.status.busy": "2020-11-10T04:38:20.625678Z",
     "iopub.status.idle": "2020-11-10T04:38:21.459664Z",
     "shell.execute_reply": "2020-11-10T04:38:21.459058Z"
    },
    "papermill": {
     "duration": 0.874649,
     "end_time": "2020-11-10T04:38:21.459791",
     "exception": false,
     "start_time": "2020-11-10T04:38:20.585142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_dist = targets.copy()\n",
    "check_dist[\"mlp_fold\"] = np.array(mlp_fold).reshape(-1,1)\n",
    "\n",
    "unbalanced_feats = []\n",
    "for i in target_feats:\n",
    "    if np.max(check_dist.loc[:,[i]+[\"mlp_fold\"]].groupby(\"mlp_fold\").sum()).values[0] == check_dist[i].sum() and check_dist[i].sum()>=5:\n",
    "        unbalanced_feats.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030962,
     "end_time": "2020-11-10T04:38:21.524269",
     "exception": false,
     "start_time": "2020-11-10T04:38:21.493307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:21.632172Z",
     "iopub.status.busy": "2020-11-10T04:38:21.629951Z",
     "iopub.status.idle": "2020-11-10T04:38:21.632877Z",
     "shell.execute_reply": "2020-11-10T04:38:21.633410Z"
    },
    "papermill": {
     "duration": 0.077201,
     "end_time": "2020-11-10T04:38:21.633561",
     "exception": false,
     "start_time": "2020-11-10T04:38:21.556360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe_simple(df, remove_features):\n",
    "    tmp = df.copy()\n",
    "    tmp.loc[:, 'cp_dose'] = tmp.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    tmp.drop(remove_features, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def fe_simple2(df):\n",
    "    tmp = df.copy()\n",
    "    tmp = pd.get_dummies(tmp, columns=['cp_time','cp_dose'])\n",
    "    tmp.drop([\"cp_type\", \"sig_id\"], axis=1, inplace=True) \n",
    "    return tmp\n",
    "\n",
    "def fe_mlp(df_train, df_test):\n",
    "    tmp_train = df_train.copy()\n",
    "    tmp_test = df_test.copy()\n",
    "    X = tmp_train.iloc[:,4:].copy().values\n",
    "    select = VarianceThreshold(threshold=0.7)\n",
    "    X_new = select.fit_transform(X)\n",
    "    drop_feats = list(np.array(tmp_train.iloc[:,4:].columns)[select.get_support()==False])\n",
    "    \n",
    "    tmp_train.drop(drop_feats, axis=1, inplace=True)\n",
    "    tmp_test.drop(drop_feats, axis=1, inplace=True)\n",
    "\n",
    "    modg_feats = [i for i in tmp_train.columns if \"g-\" in i]\n",
    "    modc_feats = [i for i in tmp_train.columns if \"c-\" in i]\n",
    "    \n",
    "    for i in modc_feats + modg_feats:\n",
    "        ss = preprocessing.QuantileTransformer(n_quantiles=1000, random_state=0, output_distribution=\"normal\")\n",
    "        ss.fit(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_train[i] = ss.transform(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_test[i] = ss.transform(tmp_test[i].values.reshape(-1,1))\n",
    "    \n",
    "    c_num = 10\n",
    "    pca_c_cols = [\"pca-c\"+str(i+1) for i in range(c_num)]\n",
    "    pca = PCA(n_components=c_num,random_state=42)\n",
    "    c_train = pca.fit_transform(tmp_train[modc_feats])\n",
    "    c_test = pca.transform(tmp_test[modc_feats])\n",
    "    c_train = pd.DataFrame(c_train, columns=pca_c_cols)\n",
    "    c_test = pd.DataFrame(c_test, columns=pca_c_cols)\n",
    "\n",
    "    g_num = 60\n",
    "    pca_g_cols = [\"pca-g\"+str(i+1) for i in range(g_num)]\n",
    "    pca = PCA(n_components=g_num, random_state=42)\n",
    "    g_train = pca.fit_transform(tmp_train[modg_feats])\n",
    "    g_test = pca.transform(tmp_test[modg_feats])\n",
    "    g_train = pd.DataFrame(g_train, columns=pca_g_cols)\n",
    "    g_test = pd.DataFrame(g_test, columns=pca_g_cols)\n",
    "\n",
    "    tmp_train = pd.concat([tmp_train, c_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, c_test],axis=1)\n",
    "    tmp_train = pd.concat([tmp_train, g_train],axis=1)\n",
    "    tmp_test = pd.concat([tmp_test, g_test],axis=1)\n",
    "    \n",
    "    return tmp_train, tmp_test\n",
    "\n",
    "def fe_stats(df):\n",
    "    tmp = df.copy()\n",
    "    modg_feats = [i for i in tmp.columns if \"g-\" in i]\n",
    "    modc_feats = [i for i in tmp.columns if \"c-\" in i]\n",
    "    tmp['g_kurt'] = tmp[modg_feats].kurtosis(axis = 1)\n",
    "    tmp['g_skew'] = tmp[modg_feats].skew(axis = 1)\n",
    "    tmp['c_kurt'] = tmp[modc_feats].kurtosis(axis = 1)\n",
    "    tmp['c_skew'] = tmp[modc_feats].skew(axis = 1)\n",
    "    return tmp\n",
    "\n",
    "def fe_lstm(df_train, df_test):\n",
    "    tmp_train = df_train.copy()\n",
    "    tmp_test = df_test.copy()\n",
    "\n",
    "    tmp_train.drop([\"cp_type\", \"sig_id\", \"cp_dose\", \"cp_time\"], axis=1, inplace=True)\n",
    "    tmp_test.drop([\"cp_type\", \"sig_id\", \"cp_dose\", \"cp_time\"], axis=1, inplace=True)\n",
    "\n",
    "    X = tmp_train.copy().values\n",
    "    select = VarianceThreshold(threshold=0.4)\n",
    "    X_new = select.fit_transform(X)\n",
    "    drop_feats = list(np.array(tmp_train.columns)[select.get_support()==False])\n",
    "\n",
    "    tmp_train.drop(drop_feats, axis = 1, inplace=True)\n",
    "    tmp_test.drop(drop_feats, axis = 1, inplace=True)\n",
    "\n",
    "    modg_feats = [i for i in tmp_train.columns if \"g-\" in i]\n",
    "    modc_feats = [i for i in tmp_train.columns if \"c-\" in i]\n",
    "    \n",
    "    for i in modc_feats + modg_feats:\n",
    "        ss = preprocessing.RobustScaler()\n",
    "        ss.fit(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_train[i] = ss.transform(tmp_train[i].values.reshape(-1,1))\n",
    "        tmp_test[i] = ss.transform(tmp_test[i].values.reshape(-1,1))\n",
    "    \n",
    "    return tmp_train, tmp_test\n",
    "\n",
    "remove_features = [\"cp_type\" , \"sig_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:21.714493Z",
     "iopub.status.busy": "2020-11-10T04:38:21.712948Z",
     "iopub.status.idle": "2020-11-10T04:38:42.108447Z",
     "shell.execute_reply": "2020-11-10T04:38:42.109472Z"
    },
    "papermill": {
     "duration": 20.445351,
     "end_time": "2020-11-10T04:38:42.109625",
     "exception": false,
     "start_time": "2020-11-10T04:38:21.664274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 919) (3624, 918)\n",
      "(21948, 875) (3624, 874)\n",
      "(21948, 869) (3624, 868)\n",
      "(21948, 875) (3624, 874)\n"
     ]
    }
   ],
   "source": [
    "fn_train = fe_simple(train, remove_features)\n",
    "fn_test = fe_simple(test, remove_features)\n",
    "\n",
    "# pytorch mlp -----------------------------------\n",
    "mlp_train, mlp_test = fe_mlp(train, test)\n",
    "mlp_train = fe_stats(mlp_train)\n",
    "mlp_test = fe_stats(mlp_test)\n",
    "mlp_train = fe_simple2(mlp_train)\n",
    "mlp_test = fe_simple2(mlp_test)\n",
    "mlp_train[\"fold\"] = mlp_fold\n",
    "mlp_train = mlp_train.to_numpy()\n",
    "mlp_test = mlp_test.to_numpy()\n",
    "\n",
    "# pytorch tabnet ----------------------------------\n",
    "tab_train = fn_train.copy()\n",
    "tab_test = fn_test.copy()\n",
    "tab_train[\"fold\"] = tab_fold\n",
    "\n",
    "tab_train= tab_train.to_numpy()\n",
    "tab_test = tab_test.to_numpy()\n",
    "\n",
    "# pytorch lstm ------------------------------------\n",
    "lstm_train, lstm_test = fe_lstm(train, test)\n",
    "lstm_train[\"fold\"] = mlp_fold\n",
    "modg_feats = [i for i in lstm_train.columns if \"g-\" in i]\n",
    "modc_feats = [i for i in lstm_train.columns if \"c-\" in i]\n",
    "\n",
    "lstm_train = lstm_train.to_numpy()\n",
    "lstm_test = lstm_test.to_numpy()\n",
    "\n",
    "# svm-----------------------\n",
    "for i in c_feats + g_feats:\n",
    "    ss = preprocessing.StandardScaler()\n",
    "    ss.fit(fn_train[i].values.reshape(-1,1))\n",
    "    fn_train[i] = ss.transform(fn_train[i].values.reshape(-1,1))\n",
    "    fn_test[i] = ss.transform(fn_test[i].values.reshape(-1,1))\n",
    "fn_train[\"fold\"] = tab_fold\n",
    "fn_train = fn_train.to_numpy()\n",
    "fn_test = fn_test.to_numpy()\n",
    "    \n",
    "print(mlp_train.shape, mlp_test.shape)\n",
    "print(tab_train.shape, tab_test.shape)\n",
    "print(lstm_train.shape, lstm_test.shape)\n",
    "print(fn_train.shape, fn_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030986,
     "end_time": "2020-11-10T04:38:42.173014",
     "exception": false,
     "start_time": "2020-11-10T04:38:42.142028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:42.251642Z",
     "iopub.status.busy": "2020-11-10T04:38:42.249997Z",
     "iopub.status.idle": "2020-11-10T04:38:42.252403Z",
     "shell.execute_reply": "2020-11-10T04:38:42.252903Z"
    },
    "papermill": {
     "duration": 0.04776,
     "end_time": "2020-11-10T04:38:42.253028",
     "exception": false,
     "start_time": "2020-11-10T04:38:42.205268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(device) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:42.356020Z",
     "iopub.status.busy": "2020-11-10T04:38:42.330275Z",
     "iopub.status.idle": "2020-11-10T04:38:42.386803Z",
     "shell.execute_reply": "2020-11-10T04:38:42.386289Z"
    },
    "papermill": {
     "duration": 0.102248,
     "end_time": "2020-11-10T04:38:42.386903",
     "exception": false,
     "start_time": "2020-11-10T04:38:42.284655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128\n",
    "n_folds=7\n",
    "train_epochs = 20\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 1024))\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(1024, 1024))\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, last_num))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=224)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test = torch.utils.data.TensorDataset(X_test2) \n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    \n",
    "    scores = []\n",
    "    for fold in range(n_folds):\n",
    "        valid_index = X_train[:,-1] == fold\n",
    "        train_index = X_train[:,-1] != fold\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        X_train2 = X_train2[:,:-1]\n",
    "        X_valid2 = X_valid2[:,:-1]\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        loss_fn = SmoothCrossEntropyLoss(smoothing=smoothing)\n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                              max_lr=1e-2, epochs=train_epochs, steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            \n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            #scheduler.step() #avg_val_loss # maybe mistake\n",
    "                    \n",
    "            if sm_avg_val_loss < best_val_loss:\n",
    "                best_val_loss = sm_avg_val_loss\n",
    "                print('Best: Epoch {} \\t loss={:.5f} \\t val_loss={:.5f} \\t sm_loss={:.5f} \\t sm_val_loss={:.5f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))         \n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max)\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:38:42.458191Z",
     "iopub.status.busy": "2020-11-10T04:38:42.457450Z",
     "iopub.status.idle": "2020-11-10T04:51:17.787747Z",
     "shell.execute_reply": "2020-11-10T04:51:17.788260Z"
    },
    "papermill": {
     "duration": 755.370131,
     "end_time": "2020-11-10T04:51:17.788464",
     "exception": false,
     "start_time": "2020-11-10T04:38:42.418333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.41376 \t val_loss=0.02255 \t sm_loss=0.41373 \t sm_val_loss=0.02252 \t time=1.21s\n",
      "Best: Epoch 2 \t loss=0.02017 \t val_loss=0.01888 \t sm_loss=0.02011 \t sm_val_loss=0.01882 \t time=0.91s\n",
      "Best: Epoch 3 \t loss=0.01819 \t val_loss=0.01834 \t sm_loss=0.01820 \t sm_val_loss=0.01839 \t time=1.00s\n",
      "Best: Epoch 4 \t loss=0.01749 \t val_loss=0.01800 \t sm_loss=0.01757 \t sm_val_loss=0.01803 \t time=1.37s\n",
      "Best: Epoch 5 \t loss=0.01735 \t val_loss=0.01791 \t sm_loss=0.01746 \t sm_val_loss=0.01790 \t time=0.95s\n",
      "Best: Epoch 9 \t loss=0.01720 \t val_loss=0.01767 \t sm_loss=0.01733 \t sm_val_loss=0.01772 \t time=0.88s\n",
      "Best: Epoch 11 \t loss=0.01691 \t val_loss=0.01770 \t sm_loss=0.01704 \t sm_val_loss=0.01771 \t time=0.88s\n",
      "Best: Epoch 12 \t loss=0.01667 \t val_loss=0.01755 \t sm_loss=0.01682 \t sm_val_loss=0.01760 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01645 \t val_loss=0.01740 \t sm_loss=0.01661 \t sm_val_loss=0.01742 \t time=0.88s\n",
      "Best: Epoch 14 \t loss=0.01620 \t val_loss=0.01738 \t sm_loss=0.01637 \t sm_val_loss=0.01738 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01578 \t val_loss=0.01718 \t sm_loss=0.01595 \t sm_val_loss=0.01719 \t time=1.13s\n",
      "Best: Epoch 16 \t loss=0.01535 \t val_loss=0.01719 \t sm_loss=0.01554 \t sm_val_loss=0.01718 \t time=0.91s\n",
      "Best: Epoch 17 \t loss=0.01489 \t val_loss=0.01704 \t sm_loss=0.01509 \t sm_val_loss=0.01706 \t time=0.90s\n",
      "Best: Epoch 18 \t loss=0.01439 \t val_loss=0.01701 \t sm_loss=0.01461 \t sm_val_loss=0.01702 \t time=0.88s\n",
      "Best: Epoch 19 \t loss=0.01398 \t val_loss=0.01701 \t sm_loss=0.01420 \t sm_val_loss=0.01701 \t time=0.89s\n",
      "Fold 1 log loss: 0.017113289632990872\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.41428 \t val_loss=0.02199 \t sm_loss=0.41425 \t sm_val_loss=0.02198 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02018 \t val_loss=0.01862 \t sm_loss=0.02013 \t sm_val_loss=0.01863 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01814 \t val_loss=0.01765 \t sm_loss=0.01818 \t sm_val_loss=0.01770 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01745 \t val_loss=0.01764 \t sm_loss=0.01755 \t sm_val_loss=0.01760 \t time=0.94s\n",
      "Best: Epoch 5 \t loss=0.01735 \t val_loss=0.01729 \t sm_loss=0.01747 \t sm_val_loss=0.01729 \t time=1.18s\n",
      "Best: Epoch 11 \t loss=0.01686 \t val_loss=0.01716 \t sm_loss=0.01700 \t sm_val_loss=0.01719 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01666 \t val_loss=0.01711 \t sm_loss=0.01681 \t sm_val_loss=0.01715 \t time=0.92s\n",
      "Best: Epoch 13 \t loss=0.01640 \t val_loss=0.01695 \t sm_loss=0.01656 \t sm_val_loss=0.01699 \t time=0.90s\n",
      "Best: Epoch 16 \t loss=0.01541 \t val_loss=0.01692 \t sm_loss=0.01561 \t sm_val_loss=0.01695 \t time=1.17s\n",
      "Best: Epoch 17 \t loss=0.01489 \t val_loss=0.01692 \t sm_loss=0.01510 \t sm_val_loss=0.01694 \t time=0.92s\n",
      "Fold 2 log loss: 0.017049994689241232\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.41404 \t val_loss=0.02257 \t sm_loss=0.41396 \t sm_val_loss=0.02254 \t time=0.89s\n",
      "Best: Epoch 2 \t loss=0.02000 \t val_loss=0.02003 \t sm_loss=0.01997 \t sm_val_loss=0.01999 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01792 \t val_loss=0.01832 \t sm_loss=0.01798 \t sm_val_loss=0.01836 \t time=0.90s\n",
      "Best: Epoch 8 \t loss=0.01710 \t val_loss=0.01811 \t sm_loss=0.01722 \t sm_val_loss=0.01811 \t time=0.95s\n",
      "Best: Epoch 11 \t loss=0.01679 \t val_loss=0.01789 \t sm_loss=0.01693 \t sm_val_loss=0.01787 \t time=0.88s\n",
      "Best: Epoch 13 \t loss=0.01638 \t val_loss=0.01769 \t sm_loss=0.01654 \t sm_val_loss=0.01771 \t time=0.88s\n",
      "Best: Epoch 14 \t loss=0.01608 \t val_loss=0.01764 \t sm_loss=0.01625 \t sm_val_loss=0.01767 \t time=0.93s\n",
      "Best: Epoch 15 \t loss=0.01572 \t val_loss=0.01762 \t sm_loss=0.01590 \t sm_val_loss=0.01764 \t time=0.93s\n",
      "Best: Epoch 16 \t loss=0.01530 \t val_loss=0.01763 \t sm_loss=0.01549 \t sm_val_loss=0.01761 \t time=0.92s\n",
      "Best: Epoch 17 \t loss=0.01482 \t val_loss=0.01761 \t sm_loss=0.01503 \t sm_val_loss=0.01761 \t time=1.38s\n",
      "Best: Epoch 18 \t loss=0.01433 \t val_loss=0.01756 \t sm_loss=0.01455 \t sm_val_loss=0.01755 \t time=1.86s\n",
      "Fold 3 log loss: 0.01750418235532065\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.41387 \t val_loss=0.02273 \t sm_loss=0.41382 \t sm_val_loss=0.02269 \t time=1.00s\n",
      "Best: Epoch 3 \t loss=0.01804 \t val_loss=0.01868 \t sm_loss=0.01811 \t sm_val_loss=0.01870 \t time=0.94s\n",
      "Best: Epoch 4 \t loss=0.01738 \t val_loss=0.01849 \t sm_loss=0.01750 \t sm_val_loss=0.01847 \t time=0.92s\n",
      "Best: Epoch 6 \t loss=0.01723 \t val_loss=0.01841 \t sm_loss=0.01736 \t sm_val_loss=0.01842 \t time=1.16s\n",
      "Best: Epoch 7 \t loss=0.01720 \t val_loss=0.01841 \t sm_loss=0.01733 \t sm_val_loss=0.01839 \t time=0.96s\n",
      "Best: Epoch 9 \t loss=0.01707 \t val_loss=0.01820 \t sm_loss=0.01721 \t sm_val_loss=0.01819 \t time=1.09s\n",
      "Best: Epoch 11 \t loss=0.01685 \t val_loss=0.01792 \t sm_loss=0.01700 \t sm_val_loss=0.01791 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01640 \t val_loss=0.01783 \t sm_loss=0.01656 \t sm_val_loss=0.01783 \t time=0.91s\n",
      "Best: Epoch 15 \t loss=0.01566 \t val_loss=0.01780 \t sm_loss=0.01584 \t sm_val_loss=0.01781 \t time=0.90s\n",
      "Best: Epoch 16 \t loss=0.01529 \t val_loss=0.01778 \t sm_loss=0.01549 \t sm_val_loss=0.01777 \t time=0.92s\n",
      "Best: Epoch 17 \t loss=0.01479 \t val_loss=0.01773 \t sm_loss=0.01501 \t sm_val_loss=0.01772 \t time=1.23s\n",
      "Best: Epoch 20 \t loss=0.01367 \t val_loss=0.01772 \t sm_loss=0.01390 \t sm_val_loss=0.01771 \t time=0.94s\n",
      "Fold 4 log loss: 0.017804519168280273\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.41444 \t val_loss=0.02263 \t sm_loss=0.41439 \t sm_val_loss=0.02262 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02010 \t val_loss=0.01931 \t sm_loss=0.02005 \t sm_val_loss=0.01929 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01836 \t val_loss=0.01844 \t sm_loss=0.01836 \t sm_val_loss=0.01845 \t time=0.90s\n",
      "Best: Epoch 5 \t loss=0.01737 \t val_loss=0.01801 \t sm_loss=0.01750 \t sm_val_loss=0.01802 \t time=0.91s\n",
      "Best: Epoch 6 \t loss=0.01727 \t val_loss=0.01796 \t sm_loss=0.01740 \t sm_val_loss=0.01796 \t time=1.51s\n",
      "Best: Epoch 9 \t loss=0.01713 \t val_loss=0.01779 \t sm_loss=0.01726 \t sm_val_loss=0.01781 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01688 \t val_loss=0.01773 \t sm_loss=0.01702 \t sm_val_loss=0.01773 \t time=0.94s\n",
      "Best: Epoch 12 \t loss=0.01660 \t val_loss=0.01750 \t sm_loss=0.01675 \t sm_val_loss=0.01750 \t time=0.90s\n",
      "Best: Epoch 13 \t loss=0.01637 \t val_loss=0.01728 \t sm_loss=0.01653 \t sm_val_loss=0.01730 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01609 \t val_loss=0.01724 \t sm_loss=0.01626 \t sm_val_loss=0.01724 \t time=0.91s\n",
      "Best: Epoch 16 \t loss=0.01532 \t val_loss=0.01704 \t sm_loss=0.01551 \t sm_val_loss=0.01705 \t time=0.93s\n",
      "Best: Epoch 18 \t loss=0.01431 \t val_loss=0.01698 \t sm_loss=0.01453 \t sm_val_loss=0.01698 \t time=1.17s\n",
      "Fold 5 log loss: 0.017231030051370815\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.41385 \t val_loss=0.02254 \t sm_loss=0.41378 \t sm_val_loss=0.02252 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02009 \t val_loss=0.02210 \t sm_loss=0.02004 \t sm_val_loss=0.02127 \t time=0.88s\n",
      "Best: Epoch 3 \t loss=0.01817 \t val_loss=0.01851 \t sm_loss=0.01820 \t sm_val_loss=0.01851 \t time=0.91s\n",
      "Best: Epoch 4 \t loss=0.01764 \t val_loss=0.01824 \t sm_loss=0.01774 \t sm_val_loss=0.01832 \t time=0.89s\n",
      "Best: Epoch 8 \t loss=0.01725 \t val_loss=0.01786 \t sm_loss=0.01737 \t sm_val_loss=0.01795 \t time=1.13s\n",
      "Best: Epoch 10 \t loss=0.01708 \t val_loss=0.01773 \t sm_loss=0.01719 \t sm_val_loss=0.01780 \t time=0.88s\n",
      "Best: Epoch 11 \t loss=0.01691 \t val_loss=0.01772 \t sm_loss=0.01704 \t sm_val_loss=0.01779 \t time=0.93s\n",
      "Best: Epoch 12 \t loss=0.01674 \t val_loss=0.01751 \t sm_loss=0.01688 \t sm_val_loss=0.01761 \t time=0.88s\n",
      "Best: Epoch 14 \t loss=0.01629 \t val_loss=0.01746 \t sm_loss=0.01644 \t sm_val_loss=0.01752 \t time=1.19s\n",
      "Best: Epoch 15 \t loss=0.01586 \t val_loss=0.01728 \t sm_loss=0.01602 \t sm_val_loss=0.01736 \t time=1.00s\n",
      "Best: Epoch 16 \t loss=0.01546 \t val_loss=0.01723 \t sm_loss=0.01564 \t sm_val_loss=0.01732 \t time=0.91s\n",
      "Best: Epoch 17 \t loss=0.01504 \t val_loss=0.01722 \t sm_loss=0.01523 \t sm_val_loss=0.01731 \t time=1.23s\n",
      "Best: Epoch 18 \t loss=0.01450 \t val_loss=0.01722 \t sm_loss=0.01471 \t sm_val_loss=0.01730 \t time=1.08s\n",
      "Fold 6 log loss: 0.01733028201988104\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.41437 \t val_loss=0.02302 \t sm_loss=0.41431 \t sm_val_loss=0.02301 \t time=0.88s\n",
      "Best: Epoch 2 \t loss=0.02020 \t val_loss=0.01936 \t sm_loss=0.02014 \t sm_val_loss=0.01929 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01803 \t val_loss=0.01887 \t sm_loss=0.01808 \t sm_val_loss=0.01890 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01745 \t val_loss=0.01796 \t sm_loss=0.01754 \t sm_val_loss=0.01802 \t time=0.92s\n",
      "Best: Epoch 5 \t loss=0.01729 \t val_loss=0.01780 \t sm_loss=0.01740 \t sm_val_loss=0.01785 \t time=0.89s\n",
      "Best: Epoch 10 \t loss=0.01701 \t val_loss=0.01775 \t sm_loss=0.01714 \t sm_val_loss=0.01781 \t time=0.89s\n",
      "Best: Epoch 11 \t loss=0.01680 \t val_loss=0.01755 \t sm_loss=0.01694 \t sm_val_loss=0.01761 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01666 \t val_loss=0.01734 \t sm_loss=0.01681 \t sm_val_loss=0.01743 \t time=0.89s\n",
      "Best: Epoch 14 \t loss=0.01618 \t val_loss=0.01720 \t sm_loss=0.01634 \t sm_val_loss=0.01725 \t time=0.90s\n",
      "Best: Epoch 16 \t loss=0.01529 \t val_loss=0.01709 \t sm_loss=0.01548 \t sm_val_loss=0.01714 \t time=0.90s\n",
      "Best: Epoch 17 \t loss=0.01484 \t val_loss=0.01705 \t sm_loss=0.01505 \t sm_val_loss=0.01710 \t time=0.91s\n",
      "Best: Epoch 18 \t loss=0.01431 \t val_loss=0.01696 \t sm_loss=0.01453 \t sm_val_loss=0.01702 \t time=0.91s\n",
      "Best: Epoch 19 \t loss=0.01386 \t val_loss=0.01695 \t sm_loss=0.01409 \t sm_val_loss=0.01700 \t time=1.02s\n",
      "Fold 7 log loss: 0.017022507464739062\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.017113289632990872\n",
      "Fold 2 log loss: 0.017049994689241232\n",
      "Fold 3 log loss: 0.01750418235532065\n",
      "Fold 4 log loss: 0.017804519168280273\n",
      "Fold 5 log loss: 0.017231030051370815\n",
      "Fold 6 log loss: 0.01733028201988104\n",
      "Fold 7 log loss: 0.017022507464739062\n",
      "Std of log loss: 0.00026076546116121505\n",
      "Total log loss: 0.01729337550164496\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.41504 \t val_loss=0.02308 \t sm_loss=0.41500 \t sm_val_loss=0.02304 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02017 \t val_loss=0.01958 \t sm_loss=0.02012 \t sm_val_loss=0.01953 \t time=0.89s\n",
      "Best: Epoch 4 \t loss=0.01741 \t val_loss=0.01799 \t sm_loss=0.01752 \t sm_val_loss=0.01799 \t time=0.91s\n",
      "Best: Epoch 7 \t loss=0.01729 \t val_loss=0.01794 \t sm_loss=0.01740 \t sm_val_loss=0.01795 \t time=0.90s\n",
      "Best: Epoch 8 \t loss=0.01718 \t val_loss=0.01772 \t sm_loss=0.01730 \t sm_val_loss=0.01775 \t time=1.09s\n",
      "Best: Epoch 10 \t loss=0.01703 \t val_loss=0.01756 \t sm_loss=0.01715 \t sm_val_loss=0.01756 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01649 \t val_loss=0.01736 \t sm_loss=0.01664 \t sm_val_loss=0.01737 \t time=0.92s\n",
      "Best: Epoch 14 \t loss=0.01618 \t val_loss=0.01735 \t sm_loss=0.01634 \t sm_val_loss=0.01737 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01588 \t val_loss=0.01719 \t sm_loss=0.01605 \t sm_val_loss=0.01718 \t time=0.91s\n",
      "Best: Epoch 16 \t loss=0.01544 \t val_loss=0.01705 \t sm_loss=0.01563 \t sm_val_loss=0.01706 \t time=0.90s\n",
      "Best: Epoch 18 \t loss=0.01455 \t val_loss=0.01693 \t sm_loss=0.01476 \t sm_val_loss=0.01695 \t time=0.90s\n",
      "Fold 1 log loss: 0.01705126170858484\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.41372 \t val_loss=0.02157 \t sm_loss=0.41367 \t sm_val_loss=0.02156 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02023 \t val_loss=0.01814 \t sm_loss=0.02019 \t sm_val_loss=0.01815 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01802 \t val_loss=0.01796 \t sm_loss=0.01806 \t sm_val_loss=0.01796 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01744 \t val_loss=0.01763 \t sm_loss=0.01755 \t sm_val_loss=0.01762 \t time=0.91s\n",
      "Best: Epoch 5 \t loss=0.01731 \t val_loss=0.01741 \t sm_loss=0.01743 \t sm_val_loss=0.01742 \t time=0.90s\n",
      "Best: Epoch 9 \t loss=0.01714 \t val_loss=0.01733 \t sm_loss=0.01727 \t sm_val_loss=0.01735 \t time=0.91s\n",
      "Best: Epoch 10 \t loss=0.01698 \t val_loss=0.01729 \t sm_loss=0.01712 \t sm_val_loss=0.01731 \t time=1.44s\n",
      "Best: Epoch 11 \t loss=0.01689 \t val_loss=0.01727 \t sm_loss=0.01703 \t sm_val_loss=0.01730 \t time=1.00s\n",
      "Best: Epoch 12 \t loss=0.01671 \t val_loss=0.01707 \t sm_loss=0.01685 \t sm_val_loss=0.01711 \t time=0.96s\n",
      "Best: Epoch 14 \t loss=0.01617 \t val_loss=0.01705 \t sm_loss=0.01634 \t sm_val_loss=0.01709 \t time=0.96s\n",
      "Best: Epoch 15 \t loss=0.01582 \t val_loss=0.01685 \t sm_loss=0.01600 \t sm_val_loss=0.01688 \t time=0.97s\n",
      "Fold 2 log loss: 0.017004102171724527\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.41474 \t val_loss=0.02394 \t sm_loss=0.41469 \t sm_val_loss=0.02392 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02007 \t val_loss=0.01890 \t sm_loss=0.01999 \t sm_val_loss=0.01895 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01795 \t val_loss=0.01836 \t sm_loss=0.01799 \t sm_val_loss=0.01839 \t time=0.90s\n",
      "Best: Epoch 7 \t loss=0.01709 \t val_loss=0.01813 \t sm_loss=0.01723 \t sm_val_loss=0.01812 \t time=0.89s\n",
      "Best: Epoch 8 \t loss=0.01707 \t val_loss=0.01817 \t sm_loss=0.01721 \t sm_val_loss=0.01811 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01670 \t val_loss=0.01790 \t sm_loss=0.01685 \t sm_val_loss=0.01790 \t time=0.88s\n",
      "Best: Epoch 12 \t loss=0.01657 \t val_loss=0.01786 \t sm_loss=0.01672 \t sm_val_loss=0.01786 \t time=0.88s\n",
      "Best: Epoch 13 \t loss=0.01633 \t val_loss=0.01775 \t sm_loss=0.01648 \t sm_val_loss=0.01777 \t time=0.90s\n",
      "Best: Epoch 14 \t loss=0.01601 \t val_loss=0.01772 \t sm_loss=0.01617 \t sm_val_loss=0.01775 \t time=0.91s\n",
      "Best: Epoch 16 \t loss=0.01522 \t val_loss=0.01763 \t sm_loss=0.01542 \t sm_val_loss=0.01763 \t time=0.90s\n",
      "Best: Epoch 17 \t loss=0.01471 \t val_loss=0.01760 \t sm_loss=0.01492 \t sm_val_loss=0.01762 \t time=0.89s\n",
      "Best: Epoch 18 \t loss=0.01418 \t val_loss=0.01751 \t sm_loss=0.01440 \t sm_val_loss=0.01751 \t time=0.89s\n",
      "Fold 3 log loss: 0.017463404467133407\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.41378 \t val_loss=0.02243 \t sm_loss=0.41369 \t sm_val_loss=0.02242 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.01997 \t val_loss=0.02015 \t sm_loss=0.01994 \t sm_val_loss=0.01993 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01786 \t val_loss=0.01940 \t sm_loss=0.01794 \t sm_val_loss=0.01919 \t time=0.92s\n",
      "Best: Epoch 4 \t loss=0.01747 \t val_loss=0.01847 \t sm_loss=0.01757 \t sm_val_loss=0.01841 \t time=0.92s\n",
      "Best: Epoch 6 \t loss=0.01715 \t val_loss=0.01819 \t sm_loss=0.01728 \t sm_val_loss=0.01817 \t time=1.15s\n",
      "Best: Epoch 8 \t loss=0.01710 \t val_loss=0.01817 \t sm_loss=0.01722 \t sm_val_loss=0.01816 \t time=0.90s\n",
      "Best: Epoch 10 \t loss=0.01688 \t val_loss=0.01808 \t sm_loss=0.01702 \t sm_val_loss=0.01807 \t time=1.08s\n",
      "Best: Epoch 11 \t loss=0.01673 \t val_loss=0.01795 \t sm_loss=0.01687 \t sm_val_loss=0.01795 \t time=0.98s\n",
      "Best: Epoch 12 \t loss=0.01651 \t val_loss=0.01778 \t sm_loss=0.01667 \t sm_val_loss=0.01776 \t time=0.92s\n",
      "Best: Epoch 16 \t loss=0.01522 \t val_loss=0.01768 \t sm_loss=0.01541 \t sm_val_loss=0.01768 \t time=0.89s\n",
      "Best: Epoch 17 \t loss=0.01474 \t val_loss=0.01767 \t sm_loss=0.01495 \t sm_val_loss=0.01766 \t time=0.90s\n",
      "Fold 4 log loss: 0.017758500304861553\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.41486 \t val_loss=0.02257 \t sm_loss=0.41481 \t sm_val_loss=0.02251 \t time=1.07s\n",
      "Best: Epoch 2 \t loss=0.02013 \t val_loss=0.02069 \t sm_loss=0.02007 \t sm_val_loss=0.02068 \t time=0.95s\n",
      "Best: Epoch 3 \t loss=0.01814 \t val_loss=0.01867 \t sm_loss=0.01818 \t sm_val_loss=0.01871 \t time=0.95s\n",
      "Best: Epoch 4 \t loss=0.01744 \t val_loss=0.01832 \t sm_loss=0.01756 \t sm_val_loss=0.01829 \t time=1.44s\n",
      "Best: Epoch 5 \t loss=0.01724 \t val_loss=0.01808 \t sm_loss=0.01737 \t sm_val_loss=0.01806 \t time=1.01s\n",
      "Best: Epoch 8 \t loss=0.01720 \t val_loss=0.01790 \t sm_loss=0.01733 \t sm_val_loss=0.01792 \t time=0.94s\n",
      "Best: Epoch 9 \t loss=0.01711 \t val_loss=0.01773 \t sm_loss=0.01724 \t sm_val_loss=0.01771 \t time=0.95s\n",
      "Best: Epoch 10 \t loss=0.01701 \t val_loss=0.01763 \t sm_loss=0.01715 \t sm_val_loss=0.01765 \t time=0.93s\n",
      "Best: Epoch 11 \t loss=0.01681 \t val_loss=0.01760 \t sm_loss=0.01695 \t sm_val_loss=0.01761 \t time=0.97s\n",
      "Best: Epoch 12 \t loss=0.01672 \t val_loss=0.01754 \t sm_loss=0.01686 \t sm_val_loss=0.01754 \t time=1.21s\n",
      "Best: Epoch 13 \t loss=0.01638 \t val_loss=0.01743 \t sm_loss=0.01653 \t sm_val_loss=0.01742 \t time=0.92s\n",
      "Best: Epoch 14 \t loss=0.01617 \t val_loss=0.01720 \t sm_loss=0.01634 \t sm_val_loss=0.01721 \t time=0.93s\n",
      "Best: Epoch 16 \t loss=0.01539 \t val_loss=0.01714 \t sm_loss=0.01558 \t sm_val_loss=0.01717 \t time=1.06s\n",
      "Best: Epoch 17 \t loss=0.01490 \t val_loss=0.01703 \t sm_loss=0.01511 \t sm_val_loss=0.01705 \t time=1.10s\n",
      "Best: Epoch 18 \t loss=0.01442 \t val_loss=0.01697 \t sm_loss=0.01463 \t sm_val_loss=0.01698 \t time=0.90s\n",
      "Best: Epoch 19 \t loss=0.01406 \t val_loss=0.01697 \t sm_loss=0.01428 \t sm_val_loss=0.01697 \t time=0.90s\n",
      "Fold 5 log loss: 0.017212529109482812\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.41462 \t val_loss=0.02295 \t sm_loss=0.41456 \t sm_val_loss=0.02293 \t time=0.91s\n",
      "Best: Epoch 2 \t loss=0.02001 \t val_loss=0.02236 \t sm_loss=0.01995 \t sm_val_loss=0.02237 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01811 \t val_loss=0.01801 \t sm_loss=0.01812 \t sm_val_loss=0.01808 \t time=0.90s\n",
      "Best: Epoch 8 \t loss=0.01733 \t val_loss=0.01791 \t sm_loss=0.01745 \t sm_val_loss=0.01799 \t time=0.90s\n",
      "Best: Epoch 9 \t loss=0.01722 \t val_loss=0.01779 \t sm_loss=0.01733 \t sm_val_loss=0.01786 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01684 \t val_loss=0.01760 \t sm_loss=0.01696 \t sm_val_loss=0.01770 \t time=0.98s\n",
      "Best: Epoch 13 \t loss=0.01648 \t val_loss=0.01752 \t sm_loss=0.01663 \t sm_val_loss=0.01760 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01612 \t val_loss=0.01748 \t sm_loss=0.01629 \t sm_val_loss=0.01755 \t time=0.92s\n",
      "Best: Epoch 15 \t loss=0.01580 \t val_loss=0.01738 \t sm_loss=0.01597 \t sm_val_loss=0.01746 \t time=0.92s\n",
      "Best: Epoch 16 \t loss=0.01536 \t val_loss=0.01738 \t sm_loss=0.01555 \t sm_val_loss=0.01746 \t time=0.89s\n",
      "Best: Epoch 17 \t loss=0.01488 \t val_loss=0.01729 \t sm_loss=0.01508 \t sm_val_loss=0.01736 \t time=0.89s\n",
      "Fold 6 log loss: 0.017391987919513786\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.41481 \t val_loss=0.02261 \t sm_loss=0.41477 \t sm_val_loss=0.02260 \t time=1.06s\n",
      "Best: Epoch 2 \t loss=0.02005 \t val_loss=0.01879 \t sm_loss=0.01999 \t sm_val_loss=0.01884 \t time=1.15s\n",
      "Best: Epoch 3 \t loss=0.01817 \t val_loss=0.01785 \t sm_loss=0.01818 \t sm_val_loss=0.01792 \t time=0.96s\n",
      "Best: Epoch 8 \t loss=0.01721 \t val_loss=0.01774 \t sm_loss=0.01733 \t sm_val_loss=0.01778 \t time=0.99s\n",
      "Best: Epoch 9 \t loss=0.01708 \t val_loss=0.01765 \t sm_loss=0.01720 \t sm_val_loss=0.01770 \t time=0.99s\n",
      "Best: Epoch 11 \t loss=0.01689 \t val_loss=0.01752 \t sm_loss=0.01702 \t sm_val_loss=0.01757 \t time=0.95s\n",
      "Best: Epoch 12 \t loss=0.01668 \t val_loss=0.01736 \t sm_loss=0.01682 \t sm_val_loss=0.01743 \t time=1.17s\n",
      "Best: Epoch 13 \t loss=0.01645 \t val_loss=0.01719 \t sm_loss=0.01660 \t sm_val_loss=0.01727 \t time=0.96s\n",
      "Best: Epoch 15 \t loss=0.01576 \t val_loss=0.01720 \t sm_loss=0.01594 \t sm_val_loss=0.01726 \t time=0.93s\n",
      "Best: Epoch 16 \t loss=0.01536 \t val_loss=0.01704 \t sm_loss=0.01555 \t sm_val_loss=0.01712 \t time=0.94s\n",
      "Best: Epoch 17 \t loss=0.01484 \t val_loss=0.01698 \t sm_loss=0.01504 \t sm_val_loss=0.01706 \t time=0.92s\n",
      "Best: Epoch 19 \t loss=0.01390 \t val_loss=0.01695 \t sm_loss=0.01412 \t sm_val_loss=0.01702 \t time=0.92s\n",
      "Fold 7 log loss: 0.017041612463113538\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.01705126170858484\n",
      "Fold 2 log loss: 0.017004102171724527\n",
      "Fold 3 log loss: 0.017463404467133407\n",
      "Fold 4 log loss: 0.017758500304861553\n",
      "Fold 5 log loss: 0.017212529109482812\n",
      "Fold 6 log loss: 0.017391987919513786\n",
      "Fold 7 log loss: 0.017041612463113538\n",
      "Std of log loss: 0.0002576740481782135\n",
      "Total log loss: 0.01727447273181132\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.41434 \t val_loss=0.02256 \t sm_loss=0.41429 \t sm_val_loss=0.02252 \t time=1.17s\n",
      "Best: Epoch 2 \t loss=0.02010 \t val_loss=0.01879 \t sm_loss=0.02005 \t sm_val_loss=0.01877 \t time=0.94s\n",
      "Best: Epoch 3 \t loss=0.01801 \t val_loss=0.01835 \t sm_loss=0.01804 \t sm_val_loss=0.01841 \t time=0.92s\n",
      "Best: Epoch 4 \t loss=0.01742 \t val_loss=0.01783 \t sm_loss=0.01752 \t sm_val_loss=0.01781 \t time=0.95s\n",
      "Best: Epoch 11 \t loss=0.01688 \t val_loss=0.01754 \t sm_loss=0.01702 \t sm_val_loss=0.01759 \t time=1.19s\n",
      "Best: Epoch 13 \t loss=0.01647 \t val_loss=0.01746 \t sm_loss=0.01662 \t sm_val_loss=0.01750 \t time=0.89s\n",
      "Best: Epoch 14 \t loss=0.01615 \t val_loss=0.01727 \t sm_loss=0.01632 \t sm_val_loss=0.01730 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01579 \t val_loss=0.01717 \t sm_loss=0.01597 \t sm_val_loss=0.01718 \t time=1.25s\n",
      "Best: Epoch 17 \t loss=0.01489 \t val_loss=0.01702 \t sm_loss=0.01510 \t sm_val_loss=0.01705 \t time=0.91s\n",
      "Best: Epoch 19 \t loss=0.01396 \t val_loss=0.01699 \t sm_loss=0.01419 \t sm_val_loss=0.01701 \t time=0.91s\n",
      "Fold 1 log loss: 0.01711217684578401\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.41383 \t val_loss=0.02216 \t sm_loss=0.41378 \t sm_val_loss=0.02214 \t time=1.13s\n",
      "Best: Epoch 2 \t loss=0.01997 \t val_loss=0.01820 \t sm_loss=0.01994 \t sm_val_loss=0.01826 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01850 \t val_loss=0.01788 \t sm_loss=0.01845 \t sm_val_loss=0.01792 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01751 \t val_loss=0.01759 \t sm_loss=0.01761 \t sm_val_loss=0.01765 \t time=0.94s\n",
      "Best: Epoch 5 \t loss=0.01739 \t val_loss=0.01759 \t sm_loss=0.01752 \t sm_val_loss=0.01763 \t time=0.93s\n",
      "Best: Epoch 7 \t loss=0.01734 \t val_loss=0.01753 \t sm_loss=0.01747 \t sm_val_loss=0.01754 \t time=0.89s\n",
      "Best: Epoch 9 \t loss=0.01717 \t val_loss=0.01730 \t sm_loss=0.01729 \t sm_val_loss=0.01734 \t time=0.90s\n",
      "Best: Epoch 14 \t loss=0.01621 \t val_loss=0.01714 \t sm_loss=0.01637 \t sm_val_loss=0.01713 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01581 \t val_loss=0.01705 \t sm_loss=0.01599 \t sm_val_loss=0.01708 \t time=0.92s\n",
      "Best: Epoch 18 \t loss=0.01443 \t val_loss=0.01700 \t sm_loss=0.01465 \t sm_val_loss=0.01702 \t time=0.91s\n",
      "Fold 2 log loss: 0.017121918826191393\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.41482 \t val_loss=0.02237 \t sm_loss=0.41476 \t sm_val_loss=0.02236 \t time=0.95s\n",
      "Best: Epoch 2 \t loss=0.02021 \t val_loss=0.01868 \t sm_loss=0.02016 \t sm_val_loss=0.01874 \t time=1.12s\n",
      "Best: Epoch 3 \t loss=0.01792 \t val_loss=0.01832 \t sm_loss=0.01797 \t sm_val_loss=0.01837 \t time=0.94s\n",
      "Best: Epoch 5 \t loss=0.01728 \t val_loss=0.01825 \t sm_loss=0.01741 \t sm_val_loss=0.01824 \t time=1.24s\n",
      "Best: Epoch 9 \t loss=0.01703 \t val_loss=0.01794 \t sm_loss=0.01716 \t sm_val_loss=0.01796 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01685 \t val_loss=0.01792 \t sm_loss=0.01699 \t sm_val_loss=0.01793 \t time=0.90s\n",
      "Best: Epoch 13 \t loss=0.01633 \t val_loss=0.01766 \t sm_loss=0.01648 \t sm_val_loss=0.01768 \t time=0.99s\n",
      "Best: Epoch 14 \t loss=0.01602 \t val_loss=0.01765 \t sm_loss=0.01619 \t sm_val_loss=0.01767 \t time=1.07s\n",
      "Best: Epoch 16 \t loss=0.01520 \t val_loss=0.01752 \t sm_loss=0.01540 \t sm_val_loss=0.01752 \t time=1.30s\n",
      "Best: Epoch 19 \t loss=0.01374 \t val_loss=0.01752 \t sm_loss=0.01397 \t sm_val_loss=0.01751 \t time=1.19s\n",
      "Best: Epoch 20 \t loss=0.01348 \t val_loss=0.01752 \t sm_loss=0.01371 \t sm_val_loss=0.01751 \t time=0.90s\n",
      "Fold 3 log loss: 0.017468441953835206\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.41324 \t val_loss=0.02301 \t sm_loss=0.41317 \t sm_val_loss=0.02301 \t time=0.95s\n",
      "Best: Epoch 2 \t loss=0.01993 \t val_loss=0.02082 \t sm_loss=0.01992 \t sm_val_loss=0.02063 \t time=1.02s\n",
      "Best: Epoch 3 \t loss=0.01789 \t val_loss=0.01923 \t sm_loss=0.01797 \t sm_val_loss=0.01873 \t time=1.11s\n",
      "Best: Epoch 5 \t loss=0.01724 \t val_loss=0.01841 \t sm_loss=0.01737 \t sm_val_loss=0.01840 \t time=0.93s\n",
      "Best: Epoch 7 \t loss=0.01720 \t val_loss=0.01828 \t sm_loss=0.01733 \t sm_val_loss=0.01824 \t time=0.93s\n",
      "Best: Epoch 8 \t loss=0.01710 \t val_loss=0.01822 \t sm_loss=0.01724 \t sm_val_loss=0.01820 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.01671 \t val_loss=0.01797 \t sm_loss=0.01685 \t sm_val_loss=0.01797 \t time=0.91s\n",
      "Best: Epoch 12 \t loss=0.01656 \t val_loss=0.01791 \t sm_loss=0.01672 \t sm_val_loss=0.01791 \t time=0.97s\n",
      "Best: Epoch 13 \t loss=0.01634 \t val_loss=0.01782 \t sm_loss=0.01650 \t sm_val_loss=0.01779 \t time=0.95s\n",
      "Best: Epoch 15 \t loss=0.01560 \t val_loss=0.01763 \t sm_loss=0.01578 \t sm_val_loss=0.01764 \t time=1.19s\n",
      "Best: Epoch 19 \t loss=0.01381 \t val_loss=0.01761 \t sm_loss=0.01404 \t sm_val_loss=0.01760 \t time=0.92s\n",
      "Fold 4 log loss: 0.017700802099298828\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.41407 \t val_loss=0.02254 \t sm_loss=0.41402 \t sm_val_loss=0.02252 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.02030 \t val_loss=0.01917 \t sm_loss=0.02020 \t sm_val_loss=0.01919 \t time=0.91s\n",
      "Best: Epoch 3 \t loss=0.01804 \t val_loss=0.01810 \t sm_loss=0.01811 \t sm_val_loss=0.01811 \t time=1.09s\n",
      "Best: Epoch 4 \t loss=0.01742 \t val_loss=0.01797 \t sm_loss=0.01752 \t sm_val_loss=0.01797 \t time=1.07s\n",
      "Best: Epoch 5 \t loss=0.01725 \t val_loss=0.01783 \t sm_loss=0.01738 \t sm_val_loss=0.01784 \t time=0.97s\n",
      "Best: Epoch 7 \t loss=0.01721 \t val_loss=0.01776 \t sm_loss=0.01734 \t sm_val_loss=0.01776 \t time=0.95s\n",
      "Best: Epoch 10 \t loss=0.01694 \t val_loss=0.01768 \t sm_loss=0.01707 \t sm_val_loss=0.01764 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01681 \t val_loss=0.01762 \t sm_loss=0.01695 \t sm_val_loss=0.01762 \t time=0.93s\n",
      "Best: Epoch 12 \t loss=0.01664 \t val_loss=0.01730 \t sm_loss=0.01679 \t sm_val_loss=0.01733 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01613 \t val_loss=0.01728 \t sm_loss=0.01630 \t sm_val_loss=0.01730 \t time=0.92s\n",
      "Best: Epoch 16 \t loss=0.01534 \t val_loss=0.01721 \t sm_loss=0.01554 \t sm_val_loss=0.01719 \t time=0.90s\n",
      "Best: Epoch 17 \t loss=0.01487 \t val_loss=0.01707 \t sm_loss=0.01508 \t sm_val_loss=0.01707 \t time=0.91s\n",
      "Best: Epoch 18 \t loss=0.01439 \t val_loss=0.01697 \t sm_loss=0.01461 \t sm_val_loss=0.01697 \t time=0.91s\n",
      "Fold 5 log loss: 0.01721973917108187\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.41414 \t val_loss=0.02279 \t sm_loss=0.41407 \t sm_val_loss=0.02278 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.02001 \t val_loss=0.02059 \t sm_loss=0.01998 \t sm_val_loss=0.02061 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01808 \t val_loss=0.01843 \t sm_loss=0.01814 \t sm_val_loss=0.01849 \t time=1.12s\n",
      "Best: Epoch 4 \t loss=0.01760 \t val_loss=0.01817 \t sm_loss=0.01770 \t sm_val_loss=0.01824 \t time=1.16s\n",
      "Best: Epoch 6 \t loss=0.01744 \t val_loss=0.01805 \t sm_loss=0.01755 \t sm_val_loss=0.01813 \t time=1.05s\n",
      "Best: Epoch 7 \t loss=0.01730 \t val_loss=0.01795 \t sm_loss=0.01742 \t sm_val_loss=0.01802 \t time=0.92s\n",
      "Best: Epoch 8 \t loss=0.01732 \t val_loss=0.01790 \t sm_loss=0.01743 \t sm_val_loss=0.01797 \t time=0.89s\n",
      "Best: Epoch 10 \t loss=0.01703 \t val_loss=0.01781 \t sm_loss=0.01715 \t sm_val_loss=0.01786 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.01692 \t val_loss=0.01758 \t sm_loss=0.01705 \t sm_val_loss=0.01764 \t time=0.91s\n",
      "Best: Epoch 12 \t loss=0.01671 \t val_loss=0.01749 \t sm_loss=0.01685 \t sm_val_loss=0.01759 \t time=1.39s\n",
      "Best: Epoch 14 \t loss=0.01622 \t val_loss=0.01752 \t sm_loss=0.01638 \t sm_val_loss=0.01758 \t time=0.94s\n",
      "Best: Epoch 15 \t loss=0.01578 \t val_loss=0.01723 \t sm_loss=0.01595 \t sm_val_loss=0.01731 \t time=0.99s\n",
      "Best: Epoch 18 \t loss=0.01445 \t val_loss=0.01717 \t sm_loss=0.01465 \t sm_val_loss=0.01726 \t time=0.96s\n",
      "Fold 6 log loss: 0.017309055830451555\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.41375 \t val_loss=0.02254 \t sm_loss=0.41371 \t sm_val_loss=0.02253 \t time=0.91s\n",
      "Best: Epoch 3 \t loss=0.01815 \t val_loss=0.01783 \t sm_loss=0.01819 \t sm_val_loss=0.01791 \t time=0.91s\n",
      "Best: Epoch 7 \t loss=0.01728 \t val_loss=0.01784 \t sm_loss=0.01740 \t sm_val_loss=0.01789 \t time=0.89s\n",
      "Best: Epoch 9 \t loss=0.01711 \t val_loss=0.01781 \t sm_loss=0.01724 \t sm_val_loss=0.01787 \t time=0.91s\n",
      "Best: Epoch 10 \t loss=0.01700 \t val_loss=0.01764 \t sm_loss=0.01713 \t sm_val_loss=0.01770 \t time=0.98s\n",
      "Best: Epoch 11 \t loss=0.01683 \t val_loss=0.01748 \t sm_loss=0.01697 \t sm_val_loss=0.01756 \t time=0.91s\n",
      "Best: Epoch 12 \t loss=0.01663 \t val_loss=0.01734 \t sm_loss=0.01677 \t sm_val_loss=0.01741 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01615 \t val_loss=0.01731 \t sm_loss=0.01631 \t sm_val_loss=0.01739 \t time=0.92s\n",
      "Best: Epoch 15 \t loss=0.01574 \t val_loss=0.01706 \t sm_loss=0.01592 \t sm_val_loss=0.01713 \t time=0.95s\n",
      "Best: Epoch 16 \t loss=0.01533 \t val_loss=0.01702 \t sm_loss=0.01552 \t sm_val_loss=0.01708 \t time=1.33s\n",
      "Best: Epoch 17 \t loss=0.01484 \t val_loss=0.01702 \t sm_loss=0.01505 \t sm_val_loss=0.01707 \t time=0.99s\n",
      "Best: Epoch 18 \t loss=0.01433 \t val_loss=0.01696 \t sm_loss=0.01455 \t sm_val_loss=0.01701 \t time=0.92s\n",
      "Best: Epoch 19 \t loss=0.01391 \t val_loss=0.01695 \t sm_loss=0.01413 \t sm_val_loss=0.01700 \t time=0.91s\n",
      "Fold 7 log loss: 0.01701579424366475\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.01711217684578401\n",
      "Fold 2 log loss: 0.017121918826191393\n",
      "Fold 3 log loss: 0.017468441953835206\n",
      "Fold 4 log loss: 0.017700802099298828\n",
      "Fold 5 log loss: 0.01721973917108187\n",
      "Fold 6 log loss: 0.017309055830451555\n",
      "Fold 7 log loss: 0.01701579424366475\n",
      "Std of log loss: 0.00022038878637838544\n",
      "Total log loss: 0.017277915685424167\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.41484 \t val_loss=0.02271 \t sm_loss=0.41478 \t sm_val_loss=0.02268 \t time=0.90s\n",
      "Best: Epoch 3 \t loss=0.01815 \t val_loss=0.01793 \t sm_loss=0.01816 \t sm_val_loss=0.01796 \t time=0.92s\n",
      "Best: Epoch 7 \t loss=0.01725 \t val_loss=0.01780 \t sm_loss=0.01738 \t sm_val_loss=0.01783 \t time=0.92s\n",
      "Best: Epoch 10 \t loss=0.01699 \t val_loss=0.01770 \t sm_loss=0.01712 \t sm_val_loss=0.01773 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01673 \t val_loss=0.01748 \t sm_loss=0.01687 \t sm_val_loss=0.01751 \t time=0.89s\n",
      "Best: Epoch 14 \t loss=0.01614 \t val_loss=0.01728 \t sm_loss=0.01631 \t sm_val_loss=0.01728 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01576 \t val_loss=0.01712 \t sm_loss=0.01594 \t sm_val_loss=0.01713 \t time=1.10s\n",
      "Best: Epoch 18 \t loss=0.01442 \t val_loss=0.01713 \t sm_loss=0.01463 \t sm_val_loss=0.01713 \t time=0.90s\n",
      "Best: Epoch 19 \t loss=0.01400 \t val_loss=0.01709 \t sm_loss=0.01422 \t sm_val_loss=0.01708 \t time=0.98s\n",
      "Best: Epoch 20 \t loss=0.01377 \t val_loss=0.01708 \t sm_loss=0.01399 \t sm_val_loss=0.01708 \t time=0.90s\n",
      "Fold 1 log loss: 0.01717298648313837\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.41495 \t val_loss=0.02254 \t sm_loss=0.41488 \t sm_val_loss=0.02254 \t time=0.91s\n",
      "Best: Epoch 2 \t loss=0.02010 \t val_loss=0.01823 \t sm_loss=0.02005 \t sm_val_loss=0.01828 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01802 \t val_loss=0.01767 \t sm_loss=0.01806 \t sm_val_loss=0.01767 \t time=0.91s\n",
      "Best: Epoch 5 \t loss=0.01726 \t val_loss=0.01756 \t sm_loss=0.01739 \t sm_val_loss=0.01754 \t time=1.77s\n",
      "Best: Epoch 8 \t loss=0.01725 \t val_loss=0.01744 \t sm_loss=0.01738 \t sm_val_loss=0.01750 \t time=0.90s\n",
      "Best: Epoch 10 \t loss=0.01713 \t val_loss=0.01739 \t sm_loss=0.01726 \t sm_val_loss=0.01742 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.01693 \t val_loss=0.01732 \t sm_loss=0.01706 \t sm_val_loss=0.01736 \t time=1.19s\n",
      "Best: Epoch 12 \t loss=0.01671 \t val_loss=0.01729 \t sm_loss=0.01686 \t sm_val_loss=0.01732 \t time=0.92s\n",
      "Best: Epoch 13 \t loss=0.01651 \t val_loss=0.01704 \t sm_loss=0.01666 \t sm_val_loss=0.01706 \t time=0.97s\n",
      "Best: Epoch 14 \t loss=0.01616 \t val_loss=0.01690 \t sm_loss=0.01632 \t sm_val_loss=0.01693 \t time=1.01s\n",
      "Best: Epoch 15 \t loss=0.01579 \t val_loss=0.01685 \t sm_loss=0.01597 \t sm_val_loss=0.01691 \t time=1.02s\n",
      "Fold 2 log loss: 0.017040519526601745\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.41371 \t val_loss=0.02243 \t sm_loss=0.41366 \t sm_val_loss=0.02241 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02010 \t val_loss=0.02038 \t sm_loss=0.02006 \t sm_val_loss=0.02018 \t time=0.91s\n",
      "Best: Epoch 3 \t loss=0.01820 \t val_loss=0.01824 \t sm_loss=0.01821 \t sm_val_loss=0.01830 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01733 \t val_loss=0.01821 \t sm_loss=0.01745 \t sm_val_loss=0.01821 \t time=0.98s\n",
      "Best: Epoch 7 \t loss=0.01712 \t val_loss=0.01818 \t sm_loss=0.01725 \t sm_val_loss=0.01814 \t time=0.90s\n",
      "Best: Epoch 10 \t loss=0.01700 \t val_loss=0.01810 \t sm_loss=0.01713 \t sm_val_loss=0.01807 \t time=0.88s\n",
      "Best: Epoch 11 \t loss=0.01680 \t val_loss=0.01807 \t sm_loss=0.01694 \t sm_val_loss=0.01806 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01657 \t val_loss=0.01790 \t sm_loss=0.01672 \t sm_val_loss=0.01788 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01610 \t val_loss=0.01764 \t sm_loss=0.01626 \t sm_val_loss=0.01764 \t time=0.93s\n",
      "Best: Epoch 16 \t loss=0.01536 \t val_loss=0.01747 \t sm_loss=0.01555 \t sm_val_loss=0.01747 \t time=1.13s\n",
      "Fold 3 log loss: 0.01743191730516925\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.41365 \t val_loss=0.02274 \t sm_loss=0.41361 \t sm_val_loss=0.02270 \t time=0.94s\n",
      "Best: Epoch 2 \t loss=0.01983 \t val_loss=0.02004 \t sm_loss=0.01982 \t sm_val_loss=0.01973 \t time=0.91s\n",
      "Best: Epoch 3 \t loss=0.01789 \t val_loss=0.01976 \t sm_loss=0.01795 \t sm_val_loss=0.01963 \t time=0.91s\n",
      "Best: Epoch 4 \t loss=0.01735 \t val_loss=0.01877 \t sm_loss=0.01746 \t sm_val_loss=0.01869 \t time=0.91s\n",
      "Best: Epoch 6 \t loss=0.01717 \t val_loss=0.01825 \t sm_loss=0.01730 \t sm_val_loss=0.01824 \t time=1.13s\n",
      "Best: Epoch 10 \t loss=0.01688 \t val_loss=0.01811 \t sm_loss=0.01702 \t sm_val_loss=0.01807 \t time=0.92s\n",
      "Best: Epoch 12 \t loss=0.01653 \t val_loss=0.01802 \t sm_loss=0.01668 \t sm_val_loss=0.01802 \t time=0.93s\n",
      "Best: Epoch 14 \t loss=0.01597 \t val_loss=0.01793 \t sm_loss=0.01615 \t sm_val_loss=0.01793 \t time=0.91s\n",
      "Best: Epoch 15 \t loss=0.01567 \t val_loss=0.01776 \t sm_loss=0.01585 \t sm_val_loss=0.01777 \t time=0.90s\n",
      "Best: Epoch 17 \t loss=0.01476 \t val_loss=0.01777 \t sm_loss=0.01497 \t sm_val_loss=0.01775 \t time=1.05s\n",
      "Best: Epoch 18 \t loss=0.01422 \t val_loss=0.01773 \t sm_loss=0.01445 \t sm_val_loss=0.01771 \t time=1.03s\n",
      "Fold 4 log loss: 0.017804498035552643\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.41518 \t val_loss=0.02250 \t sm_loss=0.41514 \t sm_val_loss=0.02250 \t time=0.89s\n",
      "Best: Epoch 2 \t loss=0.02013 \t val_loss=0.01990 \t sm_loss=0.02008 \t sm_val_loss=0.01969 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01795 \t val_loss=0.01827 \t sm_loss=0.01800 \t sm_val_loss=0.01827 \t time=1.31s\n",
      "Best: Epoch 4 \t loss=0.01737 \t val_loss=0.01819 \t sm_loss=0.01749 \t sm_val_loss=0.01819 \t time=1.04s\n",
      "Best: Epoch 6 \t loss=0.01726 \t val_loss=0.01813 \t sm_loss=0.01739 \t sm_val_loss=0.01808 \t time=1.11s\n",
      "Best: Epoch 7 \t loss=0.01724 \t val_loss=0.01792 \t sm_loss=0.01736 \t sm_val_loss=0.01789 \t time=1.44s\n",
      "Best: Epoch 8 \t loss=0.01716 \t val_loss=0.01771 \t sm_loss=0.01729 \t sm_val_loss=0.01770 \t time=0.98s\n",
      "Best: Epoch 10 \t loss=0.01690 \t val_loss=0.01745 \t sm_loss=0.01703 \t sm_val_loss=0.01747 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01661 \t val_loss=0.01742 \t sm_loss=0.01676 \t sm_val_loss=0.01744 \t time=0.98s\n",
      "Best: Epoch 13 \t loss=0.01634 \t val_loss=0.01729 \t sm_loss=0.01650 \t sm_val_loss=0.01731 \t time=0.91s\n",
      "Best: Epoch 15 \t loss=0.01568 \t val_loss=0.01723 \t sm_loss=0.01586 \t sm_val_loss=0.01722 \t time=0.89s\n",
      "Best: Epoch 16 \t loss=0.01528 \t val_loss=0.01715 \t sm_loss=0.01548 \t sm_val_loss=0.01716 \t time=0.90s\n",
      "Best: Epoch 17 \t loss=0.01479 \t val_loss=0.01704 \t sm_loss=0.01500 \t sm_val_loss=0.01704 \t time=1.09s\n",
      "Best: Epoch 19 \t loss=0.01387 \t val_loss=0.01700 \t sm_loss=0.01410 \t sm_val_loss=0.01699 \t time=0.89s\n",
      "Fold 5 log loss: 0.017188675022130492\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.41172 \t val_loss=0.02253 \t sm_loss=0.41163 \t sm_val_loss=0.02251 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01903 \t val_loss=0.01847 \t sm_loss=0.01893 \t sm_val_loss=0.01855 \t time=0.90s\n",
      "Best: Epoch 4 \t loss=0.01766 \t val_loss=0.01810 \t sm_loss=0.01777 \t sm_val_loss=0.01818 \t time=0.91s\n",
      "Best: Epoch 6 \t loss=0.01739 \t val_loss=0.01808 \t sm_loss=0.01751 \t sm_val_loss=0.01816 \t time=0.89s\n",
      "Best: Epoch 7 \t loss=0.01740 \t val_loss=0.01797 \t sm_loss=0.01751 \t sm_val_loss=0.01805 \t time=1.19s\n",
      "Best: Epoch 9 \t loss=0.01717 \t val_loss=0.01782 \t sm_loss=0.01729 \t sm_val_loss=0.01790 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.01697 \t val_loss=0.01776 \t sm_loss=0.01710 \t sm_val_loss=0.01783 \t time=0.88s\n",
      "Best: Epoch 12 \t loss=0.01679 \t val_loss=0.01758 \t sm_loss=0.01693 \t sm_val_loss=0.01765 \t time=0.88s\n",
      "Best: Epoch 14 \t loss=0.01632 \t val_loss=0.01748 \t sm_loss=0.01647 \t sm_val_loss=0.01757 \t time=0.91s\n",
      "Best: Epoch 15 \t loss=0.01582 \t val_loss=0.01744 \t sm_loss=0.01599 \t sm_val_loss=0.01752 \t time=1.03s\n",
      "Best: Epoch 16 \t loss=0.01550 \t val_loss=0.01738 \t sm_loss=0.01569 \t sm_val_loss=0.01746 \t time=1.08s\n",
      "Best: Epoch 17 \t loss=0.01500 \t val_loss=0.01725 \t sm_loss=0.01520 \t sm_val_loss=0.01733 \t time=0.90s\n",
      "Best: Epoch 19 \t loss=0.01410 \t val_loss=0.01725 \t sm_loss=0.01431 \t sm_val_loss=0.01733 \t time=1.10s\n",
      "Fold 6 log loss: 0.0173701950715657\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.41617 \t val_loss=0.02282 \t sm_loss=0.41611 \t sm_val_loss=0.02274 \t time=0.88s\n",
      "Best: Epoch 3 \t loss=0.01836 \t val_loss=0.01819 \t sm_loss=0.01834 \t sm_val_loss=0.01824 \t time=0.88s\n",
      "Best: Epoch 5 \t loss=0.01731 \t val_loss=0.01805 \t sm_loss=0.01742 \t sm_val_loss=0.01812 \t time=0.88s\n",
      "Best: Epoch 6 \t loss=0.01726 \t val_loss=0.01796 \t sm_loss=0.01739 \t sm_val_loss=0.01802 \t time=0.87s\n",
      "Best: Epoch 7 \t loss=0.01726 \t val_loss=0.01762 \t sm_loss=0.01739 \t sm_val_loss=0.01767 \t time=0.89s\n",
      "Best: Epoch 11 \t loss=0.01689 \t val_loss=0.01756 \t sm_loss=0.01703 \t sm_val_loss=0.01762 \t time=0.88s\n",
      "Best: Epoch 12 \t loss=0.01666 \t val_loss=0.01743 \t sm_loss=0.01680 \t sm_val_loss=0.01750 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01652 \t val_loss=0.01726 \t sm_loss=0.01666 \t sm_val_loss=0.01734 \t time=0.88s\n",
      "Best: Epoch 14 \t loss=0.01617 \t val_loss=0.01706 \t sm_loss=0.01633 \t sm_val_loss=0.01714 \t time=0.87s\n",
      "Best: Epoch 16 \t loss=0.01540 \t val_loss=0.01695 \t sm_loss=0.01558 \t sm_val_loss=0.01701 \t time=0.88s\n",
      "Best: Epoch 19 \t loss=0.01399 \t val_loss=0.01688 \t sm_loss=0.01421 \t sm_val_loss=0.01695 \t time=0.95s\n",
      "Fold 7 log loss: 0.01697732237428382\n",
      "Seed 3\n",
      "Fold 1 log loss: 0.01717298648313837\n",
      "Fold 2 log loss: 0.017040519526601745\n",
      "Fold 3 log loss: 0.01743191730516925\n",
      "Fold 4 log loss: 0.017804498035552643\n",
      "Fold 5 log loss: 0.017188675022130492\n",
      "Fold 6 log loss: 0.0173701950715657\n",
      "Fold 7 log loss: 0.01697732237428382\n",
      "Std of log loss: 0.0002604862419393355\n",
      "Total log loss: 0.017283274632953167\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.41448 \t val_loss=0.02249 \t sm_loss=0.41442 \t sm_val_loss=0.02246 \t time=0.92s\n",
      "Best: Epoch 2 \t loss=0.01995 \t val_loss=0.01883 \t sm_loss=0.01992 \t sm_val_loss=0.01884 \t time=1.02s\n",
      "Best: Epoch 3 \t loss=0.01809 \t val_loss=0.01814 \t sm_loss=0.01811 \t sm_val_loss=0.01812 \t time=1.17s\n",
      "Best: Epoch 5 \t loss=0.01731 \t val_loss=0.01808 \t sm_loss=0.01743 \t sm_val_loss=0.01809 \t time=0.96s\n",
      "Best: Epoch 7 \t loss=0.01723 \t val_loss=0.01776 \t sm_loss=0.01736 \t sm_val_loss=0.01777 \t time=0.95s\n",
      "Best: Epoch 10 \t loss=0.01699 \t val_loss=0.01771 \t sm_loss=0.01713 \t sm_val_loss=0.01775 \t time=0.90s\n",
      "Best: Epoch 12 \t loss=0.01660 \t val_loss=0.01750 \t sm_loss=0.01675 \t sm_val_loss=0.01752 \t time=0.88s\n",
      "Best: Epoch 13 \t loss=0.01645 \t val_loss=0.01743 \t sm_loss=0.01660 \t sm_val_loss=0.01743 \t time=0.91s\n",
      "Best: Epoch 14 \t loss=0.01617 \t val_loss=0.01733 \t sm_loss=0.01633 \t sm_val_loss=0.01735 \t time=0.90s\n",
      "Best: Epoch 15 \t loss=0.01581 \t val_loss=0.01716 \t sm_loss=0.01599 \t sm_val_loss=0.01717 \t time=0.88s\n",
      "Best: Epoch 17 \t loss=0.01488 \t val_loss=0.01707 \t sm_loss=0.01509 \t sm_val_loss=0.01709 \t time=0.87s\n",
      "Best: Epoch 18 \t loss=0.01442 \t val_loss=0.01707 \t sm_loss=0.01464 \t sm_val_loss=0.01708 \t time=0.89s\n",
      "Fold 1 log loss: 0.017178599907108048\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.41293 \t val_loss=0.02188 \t sm_loss=0.41289 \t sm_val_loss=0.02188 \t time=0.88s\n",
      "Best: Epoch 2 \t loss=0.02015 \t val_loss=0.01917 \t sm_loss=0.02011 \t sm_val_loss=0.01917 \t time=0.88s\n",
      "Best: Epoch 3 \t loss=0.01884 \t val_loss=0.01769 \t sm_loss=0.01879 \t sm_val_loss=0.01775 \t time=0.89s\n",
      "Best: Epoch 4 \t loss=0.01769 \t val_loss=0.01749 \t sm_loss=0.01780 \t sm_val_loss=0.01749 \t time=0.89s\n",
      "Best: Epoch 6 \t loss=0.01732 \t val_loss=0.01732 \t sm_loss=0.01745 \t sm_val_loss=0.01734 \t time=0.89s\n",
      "Best: Epoch 7 \t loss=0.01730 \t val_loss=0.01726 \t sm_loss=0.01743 \t sm_val_loss=0.01727 \t time=0.98s\n",
      "Best: Epoch 12 \t loss=0.01669 \t val_loss=0.01723 \t sm_loss=0.01684 \t sm_val_loss=0.01723 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01650 \t val_loss=0.01703 \t sm_loss=0.01664 \t sm_val_loss=0.01706 \t time=0.92s\n",
      "Best: Epoch 20 \t loss=0.01368 \t val_loss=0.01697 \t sm_loss=0.01391 \t sm_val_loss=0.01698 \t time=0.88s\n",
      "Fold 2 log loss: 0.017105581566924162\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.41534 \t val_loss=0.02300 \t sm_loss=0.41527 \t sm_val_loss=0.02298 \t time=0.89s\n",
      "Best: Epoch 2 \t loss=0.02001 \t val_loss=0.01902 \t sm_loss=0.01998 \t sm_val_loss=0.01906 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.01806 \t val_loss=0.01810 \t sm_loss=0.01811 \t sm_val_loss=0.01815 \t time=0.89s\n",
      "Best: Epoch 4 \t loss=0.01733 \t val_loss=0.01814 \t sm_loss=0.01745 \t sm_val_loss=0.01814 \t time=0.89s\n",
      "Best: Epoch 5 \t loss=0.01726 \t val_loss=0.01806 \t sm_loss=0.01739 \t sm_val_loss=0.01806 \t time=0.89s\n",
      "Best: Epoch 9 \t loss=0.01707 \t val_loss=0.01798 \t sm_loss=0.01720 \t sm_val_loss=0.01801 \t time=0.88s\n",
      "Best: Epoch 12 \t loss=0.01658 \t val_loss=0.01796 \t sm_loss=0.01673 \t sm_val_loss=0.01798 \t time=1.00s\n",
      "Best: Epoch 13 \t loss=0.01637 \t val_loss=0.01783 \t sm_loss=0.01653 \t sm_val_loss=0.01781 \t time=0.94s\n",
      "Best: Epoch 14 \t loss=0.01602 \t val_loss=0.01768 \t sm_loss=0.01619 \t sm_val_loss=0.01768 \t time=0.94s\n",
      "Best: Epoch 16 \t loss=0.01524 \t val_loss=0.01765 \t sm_loss=0.01543 \t sm_val_loss=0.01765 \t time=0.89s\n",
      "Best: Epoch 18 \t loss=0.01420 \t val_loss=0.01757 \t sm_loss=0.01441 \t sm_val_loss=0.01754 \t time=0.92s\n",
      "Best: Epoch 19 \t loss=0.01379 \t val_loss=0.01756 \t sm_loss=0.01402 \t sm_val_loss=0.01754 \t time=1.00s\n",
      "Fold 3 log loss: 0.01749427825215393\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.41369 \t val_loss=0.02315 \t sm_loss=0.41364 \t sm_val_loss=0.02310 \t time=1.12s\n",
      "Best: Epoch 2 \t loss=0.01986 \t val_loss=0.02082 \t sm_loss=0.01985 \t sm_val_loss=0.02006 \t time=0.99s\n",
      "Best: Epoch 3 \t loss=0.01797 \t val_loss=0.01876 \t sm_loss=0.01803 \t sm_val_loss=0.01861 \t time=0.95s\n",
      "Best: Epoch 4 \t loss=0.01730 \t val_loss=0.01860 \t sm_loss=0.01743 \t sm_val_loss=0.01846 \t time=0.94s\n",
      "Best: Epoch 5 \t loss=0.01725 \t val_loss=0.01838 \t sm_loss=0.01738 \t sm_val_loss=0.01838 \t time=1.30s\n",
      "Best: Epoch 6 \t loss=0.01719 \t val_loss=0.01838 \t sm_loss=0.01732 \t sm_val_loss=0.01832 \t time=0.90s\n",
      "Best: Epoch 7 \t loss=0.01711 \t val_loss=0.01807 \t sm_loss=0.01724 \t sm_val_loss=0.01804 \t time=0.89s\n",
      "Best: Epoch 10 \t loss=0.01687 \t val_loss=0.01805 \t sm_loss=0.01700 \t sm_val_loss=0.01803 \t time=0.88s\n",
      "Best: Epoch 11 \t loss=0.01671 \t val_loss=0.01797 \t sm_loss=0.01685 \t sm_val_loss=0.01797 \t time=1.05s\n",
      "Best: Epoch 14 \t loss=0.01596 \t val_loss=0.01794 \t sm_loss=0.01613 \t sm_val_loss=0.01796 \t time=0.89s\n",
      "Best: Epoch 15 \t loss=0.01562 \t val_loss=0.01769 \t sm_loss=0.01581 \t sm_val_loss=0.01770 \t time=0.89s\n",
      "Fold 4 log loss: 0.017796781393628846\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.41545 \t val_loss=0.02219 \t sm_loss=0.41540 \t sm_val_loss=0.02219 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02005 \t val_loss=0.01940 \t sm_loss=0.01999 \t sm_val_loss=0.01943 \t time=1.13s\n",
      "Best: Epoch 4 \t loss=0.01747 \t val_loss=0.01790 \t sm_loss=0.01756 \t sm_val_loss=0.01793 \t time=0.89s\n",
      "Best: Epoch 6 \t loss=0.01731 \t val_loss=0.01790 \t sm_loss=0.01744 \t sm_val_loss=0.01789 \t time=0.89s\n",
      "Best: Epoch 8 \t loss=0.01723 \t val_loss=0.01779 \t sm_loss=0.01736 \t sm_val_loss=0.01778 \t time=0.89s\n",
      "Best: Epoch 11 \t loss=0.01685 \t val_loss=0.01761 \t sm_loss=0.01699 \t sm_val_loss=0.01761 \t time=0.89s\n",
      "Best: Epoch 12 \t loss=0.01667 \t val_loss=0.01737 \t sm_loss=0.01681 \t sm_val_loss=0.01736 \t time=0.94s\n",
      "Best: Epoch 13 \t loss=0.01646 \t val_loss=0.01737 \t sm_loss=0.01662 \t sm_val_loss=0.01736 \t time=1.13s\n",
      "Best: Epoch 14 \t loss=0.01611 \t val_loss=0.01739 \t sm_loss=0.01628 \t sm_val_loss=0.01735 \t time=0.97s\n",
      "Best: Epoch 15 \t loss=0.01578 \t val_loss=0.01720 \t sm_loss=0.01597 \t sm_val_loss=0.01721 \t time=0.90s\n",
      "Best: Epoch 16 \t loss=0.01539 \t val_loss=0.01714 \t sm_loss=0.01558 \t sm_val_loss=0.01712 \t time=1.00s\n",
      "Best: Epoch 17 \t loss=0.01489 \t val_loss=0.01702 \t sm_loss=0.01510 \t sm_val_loss=0.01701 \t time=1.17s\n",
      "Best: Epoch 18 \t loss=0.01437 \t val_loss=0.01701 \t sm_loss=0.01459 \t sm_val_loss=0.01700 \t time=0.95s\n",
      "Fold 5 log loss: 0.017204310320810514\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.41407 \t val_loss=0.02242 \t sm_loss=0.41400 \t sm_val_loss=0.02240 \t time=0.94s\n",
      "Best: Epoch 2 \t loss=0.02023 \t val_loss=0.02051 \t sm_loss=0.02014 \t sm_val_loss=0.02051 \t time=1.09s\n",
      "Best: Epoch 3 \t loss=0.01825 \t val_loss=0.02074 \t sm_loss=0.01824 \t sm_val_loss=0.02011 \t time=1.03s\n",
      "Best: Epoch 4 \t loss=0.01762 \t val_loss=0.01812 \t sm_loss=0.01772 \t sm_val_loss=0.01819 \t time=0.95s\n",
      "Best: Epoch 7 \t loss=0.01726 \t val_loss=0.01807 \t sm_loss=0.01738 \t sm_val_loss=0.01813 \t time=0.90s\n",
      "Best: Epoch 8 \t loss=0.01730 \t val_loss=0.01792 \t sm_loss=0.01742 \t sm_val_loss=0.01797 \t time=0.91s\n",
      "Best: Epoch 9 \t loss=0.01713 \t val_loss=0.01789 \t sm_loss=0.01725 \t sm_val_loss=0.01794 \t time=0.91s\n",
      "Best: Epoch 10 \t loss=0.01703 \t val_loss=0.01767 \t sm_loss=0.01716 \t sm_val_loss=0.01774 \t time=0.91s\n",
      "Best: Epoch 11 \t loss=0.01693 \t val_loss=0.01753 \t sm_loss=0.01706 \t sm_val_loss=0.01762 \t time=0.91s\n",
      "Best: Epoch 13 \t loss=0.01646 \t val_loss=0.01751 \t sm_loss=0.01661 \t sm_val_loss=0.01761 \t time=0.90s\n",
      "Best: Epoch 14 \t loss=0.01620 \t val_loss=0.01748 \t sm_loss=0.01636 \t sm_val_loss=0.01756 \t time=1.70s\n",
      "Best: Epoch 15 \t loss=0.01582 \t val_loss=0.01736 \t sm_loss=0.01599 \t sm_val_loss=0.01745 \t time=0.92s\n",
      "Best: Epoch 16 \t loss=0.01544 \t val_loss=0.01726 \t sm_loss=0.01562 \t sm_val_loss=0.01735 \t time=1.05s\n",
      "Best: Epoch 19 \t loss=0.01418 \t val_loss=0.01722 \t sm_loss=0.01439 \t sm_val_loss=0.01731 \t time=1.12s\n",
      "Fold 6 log loss: 0.017340261576253676\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.41390 \t val_loss=0.02308 \t sm_loss=0.41386 \t sm_val_loss=0.02307 \t time=0.90s\n",
      "Best: Epoch 2 \t loss=0.02015 \t val_loss=0.01889 \t sm_loss=0.02009 \t sm_val_loss=0.01886 \t time=0.99s\n",
      "Best: Epoch 3 \t loss=0.01814 \t val_loss=0.01891 \t sm_loss=0.01816 \t sm_val_loss=0.01876 \t time=1.26s\n",
      "Best: Epoch 5 \t loss=0.01735 \t val_loss=0.01782 \t sm_loss=0.01747 \t sm_val_loss=0.01784 \t time=1.14s\n",
      "Best: Epoch 9 \t loss=0.01718 \t val_loss=0.01774 \t sm_loss=0.01730 \t sm_val_loss=0.01780 \t time=0.90s\n",
      "Best: Epoch 10 \t loss=0.01707 \t val_loss=0.01764 \t sm_loss=0.01720 \t sm_val_loss=0.01771 \t time=0.90s\n",
      "Best: Epoch 11 \t loss=0.01688 \t val_loss=0.01754 \t sm_loss=0.01701 \t sm_val_loss=0.01760 \t time=0.89s\n",
      "Best: Epoch 12 \t loss=0.01664 \t val_loss=0.01749 \t sm_loss=0.01678 \t sm_val_loss=0.01754 \t time=0.89s\n",
      "Best: Epoch 13 \t loss=0.01651 \t val_loss=0.01746 \t sm_loss=0.01666 \t sm_val_loss=0.01752 \t time=0.89s\n",
      "Best: Epoch 14 \t loss=0.01616 \t val_loss=0.01735 \t sm_loss=0.01633 \t sm_val_loss=0.01742 \t time=1.13s\n",
      "Best: Epoch 15 \t loss=0.01583 \t val_loss=0.01706 \t sm_loss=0.01601 \t sm_val_loss=0.01714 \t time=0.89s\n",
      "Best: Epoch 16 \t loss=0.01538 \t val_loss=0.01706 \t sm_loss=0.01557 \t sm_val_loss=0.01714 \t time=0.92s\n",
      "Best: Epoch 17 \t loss=0.01492 \t val_loss=0.01694 \t sm_loss=0.01512 \t sm_val_loss=0.01701 \t time=0.89s\n",
      "Best: Epoch 18 \t loss=0.01440 \t val_loss=0.01690 \t sm_loss=0.01462 \t sm_val_loss=0.01697 \t time=1.07s\n",
      "Best: Epoch 19 \t loss=0.01405 \t val_loss=0.01687 \t sm_loss=0.01427 \t sm_val_loss=0.01695 \t time=0.88s\n",
      "Fold 7 log loss: 0.016973093764633303\n",
      "Seed 4\n",
      "Fold 1 log loss: 0.017178599907108048\n",
      "Fold 2 log loss: 0.017105581566924162\n",
      "Fold 3 log loss: 0.01749427825215393\n",
      "Fold 4 log loss: 0.017796781393628846\n",
      "Fold 5 log loss: 0.017204310320810514\n",
      "Fold 6 log loss: 0.017340261576253676\n",
      "Fold 7 log loss: 0.016973093764633303\n",
      "Std of log loss: 0.00025486980156787296\n",
      "Total log loss: 0.01729849942740955\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "mlp1_oof = np.zeros([len(mlp_train),fn_targets.shape[1]])\n",
    "mlp1_test = np.zeros([len(mlp_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, pytorch_pred = modelling_torch(mlp_train, fn_targets, mlp_test, seed_, mlp_train.shape[1]-1, fn_targets.shape[1])\n",
    "    mlp1_oof += oof / len(seeds)\n",
    "    mlp1_test += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:51:18.210856Z",
     "iopub.status.busy": "2020-11-10T04:51:18.209876Z",
     "iopub.status.idle": "2020-11-10T04:51:19.482131Z",
     "shell.execute_reply": "2020-11-10T04:51:19.481603Z"
    },
    "papermill": {
     "duration": 1.487104,
     "end_time": "2020-11-10T04:51:19.482268",
     "exception": false,
     "start_time": "2020-11-10T04:51:17.995164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.01571810721478206\n"
     ]
    }
   ],
   "source": [
    "check_mlp = np.zeros([y.shape[0], y.shape[1]])\n",
    "check_mlp[cons_train_index,:] = mlp1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:51:20.100425Z",
     "iopub.status.busy": "2020-11-10T04:51:20.099281Z",
     "iopub.status.idle": "2020-11-10T04:51:21.447602Z",
     "shell.execute_reply": "2020-11-10T04:51:21.448581Z"
    },
    "papermill": {
     "duration": 1.695002,
     "end_time": "2020-11-10T04:51:21.448745",
     "exception": false,
     "start_time": "2020-11-10T04:51:19.753743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC : 0.6741695449519476\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "for task_id in range(y.shape[1]):\n",
    "    aucs.append(roc_auc_score(y_true=y.iloc[:, task_id].values,\n",
    "                              y_score=check_mlp[:, task_id]))\n",
    "print(f\"Overall AUC : {np.mean(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.20876,
     "end_time": "2020-11-10T04:51:21.867442",
     "exception": false,
     "start_time": "2020-11-10T04:51:21.658682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:51:22.303507Z",
     "iopub.status.busy": "2020-11-10T04:51:22.302391Z",
     "iopub.status.idle": "2020-11-10T04:51:22.305660Z",
     "shell.execute_reply": "2020-11-10T04:51:22.305047Z"
    },
    "papermill": {
     "duration": 0.222789,
     "end_time": "2020-11-10T04:51:22.305767",
     "exception": false,
     "start_time": "2020-11-10T04:51:22.082978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        \n",
    "        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:51:22.753353Z",
     "iopub.status.busy": "2020-11-10T04:51:22.747949Z",
     "iopub.status.idle": "2020-11-10T04:51:22.756000Z",
     "shell.execute_reply": "2020-11-10T04:51:22.755482Z"
    },
    "papermill": {
     "duration": 0.240226,
     "end_time": "2020-11-10T04:51:22.756109",
     "exception": false,
     "start_time": "2020-11-10T04:51:22.515883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH=200\n",
    "\n",
    "def seed_tabnet_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "def modelling_tabnet(tr, target, te, sample_seed):\n",
    "    seed_tabnet_everything(sample_seed) \n",
    "    tabnet_params = dict(n_d=12, n_a=12, n_steps=1, gamma=1.3, seed = sample_seed,\n",
    "                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n",
    "                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "                     mask_type='entmax',\n",
    "                     scheduler_params=dict(mode=\"min\",\n",
    "                                           patience=5,\n",
    "                                           min_lr=1e-5,\n",
    "                                           factor=0.9,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                     verbose=10,\n",
    "                     )\n",
    "    test_cv_preds = []\n",
    "    \n",
    "    NB_SPLITS = 5\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n",
    "    oof_preds = np.zeros([len(tr),target.shape[1]])\n",
    "    for fold_nb in range(NB_SPLITS):\n",
    "        print(\"FOLDS : \", fold_nb+1)\n",
    "        \n",
    "        ## model\n",
    "        val_idx = tr[:,-1] == fold_nb\n",
    "        train_idx = tr[:,-1] != fold_nb\n",
    "        X_train, y_train = tr[train_idx, :], target[train_idx, :]\n",
    "        X_val, y_val = tr[val_idx, :], target[val_idx, :]\n",
    "        X_train = np.delete(X_train, -1, 1)\n",
    "        X_val = np.delete(X_val, -1, 1)\n",
    "        \n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "        model.fit(X_train=X_train,\n",
    "              y_train=y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_name = [\"val\"],\n",
    "              eval_metric = [\"logits_ll\"],\n",
    "              max_epochs=MAX_EPOCH,\n",
    "              patience=20, batch_size=1024, virtual_batch_size=128,\n",
    "              num_workers=1, drop_last=False,\n",
    "              # use binary cross entropy as this is not a regression problem\n",
    "              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "        \n",
    "        preds_val = model.predict(X_val)\n",
    "        preds =  1 / (1 + np.exp(-preds_val))\n",
    "        oof_preds[val_idx,:] = preds\n",
    "        \n",
    "        # preds on test\n",
    "        preds_test = model.predict(te)\n",
    "        test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n",
    "\n",
    "    test_preds_all = np.stack(test_cv_preds)\n",
    "    return oof_preds, test_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:51:23.183400Z",
     "iopub.status.busy": "2020-11-10T04:51:23.180883Z",
     "iopub.status.idle": "2020-11-10T04:59:48.120027Z",
     "shell.execute_reply": "2020-11-10T04:59:48.119485Z"
    },
    "papermill": {
     "duration": 505.156939,
     "end_time": "2020-11-10T04:59:48.120156",
     "exception": false,
     "start_time": "2020-11-10T04:51:22.963217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLDS :  1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.56822 | val_logits_ll: 0.32031 |  0:00:01s\n",
      "epoch 10 | loss: 0.02073 | val_logits_ll: 0.02057 |  0:00:12s\n",
      "epoch 20 | loss: 0.01911 | val_logits_ll: 0.01923 |  0:00:22s\n",
      "epoch 30 | loss: 0.01795 | val_logits_ll: 0.01885 |  0:00:34s\n",
      "epoch 40 | loss: 0.01726 | val_logits_ll: 0.01974 |  0:00:45s\n",
      "epoch 50 | loss: 0.01696 | val_logits_ll: 0.01791 |  0:00:55s\n",
      "epoch 60 | loss: 0.01648 | val_logits_ll: 0.01779 |  0:01:05s\n",
      "epoch 70 | loss: 0.01624 | val_logits_ll: 0.01918 |  0:01:16s\n",
      "epoch 80 | loss: 0.01608 | val_logits_ll: 0.01816 |  0:01:27s\n",
      "epoch 90 | loss: 0.01591 | val_logits_ll: 0.01762 |  0:01:38s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 72 and best_val_logits_ll = 0.01751\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.56335 | val_logits_ll: 0.30359 |  0:00:01s\n",
      "epoch 10 | loss: 0.02052 | val_logits_ll: 0.02025 |  0:00:12s\n",
      "epoch 20 | loss: 0.01904 | val_logits_ll: 0.01894 |  0:00:22s\n",
      "epoch 30 | loss: 0.01772 | val_logits_ll: 0.01812 |  0:00:32s\n",
      "epoch 40 | loss: 0.01701 | val_logits_ll: 0.01985 |  0:00:44s\n",
      "epoch 50 | loss: 0.01656 | val_logits_ll: 0.02032 |  0:00:55s\n",
      "epoch 60 | loss: 0.01603 | val_logits_ll: 0.01783 |  0:01:05s\n",
      "epoch 70 | loss: 0.01587 | val_logits_ll: 0.01754 |  0:01:17s\n",
      "epoch 80 | loss: 0.01563 | val_logits_ll: 0.01765 |  0:01:27s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 69 and best_val_logits_ll = 0.01752\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.5625  | val_logits_ll: 0.2968  |  0:00:01s\n",
      "epoch 10 | loss: 0.02049 | val_logits_ll: 0.02139 |  0:00:13s\n",
      "epoch 20 | loss: 0.01836 | val_logits_ll: 0.01972 |  0:00:23s\n",
      "epoch 30 | loss: 0.01722 | val_logits_ll: 0.01938 |  0:00:34s\n",
      "epoch 40 | loss: 0.01676 | val_logits_ll: 0.02025 |  0:00:45s\n",
      "epoch 50 | loss: 0.01643 | val_logits_ll: 0.01891 |  0:00:55s\n",
      "epoch 60 | loss: 0.01633 | val_logits_ll: 0.02013 |  0:01:07s\n",
      "epoch 70 | loss: 0.01582 | val_logits_ll: 0.01848 |  0:01:18s\n",
      "epoch 80 | loss: 0.01553 | val_logits_ll: 0.02074 |  0:01:29s\n",
      "epoch 90 | loss: 0.01515 | val_logits_ll: 0.01862 |  0:01:40s\n",
      "\n",
      "Early stopping occured at epoch 90 with best_epoch = 70 and best_val_logits_ll = 0.01848\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.57363 | val_logits_ll: 0.31949 |  0:00:01s\n",
      "epoch 10 | loss: 0.02078 | val_logits_ll: 0.02061 |  0:00:11s\n",
      "epoch 20 | loss: 0.0188  | val_logits_ll: 0.01908 |  0:00:22s\n",
      "epoch 30 | loss: 0.01781 | val_logits_ll: 0.01827 |  0:00:33s\n",
      "epoch 40 | loss: 0.01703 | val_logits_ll: 0.01794 |  0:00:44s\n",
      "epoch 50 | loss: 0.01658 | val_logits_ll: 0.01932 |  0:00:54s\n",
      "epoch 60 | loss: 0.0163  | val_logits_ll: 0.01771 |  0:01:05s\n",
      "epoch 70 | loss: 0.01585 | val_logits_ll: 0.01757 |  0:01:15s\n",
      "epoch 80 | loss: 0.01553 | val_logits_ll: 0.0177  |  0:01:27s\n",
      "\n",
      "Early stopping occured at epoch 85 with best_epoch = 65 and best_val_logits_ll = 0.01756\n",
      "Best weights from best epoch are automatically used!\n",
      "FOLDS :  5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.57439 | val_logits_ll: 0.28826 |  0:00:00s\n",
      "epoch 10 | loss: 0.02083 | val_logits_ll: 0.0208  |  0:00:11s\n",
      "epoch 20 | loss: 0.01893 | val_logits_ll: 0.01969 |  0:00:21s\n",
      "epoch 30 | loss: 0.01752 | val_logits_ll: 0.01812 |  0:00:32s\n",
      "epoch 40 | loss: 0.01707 | val_logits_ll: 0.0178  |  0:00:43s\n",
      "epoch 50 | loss: 0.01662 | val_logits_ll: 0.01901 |  0:00:54s\n",
      "epoch 60 | loss: 0.0164  | val_logits_ll: 0.01769 |  0:01:05s\n",
      "epoch 70 | loss: 0.01633 | val_logits_ll: 0.01776 |  0:01:16s\n",
      "epoch 80 | loss: 0.016   | val_logits_ll: 0.01768 |  0:01:27s\n",
      "epoch 90 | loss: 0.01573 | val_logits_ll: 0.01754 |  0:01:37s\n",
      "\n",
      "Early stopping occured at epoch 99 with best_epoch = 79 and best_val_logits_ll = 0.01747\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "tabnet1_oof = np.zeros([len(tab_train),fn_targets.shape[1]])\n",
    "tabnet1_test = np.zeros([len(tab_test),fn_targets.shape[1]])\n",
    "seeds = [0]\n",
    "for seed_ in seeds:\n",
    "    oof_preds, test_preds_all = modelling_tabnet(tab_train, fn_targets, tab_test, seed_)\n",
    "    tabnet1_oof += oof_preds / len(seeds)\n",
    "    tabnet1_test += test_preds_all.mean(axis=0) / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:59:48.609274Z",
     "iopub.status.busy": "2020-11-10T04:59:48.608218Z",
     "iopub.status.idle": "2020-11-10T04:59:49.931698Z",
     "shell.execute_reply": "2020-11-10T04:59:49.930759Z"
    },
    "papermill": {
     "duration": 1.570261,
     "end_time": "2020-11-10T04:59:49.931813",
     "exception": false,
     "start_time": "2020-11-10T04:59:48.361552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016317946358678424\n"
     ]
    }
   ],
   "source": [
    "check_tabnet = np.zeros([y.shape[0], y.shape[1]])\n",
    "check_tabnet[cons_train_index,:] = tabnet1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_tabnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.232466,
     "end_time": "2020-11-10T04:59:50.407174",
     "exception": false,
     "start_time": "2020-11-10T04:59:50.174708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:59:50.880917Z",
     "iopub.status.busy": "2020-11-10T04:59:50.879024Z",
     "iopub.status.idle": "2020-11-10T04:59:50.881575Z",
     "shell.execute_reply": "2020-11-10T04:59:50.882046Z"
    },
    "papermill": {
     "duration": 0.247469,
     "end_time": "2020-11-10T04:59:50.882170",
     "exception": false,
     "start_time": "2020-11-10T04:59:50.634701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, c_lstm_hidden_size, last_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.g_layer_num = 1\n",
    "        self.c_layer_num = 1\n",
    "\n",
    "        self.hidden_dim = 512\n",
    "        self.hidden_dim_c = 10\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_hidden_size, self.hidden_dim, batch_first=True, bidirectional=True, num_layers=self.g_layer_num)\n",
    "        self.c_lstm = nn.LSTM(c_lstm_hidden_size, self.hidden_dim_c, batch_first=True, bidirectional=True, num_layers=self.c_layer_num)\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d((self.hidden_dim+self.hidden_dim_c) * 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.utils.weight_norm(nn.Linear((self.hidden_dim+self.hidden_dim_c) * 2, last_num))\n",
    "        \n",
    "    def forward(self, cont_g, cont_c): \n",
    "        cont_g = torch.unsqueeze(cont_g, 1)\n",
    "        h_lstm, lstm_out = self.lstm(cont_g) # h_lstm: 256 * 1 * (2 * 512)\n",
    "        conc_g = h_lstm.view(-1, self.hidden_dim * 2)\n",
    "        \n",
    "        cont_c = torch.unsqueeze(cont_c, 1)\n",
    "        h_lstm_c, lstm_out_c = self.c_lstm(cont_c) # h_lstm: 256 * 1 * (2 * 5)\n",
    "        conc_c = h_lstm_c.view(-1, self.hidden_dim_c * 2)\n",
    "        \n",
    "        conc = torch.cat((conc_g, conc_c),1)\n",
    "        conc = self.batch_norm(conc)\n",
    "        dropped = self.dropout(conc)\n",
    "        out = self.out(dropped)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:59:51.457968Z",
     "iopub.status.busy": "2020-11-10T04:59:51.456872Z",
     "iopub.status.idle": "2020-11-10T04:59:51.536850Z",
     "shell.execute_reply": "2020-11-10T04:59:51.535963Z"
    },
    "papermill": {
     "duration": 0.424836,
     "end_time": "2020-11-10T04:59:51.537008",
     "exception": false,
     "start_time": "2020-11-10T04:59:51.112172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_epochs = 30\n",
    "n_folds=7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "smoothing = 0.001\n",
    "p_min = smoothing\n",
    "p_max = 1 - smoothing\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_lstm_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def modelling_lstm(tr, target, te, sample_seed, last_num):\n",
    "    \n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    metric = lambda inputs, targets : F.binary_cross_entropy((torch.clamp(torch.sigmoid(inputs), p_min, p_max)), targets)\n",
    "    \n",
    "    seed_lstm_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    X_test_g = torch.tensor(X_test[:,:len(modg_feats)], dtype=torch.float32)\n",
    "    X_test_c = torch.tensor(X_test[:,len(modg_feats):], dtype=torch.float32)\n",
    "    \n",
    "    X_test = torch.utils.data.TensorDataset(X_test_g, X_test_c) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "            \n",
    "    for fold in range(n_folds):\n",
    "        valid_index = X_train[:,-1] == fold\n",
    "        train_index = X_train[:,-1] != fold        \n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2_g = torch.tensor(X_train[train_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_valid2_g = torch.tensor(X_train[valid_index,:len(modg_feats)], dtype=torch.float32)\n",
    "        X_train2_c = torch.tensor(X_train[train_index,len(modg_feats):-1], dtype=torch.float32)\n",
    "        X_valid2_c = torch.tensor(X_train[valid_index,len(modg_feats):-1], dtype=torch.float32)\n",
    "        \n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2_g, X_train2_c, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2_g, X_valid2_c, y_valid2)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "        clf = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.01, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            sm_avg_loss = 0.\n",
    "            for x_batch_g, x_batch_c, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)  \n",
    "                sm_avg_loss += metric(y_pred, y_batch) / len(train_loader) \n",
    "                \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            sm_avg_val_loss = 0.\n",
    "            for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                x_batch_g = x_batch_g.to(device)\n",
    "                x_batch_c = x_batch_c.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch_g, x_batch_c).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                sm_avg_val_loss += metric(y_pred, y_batch) / len(valid_loader)\n",
    "                \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "                    \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best: Epoch {} \\t loss={:.6f}  val_loss={:.6f}  sm_loss={:.6f} \\t sm_val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, sm_avg_loss, sm_avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "                \n",
    "            else:\n",
    "                stop_counts += 1\n",
    "        \n",
    "        pred_model = myLSTM(len(modg_feats), len(modc_feats), last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2_g.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch_g, x_batch_c, ) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch_g, x_batch_c).sigmoid().detach() #\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy() #torch.clamp(torch.sigmoid(y_pred.cpu()), p_min, p_max) #\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "\n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T04:59:52.174281Z",
     "iopub.status.busy": "2020-11-10T04:59:52.173119Z",
     "iopub.status.idle": "2020-11-10T05:09:35.545822Z",
     "shell.execute_reply": "2020-11-10T05:09:35.546437Z"
    },
    "papermill": {
     "duration": 583.661683,
     "end_time": "2020-11-10T05:09:35.546607",
     "exception": false,
     "start_time": "2020-11-10T04:59:51.884924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.137526  val_loss=0.019500  sm_loss=0.137526 \t sm_val_loss=0.019504 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018288  val_loss=0.018319  sm_loss=0.018360 \t sm_val_loss=0.018318 \t time=0.81s\n",
      "Best: Epoch 3 \t loss=0.017419  val_loss=0.018310  sm_loss=0.017526 \t sm_val_loss=0.018330 \t time=0.81s\n",
      "Best: Epoch 4 \t loss=0.016927  val_loss=0.018038  sm_loss=0.017074 \t sm_val_loss=0.018021 \t time=0.79s\n",
      "Best: Epoch 5 \t loss=0.016650  val_loss=0.017852  sm_loss=0.016814 \t sm_val_loss=0.017839 \t time=0.78s\n",
      "Best: Epoch 6 \t loss=0.016483  val_loss=0.017601  sm_loss=0.016657 \t sm_val_loss=0.017597 \t time=1.08s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014684  val_loss=0.017153  sm_loss=0.014899 \t sm_val_loss=0.017178 \t time=0.75s\n",
      "Best: Epoch 12 \t loss=0.013401  val_loss=0.017040  sm_loss=0.013635 \t sm_val_loss=0.017061 \t time=0.77s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.017353367433795477\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.138274  val_loss=0.019421  sm_loss=0.138267 \t sm_val_loss=0.019460 \t time=0.78s\n",
      "Best: Epoch 2 \t loss=0.018561  val_loss=0.017947  sm_loss=0.018619 \t sm_val_loss=0.017958 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017468  val_loss=0.017624  sm_loss=0.017581 \t sm_val_loss=0.017627 \t time=0.77s\n",
      "Best: Epoch 4 \t loss=0.016986  val_loss=0.017604  sm_loss=0.017136 \t sm_val_loss=0.017641 \t time=0.76s\n",
      "Best: Epoch 6 \t loss=0.016445  val_loss=0.017544  sm_loss=0.016622 \t sm_val_loss=0.017547 \t time=0.76s\n",
      "Best: Epoch 7 \t loss=0.016377  val_loss=0.017443  sm_loss=0.016553 \t sm_val_loss=0.017470 \t time=0.80s\n",
      "Best: Epoch 9 \t loss=0.016353  val_loss=0.017440  sm_loss=0.016530 \t sm_val_loss=0.017456 \t time=0.76s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014612  val_loss=0.016895  sm_loss=0.014828 \t sm_val_loss=0.016929 \t time=0.76s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.01720011404080099\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.135363  val_loss=0.020064  sm_loss=0.135359 \t sm_val_loss=0.020089 \t time=0.94s\n",
      "Best: Epoch 2 \t loss=0.018325  val_loss=0.018817  sm_loss=0.018395 \t sm_val_loss=0.018855 \t time=1.11s\n",
      "Best: Epoch 3 \t loss=0.017423  val_loss=0.018640  sm_loss=0.017549 \t sm_val_loss=0.018676 \t time=0.76s\n",
      "Best: Epoch 4 \t loss=0.016884  val_loss=0.018491  sm_loss=0.017035 \t sm_val_loss=0.018472 \t time=0.81s\n",
      "Best: Epoch 6 \t loss=0.016418  val_loss=0.018278  sm_loss=0.016593 \t sm_val_loss=0.018278 \t time=0.75s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014419  val_loss=0.017652  sm_loss=0.014637 \t sm_val_loss=0.017668 \t time=0.96s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017553573042820805\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.138023  val_loss=0.020075  sm_loss=0.138023 \t sm_val_loss=0.020028 \t time=1.03s\n",
      "Best: Epoch 2 \t loss=0.018279  val_loss=0.018826  sm_loss=0.018354 \t sm_val_loss=0.018840 \t time=0.78s\n",
      "Best: Epoch 3 \t loss=0.017293  val_loss=0.018616  sm_loss=0.017409 \t sm_val_loss=0.018552 \t time=0.77s\n",
      "Best: Epoch 4 \t loss=0.016842  val_loss=0.018284  sm_loss=0.016993 \t sm_val_loss=0.018301 \t time=0.76s\n",
      "Best: Epoch 7 \t loss=0.016211  val_loss=0.018220  sm_loss=0.016391 \t sm_val_loss=0.018212 \t time=0.78s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014395  val_loss=0.017627  sm_loss=0.014612 \t sm_val_loss=0.017629 \t time=0.78s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.017877905681925484\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.135228  val_loss=0.019851  sm_loss=0.135224 \t sm_val_loss=0.019856 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018224  val_loss=0.018887  sm_loss=0.018296 \t sm_val_loss=0.018854 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017471  val_loss=0.018545  sm_loss=0.017587 \t sm_val_loss=0.018519 \t time=0.99s\n",
      "Best: Epoch 4 \t loss=0.017176  val_loss=0.018503  sm_loss=0.017316 \t sm_val_loss=0.018501 \t time=0.86s\n",
      "Best: Epoch 8 \t loss=0.016473  val_loss=0.018438  sm_loss=0.016643 \t sm_val_loss=0.018362 \t time=1.19s\n",
      "Best: Epoch 9 \t loss=0.016440  val_loss=0.018314  sm_loss=0.016606 \t sm_val_loss=0.018273 \t time=0.76s\n",
      "Best: Epoch 13 \t loss=0.016546  val_loss=0.018308  sm_loss=0.016708 \t sm_val_loss=0.018253 \t time=1.01s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 18 \t loss=0.015115  val_loss=0.017551  sm_loss=0.015312 \t sm_val_loss=0.017543 \t time=0.78s\n",
      "Best: Epoch 19 \t loss=0.013821  val_loss=0.017550  sm_loss=0.014052 \t sm_val_loss=0.017522 \t time=0.79s\n",
      "Best: Epoch 20 \t loss=0.013122  val_loss=0.017499  sm_loss=0.013371 \t sm_val_loss=0.017469 \t time=0.97s\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.017447282845075567\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.143866  val_loss=0.020151  sm_loss=0.143862 \t sm_val_loss=0.020139 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018549  val_loss=0.018826  sm_loss=0.018614 \t sm_val_loss=0.018867 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017437  val_loss=0.018423  sm_loss=0.017547 \t sm_val_loss=0.018496 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016869  val_loss=0.018358  sm_loss=0.017018 \t sm_val_loss=0.018410 \t time=0.84s\n",
      "Best: Epoch 5 \t loss=0.016543  val_loss=0.018351  sm_loss=0.016709 \t sm_val_loss=0.018428 \t time=1.02s\n",
      "Best: Epoch 6 \t loss=0.016401  val_loss=0.018095  sm_loss=0.016580 \t sm_val_loss=0.018166 \t time=0.77s\n",
      "Best: Epoch 9 \t loss=0.016224  val_loss=0.017991  sm_loss=0.016404 \t sm_val_loss=0.018050 \t time=0.80s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014445  val_loss=0.017390  sm_loss=0.014664 \t sm_val_loss=0.017479 \t time=0.76s\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.017439711542328554\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.138757  val_loss=0.019830  sm_loss=0.138753 \t sm_val_loss=0.019814 \t time=0.75s\n",
      "Best: Epoch 2 \t loss=0.018441  val_loss=0.018637  sm_loss=0.018515 \t sm_val_loss=0.018627 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017506  val_loss=0.018546  sm_loss=0.017616 \t sm_val_loss=0.018568 \t time=0.76s\n",
      "Best: Epoch 4 \t loss=0.016939  val_loss=0.018220  sm_loss=0.017085 \t sm_val_loss=0.018254 \t time=0.77s\n",
      "Best: Epoch 5 \t loss=0.016685  val_loss=0.017881  sm_loss=0.016846 \t sm_val_loss=0.017940 \t time=0.82s\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 10 \t loss=0.014567  val_loss=0.017175  sm_loss=0.014782 \t sm_val_loss=0.017263 \t time=0.77s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.017213508552076644\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.017353367433795477\n",
      "Fold 2 log loss: 0.01720011404080099\n",
      "Fold 3 log loss: 0.017553573042820805\n",
      "Fold 4 log loss: 0.017877905681925484\n",
      "Fold 5 log loss: 0.017447282845075567\n",
      "Fold 6 log loss: 0.017439711542328554\n",
      "Fold 7 log loss: 0.017213508552076644\n",
      "Std of log loss: 0.0002141621323706742\n",
      "Total log loss: 0.017440809889536096\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.138404  val_loss=0.019408  sm_loss=0.138406 \t sm_val_loss=0.019416 \t time=0.91s\n",
      "Best: Epoch 2 \t loss=0.018296  val_loss=0.018250  sm_loss=0.018370 \t sm_val_loss=0.018254 \t time=0.89s\n",
      "Best: Epoch 3 \t loss=0.017380  val_loss=0.018047  sm_loss=0.017498 \t sm_val_loss=0.018060 \t time=0.76s\n",
      "Best: Epoch 4 \t loss=0.016892  val_loss=0.017918  sm_loss=0.017046 \t sm_val_loss=0.017946 \t time=0.77s\n",
      "Best: Epoch 6 \t loss=0.016462  val_loss=0.017873  sm_loss=0.016636 \t sm_val_loss=0.017830 \t time=0.76s\n",
      "Best: Epoch 7 \t loss=0.016322  val_loss=0.017869  sm_loss=0.016495 \t sm_val_loss=0.017829 \t time=0.75s\n",
      "Best: Epoch 8 \t loss=0.016293  val_loss=0.017745  sm_loss=0.016467 \t sm_val_loss=0.017714 \t time=0.76s\n",
      "Best: Epoch 11 \t loss=0.016373  val_loss=0.017714  sm_loss=0.016540 \t sm_val_loss=0.017721 \t time=1.05s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 16 \t loss=0.014660  val_loss=0.017171  sm_loss=0.014871 \t sm_val_loss=0.017198 \t time=0.92s\n",
      "Best: Epoch 17 \t loss=0.013381  val_loss=0.017056  sm_loss=0.013622 \t sm_val_loss=0.017082 \t time=0.81s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.017378951948478338\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.136256  val_loss=0.019252  sm_loss=0.136251 \t sm_val_loss=0.019273 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018405  val_loss=0.017947  sm_loss=0.018475 \t sm_val_loss=0.017985 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017468  val_loss=0.017598  sm_loss=0.017586 \t sm_val_loss=0.017645 \t time=0.76s\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 8 \t loss=0.014642  val_loss=0.016733  sm_loss=0.014852 \t sm_val_loss=0.016785 \t time=0.88s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.01705338700102671\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.140419  val_loss=0.020024  sm_loss=0.140416 \t sm_val_loss=0.020043 \t time=0.79s\n",
      "Best: Epoch 2 \t loss=0.018433  val_loss=0.019404  sm_loss=0.018501 \t sm_val_loss=0.019406 \t time=0.79s\n",
      "Best: Epoch 3 \t loss=0.017539  val_loss=0.018712  sm_loss=0.017638 \t sm_val_loss=0.018749 \t time=0.77s\n",
      "Best: Epoch 4 \t loss=0.016929  val_loss=0.018609  sm_loss=0.017083 \t sm_val_loss=0.018596 \t time=0.95s\n",
      "Best: Epoch 5 \t loss=0.016571  val_loss=0.018388  sm_loss=0.016743 \t sm_val_loss=0.018408 \t time=0.84s\n",
      "Best: Epoch 6 \t loss=0.016449  val_loss=0.018347  sm_loss=0.016625 \t sm_val_loss=0.018348 \t time=0.79s\n",
      "Best: Epoch 8 \t loss=0.016274  val_loss=0.018311  sm_loss=0.016451 \t sm_val_loss=0.018305 \t time=0.78s\n",
      "Best: Epoch 9 \t loss=0.016216  val_loss=0.018247  sm_loss=0.016393 \t sm_val_loss=0.018246 \t time=0.77s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014486  val_loss=0.017746  sm_loss=0.014693 \t sm_val_loss=0.017746 \t time=0.92s\n",
      "Best: Epoch 15 \t loss=0.013178  val_loss=0.017691  sm_loss=0.013418 \t sm_val_loss=0.017693 \t time=0.79s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017573691558414973\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.137966  val_loss=0.020052  sm_loss=0.137964 \t sm_val_loss=0.020019 \t time=0.78s\n",
      "Best: Epoch 2 \t loss=0.018369  val_loss=0.018846  sm_loss=0.018442 \t sm_val_loss=0.018794 \t time=0.80s\n",
      "Best: Epoch 3 \t loss=0.017339  val_loss=0.018585  sm_loss=0.017460 \t sm_val_loss=0.018580 \t time=0.82s\n",
      "Best: Epoch 4 \t loss=0.016783  val_loss=0.018337  sm_loss=0.016937 \t sm_val_loss=0.018318 \t time=0.97s\n",
      "Best: Epoch 8 \t loss=0.016142  val_loss=0.018290  sm_loss=0.016326 \t sm_val_loss=0.018251 \t time=0.80s\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 13 \t loss=0.014365  val_loss=0.017614  sm_loss=0.014583 \t sm_val_loss=0.017628 \t time=0.80s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.017842126817864713\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.137542  val_loss=0.019980  sm_loss=0.137539 \t sm_val_loss=0.019993 \t time=0.79s\n",
      "Best: Epoch 2 \t loss=0.018430  val_loss=0.018778  sm_loss=0.018497 \t sm_val_loss=0.018788 \t time=0.79s\n",
      "Best: Epoch 3 \t loss=0.017434  val_loss=0.018344  sm_loss=0.017554 \t sm_val_loss=0.018314 \t time=0.82s\n",
      "Best: Epoch 5 \t loss=0.016684  val_loss=0.018266  sm_loss=0.016856 \t sm_val_loss=0.018229 \t time=0.77s\n",
      "Best: Epoch 9 \t loss=0.016456  val_loss=0.018252  sm_loss=0.016626 \t sm_val_loss=0.018225 \t time=1.04s\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 14 \t loss=0.014682  val_loss=0.017602  sm_loss=0.014888 \t sm_val_loss=0.017561 \t time=0.76s\n",
      "Best: Epoch 15 \t loss=0.013360  val_loss=0.017546  sm_loss=0.013599 \t sm_val_loss=0.017517 \t time=0.81s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.017488897881826924\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.141297  val_loss=0.019973  sm_loss=0.141294 \t sm_val_loss=0.019976 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018421  val_loss=0.018778  sm_loss=0.018489 \t sm_val_loss=0.018785 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017455  val_loss=0.018342  sm_loss=0.017575 \t sm_val_loss=0.018348 \t time=1.51s\n",
      "Best: Epoch 5 \t loss=0.016590  val_loss=0.018324  sm_loss=0.016756 \t sm_val_loss=0.018393 \t time=0.79s\n",
      "Best: Epoch 6 \t loss=0.016441  val_loss=0.018149  sm_loss=0.016618 \t sm_val_loss=0.018207 \t time=0.81s\n",
      "Best: Epoch 10 \t loss=0.016279  val_loss=0.018031  sm_loss=0.016449 \t sm_val_loss=0.018120 \t time=0.83s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014729  val_loss=0.017379  sm_loss=0.014932 \t sm_val_loss=0.017472 \t time=1.00s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.01744348195553728\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.135381  val_loss=0.019779  sm_loss=0.135374 \t sm_val_loss=0.019781 \t time=0.82s\n",
      "Best: Epoch 2 \t loss=0.018368  val_loss=0.018615  sm_loss=0.018440 \t sm_val_loss=0.018614 \t time=0.79s\n",
      "Best: Epoch 3 \t loss=0.017432  val_loss=0.018392  sm_loss=0.017542 \t sm_val_loss=0.018393 \t time=0.93s\n",
      "Best: Epoch 4 \t loss=0.017015  val_loss=0.017969  sm_loss=0.017161 \t sm_val_loss=0.018005 \t time=0.88s\n",
      "Best: Epoch 7 \t loss=0.016442  val_loss=0.017909  sm_loss=0.016609 \t sm_val_loss=0.017950 \t time=0.84s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014533  val_loss=0.017186  sm_loss=0.014744 \t sm_val_loss=0.017274 \t time=0.91s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.01723136778077747\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.017378951948478338\n",
      "Fold 2 log loss: 0.01705338700102671\n",
      "Fold 3 log loss: 0.017573691558414973\n",
      "Fold 4 log loss: 0.017842126817864713\n",
      "Fold 5 log loss: 0.017488897881826924\n",
      "Fold 6 log loss: 0.01744348195553728\n",
      "Fold 7 log loss: 0.01723136778077747\n",
      "Std of log loss: 0.00023239252392796944\n",
      "Total log loss: 0.017430755562212907\n",
      "Fold 1\n",
      "Best: Epoch 1 \t loss=0.137650  val_loss=0.019558  sm_loss=0.137647 \t sm_val_loss=0.019548 \t time=0.96s\n",
      "Best: Epoch 2 \t loss=0.018379  val_loss=0.018407  sm_loss=0.018448 \t sm_val_loss=0.018370 \t time=0.78s\n",
      "Best: Epoch 3 \t loss=0.017373  val_loss=0.018200  sm_loss=0.017494 \t sm_val_loss=0.018159 \t time=0.95s\n",
      "Best: Epoch 4 \t loss=0.017060  val_loss=0.018018  sm_loss=0.017197 \t sm_val_loss=0.018037 \t time=0.81s\n",
      "Best: Epoch 5 \t loss=0.016683  val_loss=0.017873  sm_loss=0.016844 \t sm_val_loss=0.017838 \t time=0.77s\n",
      "Best: Epoch 7 \t loss=0.016363  val_loss=0.017783  sm_loss=0.016535 \t sm_val_loss=0.017802 \t time=0.76s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014640  val_loss=0.017069  sm_loss=0.014858 \t sm_val_loss=0.017088 \t time=1.06s\n",
      "Best: Epoch 13 \t loss=0.013372  val_loss=0.016995  sm_loss=0.013609 \t sm_val_loss=0.017013 \t time=1.08s\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 1 log loss: 0.01732062232066088\n",
      "Fold 2\n",
      "Best: Epoch 1 \t loss=0.142542  val_loss=0.019366  sm_loss=0.142538 \t sm_val_loss=0.019405 \t time=1.05s\n",
      "Best: Epoch 2 \t loss=0.018587  val_loss=0.017879  sm_loss=0.018655 \t sm_val_loss=0.017934 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017456  val_loss=0.017702  sm_loss=0.017576 \t sm_val_loss=0.017766 \t time=0.80s\n",
      "Best: Epoch 4 \t loss=0.016967  val_loss=0.017601  sm_loss=0.017118 \t sm_val_loss=0.017621 \t time=0.76s\n",
      "Best: Epoch 5 \t loss=0.016658  val_loss=0.017534  sm_loss=0.016835 \t sm_val_loss=0.017530 \t time=0.83s\n",
      "Best: Epoch 6 \t loss=0.016476  val_loss=0.017401  sm_loss=0.016654 \t sm_val_loss=0.017433 \t time=0.85s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014470  val_loss=0.016809  sm_loss=0.014687 \t sm_val_loss=0.016861 \t time=0.91s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 2 log loss: 0.017097060385530077\n",
      "Fold 3\n",
      "Best: Epoch 1 \t loss=0.137006  val_loss=0.020128  sm_loss=0.136997 \t sm_val_loss=0.020124 \t time=1.03s\n",
      "Best: Epoch 2 \t loss=0.018297  val_loss=0.018897  sm_loss=0.018369 \t sm_val_loss=0.018925 \t time=0.76s\n",
      "Best: Epoch 3 \t loss=0.017328  val_loss=0.018818  sm_loss=0.017446 \t sm_val_loss=0.018802 \t time=0.76s\n",
      "Best: Epoch 4 \t loss=0.016917  val_loss=0.018507  sm_loss=0.017068 \t sm_val_loss=0.018506 \t time=0.79s\n",
      "Best: Epoch 6 \t loss=0.016394  val_loss=0.018401  sm_loss=0.016567 \t sm_val_loss=0.018412 \t time=0.75s\n",
      "Best: Epoch 7 \t loss=0.016311  val_loss=0.018310  sm_loss=0.016485 \t sm_val_loss=0.018288 \t time=0.92s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014468  val_loss=0.017611  sm_loss=0.014686 \t sm_val_loss=0.017618 \t time=0.79s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 3 log loss: 0.017521418248245265\n",
      "Fold 4\n",
      "Best: Epoch 1 \t loss=0.134803  val_loss=0.020007  sm_loss=0.134801 \t sm_val_loss=0.019984 \t time=0.81s\n",
      "Best: Epoch 2 \t loss=0.018277  val_loss=0.018958  sm_loss=0.018354 \t sm_val_loss=0.018953 \t time=0.78s\n",
      "Best: Epoch 3 \t loss=0.017293  val_loss=0.018674  sm_loss=0.017415 \t sm_val_loss=0.018636 \t time=0.75s\n",
      "Best: Epoch 4 \t loss=0.016784  val_loss=0.018518  sm_loss=0.016944 \t sm_val_loss=0.018477 \t time=0.78s\n",
      "Best: Epoch 5 \t loss=0.016465  val_loss=0.018447  sm_loss=0.016639 \t sm_val_loss=0.018440 \t time=1.22s\n",
      "Best: Epoch 7 \t loss=0.016303  val_loss=0.018199  sm_loss=0.016481 \t sm_val_loss=0.018166 \t time=0.75s\n",
      "Best: Epoch 10 \t loss=0.016342  val_loss=0.018161  sm_loss=0.016515 \t sm_val_loss=0.018123 \t time=1.09s\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 15 \t loss=0.014662  val_loss=0.017716  sm_loss=0.014873 \t sm_val_loss=0.017747 \t time=0.83s\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 4 log loss: 0.017964381739147883\n",
      "Fold 5\n",
      "Best: Epoch 1 \t loss=0.137738  val_loss=0.019961  sm_loss=0.137738 \t sm_val_loss=0.019967 \t time=0.77s\n",
      "Best: Epoch 2 \t loss=0.018459  val_loss=0.018841  sm_loss=0.018528 \t sm_val_loss=0.018844 \t time=1.00s\n",
      "Best: Epoch 3 \t loss=0.017659  val_loss=0.018402  sm_loss=0.017767 \t sm_val_loss=0.018408 \t time=0.76s\n",
      "Best: Epoch 5 \t loss=0.016665  val_loss=0.018281  sm_loss=0.016831 \t sm_val_loss=0.018271 \t time=0.75s\n",
      "Best: Epoch 6 \t loss=0.016466  val_loss=0.018155  sm_loss=0.016639 \t sm_val_loss=0.018107 \t time=0.77s\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 11 \t loss=0.014595  val_loss=0.017491  sm_loss=0.014812 \t sm_val_loss=0.017474 \t time=0.75s\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 5 log loss: 0.01743110211770905\n",
      "Fold 6\n",
      "Best: Epoch 1 \t loss=0.143982  val_loss=0.020035  sm_loss=0.143982 \t sm_val_loss=0.020044 \t time=0.94s\n",
      "Best: Epoch 2 \t loss=0.018403  val_loss=0.018761  sm_loss=0.018464 \t sm_val_loss=0.018726 \t time=0.77s\n",
      "Best: Epoch 3 \t loss=0.017408  val_loss=0.018430  sm_loss=0.017517 \t sm_val_loss=0.018489 \t time=0.81s\n",
      "Best: Epoch 4 \t loss=0.017154  val_loss=0.018337  sm_loss=0.017293 \t sm_val_loss=0.018390 \t time=1.07s\n",
      "Best: Epoch 6 \t loss=0.016392  val_loss=0.018130  sm_loss=0.016564 \t sm_val_loss=0.018205 \t time=0.74s\n",
      "Best: Epoch 9 \t loss=0.016332  val_loss=0.018103  sm_loss=0.016510 \t sm_val_loss=0.018147 \t time=0.89s\n",
      "Best: Epoch 12 \t loss=0.016355  val_loss=0.018070  sm_loss=0.016526 \t sm_val_loss=0.018109 \t time=0.90s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 17 \t loss=0.014825  val_loss=0.017374  sm_loss=0.015025 \t sm_val_loss=0.017463 \t time=0.96s\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 6 log loss: 0.01744653400362781\n",
      "Fold 7\n",
      "Best: Epoch 1 \t loss=0.137200  val_loss=0.019827  sm_loss=0.137193 \t sm_val_loss=0.019844 \t time=0.84s\n",
      "Best: Epoch 2 \t loss=0.018420  val_loss=0.018656  sm_loss=0.018485 \t sm_val_loss=0.018667 \t time=0.85s\n",
      "Best: Epoch 3 \t loss=0.017573  val_loss=0.018103  sm_loss=0.017688 \t sm_val_loss=0.018138 \t time=1.18s\n",
      "Best: Epoch 4 \t loss=0.016881  val_loss=0.017949  sm_loss=0.017026 \t sm_val_loss=0.017965 \t time=0.85s\n",
      "Best: Epoch 7 \t loss=0.016312  val_loss=0.017812  sm_loss=0.016491 \t sm_val_loss=0.017854 \t time=1.04s\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Best: Epoch 12 \t loss=0.014401  val_loss=0.017078  sm_loss=0.014620 \t sm_val_loss=0.017156 \t time=0.79s\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Fold 7 log loss: 0.01715348683953257\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.01732062232066088\n",
      "Fold 2 log loss: 0.017097060385530077\n",
      "Fold 3 log loss: 0.017521418248245265\n",
      "Fold 4 log loss: 0.017964381739147883\n",
      "Fold 5 log loss: 0.01743110211770905\n",
      "Fold 6 log loss: 0.01744653400362781\n",
      "Fold 7 log loss: 0.01715348683953257\n",
      "Std of log loss: 0.00026529107324739097\n",
      "Total log loss: 0.01741929714705502\n",
      "Total log loss in targets: 0.017261153120231854\n"
     ]
    }
   ],
   "source": [
    "lstm1_oof = np.zeros([len(lstm_train),fn_targets.shape[1]])\n",
    "lstm1_test = np.zeros([len(lstm_test),fn_targets.shape[1]])\n",
    "\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, lstm_pred = modelling_lstm(lstm_train, fn_targets, lstm_test, seed_, fn_targets.shape[1])\n",
    "    lstm1_oof += oof / len(seeds)\n",
    "    lstm1_test += lstm_pred / len(seeds)\n",
    "print(\"Total log loss in targets: {}\".format(mean_log_loss(oof_targets, lstm1_oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:09:36.206679Z",
     "iopub.status.busy": "2020-11-10T05:09:36.205523Z",
     "iopub.status.idle": "2020-11-10T05:09:37.479512Z",
     "shell.execute_reply": "2020-11-10T05:09:37.477743Z"
    },
    "papermill": {
     "duration": 1.609823,
     "end_time": "2020-11-10T05:09:37.479636",
     "exception": false,
     "start_time": "2020-11-10T05:09:35.869813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.01590861630481441\n"
     ]
    }
   ],
   "source": [
    "check_lstm = np.zeros([y.shape[0], y.shape[1]])\n",
    "check_lstm[cons_train_index,:] = lstm1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_lstm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.321996,
     "end_time": "2020-11-10T05:09:38.134602",
     "exception": false,
     "start_time": "2020-11-10T05:09:37.812606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:09:38.785748Z",
     "iopub.status.busy": "2020-11-10T05:09:38.775543Z",
     "iopub.status.idle": "2020-11-10T05:19:55.011772Z",
     "shell.execute_reply": "2020-11-10T05:19:55.012566Z"
    },
    "papermill": {
     "duration": 616.563911,
     "end_time": "2020-11-10T05:19:55.012750",
     "exception": false,
     "start_time": "2020-11-10T05:09:38.448839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819f5c1000544ceaf594a448dff4e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Target ind 0 score 0.0267522871657328: 0.0048122212123293704\n",
      "SVM Target ind 1 score 0.028325951116658196: 0.006770445869106968\n",
      "SVM Target ind 2 score 0.0377679348222106: 0.008648690214872171\n",
      "SVM Target ind 3 score 0.2990344481290532: 0.048714773921495466\n",
      "SVM Target ind 4 score 0.4736868044966427: 0.07136280954991497\n",
      "SVM Target ind 5 score 0.11487746841755518: 0.02234595137756862\n",
      "SVM Target ind 6 score 0.0849778533499726: 0.01722326624283357\n",
      "SVM Target ind 7 score 0.15107877203184092: 0.027914981102869423\n",
      "SVM Target ind 8 score 0.018891922657283466: 0.004205393265934258\n",
      "SVM Target ind 9 score 0.4117833910145857: 0.06133994484728045\n",
      "SVM Target ind 10 score 0.5665533919108652: 0.08282704855719118\n",
      "SVM Target ind 11 score 0.09173731824932935: 0.018122995986820847\n",
      "SVM Target ind 12 score 0.009515743973757837: 0.0004525445380906421\n",
      "SVM Target ind 13 score 0.06609388593886781: 0.013647134323485403\n",
      "SVM Target ind 14 score 0.0188839674111058: 0.0048917614346998374\n",
      "SVM Target ind 15 score 0.018883967411105797: 0.004829948538421805\n",
      "SVM Target ind 16 score 0.0755358696444202: 0.015589540240999792\n",
      "SVM Target ind 17 score 0.14012149097942572: 0.026166388686752747\n",
      "SVM Target ind 18 score 0.12589699102527943: 0.02403934209920235\n",
      "SVM Target ind 19 score 0.0566519022333154: 0.012199328844973005\n",
      "SVM Target ind 20 score 0.058225566184240796: 0.012396844342183785\n",
      "SVM Target ind 21 score 0.11488528591524694: 0.021165237071924206\n",
      "SVM Target ind 22 score 0.009441983705553398: 0.002450428257327784\n",
      "SVM Target ind 23 score 0.06766754988979319: 0.014242380619295067\n",
      "SVM Target ind 24 score 0.0188839674111058: 0.004769667487327933\n",
      "SVM Target ind 25 score 0.020457631362031197: 0.004750364403721012\n",
      "SVM Target ind 26 score 0.018883967411105797: 0.004899251746230291\n",
      "SVM Target ind 27 score 0.028325951116658196: 0.006746934487096453\n",
      "SVM Target ind 28 score 0.11489994454125267: 0.0217504559033415\n",
      "SVM Target ind 29 score 0.0566519022333154: 0.012049516897666136\n",
      "SVM Target ind 30 score 0.0361959840422565: 0.008492628666510425\n",
      "SVM Target ind 31 score 0.07712476011032177: 0.016182980118025568\n",
      "SVM Target ind 32 score 0.0739622056934948: 0.015204090999585215\n",
      "SVM Target ind 33 score 0.009441983705553401: 0.001790930164464218\n",
      "SVM Target ind 34 score 0.0015736639509263987: 0.0015736639509263987\n",
      "SVM Target ind 35 score 0.018883967411105797: 0.002467852585101117\n",
      "SVM Target ind 36 score 0.12354041794750255: 0.025745045148729086\n",
      "SVM Target ind 37 score 0.029899615067583596: 0.0063590104219056615\n",
      "SVM Target ind 38 score 0.117027522981267: 0.015646338376196143\n",
      "SVM Target ind 39 score 0.0094419837055534: 0.0024928529974551977\n",
      "SVM Target ind 40 score 0.09443222698934238: 0.018850779777897845\n",
      "SVM Target ind 41 score 0.12596981185546863: 0.023782026165158753\n",
      "SVM Target ind 42 score 0.056651902233315385: 0.011968476697419858\n",
      "SVM Target ind 43 score 0.3021922555051231: 0.04979909120026811\n",
      "SVM Target ind 44 score 0.14010783462935597: 0.025698321714253937\n",
      "SVM Target ind 45 score 0.18102723765002543: 0.031070620296824735\n",
      "SVM Target ind 46 score 0.0110156476564788: 0.0028658268531132377\n",
      "SVM Target ind 47 score 0.04735046826965975: 0.010391381931273003\n",
      "SVM Target ind 48 score 0.05979923013516619: 0.012798917659092067\n",
      "SVM Target ind 49 score 0.10545043223729317: 0.020205678384129802\n",
      "SVM Target ind 50 score 0.037767934822210594: 0.008622660550334095\n",
      "SVM Target ind 51 score 0.08894418142118875: 0.016255558135436686\n",
      "SVM Target ind 52 score 0.04563625457683759: 0.009830086535951748\n",
      "SVM Target ind 53 score 0.0094419837055534: 0.0024484549404580197\n",
      "SVM Target ind 54 score 0.44074298736759143: 0.06799209772366961\n",
      "SVM Target ind 55 score 0.0660938859388678: 0.013825531530498734\n",
      "SVM Target ind 56 score 0.08655340827240351: 0.017192315469825734\n",
      "SVM Target ind 57 score 0.0566519022333154: 0.012193895943216966\n",
      "SVM Target ind 58 score 0.05667883987637601: 0.012181612325317896\n",
      "SVM Target ind 59 score 0.028325951116658196: 0.006897075401362036\n",
      "SVM Target ind 60 score 0.018883967411105793: 0.00480058090713985\n",
      "SVM Target ind 61 score 0.16054418659804162: 0.029686115653947517\n",
      "SVM Target ind 62 score 0.028325951116658196: 0.006621227076431886\n",
      "SVM Target ind 63 score 0.15515156887990914: 0.02454946930760223\n",
      "SVM Target ind 64 score 0.0849778533499726: 0.01713498993853533\n",
      "SVM Target ind 65 score 0.028701943512574293: 0.004401178659125449\n",
      "SVM Target ind 66 score 0.06609388593886781: 0.013766206676114221\n",
      "SVM Target ind 67 score 0.0755358696444202: 0.01562036654493434\n",
      "SVM Target ind 68 score 0.08498337260773155: 0.01685230512805121\n",
      "SVM Target ind 69 score 0.0094419837055534: 0.0025147308373676355\n",
      "SVM Target ind 70 score 0.05666795544685991: 0.010633063590078028\n",
      "SVM Target ind 71 score 0.6833149662642808: 0.09403252812332667\n",
      "SVM Target ind 72 score 0.16370162600341373: 0.030137908537439594\n",
      "SVM Target ind 73 score 0.045739991249703765: 0.0096588492143437\n",
      "SVM Target ind 74 score 0.039341598773136: 0.008752612035473568\n",
      "SVM Target ind 75 score 0.0094419837055534: 0.0025288314015760004\n",
      "SVM Target ind 76 score 0.07555741693010283: 0.015589352252261188\n",
      "SVM Target ind 77 score 0.6121959610368682: 0.08806763671619133\n",
      "SVM Target ind 78 score 0.19047378562691702: 0.033843083293371906\n",
      "SVM Target ind 79 score 0.6629748425839092: 0.09153735405159015\n",
      "SVM Target ind 80 score 0.15254272903585567: 0.021704151288719362\n",
      "SVM Target ind 81 score 0.009441983705553398: 0.0025363835790683073\n",
      "SVM Target ind 82 score 0.0015736639509263994: 0.0015736639509263994\n",
      "SVM Target ind 83 score 0.24877237152238718: 0.04199849143907281\n",
      "SVM Target ind 84 score 0.07554329595043781: 0.015759635819753644\n",
      "SVM Target ind 85 score 0.05665321535403869: 0.011604208279925575\n",
      "SVM Target ind 86 score 0.025316019953060986: 0.0036581156074888253\n",
      "SVM Target ind 87 score 0.039341598773136: 0.008808001014793704\n",
      "SVM Target ind 88 score 0.07868536907224316: 0.015315280044443737\n",
      "SVM Target ind 89 score 0.22174756338740179: 0.035803549679740365\n",
      "SVM Target ind 90 score 0.026816801226915685: 0.0037965317222974112\n",
      "SVM Target ind 91 score 0.028325951116658196: 0.006814352605739545\n",
      "SVM Target ind 92 score 0.036194270871285204: 0.00807445526582656\n",
      "SVM Target ind 93 score 0.16683628541176634: 0.030660249741942872\n",
      "SVM Target ind 94 score 0.2596889034750546: 0.04400267545093694\n",
      "SVM Target ind 95 score 0.07304508033371049: 0.013191982008942489\n",
      "SVM Target ind 96 score 0.09822436132751709: 0.01704312058059731\n",
      "SVM Target ind 97 score 0.020457631362031197: 0.005245630390193021\n",
      "SVM Target ind 98 score 0.11647925243600864: 0.0225009232905924\n",
      "SVM Target ind 99 score 0.5775796588197007: 0.08460551028518314\n",
      "SVM Target ind 100 score 0.028325951116658202: 0.006735334425151547\n",
      "SVM Target ind 101 score 0.07133256273391811: 0.013421149559675893\n",
      "SVM Target ind 102 score 0.1133038044666298: 0.0220790780104637\n",
      "SVM Target ind 103 score 0.07596720759332505: 0.01474272967628455\n",
      "SVM Target ind 104 score 0.09284715278404923: 0.018574178847245812\n",
      "SVM Target ind 105 score 0.3793431337197532: 0.059589232097869156\n",
      "SVM Target ind 106 score 0.02752061767830488: 0.00564093553182494\n",
      "SVM Target ind 107 score 0.04884876356731782: 0.00886663621842002\n",
      "SVM Target ind 108 score 0.11331339953731533: 0.021205130528530704\n",
      "SVM Target ind 109 score 0.11148219656751567: 0.014442820895742177\n",
      "SVM Target ind 110 score 0.04139778678180223: 0.00894786131591301\n",
      "SVM Target ind 111 score 0.058225566184240796: 0.011982097771449585\n",
      "SVM Target ind 112 score 0.047209918527763: 0.013812984986543887\n",
      "SVM Target ind 113 score 0.048799928903180086: 0.01069889119480984\n",
      "SVM Target ind 114 score 0.11489700713375606: 0.021986665722632776\n",
      "SVM Target ind 115 score 0.047209918527762997: 0.010413046625845862\n",
      "SVM Target ind 116 score 0.08031136716799994: 0.01673433867057325\n",
      "SVM Target ind 117 score 0.06609388593886781: 0.013992731770019196\n",
      "SVM Target ind 118 score 0.11481191529522863: 0.017862058461734123\n",
      "SVM Target ind 119 score 0.1900503821981799: 0.029106909377399907\n",
      "SVM Target ind 120 score 0.009441983705553398: 0.002518464870785094\n",
      "SVM Target ind 121 score 0.009441983705553398: 0.00241521219902723\n",
      "SVM Target ind 122 score 0.09758370206271919: 0.01954349270380328\n",
      "SVM Target ind 123 score 0.018883967411105797: 0.004910827988053111\n",
      "SVM Target ind 124 score 0.09603085705039685: 0.019145856107990588\n",
      "SVM Target ind 125 score 0.0094419837055534: 0.002089796162456942\n",
      "SVM Target ind 126 score 0.042647805184454365: 0.007580185706503541\n",
      "SVM Target ind 127 score 0.0578453715005242: 0.012395753433592867\n",
      "SVM Target ind 128 score 0.11646998518782649: 0.022791448114633583\n",
      "SVM Target ind 129 score 0.039347978680219636: 0.00901715203565252\n",
      "SVM Target ind 130 score 0.018883967411105797: 0.004892486700691938\n",
      "SVM Target ind 131 score 0.13377220290468148: 0.02513189032455173\n",
      "SVM Target ind 132 score 0.028325951116658196: 0.005487738496776081\n",
      "SVM Target ind 133 score 0.1328684504116916: 0.019439110985058062\n",
      "SVM Target ind 134 score 0.07554073462179142: 0.015486010908267024\n",
      "SVM Target ind 135 score 0.05822603656773986: 0.01235072076299401\n",
      "SVM Target ind 136 score 0.1929998294329688: 0.03881667016491858\n",
      "SVM Target ind 137 score 0.009441983705553398: 0.0024278970354662133\n",
      "SVM Target ind 138 score 0.040915262724061395: 0.009090522960673175\n",
      "SVM Target ind 139 score 0.018883967411105797: 0.006205146949600958\n",
      "SVM Target ind 140 score 0.040915262724061395: 0.009165378911992627\n",
      "SVM Target ind 141 score 0.0110156476564788: 0.003375072243705271\n",
      "SVM Target ind 142 score 0.028325951116658196: 0.0066770105345926615\n",
      "SVM Target ind 143 score 0.09599404019071243: 0.0189925740439613\n",
      "SVM Target ind 144 score 0.15110035004919148: 0.02831672826567904\n",
      "SVM Target ind 145 score 0.058225566184240796: 0.012598081736651437\n",
      "SVM Target ind 146 score 0.08889363525158744: 0.014559965277264135\n",
      "SVM Target ind 147 score 0.0377679348222106: 0.0088130222205786\n",
      "SVM Target ind 148 score 0.09600317569208881: 0.018006592830268186\n",
      "SVM Target ind 149 score 0.2238653089715262: 0.03183132650184997\n",
      "SVM Target ind 150 score 0.028325951116658196: 0.0067288317723638875\n",
      "SVM Target ind 151 score 0.4125742953475571: 0.06253510183250205\n",
      "SVM Target ind 152 score 0.039341598773136: 0.008857181921295677\n",
      "SVM Target ind 153 score 0.17817709488983066: 0.03143600877689148\n",
      "SVM Target ind 154 score 0.0487835824786884: 0.010612617947092139\n",
      "SVM Target ind 155 score 0.086551517300898: 0.017281646741547297\n",
      "SVM Target ind 156 score 0.1542430621174932: 0.0284432850817324\n",
      "SVM Target ind 157 score 0.16642570058794975: 0.02958930874661725\n",
      "SVM Target ind 158 score 0.04721161032920866: 0.010397454683358356\n",
      "SVM Target ind 159 score 0.18589619914127237: 0.031609625234598815\n",
      "SVM Target ind 160 score 0.028325951116658196: 0.006083672613123107\n",
      "SVM Target ind 161 score 0.05665971361714863: 0.012052174780785248\n",
      "SVM Target ind 162 score 0.1322021784191796: 0.02491665250065124\n",
      "SVM Target ind 163 score 0.00773799318912934: 0.0014792496198289893\n",
      "SVM Target ind 164 score 0.07553863516630119: 0.015871761006495563\n",
      "SVM Target ind 165 score 0.009441983705553398: 0.002370832990247406\n",
      "SVM Target ind 166 score 0.15758433317429665: 0.029955729481535624\n",
      "SVM Target ind 167 score 0.029899615067583596: 0.007149071440814708\n",
      "SVM Target ind 168 score 0.08812518125182339: 0.017853063857401805\n",
      "SVM Target ind 169 score 0.061469003934786666: 0.010724624244010193\n",
      "SVM Target ind 170 score 0.018883967411105797: 0.004962878254706303\n",
      "SVM Target ind 171 score 0.08625745459401825: 0.012377094749346408\n",
      "SVM Target ind 172 score 0.009441983705553398: 0.002332818049010059\n",
      "SVM Target ind 173 score 0.05354624685742569: 0.009941526794199804\n",
      "SVM Target ind 174 score 0.05708164867373365: 0.011843256067980108\n",
      "SVM Target ind 175 score 0.039341598773136: 0.009587367833978818\n",
      "SVM Target ind 176 score 0.369978132757806: 0.05899561531396269\n",
      "SVM Target ind 177 score 0.6360412257843459: 0.08850242422558528\n",
      "SVM Target ind 178 score 0.06924121384071859: 0.014529707915256205\n",
      "SVM Target ind 179 score 0.0566519022333154: 0.012075925599419382\n",
      "SVM Target ind 180 score 0.0566519022333154: 0.011959748603787318\n",
      "SVM Target ind 181 score 0.039341598773136: 0.008938946616912349\n",
      "SVM Target ind 182 score 0.4202091469008892: 0.0655633085383788\n",
      "SVM Target ind 183 score 0.039341598773136: 0.008912449532045593\n",
      "SVM Target ind 184 score 0.11057404609184526: 0.02237052070159809\n",
      "SVM Target ind 185 score 0.0094419837055534: 0.0024620484920801944\n",
      "SVM Target ind 186 score 0.02991434312404023: 0.007161752511020129\n",
      "SVM Target ind 187 score 0.09442059265892178: 0.01847934044394187\n",
      "SVM Target ind 188 score 0.0321670175631215: 0.004235796232152339\n",
      "SVM Target ind 189 score 0.029899615067583596: 0.007106731580835096\n",
      "SVM Target ind 190 score 0.058233255091614713: 0.012361000234260901\n",
      "SVM Target ind 191 score 0.047209918527763: 0.010355699242387327\n",
      "SVM Target ind 192 score 0.011015647656478798: 0.0032793905681420495\n",
      "SVM Target ind 193 score 0.0566519022333154: 0.012087228913015956\n",
      "SVM Target ind 194 score 0.1215113896016568: 0.018102208513718417\n",
      "SVM Target ind 195 score 0.028325951116658196: 0.006584128186790138\n",
      "SVM Target ind 196 score 0.009441983705553398: 0.002504593652197995\n",
      "SVM Target ind 197 score 0.039341598773136: 0.00883611224679431\n",
      "SVM Target ind 198 score 0.07553947931480413: 0.015575349891729238\n",
      "SVM Target ind 199 score 0.1443305908956718: 0.024369676931582585\n",
      "SVM Target ind 200 score 0.11488245255379737: 0.022560060774133417\n",
      "SVM Target ind 201 score 0.0094419837055534: 0.0024968956130894766\n",
      "SVM Target ind 202 score 0.22615630381954627: 0.03597423364918137\n",
      "SVM Target ind 203 score 0.040915262724061395: 0.009399874969104353\n",
      "SVM Target ind 204 score 0.039840694652504664: 0.004798286063572215\n",
      "SVM Target ind 205 score 0.047209918527763: 0.01035807691372092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_STARTS = 1\n",
    "N_SPLITS = 5\n",
    "\n",
    "svm0_oof = np.zeros([len(fn_train), fn_targets.shape[1]])\n",
    "svm0_test = np.zeros([len(fn_test), fn_targets.shape[1]])\n",
    "\n",
    "svm1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "svm1_oof = np.zeros([fn_targets.shape[0],fn_targets.shape[1]]) \n",
    "\n",
    "for ind in tqdm(range(fn_targets.shape[1])):\n",
    "    ind_target_sum = fn_targets[:, ind].sum()\n",
    "    if ind_target_sum >= N_SPLITS and target_feats[ind] not in unbalanced_feats:  \n",
    "        for seed in range(N_STARTS):\n",
    "            for n in range(N_SPLITS):\n",
    "                val_index = fn_train[:,-1] == n\n",
    "                train_index = fn_train[:,-1] != n\n",
    "                x_tr, x_val = fn_train[train_index], fn_train[val_index]\n",
    "                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "                x_tr = np.delete(x_tr, -1, 1)\n",
    "                x_val = np.delete(x_val, -1, 1)\n",
    "\n",
    "                model = SVC(C = 40, cache_size = 2000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "                svm0_test[:, ind] += model.decision_function(fn_test) / (N_SPLITS * N_STARTS)\n",
    "                svm0_oof[val_index, ind] += model.decision_function(x_val) / N_STARTS\n",
    "\n",
    "        for seed in range(N_STARTS):\n",
    "            for n in range(N_SPLITS):\n",
    "                val_index = fn_train[:,-1] == n\n",
    "                train_index = fn_train[:,-1] != n            \n",
    "\n",
    "                x_tr, x_val = svm0_oof[train_index, ind].reshape(-1, 1), svm0_oof[val_index, ind].reshape(-1, 1)\n",
    "                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "                model = LogisticRegression(C = 35, max_iter = 1000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "                svm1_test[:, ind] += model.predict_proba(svm0_test[:, ind].reshape(-1, 1))[:, 1] / (N_SPLITS * N_STARTS)\n",
    "                svm1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1] / N_STARTS\n",
    "    elif target_feats[ind] in unbalanced_feats:\n",
    "        for seed in range(N_STARTS):\n",
    "            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "\n",
    "            for n, (train_index, val_index) in enumerate(skf.split(fn_train, fn_targets[:,ind])):\n",
    "\n",
    "                x_tr, x_val = fn_train[train_index], fn_train[val_index]\n",
    "                y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "                x_tr = np.delete(x_tr, -1, 1)\n",
    "                x_val = np.delete(x_val, -1, 1)\n",
    "                \n",
    "                model = SVC(C = 40, cache_size = 2000)\n",
    "                model.fit(x_tr, y_tr)\n",
    "                svm0_test[:, ind] += model.decision_function(fn_test) / (N_SPLITS * N_STARTS)\n",
    "                svm0_oof[val_index, ind] += model.decision_function(x_val) / N_STARTS\n",
    "                \n",
    "            for seed in range(N_STARTS):\n",
    "                skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
    "\n",
    "                for n, (train_index, val_index) in enumerate(skf.split(svm0_oof, fn_targets[:,ind])):\n",
    "\n",
    "                    x_tr, x_val = svm0_oof[train_index, ind].reshape(-1, 1), svm0_oof[val_index, ind].reshape(-1, 1)\n",
    "                    y_tr, y_val = fn_targets[train_index,ind], fn_targets[val_index,ind]\n",
    "\n",
    "                    model = LogisticRegression(C = 35, max_iter = 1000)\n",
    "                    model.fit(x_tr, y_tr)\n",
    "                    svm1_test[:, ind] += model.predict_proba(svm0_test[:, ind].reshape(-1, 1))[:, 1] / (N_SPLITS * N_STARTS)\n",
    "                    svm1_oof[val_index, ind] += model.predict_proba(x_val)[:, 1] / N_STARTS\n",
    "    \n",
    "    score1 = log_loss(fn_targets[:, ind], svm0_oof[:, ind])\n",
    "    score2 = log_loss(fn_targets[:, ind], svm1_oof[:, ind])\n",
    "    print('SVM Target ind {} score {}: {}'.format(ind, score1, score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:19:55.862887Z",
     "iopub.status.busy": "2020-11-10T05:19:55.861792Z",
     "iopub.status.idle": "2020-11-10T05:19:57.415917Z",
     "shell.execute_reply": "2020-11-10T05:19:57.416401Z"
    },
    "papermill": {
     "duration": 1.989158,
     "end_time": "2020-11-10T05:19:57.416553",
     "exception": false,
     "start_time": "2020-11-10T05:19:55.427395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.01633192803048022\n"
     ]
    }
   ],
   "source": [
    "check_svm = np.zeros([y.shape[0], y.shape[1]])\n",
    "check_svm[cons_train_index,:] = svm1_oof\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check_svm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.382532,
     "end_time": "2020-11-10T05:19:58.185947",
     "exception": false,
     "start_time": "2020-11-10T05:19:57.803415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:19:59.066893Z",
     "iopub.status.busy": "2020-11-10T05:19:59.064936Z",
     "iopub.status.idle": "2020-11-10T05:19:59.067586Z",
     "shell.execute_reply": "2020-11-10T05:19:59.068197Z"
    },
    "papermill": {
     "duration": 0.419412,
     "end_time": "2020-11-10T05:19:59.068485",
     "exception": false,
     "start_time": "2020-11-10T05:19:58.649073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "def log_loss_numpy(y_pred):\n",
    "    y_true_ravel = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n",
    "    return loss.mean()\n",
    "\n",
    "def func_numpy_metric(weights):\n",
    "    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n",
    "    return log_loss_numpy(oof_blend)\n",
    "\n",
    "def grad_func(weights):\n",
    "    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n",
    "    gradients = np.zeros(oof.shape[0])\n",
    "    for i in range(oof.shape[0]):\n",
    "        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n",
    "        for j in range(oof.shape[0]):\n",
    "            if j != i:\n",
    "                c += weights[j] * oof_clip[j]\n",
    "        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n",
    "    return gradients\n",
    "\n",
    "@njit\n",
    "def grad_func_jit(weights):\n",
    "    oof_clip = np.minimum(1 - 1e-15, np.maximum(oof, 1e-15))\n",
    "    gradients = np.zeros(oof.shape[0])\n",
    "    for i in range(oof.shape[0]):\n",
    "        a, b, c = y_true, oof_clip[i], np.zeros((oof.shape[1], oof.shape[2]))\n",
    "        for j in range(oof.shape[0]):\n",
    "            if j != i:\n",
    "                c += weights[j] * oof_clip[j]\n",
    "        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:19:59.869225Z",
     "iopub.status.busy": "2020-11-10T05:19:59.868349Z",
     "iopub.status.idle": "2020-11-10T05:20:00.248389Z",
     "shell.execute_reply": "2020-11-10T05:20:00.248893Z"
    },
    "papermill": {
     "duration": 0.769313,
     "end_time": "2020-11-10T05:20:00.249042",
     "exception": false,
     "start_time": "2020-11-10T05:19:59.479729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col = 'sig_id').values\n",
    "\n",
    "oof_dict = {'Model 1': check_mlp, \n",
    "            'Model 2': check_lstm,\n",
    "            'Model 3': check_tabnet, \n",
    "            'Model 4': check_svm\n",
    "           }\n",
    "\n",
    "oof = np.zeros((len(oof_dict), y_true.shape[0], y_true.shape[1]))\n",
    "for i in range(oof.shape[0]):\n",
    "    oof[i] = list(oof_dict.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:20:01.027321Z",
     "iopub.status.busy": "2020-11-10T05:20:01.025655Z",
     "iopub.status.idle": "2020-11-10T05:20:31.808624Z",
     "shell.execute_reply": "2020-11-10T05:20:31.809333Z"
    },
    "papermill": {
     "duration": 31.179402,
     "end_time": "2020-11-10T05:20:31.809526",
     "exception": false,
     "start_time": "2020-11-10T05:20:00.630124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital Blend OOF: 0.015539882206803962\n",
      "Optimised Blend OOF: 0.015532314758984813\n",
      "Optimised Weights: [0.49751959 0.16543279 0.17033086 0.16671677]\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-10\n",
    "#init_guess = [1 / oof.shape[0]] * oof.shape[0]\n",
    "\n",
    "init_guess = [0.6, 0.15, 0.15, 0.1]\n",
    "bnds = [(0, 1) for _ in range(oof.shape[0])]\n",
    "cons = {'type': 'eq', \n",
    "        'fun': lambda x: np.sum(x) - 1, \n",
    "        'jac': lambda x: [1] * len(x)}\n",
    "\n",
    "print('Inital Blend OOF:', func_numpy_metric(init_guess))\n",
    "res_scipy = minimize(fun = func_numpy_metric, \n",
    "                     x0 = init_guess, \n",
    "                     method = 'SLSQP', \n",
    "                     jac = grad_func_jit, # grad_func \n",
    "                     bounds = bnds, \n",
    "                     constraints = cons, \n",
    "                     tol = tol)\n",
    "print('Optimised Blend OOF:', res_scipy.fun)\n",
    "print('Optimised Weights:', res_scipy.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:20:32.605360Z",
     "iopub.status.busy": "2020-11-10T05:20:32.603919Z",
     "iopub.status.idle": "2020-11-10T05:20:33.953364Z",
     "shell.execute_reply": "2020-11-10T05:20:33.952690Z"
    },
    "papermill": {
     "duration": 1.736575,
     "end_time": "2020-11-10T05:20:33.953505",
     "exception": false,
     "start_time": "2020-11-10T05:20:32.216930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.015532314758984813\n"
     ]
    }
   ],
   "source": [
    "check = 0\n",
    "for i in range(oof.shape[0]):\n",
    "    check += res_scipy.x[i] * list(oof_dict.values())[i]\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T05:20:34.751579Z",
     "iopub.status.busy": "2020-11-10T05:20:34.750546Z",
     "iopub.status.idle": "2020-11-10T05:20:37.363045Z",
     "shell.execute_reply": "2020-11-10T05:20:37.361780Z"
    },
    "papermill": {
     "duration": 3.029087,
     "end_time": "2020-11-10T05:20:37.363192",
     "exception": false,
     "start_time": "2020-11-10T05:20:34.334105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {'Model 1': mlp1_test, \n",
    "            'Model 2': lstm1_test, \n",
    "            'Model 3': tabnet1_test, \n",
    "            'Model 4': svm1_test\n",
    "           }\n",
    "final = 0\n",
    "for i in range(oof.shape[0]):\n",
    "    final += res_scipy.x[i] * list(test_dict.values())[i]\n",
    "\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub.loc[cons_test_index,target_feats] = final\n",
    "sub.loc[noncons_test_index,target_feats] = 0\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2680.735534,
   "end_time": "2020-11-10T05:20:38.359550",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-10T04:35:57.624016",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "19909dc3df0244b9ae995368ecdf0820": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_531c128e25104be58657889c086cba74",
       "placeholder": "​",
       "style": "IPY_MODEL_27ce8e1dc8f34ceea2488370194a5927",
       "value": " 206/206 [10:23&lt;00:00,  3.03s/it]"
      }
     },
     "27ce8e1dc8f34ceea2488370194a5927": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3819f5c1000544ceaf594a448dff4e43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e9982d1610774f36b733495adff728a1",
        "IPY_MODEL_19909dc3df0244b9ae995368ecdf0820"
       ],
       "layout": "IPY_MODEL_accf98a298b04b3bb35087b61fcd3824"
      }
     },
     "531c128e25104be58657889c086cba74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fa9d84a75ba4ce0a1ed56abf3154507": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "accf98a298b04b3bb35087b61fcd3824": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be05d6d7432746388239f185aef93631": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9982d1610774f36b733495adff728a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be05d6d7432746388239f185aef93631",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6fa9d84a75ba4ce0a1ed56abf3154507",
       "value": 206.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
