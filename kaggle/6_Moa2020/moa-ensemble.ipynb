{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-10T03:45:57.263377Z",
     "iopub.status.busy": "2020-10-10T03:45:57.262399Z",
     "iopub.status.idle": "2020-10-10T03:46:06.514010Z",
     "shell.execute_reply": "2020-10-10T03:46:06.514638Z"
    },
    "papermill": {
     "duration": 9.282187,
     "end_time": "2020-10-10T03:46:06.514844",
     "exception": false,
     "start_time": "2020-10-10T03:45:57.232657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017702,
     "end_time": "2020-10-10T03:46:06.551402",
     "exception": false,
     "start_time": "2020-10-10T03:46:06.533700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:06.603920Z",
     "iopub.status.busy": "2020-10-10T03:46:06.602662Z",
     "iopub.status.idle": "2020-10-10T03:46:13.297402Z",
     "shell.execute_reply": "2020-10-10T03:46:13.298107Z"
    },
    "papermill": {
     "duration": 6.727402,
     "end_time": "2020-10-10T03:46:13.298310",
     "exception": false,
     "start_time": "2020-10-10T03:46:06.570908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/lish-moa/'\n",
    "train = pd.read_csv(DATA_DIR + 'train_features.csv')\n",
    "targets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n",
    "non_targets = pd.read_csv(DATA_DIR + 'train_targets_nonscored.csv')\n",
    "test = pd.read_csv(DATA_DIR + 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:13.367490Z",
     "iopub.status.busy": "2020-10-10T03:46:13.360111Z",
     "iopub.status.idle": "2020-10-10T03:46:13.370839Z",
     "shell.execute_reply": "2020-10-10T03:46:13.372003Z"
    },
    "papermill": {
     "duration": 0.049495,
     "end_time": "2020-10-10T03:46:13.372284",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.322789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_feats = [ i for i in targets.columns if i != \"sig_id\"]\n",
    "g_feats = [i for i in train.columns if \"g-\" in i]\n",
    "c_feats = [i for i in train.columns if \"c-\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:13.443442Z",
     "iopub.status.busy": "2020-10-10T03:46:13.438380Z",
     "iopub.status.idle": "2020-10-10T03:46:13.557826Z",
     "shell.execute_reply": "2020-10-10T03:46:13.558564Z"
    },
    "papermill": {
     "duration": 0.148424,
     "end_time": "2020-10-10T03:46:13.558764",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.410340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noncons_train_index = train[train.cp_type==\"ctl_vehicle\"].index\n",
    "cons_train_index = train[train.cp_type!=\"ctl_vehicle\"].index\n",
    "noncons_test_index = test[test.cp_type==\"ctl_vehicle\"].index\n",
    "cons_test_index = test[test.cp_type!=\"ctl_vehicle\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:13.619244Z",
     "iopub.status.busy": "2020-10-10T03:46:13.618368Z",
     "iopub.status.idle": "2020-10-10T03:46:13.643681Z",
     "shell.execute_reply": "2020-10-10T03:46:13.644907Z"
    },
    "papermill": {
     "duration": 0.061485,
     "end_time": "2020-10-10T03:46:13.645117",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.583632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test[test.index.isin(cons_test_index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:13.711544Z",
     "iopub.status.busy": "2020-10-10T03:46:13.710673Z",
     "iopub.status.idle": "2020-10-10T03:46:13.747761Z",
     "shell.execute_reply": "2020-10-10T03:46:13.748940Z"
    },
    "papermill": {
     "duration": 0.078138,
     "end_time": "2020-10-10T03:46:13.749150",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.671012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categoricals = [\"cp_dose\"]\n",
    "\n",
    "def encoding(tr, te):\n",
    "    for f in categoricals:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(tr[f]))\n",
    "        tr[f] = lbl.transform(list(tr[f]))\n",
    "        te[f] = lbl.transform(list(te[f])) \n",
    "        \n",
    "    return tr, te\n",
    "\n",
    "train, test = encoding(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:13.818404Z",
     "iopub.status.busy": "2020-10-10T03:46:13.816881Z",
     "iopub.status.idle": "2020-10-10T03:46:13.957389Z",
     "shell.execute_reply": "2020-10-10T03:46:13.954587Z"
    },
    "papermill": {
     "duration": 0.175739,
     "end_time": "2020-10-10T03:46:13.957520",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.781781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 874) (3624, 874)\n"
     ]
    }
   ],
   "source": [
    "def fe(df, remove_features):\n",
    "    tmp = df.copy()\n",
    "    tmp.drop(remove_features, axis=1, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "remove_features = [\"cp_type\" , \"sig_id\"]\n",
    "        \n",
    "train = fe(train, remove_features)\n",
    "test = fe(test, remove_features)\n",
    "    \n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020581,
     "end_time": "2020-10-10T03:46:14.000265",
     "exception": false,
     "start_time": "2020-10-10T03:46:13.979684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:14.052207Z",
     "iopub.status.busy": "2020-10-10T03:46:14.051111Z",
     "iopub.status.idle": "2020-10-10T03:46:14.088296Z",
     "shell.execute_reply": "2020-10-10T03:46:14.088864Z"
    },
    "papermill": {
     "duration": 0.068175,
     "end_time": "2020-10-10T03:46:14.089017",
     "exception": false,
     "start_time": "2020-10-10T03:46:14.020842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('classify',\n",
       "                 MultiOutputClassifier(estimator=XGBClassifier(base_score=None,\n",
       "                                                               booster=None,\n",
       "                                                               colsample_bylevel=None,\n",
       "                                                               colsample_bynode=None,\n",
       "                                                               colsample_bytree=None,\n",
       "                                                               gamma=3.6975,\n",
       "                                                               gpu_id=None,\n",
       "                                                               importance_type='gain',\n",
       "                                                               interaction_constraints=None,\n",
       "                                                               learning_rate=0.0703,\n",
       "                                                               max_delta_step=2.0706,\n",
       "                                                               max_depth=10,\n",
       "                                                               min_child_weight=31.58,\n",
       "                                                               missing=nan,\n",
       "                                                               monotone_constraints=None,\n",
       "                                                               n_estimators=166,\n",
       "                                                               n_jobs=None,\n",
       "                                                               num_parallel_tree=None,\n",
       "                                                               random_state=None,\n",
       "                                                               reg_alpha=None,\n",
       "                                                               reg_lambda=None,\n",
       "                                                               scale_pos_weight=None,\n",
       "                                                               subsample=None,\n",
       "                                                               tree_method='gpu_hist',\n",
       "                                                               validate_parameters=None,\n",
       "                                                               verbosity=None)))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n",
    "\n",
    "clf = Pipeline([('classify', classifier)\n",
    "               ])\n",
    "\n",
    "params = {'classify__estimator__gamma': 3.6975,\n",
    "          'classify__estimator__learning_rate': 0.0703,\n",
    "          'classify__estimator__max_delta_step': 2.0706,\n",
    "          'classify__estimator__max_depth': 10,\n",
    "          'classify__estimator__min_child_weight': 31.5800,\n",
    "          'classify__estimator__n_estimators': 166,\n",
    "         }\n",
    "\n",
    "clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:14.151342Z",
     "iopub.status.busy": "2020-10-10T03:46:14.149332Z",
     "iopub.status.idle": "2020-10-10T03:46:14.152252Z",
     "shell.execute_reply": "2020-10-10T03:46:14.152798Z"
    },
    "papermill": {
     "duration": 0.042837,
     "end_time": "2020-10-10T03:46:14.152943",
     "exception": false,
     "start_time": "2020-10-10T03:46:14.110106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelling_xgb(X, y, X_test, seed):\n",
    "    NFOLDS=5\n",
    "    oof_preds = np.zeros(y.shape)\n",
    "    test_preds = np.zeros((X_test.shape[0], y.shape[1]))\n",
    "    oof_losses = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=seed, shuffle=True)\n",
    "    for fn, (trn_idx, val_idx) in enumerate(mskf.split(X, y)):\n",
    "        print('Starting fold: ', fn)\n",
    "        X_train, X_val = X.iloc[trn_idx,:], X.iloc[val_idx,:].to_numpy()\n",
    "        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx].to_numpy()\n",
    "    \n",
    "        X_train = X_train[X_train.index.isin(cons_train_index)].to_numpy()\n",
    "        y_train = y_train[y_train.index.isin(cons_train_index)].to_numpy()\n",
    "    \n",
    "        clf.fit(X_train, y_train)\n",
    "        val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "        val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "        oof_preds[val_idx] = val_preds\n",
    "    \n",
    "        loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n",
    "        print(loss)\n",
    "        oof_losses.append(loss)\n",
    "        preds = clf.predict_proba(X_test)\n",
    "        preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "        test_preds += preds / NFOLDS\n",
    "    \n",
    "    print(oof_losses)\n",
    "    print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "    print('STD OOF loss across folds', np.std(oof_losses))\n",
    "    return oof_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T03:46:14.206448Z",
     "iopub.status.busy": "2020-10-10T03:46:14.204913Z",
     "iopub.status.idle": "2020-10-10T05:00:26.058042Z",
     "shell.execute_reply": "2020-10-10T05:00:26.048046Z"
    },
    "papermill": {
     "duration": 4451.883786,
     "end_time": "2020-10-10T05:00:26.058198",
     "exception": false,
     "start_time": "2020-10-10T03:46:14.174412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold:  0\n",
      "0.016738832513000318\n",
      "Starting fold:  1\n",
      "0.016908957230028557\n",
      "Starting fold:  2\n",
      "0.016773734659711288\n",
      "Starting fold:  3\n",
      "0.016497753560878757\n",
      "Starting fold:  4\n",
      "0.01667905765085504\n",
      "[0.016738832513000318, 0.016908957230028557, 0.016773734659711288, 0.016497753560878757, 0.01667905765085504]\n",
      "Mean OOF loss across folds 0.01671966712289479\n",
      "STD OOF loss across folds 0.00013417608848077545\n",
      "Starting fold:  0\n",
      "0.01668812742202729\n",
      "Starting fold:  1\n",
      "0.01672661540135191\n",
      "Starting fold:  2\n",
      "0.01671405352510354\n",
      "Starting fold:  3\n",
      "0.016696373422081447\n",
      "Starting fold:  4\n",
      "0.016772998153547904\n",
      "[0.01668812742202729, 0.01672661540135191, 0.01671405352510354, 0.016696373422081447, 0.016772998153547904]\n",
      "Mean OOF loss across folds 0.016719633584822417\n",
      "STD OOF loss across folds 2.9870833562472123e-05\n",
      "Starting fold:  0\n",
      "0.016696372969820933\n",
      "Starting fold:  1\n",
      "0.01672151248200614\n",
      "Starting fold:  2\n",
      "0.016674213856001072\n",
      "Starting fold:  3\n",
      "0.016673970749857346\n",
      "Starting fold:  4\n",
      "0.016831564237686652\n",
      "[0.016696372969820933, 0.01672151248200614, 0.016674213856001072, 0.016673970749857346, 0.016831564237686652]\n",
      "Mean OOF loss across folds 0.016719526859074428\n",
      "STD OOF loss across folds 5.868738676549415e-05\n"
     ]
    }
   ],
   "source": [
    "X = train.copy()\n",
    "y = targets.drop(\"sig_id\", axis=1).copy()\n",
    "X_test = test.copy()\n",
    "\n",
    "seeds = [42,43,44]\n",
    "xgb1_oof = np.zeros(y.shape)\n",
    "xgb1_test = np.zeros((test.shape[0], y.shape[1]))\n",
    "for seed_ in seeds:\n",
    "    ind_preds, ind_test_preds = modelling_xgb(X, y, X_test, seed_)\n",
    "    xgb1_oof += ind_preds / len(seeds)\n",
    "    xgb1_test += ind_test_preds / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:00:26.206346Z",
     "iopub.status.busy": "2020-10-10T05:00:26.204898Z",
     "iopub.status.idle": "2020-10-10T05:00:31.093245Z",
     "shell.execute_reply": "2020-10-10T05:00:31.093767Z"
    },
    "papermill": {
     "duration": 4.966428,
     "end_time": "2020-10-10T05:00:31.093926",
     "exception": false,
     "start_time": "2020-10-10T05:00:26.127498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.016352895291178133\n"
     ]
    }
   ],
   "source": [
    "check_xgb1 = targets.copy()\n",
    "check_xgb1.iloc[:,1:] = xgb1_oof\n",
    "check_xgb1.loc[check_xgb1.index.isin(noncons_train_index),target_feats] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check_xgb1.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.070395,
     "end_time": "2020-10-10T05:00:31.236114",
     "exception": false,
     "start_time": "2020-10-10T05:00:31.165719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:00:31.391673Z",
     "iopub.status.busy": "2020-10-10T05:00:31.386050Z",
     "iopub.status.idle": "2020-10-10T05:00:31.457157Z",
     "shell.execute_reply": "2020-10-10T05:00:31.456436Z"
    },
    "papermill": {
     "duration": 0.15028,
     "end_time": "2020-10-10T05:00:31.457279",
     "exception": false,
     "start_time": "2020-10-10T05:00:31.306999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "batch_size = 128\n",
    "train_epochs = 40\n",
    "n_folds=5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    metrics = []\n",
    "    for i, target in enumerate(target_feats):\n",
    "        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def seed_everything(seed=1234): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class MoaModel(nn.Module):\n",
    "    def __init__(self, num_columns, last_columns_num):\n",
    "        super(MoaModel, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1048))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1048)\n",
    "        self.dropout3 = nn.Dropout(0.6)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1048, last_columns_num))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def modelling_torch(tr, target, te, sample_seed, init_num, last_num, layer):\n",
    "    seed_everything(seed=sample_seed) \n",
    "    X_train = tr.copy()\n",
    "    y_train = target.copy()\n",
    "    X_test = te.copy()\n",
    "    test_len = X_test.shape[0]\n",
    "\n",
    "    mskf=MultilabelStratifiedKFold(n_splits = n_folds, shuffle=True, random_state=2)\n",
    "    models = []\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    X_test = torch.utils.data.TensorDataset(X_test) \n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    oof = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    oof_targets = np.zeros([len(X_train),y_train.shape[1]])\n",
    "    pred_value = np.zeros([test_len, y_train.shape[1]])\n",
    "    scores = []\n",
    "    for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(fold+1))\n",
    "        X_train2 = torch.tensor(X_train[train_index,:], dtype=torch.float32)\n",
    "        y_train2 = torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "\n",
    "        X_valid2 = torch.tensor(X_train[valid_index,:], dtype=torch.float32)\n",
    "        y_valid2 = torch.tensor(y_train[valid_index], dtype=torch.float32)\n",
    "            \n",
    "        clf = MoaModel(init_num, last_num)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(clf.parameters(), lr = 0.001, weight_decay=1e-5) \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        \n",
    "        train = torch.utils.data.TensorDataset(X_train2, y_train2)\n",
    "        valid = torch.utils.data.TensorDataset(X_valid2, y_valid2)\n",
    "        \n",
    "        clf.to(device)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        stop_counts = 0\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            clf.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch) \n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader) \n",
    "    \n",
    "            clf.eval()\n",
    "            avg_val_loss = 0.\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = clf(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        \n",
    "            elapsed_time = time.time() - start_time \n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                stop_counts = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                print('Best model: Epoch {} \\t loss={:.6f} \\t val_loss={:.6f} \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, avg_loss, avg_val_loss, elapsed_time))\n",
    "                torch.save(clf.state_dict(), 'best-model-parameters.pt')\n",
    "            else:\n",
    "                stop_counts += 1\n",
    "         \n",
    "        pred_model = MoaModel(init_num, last_num)\n",
    "        pred_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        pred_model.eval()\n",
    "        \n",
    "        # validation check ----------------\n",
    "        oof_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        target_epoch = np.zeros([X_valid2.size(0), y_train.shape[1]])\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "                y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "                oof_epoch[i * batch_size:(i+1) * batch_size,:] = y_pred.cpu().numpy()\n",
    "                target_epoch[i * batch_size:(i+1) * batch_size,:] = y_batch.cpu().numpy()\n",
    "        print(\"Fold {} log loss: {}\".format(fold+1, mean_log_loss(target_epoch, oof_epoch)))\n",
    "        scores.append(mean_log_loss(target_epoch, oof_epoch))\n",
    "        oof[valid_index,:] = oof_epoch\n",
    "        oof_targets[valid_index,:] = target_epoch\n",
    "        #-----------------------------------\n",
    "        \n",
    "        # test predcition --------------\n",
    "        test_preds = np.zeros([test_len, y_train.shape[1]])\n",
    "        for i, (x_batch,) in enumerate(test_loader): \n",
    "            y_pred = pred_model(x_batch).sigmoid().detach()\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred.cpu().numpy()\n",
    "        pred_value += test_preds / n_folds\n",
    "        # ------------------------------\n",
    "        \n",
    "    print(\"Seed {}\".format(seed_))\n",
    "    for i, ele in enumerate(scores):\n",
    "        print(\"Fold {} log loss: {}\".format(i+1, scores[i]))\n",
    "    print(\"Std of log loss: {}\".format(np.std(scores)))\n",
    "    print(\"Total log loss: {}\".format(mean_log_loss(oof_targets, oof)))\n",
    "    \n",
    "    return oof, oof_targets, pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:00:31.613842Z",
     "iopub.status.busy": "2020-10-10T05:00:31.612432Z",
     "iopub.status.idle": "2020-10-10T05:18:10.824529Z",
     "shell.execute_reply": "2020-10-10T05:18:10.823480Z"
    },
    "papermill": {
     "duration": 1059.296096,
     "end_time": "2020-10-10T05:18:10.824669",
     "exception": false,
     "start_time": "2020-10-10T05:00:31.528573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412591 \t val_loss=0.077221 \t time=1.62s\n",
      "Best model: Epoch 2 \t loss=0.048580 \t val_loss=0.028151 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.027003 \t val_loss=0.022851 \t time=0.87s\n",
      "Best model: Epoch 4 \t loss=0.023460 \t val_loss=0.021035 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021372 \t val_loss=0.019930 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020710 \t val_loss=0.019312 \t time=0.84s\n",
      "Best model: Epoch 7 \t loss=0.020091 \t val_loss=0.019111 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019413 \t val_loss=0.018614 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.018994 \t val_loss=0.018114 \t time=1.14s\n",
      "Best model: Epoch 10 \t loss=0.018733 \t val_loss=0.017949 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018251 \t val_loss=0.017597 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017651 \t val_loss=0.017239 \t time=0.87s\n",
      "Best model: Epoch 14 \t loss=0.017350 \t val_loss=0.017176 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017291 \t val_loss=0.017116 \t time=1.19s\n",
      "Best model: Epoch 16 \t loss=0.017107 \t val_loss=0.016993 \t time=0.85s\n",
      "Best model: Epoch 17 \t loss=0.016768 \t val_loss=0.016788 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016622 \t val_loss=0.016715 \t time=0.86s\n",
      "Best model: Epoch 21 \t loss=0.016251 \t val_loss=0.016576 \t time=1.08s\n",
      "Best model: Epoch 22 \t loss=0.016066 \t val_loss=0.016500 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.015943 \t val_loss=0.016470 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015734 \t val_loss=0.016424 \t time=0.87s\n",
      "Best model: Epoch 28 \t loss=0.015489 \t val_loss=0.016377 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.015327 \t val_loss=0.016313 \t time=0.94s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014506 \t val_loss=0.016155 \t time=1.26s\n",
      "Best model: Epoch 37 \t loss=0.014171 \t val_loss=0.016128 \t time=0.97s\n",
      "Best model: Epoch 38 \t loss=0.014019 \t val_loss=0.016098 \t time=0.86s\n",
      "Best model: Epoch 39 \t loss=0.013838 \t val_loss=0.016042 \t time=0.91s\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414173 \t val_loss=0.076114 \t time=1.11s\n",
      "Best model: Epoch 2 \t loss=0.048296 \t val_loss=0.029272 \t time=0.91s\n",
      "Best model: Epoch 3 \t loss=0.027405 \t val_loss=0.022898 \t time=0.90s\n",
      "Best model: Epoch 4 \t loss=0.023185 \t val_loss=0.021045 \t time=0.92s\n",
      "Best model: Epoch 5 \t loss=0.021348 \t val_loss=0.019655 \t time=1.21s\n",
      "Best model: Epoch 6 \t loss=0.020497 \t val_loss=0.019208 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.019868 \t val_loss=0.018997 \t time=0.87s\n",
      "Best model: Epoch 8 \t loss=0.019279 \t val_loss=0.018520 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018965 \t val_loss=0.017982 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018675 \t val_loss=0.017759 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018200 \t val_loss=0.017652 \t time=0.97s\n",
      "Best model: Epoch 12 \t loss=0.017911 \t val_loss=0.017377 \t time=1.03s\n",
      "Best model: Epoch 13 \t loss=0.017592 \t val_loss=0.017221 \t time=0.85s\n",
      "Best model: Epoch 15 \t loss=0.017180 \t val_loss=0.017090 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017055 \t val_loss=0.016951 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016813 \t val_loss=0.016760 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016564 \t val_loss=0.016719 \t time=0.91s\n",
      "Best model: Epoch 19 \t loss=0.016340 \t val_loss=0.016593 \t time=0.87s\n",
      "Best model: Epoch 20 \t loss=0.016182 \t val_loss=0.016538 \t time=0.87s\n",
      "Best model: Epoch 21 \t loss=0.016051 \t val_loss=0.016499 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016487 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015813 \t val_loss=0.016389 \t time=1.00s\n",
      "Best model: Epoch 25 \t loss=0.015707 \t val_loss=0.016370 \t time=0.86s\n",
      "Best model: Epoch 26 \t loss=0.015672 \t val_loss=0.016358 \t time=0.88s\n",
      "Best model: Epoch 27 \t loss=0.015543 \t val_loss=0.016294 \t time=0.87s\n",
      "Best model: Epoch 28 \t loss=0.015430 \t val_loss=0.016279 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015341 \t val_loss=0.016245 \t time=0.86s\n",
      "Best model: Epoch 30 \t loss=0.015306 \t val_loss=0.016228 \t time=0.88s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014418 \t val_loss=0.016175 \t time=0.95s\n",
      "Best model: Epoch 36 \t loss=0.014187 \t val_loss=0.016067 \t time=1.04s\n",
      "Best model: Epoch 37 \t loss=0.013917 \t val_loss=0.016034 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.013762 \t val_loss=0.016030 \t time=0.86s\n",
      "Best model: Epoch 39 \t loss=0.013603 \t val_loss=0.015983 \t time=1.19s\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.413986 \t val_loss=0.076896 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.049192 \t val_loss=0.028994 \t time=0.88s\n",
      "Best model: Epoch 3 \t loss=0.027106 \t val_loss=0.022515 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023059 \t val_loss=0.020760 \t time=1.11s\n",
      "Best model: Epoch 5 \t loss=0.021690 \t val_loss=0.020538 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020625 \t val_loss=0.019409 \t time=0.90s\n",
      "Best model: Epoch 7 \t loss=0.019750 \t val_loss=0.018819 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019329 \t val_loss=0.018315 \t time=0.86s\n",
      "Best model: Epoch 9 \t loss=0.019057 \t val_loss=0.018125 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018609 \t val_loss=0.017987 \t time=0.88s\n",
      "Best model: Epoch 11 \t loss=0.018311 \t val_loss=0.017812 \t time=0.90s\n",
      "Best model: Epoch 12 \t loss=0.018039 \t val_loss=0.017600 \t time=0.99s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017359 \t time=1.16s\n",
      "Best model: Epoch 14 \t loss=0.017500 \t val_loss=0.017197 \t time=0.92s\n",
      "Best model: Epoch 15 \t loss=0.017385 \t val_loss=0.017085 \t time=1.11s\n",
      "Best model: Epoch 16 \t loss=0.017066 \t val_loss=0.016969 \t time=0.89s\n",
      "Best model: Epoch 17 \t loss=0.016860 \t val_loss=0.016758 \t time=1.16s\n",
      "Best model: Epoch 18 \t loss=0.016605 \t val_loss=0.016644 \t time=0.94s\n",
      "Best model: Epoch 19 \t loss=0.016521 \t val_loss=0.016627 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016267 \t val_loss=0.016498 \t time=0.89s\n",
      "Best model: Epoch 23 \t loss=0.015995 \t val_loss=0.016465 \t time=0.91s\n",
      "Best model: Epoch 24 \t loss=0.015823 \t val_loss=0.016417 \t time=0.86s\n",
      "Best model: Epoch 25 \t loss=0.015703 \t val_loss=0.016356 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015659 \t val_loss=0.016288 \t time=1.15s\n",
      "Best model: Epoch 30 \t loss=0.015393 \t val_loss=0.016223 \t time=0.91s\n",
      "Best model: Epoch 33 \t loss=0.015215 \t val_loss=0.016222 \t time=0.85s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014525 \t val_loss=0.016054 \t time=0.86s\n",
      "Best model: Epoch 36 \t loss=0.014278 \t val_loss=0.015991 \t time=0.86s\n",
      "Best model: Epoch 37 \t loss=0.014126 \t val_loss=0.015968 \t time=0.98s\n",
      "Best model: Epoch 38 \t loss=0.013885 \t val_loss=0.015964 \t time=1.01s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015933 \t time=0.88s\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414550 \t val_loss=0.077203 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.050157 \t val_loss=0.030820 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.028654 \t val_loss=0.022643 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.022900 \t val_loss=0.020759 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021488 \t val_loss=0.019617 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020599 \t val_loss=0.019256 \t time=1.01s\n",
      "Best model: Epoch 7 \t loss=0.020185 \t val_loss=0.018790 \t time=0.95s\n",
      "Best model: Epoch 8 \t loss=0.019538 \t val_loss=0.018186 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019437 \t val_loss=0.017978 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018881 \t val_loss=0.017777 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018440 \t val_loss=0.017634 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.018149 \t val_loss=0.017331 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017822 \t val_loss=0.017247 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017593 \t val_loss=0.017178 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017360 \t val_loss=0.017018 \t time=0.90s\n",
      "Best model: Epoch 16 \t loss=0.017128 \t val_loss=0.016771 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016888 \t val_loss=0.016649 \t time=1.15s\n",
      "Best model: Epoch 19 \t loss=0.016572 \t val_loss=0.016569 \t time=0.95s\n",
      "Best model: Epoch 20 \t loss=0.016374 \t val_loss=0.016445 \t time=1.21s\n",
      "Best model: Epoch 22 \t loss=0.016243 \t val_loss=0.016368 \t time=0.89s\n",
      "Best model: Epoch 23 \t loss=0.016123 \t val_loss=0.016358 \t time=0.92s\n",
      "Best model: Epoch 25 \t loss=0.015849 \t val_loss=0.016256 \t time=0.87s\n",
      "Best model: Epoch 27 \t loss=0.015719 \t val_loss=0.016254 \t time=0.90s\n",
      "Best model: Epoch 28 \t loss=0.015601 \t val_loss=0.016216 \t time=0.93s\n",
      "Best model: Epoch 29 \t loss=0.015491 \t val_loss=0.016183 \t time=1.10s\n",
      "Best model: Epoch 30 \t loss=0.015505 \t val_loss=0.016177 \t time=0.85s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 35 \t loss=0.014550 \t val_loss=0.016025 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.014275 \t val_loss=0.015951 \t time=0.89s\n",
      "Best model: Epoch 37 \t loss=0.014051 \t val_loss=0.015898 \t time=1.07s\n",
      "Best model: Epoch 39 \t loss=0.013728 \t val_loss=0.015897 \t time=0.89s\n",
      "Best model: Epoch 40 \t loss=0.013633 \t val_loss=0.015858 \t time=1.08s\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412187 \t val_loss=0.088095 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.049344 \t val_loss=0.028300 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027639 \t val_loss=0.022942 \t time=0.87s\n",
      "Best model: Epoch 4 \t loss=0.023355 \t val_loss=0.020933 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021415 \t val_loss=0.019995 \t time=0.85s\n",
      "Best model: Epoch 6 \t loss=0.020525 \t val_loss=0.019669 \t time=0.85s\n",
      "Best model: Epoch 7 \t loss=0.019796 \t val_loss=0.018820 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019289 \t val_loss=0.018472 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019185 \t val_loss=0.018166 \t time=1.10s\n",
      "Best model: Epoch 11 \t loss=0.018588 \t val_loss=0.017788 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.018107 \t val_loss=0.017631 \t time=0.96s\n",
      "Best model: Epoch 13 \t loss=0.017794 \t val_loss=0.017333 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017391 \t val_loss=0.017204 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017268 \t val_loss=0.017087 \t time=0.89s\n",
      "Best model: Epoch 16 \t loss=0.017022 \t val_loss=0.016977 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016798 \t val_loss=0.016941 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016580 \t val_loss=0.016778 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016403 \t val_loss=0.016737 \t time=0.90s\n",
      "Best model: Epoch 21 \t loss=0.016202 \t val_loss=0.016591 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016031 \t val_loss=0.016581 \t time=0.89s\n",
      "Best model: Epoch 23 \t loss=0.015834 \t val_loss=0.016542 \t time=0.86s\n",
      "Best model: Epoch 24 \t loss=0.015865 \t val_loss=0.016475 \t time=0.90s\n",
      "Best model: Epoch 27 \t loss=0.015609 \t val_loss=0.016414 \t time=0.85s\n",
      "Best model: Epoch 30 \t loss=0.015342 \t val_loss=0.016413 \t time=0.87s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014680 \t val_loss=0.016273 \t time=1.12s\n",
      "Best model: Epoch 33 \t loss=0.014344 \t val_loss=0.016219 \t time=0.86s\n",
      "Best model: Epoch 34 \t loss=0.014130 \t val_loss=0.016189 \t time=0.87s\n",
      "Best model: Epoch 35 \t loss=0.014023 \t val_loss=0.016185 \t time=0.85s\n",
      "Best model: Epoch 36 \t loss=0.013851 \t val_loss=0.016143 \t time=0.86s\n",
      "Best model: Epoch 37 \t loss=0.013768 \t val_loss=0.016134 \t time=0.88s\n",
      "Best model: Epoch 40 \t loss=0.013329 \t val_loss=0.016125 \t time=0.89s\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Seed 0\n",
      "Fold 1 log loss: 0.016144690589180097\n",
      "Fold 2 log loss: 0.016044396281569673\n",
      "Fold 3 log loss: 0.015932693904937074\n",
      "Fold 4 log loss: 0.01591978898403236\n",
      "Fold 5 log loss: 0.016124345038051664\n",
      "Std of log loss: 9.362313645021518e-05\n",
      "Total log loss: 0.016033178295101806\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.412054 \t val_loss=0.076325 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.048526 \t val_loss=0.028851 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.026873 \t val_loss=0.023145 \t time=0.84s\n",
      "Best model: Epoch 4 \t loss=0.023518 \t val_loss=0.020954 \t time=0.98s\n",
      "Best model: Epoch 5 \t loss=0.021069 \t val_loss=0.019802 \t time=1.10s\n",
      "Best model: Epoch 6 \t loss=0.020366 \t val_loss=0.019167 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.019715 \t val_loss=0.018802 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019131 \t val_loss=0.018499 \t time=1.08s\n",
      "Best model: Epoch 9 \t loss=0.018863 \t val_loss=0.018232 \t time=1.19s\n",
      "Best model: Epoch 10 \t loss=0.018576 \t val_loss=0.017987 \t time=0.91s\n",
      "Best model: Epoch 11 \t loss=0.018388 \t val_loss=0.017902 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.018011 \t val_loss=0.017349 \t time=0.91s\n",
      "Best model: Epoch 13 \t loss=0.017622 \t val_loss=0.017302 \t time=0.89s\n",
      "Best model: Epoch 14 \t loss=0.017534 \t val_loss=0.017155 \t time=0.93s\n",
      "Best model: Epoch 16 \t loss=0.017059 \t val_loss=0.016986 \t time=0.85s\n",
      "Best model: Epoch 17 \t loss=0.016869 \t val_loss=0.016890 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016664 \t val_loss=0.016821 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016417 \t val_loss=0.016683 \t time=0.94s\n",
      "Best model: Epoch 20 \t loss=0.016225 \t val_loss=0.016591 \t time=1.04s\n",
      "Best model: Epoch 22 \t loss=0.015986 \t val_loss=0.016465 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015784 \t val_loss=0.016374 \t time=0.85s\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 29 \t loss=0.014782 \t val_loss=0.016199 \t time=0.88s\n",
      "Best model: Epoch 30 \t loss=0.014521 \t val_loss=0.016131 \t time=0.88s\n",
      "Best model: Epoch 31 \t loss=0.014358 \t val_loss=0.016063 \t time=1.69s\n",
      "Best model: Epoch 32 \t loss=0.014177 \t val_loss=0.016035 \t time=0.88s\n",
      "Best model: Epoch 34 \t loss=0.013939 \t val_loss=0.016024 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.013725 \t val_loss=0.016020 \t time=0.86s\n",
      "Best model: Epoch 36 \t loss=0.013646 \t val_loss=0.016016 \t time=0.87s\n",
      "Best model: Epoch 40 \t loss=0.013190 \t val_loss=0.015981 \t time=0.87s\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.414855 \t val_loss=0.075799 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.048749 \t val_loss=0.029421 \t time=0.85s\n",
      "Best model: Epoch 3 \t loss=0.027499 \t val_loss=0.023112 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020927 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021469 \t val_loss=0.019786 \t time=0.85s\n",
      "Best model: Epoch 6 \t loss=0.020582 \t val_loss=0.019286 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.020079 \t val_loss=0.018804 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019293 \t val_loss=0.018348 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019088 \t val_loss=0.018044 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018684 \t val_loss=0.018009 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.018275 \t val_loss=0.017549 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.017908 \t val_loss=0.017357 \t time=1.06s\n",
      "Best model: Epoch 13 \t loss=0.017698 \t val_loss=0.017252 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017397 \t val_loss=0.017156 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017207 \t val_loss=0.016991 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.016897 \t val_loss=0.016768 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016568 \t val_loss=0.016672 \t time=0.88s\n",
      "Best model: Epoch 19 \t loss=0.016429 \t val_loss=0.016570 \t time=0.87s\n",
      "Best model: Epoch 20 \t loss=0.016223 \t val_loss=0.016533 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016095 \t val_loss=0.016416 \t time=0.98s\n",
      "Best model: Epoch 23 \t loss=0.015917 \t val_loss=0.016410 \t time=1.20s\n",
      "Best model: Epoch 24 \t loss=0.015726 \t val_loss=0.016333 \t time=1.27s\n",
      "Best model: Epoch 27 \t loss=0.015412 \t val_loss=0.016291 \t time=1.14s\n",
      "Best model: Epoch 29 \t loss=0.015315 \t val_loss=0.016222 \t time=0.86s\n",
      "Best model: Epoch 32 \t loss=0.015167 \t val_loss=0.016173 \t time=0.92s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014376 \t val_loss=0.016120 \t time=0.86s\n",
      "Best model: Epoch 38 \t loss=0.014005 \t val_loss=0.016016 \t time=0.84s\n",
      "Best model: Epoch 40 \t loss=0.013573 \t val_loss=0.016004 \t time=0.86s\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.415098 \t val_loss=0.080733 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.048767 \t val_loss=0.028509 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.026784 \t val_loss=0.023150 \t time=1.14s\n",
      "Best model: Epoch 4 \t loss=0.023161 \t val_loss=0.021295 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021450 \t val_loss=0.019731 \t time=0.87s\n",
      "Best model: Epoch 6 \t loss=0.020675 \t val_loss=0.019469 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.020227 \t val_loss=0.019188 \t time=0.87s\n",
      "Best model: Epoch 8 \t loss=0.019444 \t val_loss=0.018675 \t time=0.92s\n",
      "Best model: Epoch 9 \t loss=0.019434 \t val_loss=0.018288 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.018643 \t val_loss=0.018101 \t time=1.02s\n",
      "Best model: Epoch 11 \t loss=0.018215 \t val_loss=0.017674 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.017846 \t val_loss=0.017324 \t time=1.11s\n",
      "Best model: Epoch 15 \t loss=0.017431 \t val_loss=0.017053 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017095 \t val_loss=0.016751 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016675 \t val_loss=0.016733 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016288 \t val_loss=0.016438 \t time=0.85s\n",
      "Best model: Epoch 22 \t loss=0.016149 \t val_loss=0.016371 \t time=0.85s\n",
      "Best model: Epoch 24 \t loss=0.015809 \t val_loss=0.016315 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015760 \t val_loss=0.016313 \t time=0.86s\n",
      "Best model: Epoch 26 \t loss=0.015703 \t val_loss=0.016287 \t time=1.12s\n",
      "Best model: Epoch 27 \t loss=0.015558 \t val_loss=0.016223 \t time=0.89s\n",
      "Best model: Epoch 29 \t loss=0.015499 \t val_loss=0.016220 \t time=0.85s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014648 \t val_loss=0.016064 \t time=0.86s\n",
      "Best model: Epoch 35 \t loss=0.014381 \t val_loss=0.016009 \t time=0.87s\n",
      "Best model: Epoch 36 \t loss=0.014148 \t val_loss=0.015964 \t time=0.86s\n",
      "Best model: Epoch 37 \t loss=0.014036 \t val_loss=0.015958 \t time=0.85s\n",
      "Best model: Epoch 38 \t loss=0.013845 \t val_loss=0.015945 \t time=1.10s\n",
      "Best model: Epoch 39 \t loss=0.013695 \t val_loss=0.015936 \t time=0.91s\n",
      "Best model: Epoch 40 \t loss=0.013596 \t val_loss=0.015928 \t time=0.88s\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.413201 \t val_loss=0.077351 \t time=1.27s\n",
      "Best model: Epoch 2 \t loss=0.047845 \t val_loss=0.027291 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.027879 \t val_loss=0.022880 \t time=1.13s\n",
      "Best model: Epoch 4 \t loss=0.023083 \t val_loss=0.021099 \t time=1.23s\n",
      "Best model: Epoch 5 \t loss=0.021983 \t val_loss=0.020159 \t time=0.95s\n",
      "Best model: Epoch 6 \t loss=0.020838 \t val_loss=0.019249 \t time=1.03s\n",
      "Best model: Epoch 7 \t loss=0.019902 \t val_loss=0.018613 \t time=0.90s\n",
      "Best model: Epoch 8 \t loss=0.019397 \t val_loss=0.018178 \t time=0.94s\n",
      "Best model: Epoch 9 \t loss=0.018991 \t val_loss=0.017956 \t time=0.91s\n",
      "Best model: Epoch 10 \t loss=0.018744 \t val_loss=0.017936 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018373 \t val_loss=0.017435 \t time=0.91s\n",
      "Best model: Epoch 12 \t loss=0.017988 \t val_loss=0.017253 \t time=0.93s\n",
      "Best model: Epoch 13 \t loss=0.017706 \t val_loss=0.017092 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017475 \t val_loss=0.016959 \t time=0.95s\n",
      "Best model: Epoch 15 \t loss=0.017304 \t val_loss=0.016845 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.016985 \t val_loss=0.016721 \t time=0.86s\n",
      "Best model: Epoch 17 \t loss=0.016821 \t val_loss=0.016602 \t time=1.19s\n",
      "Best model: Epoch 18 \t loss=0.016629 \t val_loss=0.016580 \t time=0.92s\n",
      "Best model: Epoch 19 \t loss=0.016474 \t val_loss=0.016482 \t time=0.94s\n",
      "Best model: Epoch 20 \t loss=0.016306 \t val_loss=0.016427 \t time=0.93s\n",
      "Best model: Epoch 21 \t loss=0.016230 \t val_loss=0.016412 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016012 \t val_loss=0.016284 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.015916 \t val_loss=0.016250 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015605 \t val_loss=0.016187 \t time=0.85s\n",
      "Best model: Epoch 29 \t loss=0.015414 \t val_loss=0.016160 \t time=1.00s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014523 \t val_loss=0.016007 \t time=1.00s\n",
      "Best model: Epoch 35 \t loss=0.014326 \t val_loss=0.015953 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.014128 \t val_loss=0.015927 \t time=0.88s\n",
      "Best model: Epoch 37 \t loss=0.013961 \t val_loss=0.015913 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.013800 \t val_loss=0.015902 \t time=1.19s\n",
      "Best model: Epoch 39 \t loss=0.013707 \t val_loss=0.015887 \t time=0.91s\n",
      "Best model: Epoch 40 \t loss=0.013494 \t val_loss=0.015874 \t time=1.12s\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.414129 \t val_loss=0.077634 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.048715 \t val_loss=0.028425 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027408 \t val_loss=0.022374 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023334 \t val_loss=0.020878 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021927 \t val_loss=0.020042 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020496 \t val_loss=0.019306 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.019769 \t val_loss=0.018887 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019723 \t val_loss=0.018739 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018921 \t val_loss=0.018396 \t time=1.15s\n",
      "Best model: Epoch 10 \t loss=0.018596 \t val_loss=0.017923 \t time=0.87s\n",
      "Best model: Epoch 11 \t loss=0.018246 \t val_loss=0.017664 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.018002 \t val_loss=0.017398 \t time=0.86s\n",
      "Best model: Epoch 13 \t loss=0.017603 \t val_loss=0.017307 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017409 \t val_loss=0.017181 \t time=0.88s\n",
      "Best model: Epoch 16 \t loss=0.017084 \t val_loss=0.017008 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016757 \t val_loss=0.016826 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016483 \t val_loss=0.016648 \t time=0.87s\n",
      "Best model: Epoch 20 \t loss=0.016282 \t val_loss=0.016632 \t time=1.18s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016561 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.015977 \t val_loss=0.016516 \t time=1.04s\n",
      "Best model: Epoch 24 \t loss=0.015867 \t val_loss=0.016485 \t time=0.89s\n",
      "Best model: Epoch 25 \t loss=0.015713 \t val_loss=0.016440 \t time=1.18s\n",
      "Best model: Epoch 28 \t loss=0.015600 \t val_loss=0.016433 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015441 \t val_loss=0.016363 \t time=1.34s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014609 \t val_loss=0.016205 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.014249 \t val_loss=0.016198 \t time=0.87s\n",
      "Best model: Epoch 36 \t loss=0.014074 \t val_loss=0.016103 \t time=0.90s\n",
      "Best model: Epoch 37 \t loss=0.013971 \t val_loss=0.016069 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.013754 \t val_loss=0.016059 \t time=0.87s\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Seed 1\n",
      "Fold 1 log loss: 0.016079840868431054\n",
      "Fold 2 log loss: 0.016064742658142592\n",
      "Fold 3 log loss: 0.01592049020117655\n",
      "Fold 4 log loss: 0.01592111042134359\n",
      "Fold 5 log loss: 0.016063688369179374\n",
      "Std of log loss: 7.303465552417108e-05\n",
      "Total log loss: 0.01600996956097101\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.414243 \t val_loss=0.075401 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.049196 \t val_loss=0.028064 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.026940 \t val_loss=0.022525 \t time=0.85s\n",
      "Best model: Epoch 4 \t loss=0.023264 \t val_loss=0.021226 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021407 \t val_loss=0.019807 \t time=0.85s\n",
      "Best model: Epoch 6 \t loss=0.020540 \t val_loss=0.019256 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.019968 \t val_loss=0.018993 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019501 \t val_loss=0.018829 \t time=1.12s\n",
      "Best model: Epoch 9 \t loss=0.019202 \t val_loss=0.018372 \t time=1.08s\n",
      "Best model: Epoch 10 \t loss=0.019079 \t val_loss=0.018125 \t time=0.85s\n",
      "Best model: Epoch 11 \t loss=0.018344 \t val_loss=0.017942 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.018138 \t val_loss=0.017556 \t time=0.86s\n",
      "Best model: Epoch 13 \t loss=0.017707 \t val_loss=0.017321 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017455 \t val_loss=0.017192 \t time=0.89s\n",
      "Best model: Epoch 15 \t loss=0.017177 \t val_loss=0.017051 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017017 \t val_loss=0.016919 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016662 \t val_loss=0.016903 \t time=1.02s\n",
      "Best model: Epoch 19 \t loss=0.016553 \t val_loss=0.016804 \t time=1.00s\n",
      "Best model: Epoch 20 \t loss=0.016457 \t val_loss=0.016643 \t time=0.95s\n",
      "Best model: Epoch 21 \t loss=0.016250 \t val_loss=0.016637 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016172 \t val_loss=0.016621 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.016010 \t val_loss=0.016544 \t time=0.86s\n",
      "Best model: Epoch 25 \t loss=0.015790 \t val_loss=0.016434 \t time=0.89s\n",
      "Best model: Epoch 26 \t loss=0.015725 \t val_loss=0.016409 \t time=0.87s\n",
      "Best model: Epoch 28 \t loss=0.015650 \t val_loss=0.016400 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015497 \t val_loss=0.016379 \t time=0.85s\n",
      "Best model: Epoch 33 \t loss=0.015218 \t val_loss=0.016377 \t time=0.87s\n",
      "Best model: Epoch 35 \t loss=0.015280 \t val_loss=0.016351 \t time=0.86s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014390 \t val_loss=0.016189 \t time=0.86s\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.412084 \t val_loss=0.076539 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.049085 \t val_loss=0.028798 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.026930 \t val_loss=0.022483 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.023605 \t val_loss=0.021048 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021499 \t val_loss=0.019946 \t time=0.94s\n",
      "Best model: Epoch 6 \t loss=0.020550 \t val_loss=0.019083 \t time=0.89s\n",
      "Best model: Epoch 7 \t loss=0.019963 \t val_loss=0.018862 \t time=0.94s\n",
      "Best model: Epoch 8 \t loss=0.019379 \t val_loss=0.018356 \t time=1.03s\n",
      "Best model: Epoch 9 \t loss=0.018966 \t val_loss=0.018103 \t time=1.13s\n",
      "Best model: Epoch 10 \t loss=0.018577 \t val_loss=0.017829 \t time=0.95s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017426 \t time=0.94s\n",
      "Best model: Epoch 13 \t loss=0.017755 \t val_loss=0.017169 \t time=0.91s\n",
      "Best model: Epoch 15 \t loss=0.017197 \t val_loss=0.016995 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.016993 \t val_loss=0.016966 \t time=0.88s\n",
      "Best model: Epoch 17 \t loss=0.016980 \t val_loss=0.016896 \t time=0.85s\n",
      "Best model: Epoch 18 \t loss=0.016810 \t val_loss=0.016787 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016601 \t val_loss=0.016749 \t time=0.86s\n",
      "Best model: Epoch 20 \t loss=0.016273 \t val_loss=0.016588 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016209 \t val_loss=0.016517 \t time=0.85s\n",
      "Best model: Epoch 23 \t loss=0.015865 \t val_loss=0.016396 \t time=0.85s\n",
      "Best model: Epoch 25 \t loss=0.015726 \t val_loss=0.016323 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015473 \t val_loss=0.016273 \t time=0.88s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014607 \t val_loss=0.016130 \t time=1.29s\n",
      "Best model: Epoch 35 \t loss=0.014302 \t val_loss=0.016075 \t time=0.87s\n",
      "Best model: Epoch 36 \t loss=0.014098 \t val_loss=0.016072 \t time=0.88s\n",
      "Best model: Epoch 37 \t loss=0.014008 \t val_loss=0.016029 \t time=0.85s\n",
      "Best model: Epoch 38 \t loss=0.013877 \t val_loss=0.016023 \t time=0.88s\n",
      "Best model: Epoch 40 \t loss=0.013495 \t val_loss=0.016019 \t time=0.91s\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.411402 \t val_loss=0.077279 \t time=1.09s\n",
      "Best model: Epoch 2 \t loss=0.049547 \t val_loss=0.029069 \t time=0.93s\n",
      "Best model: Epoch 3 \t loss=0.027701 \t val_loss=0.022526 \t time=1.04s\n",
      "Best model: Epoch 4 \t loss=0.022974 \t val_loss=0.020914 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021929 \t val_loss=0.020299 \t time=0.85s\n",
      "Best model: Epoch 6 \t loss=0.020641 \t val_loss=0.019507 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.020131 \t val_loss=0.019061 \t time=0.92s\n",
      "Best model: Epoch 9 \t loss=0.019336 \t val_loss=0.018609 \t time=0.87s\n",
      "Best model: Epoch 10 \t loss=0.018623 \t val_loss=0.017850 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.018119 \t val_loss=0.017799 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017806 \t val_loss=0.017264 \t time=1.18s\n",
      "Best model: Epoch 15 \t loss=0.017459 \t val_loss=0.017123 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017064 \t val_loss=0.016860 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016741 \t val_loss=0.016715 \t time=0.90s\n",
      "Best model: Epoch 19 \t loss=0.016492 \t val_loss=0.016638 \t time=1.14s\n",
      "Best model: Epoch 20 \t loss=0.016265 \t val_loss=0.016467 \t time=0.91s\n",
      "Best model: Epoch 21 \t loss=0.016243 \t val_loss=0.016437 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.016021 \t val_loss=0.016410 \t time=0.89s\n",
      "Best model: Epoch 24 \t loss=0.015786 \t val_loss=0.016352 \t time=1.12s\n",
      "Best model: Epoch 26 \t loss=0.015692 \t val_loss=0.016319 \t time=0.87s\n",
      "Best model: Epoch 27 \t loss=0.015599 \t val_loss=0.016292 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015427 \t val_loss=0.016235 \t time=0.86s\n",
      "Best model: Epoch 30 \t loss=0.015397 \t val_loss=0.016205 \t time=0.87s\n",
      "Best model: Epoch 31 \t loss=0.015269 \t val_loss=0.016160 \t time=0.86s\n",
      "Best model: Epoch 33 \t loss=0.015170 \t val_loss=0.016138 \t time=0.94s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 38 \t loss=0.014412 \t val_loss=0.016074 \t time=0.86s\n",
      "Best model: Epoch 39 \t loss=0.014103 \t val_loss=0.016024 \t time=0.86s\n",
      "Best model: Epoch 40 \t loss=0.013940 \t val_loss=0.015989 \t time=0.86s\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414447 \t val_loss=0.083103 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.048190 \t val_loss=0.029241 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.028207 \t val_loss=0.022736 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023313 \t val_loss=0.020537 \t time=1.10s\n",
      "Best model: Epoch 5 \t loss=0.021788 \t val_loss=0.019921 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020538 \t val_loss=0.019117 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.020011 \t val_loss=0.018677 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019755 \t val_loss=0.018538 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.019270 \t val_loss=0.017984 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.018757 \t val_loss=0.017874 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018413 \t val_loss=0.017491 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.018074 \t val_loss=0.017276 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017728 \t val_loss=0.017107 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.017003 \t time=1.11s\n",
      "Best model: Epoch 15 \t loss=0.017251 \t val_loss=0.016845 \t time=0.99s\n",
      "Best model: Epoch 16 \t loss=0.017310 \t val_loss=0.016813 \t time=1.00s\n",
      "Best model: Epoch 17 \t loss=0.017063 \t val_loss=0.016771 \t time=0.88s\n",
      "Best model: Epoch 18 \t loss=0.016799 \t val_loss=0.016573 \t time=0.87s\n",
      "Best model: Epoch 19 \t loss=0.016628 \t val_loss=0.016536 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016486 \t val_loss=0.016442 \t time=0.86s\n",
      "Best model: Epoch 21 \t loss=0.016320 \t val_loss=0.016359 \t time=0.87s\n",
      "Best model: Epoch 22 \t loss=0.016065 \t val_loss=0.016320 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.016071 \t val_loss=0.016278 \t time=0.90s\n",
      "Best model: Epoch 24 \t loss=0.015930 \t val_loss=0.016261 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015693 \t val_loss=0.016257 \t time=0.99s\n",
      "Best model: Epoch 28 \t loss=0.015580 \t val_loss=0.016195 \t time=0.85s\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 33 \t loss=0.014672 \t val_loss=0.016048 \t time=0.89s\n",
      "Best model: Epoch 34 \t loss=0.014376 \t val_loss=0.015966 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.014214 \t val_loss=0.015932 \t time=0.88s\n",
      "Best model: Epoch 37 \t loss=0.013900 \t val_loss=0.015920 \t time=1.11s\n",
      "Best model: Epoch 40 \t loss=0.013575 \t val_loss=0.015914 \t time=0.90s\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413087 \t val_loss=0.073699 \t time=0.87s\n",
      "Best model: Epoch 2 \t loss=0.049084 \t val_loss=0.029309 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.028082 \t val_loss=0.023504 \t time=1.13s\n",
      "Best model: Epoch 4 \t loss=0.023135 \t val_loss=0.020912 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021648 \t val_loss=0.020058 \t time=0.88s\n",
      "Best model: Epoch 6 \t loss=0.020544 \t val_loss=0.019231 \t time=0.88s\n",
      "Best model: Epoch 7 \t loss=0.019869 \t val_loss=0.018961 \t time=1.10s\n",
      "Best model: Epoch 8 \t loss=0.019517 \t val_loss=0.018423 \t time=0.85s\n",
      "Best model: Epoch 9 \t loss=0.018956 \t val_loss=0.018169 \t time=0.88s\n",
      "Best model: Epoch 10 \t loss=0.018656 \t val_loss=0.017995 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018387 \t val_loss=0.017671 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.017993 \t val_loss=0.017505 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017741 \t val_loss=0.017458 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017536 \t val_loss=0.017309 \t time=0.88s\n",
      "Best model: Epoch 15 \t loss=0.017324 \t val_loss=0.017063 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017025 \t val_loss=0.017025 \t time=1.20s\n",
      "Best model: Epoch 17 \t loss=0.016810 \t val_loss=0.016959 \t time=0.91s\n",
      "Best model: Epoch 18 \t loss=0.016821 \t val_loss=0.016803 \t time=1.15s\n",
      "Best model: Epoch 19 \t loss=0.016505 \t val_loss=0.016753 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016673 \t time=0.90s\n",
      "Best model: Epoch 22 \t loss=0.016069 \t val_loss=0.016543 \t time=0.88s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016523 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015810 \t val_loss=0.016492 \t time=0.88s\n",
      "Best model: Epoch 26 \t loss=0.015618 \t val_loss=0.016482 \t time=0.90s\n",
      "Best model: Epoch 27 \t loss=0.015527 \t val_loss=0.016386 \t time=0.87s\n",
      "Best model: Epoch 28 \t loss=0.015535 \t val_loss=0.016386 \t time=0.86s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014660 \t val_loss=0.016190 \t time=0.89s\n",
      "Best model: Epoch 34 \t loss=0.014195 \t val_loss=0.016131 \t time=0.86s\n",
      "Best model: Epoch 35 \t loss=0.014032 \t val_loss=0.016129 \t time=0.87s\n",
      "Best model: Epoch 36 \t loss=0.013952 \t val_loss=0.016119 \t time=1.07s\n",
      "Best model: Epoch 38 \t loss=0.013619 \t val_loss=0.016104 \t time=0.89s\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Seed 2\n",
      "Fold 1 log loss: 0.016280616069086786\n",
      "Fold 2 log loss: 0.0160819916597495\n",
      "Fold 3 log loss: 0.01598215567903187\n",
      "Fold 4 log loss: 0.015969970162948186\n",
      "Fold 5 log loss: 0.016105637577698048\n",
      "Std of log loss: 0.0001118079949946509\n",
      "Total log loss: 0.01608407334211503\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.413482 \t val_loss=0.085006 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.048716 \t val_loss=0.028552 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.027498 \t val_loss=0.023904 \t time=0.94s\n",
      "Best model: Epoch 4 \t loss=0.023643 \t val_loss=0.022032 \t time=1.17s\n",
      "Best model: Epoch 5 \t loss=0.021593 \t val_loss=0.019921 \t time=1.25s\n",
      "Best model: Epoch 6 \t loss=0.020638 \t val_loss=0.019664 \t time=1.12s\n",
      "Best model: Epoch 7 \t loss=0.019979 \t val_loss=0.019123 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019318 \t val_loss=0.018529 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.019013 \t val_loss=0.018134 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018321 \t val_loss=0.017871 \t time=0.89s\n",
      "Best model: Epoch 12 \t loss=0.018269 \t val_loss=0.017847 \t time=0.95s\n",
      "Best model: Epoch 13 \t loss=0.017851 \t val_loss=0.017346 \t time=0.93s\n",
      "Best model: Epoch 14 \t loss=0.017423 \t val_loss=0.017188 \t time=0.92s\n",
      "Best model: Epoch 16 \t loss=0.017074 \t val_loss=0.016996 \t time=0.90s\n",
      "Best model: Epoch 17 \t loss=0.016863 \t val_loss=0.016935 \t time=1.20s\n",
      "Best model: Epoch 18 \t loss=0.016690 \t val_loss=0.016766 \t time=0.93s\n",
      "Best model: Epoch 19 \t loss=0.016395 \t val_loss=0.016740 \t time=0.92s\n",
      "Best model: Epoch 20 \t loss=0.016392 \t val_loss=0.016729 \t time=0.91s\n",
      "Best model: Epoch 21 \t loss=0.016257 \t val_loss=0.016575 \t time=1.16s\n",
      "Best model: Epoch 22 \t loss=0.016079 \t val_loss=0.016537 \t time=0.88s\n",
      "Best model: Epoch 24 \t loss=0.015940 \t val_loss=0.016440 \t time=0.90s\n",
      "Best model: Epoch 26 \t loss=0.015711 \t val_loss=0.016439 \t time=0.86s\n",
      "Best model: Epoch 27 \t loss=0.015655 \t val_loss=0.016355 \t time=0.89s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014718 \t val_loss=0.016233 \t time=0.86s\n",
      "Best model: Epoch 33 \t loss=0.014492 \t val_loss=0.016164 \t time=0.88s\n",
      "Best model: Epoch 34 \t loss=0.014242 \t val_loss=0.016162 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.014062 \t val_loss=0.016079 \t time=0.88s\n",
      "Best model: Epoch 38 \t loss=0.013740 \t val_loss=0.016068 \t time=0.88s\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410309 \t val_loss=0.088772 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.049774 \t val_loss=0.029268 \t time=0.86s\n",
      "Best model: Epoch 3 \t loss=0.027053 \t val_loss=0.022645 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.023236 \t val_loss=0.021036 \t time=0.88s\n",
      "Best model: Epoch 5 \t loss=0.021846 \t val_loss=0.019918 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020478 \t val_loss=0.019441 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.020034 \t val_loss=0.019010 \t time=0.87s\n",
      "Best model: Epoch 8 \t loss=0.019641 \t val_loss=0.018800 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.019041 \t val_loss=0.018087 \t time=1.13s\n",
      "Best model: Epoch 10 \t loss=0.018651 \t val_loss=0.017874 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018348 \t val_loss=0.017594 \t time=1.12s\n",
      "Best model: Epoch 12 \t loss=0.017880 \t val_loss=0.017560 \t time=0.86s\n",
      "Best model: Epoch 13 \t loss=0.017816 \t val_loss=0.017224 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017406 \t val_loss=0.017038 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017159 \t val_loss=0.016993 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017050 \t val_loss=0.016916 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016693 \t val_loss=0.016737 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016392 \t val_loss=0.016562 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016198 \t val_loss=0.016541 \t time=0.89s\n",
      "Best model: Epoch 22 \t loss=0.016053 \t val_loss=0.016421 \t time=0.86s\n",
      "Best model: Epoch 24 \t loss=0.015905 \t val_loss=0.016389 \t time=1.31s\n",
      "Best model: Epoch 25 \t loss=0.015724 \t val_loss=0.016362 \t time=1.12s\n",
      "Best model: Epoch 27 \t loss=0.015611 \t val_loss=0.016351 \t time=0.87s\n",
      "Best model: Epoch 29 \t loss=0.015418 \t val_loss=0.016304 \t time=0.98s\n",
      "Best model: Epoch 30 \t loss=0.015379 \t val_loss=0.016291 \t time=0.88s\n",
      "Best model: Epoch 31 \t loss=0.015286 \t val_loss=0.016288 \t time=1.18s\n",
      "Best model: Epoch 35 \t loss=0.015174 \t val_loss=0.016268 \t time=0.94s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014326 \t val_loss=0.016078 \t time=0.87s\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.416642 \t val_loss=0.077390 \t time=1.14s\n",
      "Best model: Epoch 2 \t loss=0.049910 \t val_loss=0.028385 \t time=0.92s\n",
      "Best model: Epoch 3 \t loss=0.027063 \t val_loss=0.022448 \t time=0.93s\n",
      "Best model: Epoch 4 \t loss=0.023517 \t val_loss=0.021003 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021520 \t val_loss=0.020122 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020809 \t val_loss=0.019284 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.019977 \t val_loss=0.018849 \t time=0.88s\n",
      "Best model: Epoch 8 \t loss=0.019361 \t val_loss=0.018720 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019017 \t val_loss=0.018142 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018557 \t val_loss=0.017915 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018590 \t val_loss=0.017653 \t time=1.15s\n",
      "Best model: Epoch 12 \t loss=0.018100 \t val_loss=0.017520 \t time=0.87s\n",
      "Best model: Epoch 13 \t loss=0.017907 \t val_loss=0.017251 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017499 \t val_loss=0.017095 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017316 \t val_loss=0.017086 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017026 \t val_loss=0.016796 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016643 \t val_loss=0.016680 \t time=0.90s\n",
      "Best model: Epoch 20 \t loss=0.016413 \t val_loss=0.016520 \t time=0.85s\n",
      "Best model: Epoch 21 \t loss=0.016271 \t val_loss=0.016456 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016094 \t val_loss=0.016416 \t time=1.04s\n",
      "Best model: Epoch 23 \t loss=0.015980 \t val_loss=0.016379 \t time=0.92s\n",
      "Best model: Epoch 25 \t loss=0.015681 \t val_loss=0.016315 \t time=0.85s\n",
      "Best model: Epoch 26 \t loss=0.015640 \t val_loss=0.016247 \t time=0.86s\n",
      "Best model: Epoch 30 \t loss=0.015300 \t val_loss=0.016237 \t time=0.85s\n",
      "Best model: Epoch 31 \t loss=0.015267 \t val_loss=0.016224 \t time=0.85s\n",
      "Best model: Epoch 32 \t loss=0.015209 \t val_loss=0.016183 \t time=0.86s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 37 \t loss=0.014489 \t val_loss=0.016069 \t time=1.17s\n",
      "Best model: Epoch 38 \t loss=0.014129 \t val_loss=0.015990 \t time=0.87s\n",
      "Best model: Epoch 40 \t loss=0.013782 \t val_loss=0.015978 \t time=0.85s\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.408462 \t val_loss=0.074284 \t time=0.88s\n",
      "Best model: Epoch 2 \t loss=0.049468 \t val_loss=0.028216 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027922 \t val_loss=0.022309 \t time=1.11s\n",
      "Best model: Epoch 4 \t loss=0.023475 \t val_loss=0.020758 \t time=1.09s\n",
      "Best model: Epoch 5 \t loss=0.021691 \t val_loss=0.019736 \t time=0.95s\n",
      "Best model: Epoch 6 \t loss=0.020567 \t val_loss=0.019042 \t time=0.85s\n",
      "Best model: Epoch 7 \t loss=0.019984 \t val_loss=0.018598 \t time=0.94s\n",
      "Best model: Epoch 8 \t loss=0.019345 \t val_loss=0.018219 \t time=0.89s\n",
      "Best model: Epoch 9 \t loss=0.019050 \t val_loss=0.017943 \t time=0.98s\n",
      "Best model: Epoch 10 \t loss=0.018741 \t val_loss=0.017613 \t time=1.25s\n",
      "Best model: Epoch 12 \t loss=0.017970 \t val_loss=0.017243 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017718 \t val_loss=0.017188 \t time=1.05s\n",
      "Best model: Epoch 14 \t loss=0.017529 \t val_loss=0.016956 \t time=0.96s\n",
      "Best model: Epoch 15 \t loss=0.017444 \t val_loss=0.016822 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017227 \t val_loss=0.016729 \t time=0.86s\n",
      "Best model: Epoch 17 \t loss=0.016915 \t val_loss=0.016672 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016773 \t val_loss=0.016519 \t time=0.87s\n",
      "Best model: Epoch 21 \t loss=0.016231 \t val_loss=0.016478 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.016146 \t val_loss=0.016421 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.016069 \t val_loss=0.016306 \t time=0.87s\n",
      "Best model: Epoch 24 \t loss=0.015932 \t val_loss=0.016244 \t time=0.86s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016208 \t time=1.12s\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 30 \t loss=0.014801 \t val_loss=0.016035 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.014562 \t val_loss=0.015951 \t time=0.88s\n",
      "Best model: Epoch 32 \t loss=0.014330 \t val_loss=0.015949 \t time=0.87s\n",
      "Best model: Epoch 33 \t loss=0.014244 \t val_loss=0.015931 \t time=0.87s\n",
      "Best model: Epoch 34 \t loss=0.014103 \t val_loss=0.015871 \t time=0.85s\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.412365 \t val_loss=0.078813 \t time=0.99s\n",
      "Best model: Epoch 2 \t loss=0.049562 \t val_loss=0.030582 \t time=1.16s\n",
      "Best model: Epoch 3 \t loss=0.027535 \t val_loss=0.022771 \t time=0.93s\n",
      "Best model: Epoch 4 \t loss=0.023436 \t val_loss=0.020868 \t time=0.91s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.019981 \t time=1.13s\n",
      "Best model: Epoch 6 \t loss=0.020607 \t val_loss=0.019295 \t time=0.87s\n",
      "Best model: Epoch 7 \t loss=0.020060 \t val_loss=0.019073 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019507 \t val_loss=0.018563 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.019204 \t val_loss=0.018202 \t time=0.92s\n",
      "Best model: Epoch 10 \t loss=0.018767 \t val_loss=0.017955 \t time=0.93s\n",
      "Best model: Epoch 11 \t loss=0.018517 \t val_loss=0.017727 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.018120 \t val_loss=0.017525 \t time=0.87s\n",
      "Best model: Epoch 13 \t loss=0.017746 \t val_loss=0.017468 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017485 \t val_loss=0.017203 \t time=0.86s\n",
      "Best model: Epoch 15 \t loss=0.017232 \t val_loss=0.017094 \t time=0.88s\n",
      "Best model: Epoch 16 \t loss=0.017003 \t val_loss=0.017025 \t time=1.46s\n",
      "Best model: Epoch 17 \t loss=0.016785 \t val_loss=0.016926 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016714 \t val_loss=0.016916 \t time=1.23s\n",
      "Best model: Epoch 19 \t loss=0.016531 \t val_loss=0.016807 \t time=0.88s\n",
      "Best model: Epoch 20 \t loss=0.016316 \t val_loss=0.016705 \t time=0.94s\n",
      "Best model: Epoch 21 \t loss=0.016168 \t val_loss=0.016531 \t time=1.08s\n",
      "Best model: Epoch 22 \t loss=0.016100 \t val_loss=0.016499 \t time=0.91s\n",
      "Best model: Epoch 25 \t loss=0.015851 \t val_loss=0.016459 \t time=0.87s\n",
      "Best model: Epoch 29 \t loss=0.015470 \t val_loss=0.016444 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.015325 \t val_loss=0.016384 \t time=0.93s\n",
      "Best model: Epoch 35 \t loss=0.015085 \t val_loss=0.016368 \t time=1.24s\n",
      "Best model: Epoch 39 \t loss=0.014981 \t val_loss=0.016356 \t time=1.11s\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Seed 3\n",
      "Fold 1 log loss: 0.01615919115850787\n",
      "Fold 2 log loss: 0.016131589753943455\n",
      "Fold 3 log loss: 0.015970096888850625\n",
      "Fold 4 log loss: 0.01593554139806171\n",
      "Fold 5 log loss: 0.01637002395976194\n",
      "Std of log loss: 0.0001552114403026376\n",
      "Total log loss: 0.01611327610054892\n",
      "Fold 1\n",
      "Best model: Epoch 1 \t loss=0.417376 \t val_loss=0.076125 \t time=0.85s\n",
      "Best model: Epoch 2 \t loss=0.050306 \t val_loss=0.028216 \t time=1.15s\n",
      "Best model: Epoch 3 \t loss=0.026912 \t val_loss=0.022988 \t time=0.86s\n",
      "Best model: Epoch 4 \t loss=0.023302 \t val_loss=0.021196 \t time=1.06s\n",
      "Best model: Epoch 5 \t loss=0.021557 \t val_loss=0.020048 \t time=0.90s\n",
      "Best model: Epoch 6 \t loss=0.020636 \t val_loss=0.019492 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.019718 \t val_loss=0.018851 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019296 \t val_loss=0.018580 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.019243 \t val_loss=0.018538 \t time=0.85s\n",
      "Best model: Epoch 10 \t loss=0.018776 \t val_loss=0.018315 \t time=0.88s\n",
      "Best model: Epoch 11 \t loss=0.018431 \t val_loss=0.018107 \t time=0.85s\n",
      "Best model: Epoch 12 \t loss=0.018065 \t val_loss=0.017536 \t time=0.85s\n",
      "Best model: Epoch 13 \t loss=0.017735 \t val_loss=0.017326 \t time=0.88s\n",
      "Best model: Epoch 14 \t loss=0.017458 \t val_loss=0.017205 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017398 \t val_loss=0.017164 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017067 \t val_loss=0.017005 \t time=1.11s\n",
      "Best model: Epoch 17 \t loss=0.016868 \t val_loss=0.016867 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016657 \t val_loss=0.016804 \t time=0.87s\n",
      "Best model: Epoch 19 \t loss=0.016450 \t val_loss=0.016644 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016293 \t val_loss=0.016622 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016240 \t val_loss=0.016574 \t time=0.88s\n",
      "Best model: Epoch 22 \t loss=0.016049 \t val_loss=0.016538 \t time=0.86s\n",
      "Best model: Epoch 23 \t loss=0.015954 \t val_loss=0.016425 \t time=1.06s\n",
      "Best model: Epoch 25 \t loss=0.015780 \t val_loss=0.016411 \t time=0.87s\n",
      "Best model: Epoch 26 \t loss=0.015589 \t val_loss=0.016411 \t time=0.92s\n",
      "Best model: Epoch 27 \t loss=0.015555 \t val_loss=0.016378 \t time=1.13s\n",
      "Best model: Epoch 30 \t loss=0.015372 \t val_loss=0.016330 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.015312 \t val_loss=0.016320 \t time=0.85s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014407 \t val_loss=0.016174 \t time=1.21s\n",
      "Best model: Epoch 37 \t loss=0.014183 \t val_loss=0.016169 \t time=0.88s\n",
      "Best model: Epoch 38 \t loss=0.013891 \t val_loss=0.016102 \t time=1.10s\n",
      "Best model: Epoch 40 \t loss=0.013597 \t val_loss=0.016079 \t time=0.90s\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2\n",
      "Best model: Epoch 1 \t loss=0.410688 \t val_loss=0.080734 \t time=0.91s\n",
      "Best model: Epoch 2 \t loss=0.048953 \t val_loss=0.028426 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027673 \t val_loss=0.022677 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023493 \t val_loss=0.021433 \t time=0.90s\n",
      "Best model: Epoch 5 \t loss=0.021537 \t val_loss=0.020268 \t time=0.89s\n",
      "Best model: Epoch 6 \t loss=0.020399 \t val_loss=0.019240 \t time=0.98s\n",
      "Best model: Epoch 7 \t loss=0.019899 \t val_loss=0.018880 \t time=1.03s\n",
      "Best model: Epoch 8 \t loss=0.019432 \t val_loss=0.018494 \t time=0.90s\n",
      "Best model: Epoch 9 \t loss=0.018890 \t val_loss=0.018014 \t time=0.86s\n",
      "Best model: Epoch 10 \t loss=0.018555 \t val_loss=0.017699 \t time=0.89s\n",
      "Best model: Epoch 11 \t loss=0.018488 \t val_loss=0.017471 \t time=0.87s\n",
      "Best model: Epoch 12 \t loss=0.017990 \t val_loss=0.017407 \t time=0.88s\n",
      "Best model: Epoch 13 \t loss=0.017646 \t val_loss=0.017239 \t time=0.85s\n",
      "Best model: Epoch 14 \t loss=0.017444 \t val_loss=0.017084 \t time=1.29s\n",
      "Best model: Epoch 15 \t loss=0.017198 \t val_loss=0.017004 \t time=0.87s\n",
      "Best model: Epoch 16 \t loss=0.017013 \t val_loss=0.016855 \t time=0.87s\n",
      "Best model: Epoch 18 \t loss=0.016731 \t val_loss=0.016750 \t time=1.08s\n",
      "Best model: Epoch 19 \t loss=0.016473 \t val_loss=0.016698 \t time=0.89s\n",
      "Best model: Epoch 20 \t loss=0.016372 \t val_loss=0.016563 \t time=0.86s\n",
      "Best model: Epoch 21 \t loss=0.016135 \t val_loss=0.016538 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.016041 \t val_loss=0.016417 \t time=0.86s\n",
      "Best model: Epoch 24 \t loss=0.015880 \t val_loss=0.016353 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015686 \t val_loss=0.016303 \t time=1.18s\n",
      "Best model: Epoch 26 \t loss=0.015627 \t val_loss=0.016299 \t time=0.89s\n",
      "Best model: Epoch 27 \t loss=0.015532 \t val_loss=0.016210 \t time=0.86s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 32 \t loss=0.014590 \t val_loss=0.016123 \t time=0.87s\n",
      "Best model: Epoch 33 \t loss=0.014261 \t val_loss=0.016048 \t time=0.88s\n",
      "Best model: Epoch 35 \t loss=0.013990 \t val_loss=0.016027 \t time=0.87s\n",
      "Best model: Epoch 37 \t loss=0.013710 \t val_loss=0.016001 \t time=0.88s\n",
      "Best model: Epoch 39 \t loss=0.013421 \t val_loss=0.015993 \t time=0.87s\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3\n",
      "Best model: Epoch 1 \t loss=0.410937 \t val_loss=0.082875 \t time=0.86s\n",
      "Best model: Epoch 2 \t loss=0.049043 \t val_loss=0.027972 \t time=0.89s\n",
      "Best model: Epoch 3 \t loss=0.027629 \t val_loss=0.022424 \t time=0.88s\n",
      "Best model: Epoch 4 \t loss=0.023497 \t val_loss=0.020791 \t time=0.86s\n",
      "Best model: Epoch 5 \t loss=0.021301 \t val_loss=0.019844 \t time=0.94s\n",
      "Best model: Epoch 6 \t loss=0.020559 \t val_loss=0.019263 \t time=1.14s\n",
      "Best model: Epoch 7 \t loss=0.020173 \t val_loss=0.018889 \t time=0.86s\n",
      "Best model: Epoch 8 \t loss=0.019406 \t val_loss=0.018523 \t time=0.88s\n",
      "Best model: Epoch 9 \t loss=0.019178 \t val_loss=0.018472 \t time=0.91s\n",
      "Best model: Epoch 10 \t loss=0.018764 \t val_loss=0.017898 \t time=1.36s\n",
      "Best model: Epoch 11 \t loss=0.018385 \t val_loss=0.017611 \t time=0.94s\n",
      "Best model: Epoch 13 \t loss=0.018019 \t val_loss=0.017296 \t time=0.90s\n",
      "Best model: Epoch 14 \t loss=0.017689 \t val_loss=0.017133 \t time=1.56s\n",
      "Best model: Epoch 15 \t loss=0.017383 \t val_loss=0.016993 \t time=0.86s\n",
      "Best model: Epoch 16 \t loss=0.017100 \t val_loss=0.016782 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016827 \t val_loss=0.016761 \t time=0.89s\n",
      "Best model: Epoch 18 \t loss=0.016695 \t val_loss=0.016640 \t time=0.89s\n",
      "Best model: Epoch 19 \t loss=0.016421 \t val_loss=0.016541 \t time=0.91s\n",
      "Best model: Epoch 20 \t loss=0.016383 \t val_loss=0.016434 \t time=1.15s\n",
      "Best model: Epoch 22 \t loss=0.016122 \t val_loss=0.016432 \t time=0.90s\n",
      "Best model: Epoch 23 \t loss=0.015940 \t val_loss=0.016393 \t time=0.90s\n",
      "Best model: Epoch 24 \t loss=0.015808 \t val_loss=0.016330 \t time=0.90s\n",
      "Best model: Epoch 25 \t loss=0.015735 \t val_loss=0.016300 \t time=0.93s\n",
      "Best model: Epoch 27 \t loss=0.015601 \t val_loss=0.016294 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015337 \t val_loss=0.016287 \t time=0.86s\n",
      "Best model: Epoch 31 \t loss=0.015245 \t val_loss=0.016173 \t time=0.86s\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 36 \t loss=0.014393 \t val_loss=0.016052 \t time=0.88s\n",
      "Best model: Epoch 37 \t loss=0.014168 \t val_loss=0.015955 \t time=0.86s\n",
      "Best model: Epoch 38 \t loss=0.013916 \t val_loss=0.015939 \t time=0.85s\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4\n",
      "Best model: Epoch 1 \t loss=0.414284 \t val_loss=0.077726 \t time=1.03s\n",
      "Best model: Epoch 2 \t loss=0.048858 \t val_loss=0.027310 \t time=0.87s\n",
      "Best model: Epoch 3 \t loss=0.027433 \t val_loss=0.022797 \t time=0.89s\n",
      "Best model: Epoch 4 \t loss=0.023323 \t val_loss=0.020871 \t time=1.20s\n",
      "Best model: Epoch 5 \t loss=0.021639 \t val_loss=0.019794 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020495 \t val_loss=0.019132 \t time=0.85s\n",
      "Best model: Epoch 7 \t loss=0.019782 \t val_loss=0.018384 \t time=0.85s\n",
      "Best model: Epoch 8 \t loss=0.019526 \t val_loss=0.018299 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.019129 \t val_loss=0.017967 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018860 \t val_loss=0.017541 \t time=0.88s\n",
      "Best model: Epoch 12 \t loss=0.018034 \t val_loss=0.017256 \t time=1.09s\n",
      "Best model: Epoch 13 \t loss=0.017711 \t val_loss=0.017202 \t time=0.89s\n",
      "Best model: Epoch 14 \t loss=0.017492 \t val_loss=0.016948 \t time=0.85s\n",
      "Best model: Epoch 15 \t loss=0.017307 \t val_loss=0.016830 \t time=0.85s\n",
      "Best model: Epoch 16 \t loss=0.017385 \t val_loss=0.016827 \t time=0.87s\n",
      "Best model: Epoch 17 \t loss=0.016998 \t val_loss=0.016789 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016722 \t val_loss=0.016612 \t time=0.85s\n",
      "Best model: Epoch 19 \t loss=0.016558 \t val_loss=0.016508 \t time=0.85s\n",
      "Best model: Epoch 20 \t loss=0.016337 \t val_loss=0.016418 \t time=0.89s\n",
      "Best model: Epoch 21 \t loss=0.016235 \t val_loss=0.016326 \t time=0.86s\n",
      "Best model: Epoch 22 \t loss=0.016127 \t val_loss=0.016278 \t time=0.88s\n",
      "Best model: Epoch 25 \t loss=0.015683 \t val_loss=0.016225 \t time=0.86s\n",
      "Best model: Epoch 28 \t loss=0.015565 \t val_loss=0.016198 \t time=0.88s\n",
      "Best model: Epoch 29 \t loss=0.015424 \t val_loss=0.016165 \t time=0.85s\n",
      "Best model: Epoch 31 \t loss=0.015384 \t val_loss=0.016151 \t time=1.29s\n",
      "Best model: Epoch 35 \t loss=0.015113 \t val_loss=0.016146 \t time=1.14s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 40 \t loss=0.014338 \t val_loss=0.016034 \t time=1.26s\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5\n",
      "Best model: Epoch 1 \t loss=0.413252 \t val_loss=0.074956 \t time=0.89s\n",
      "Best model: Epoch 2 \t loss=0.048736 \t val_loss=0.028345 \t time=0.90s\n",
      "Best model: Epoch 3 \t loss=0.027761 \t val_loss=0.022420 \t time=1.02s\n",
      "Best model: Epoch 4 \t loss=0.023178 \t val_loss=0.021009 \t time=0.93s\n",
      "Best model: Epoch 5 \t loss=0.021552 \t val_loss=0.019836 \t time=0.86s\n",
      "Best model: Epoch 6 \t loss=0.020723 \t val_loss=0.019424 \t time=0.86s\n",
      "Best model: Epoch 7 \t loss=0.020016 \t val_loss=0.019338 \t time=0.89s\n",
      "Best model: Epoch 8 \t loss=0.019493 \t val_loss=0.018481 \t time=0.87s\n",
      "Best model: Epoch 9 \t loss=0.018870 \t val_loss=0.018145 \t time=0.86s\n",
      "Best model: Epoch 11 \t loss=0.018279 \t val_loss=0.017718 \t time=0.86s\n",
      "Best model: Epoch 12 \t loss=0.017971 \t val_loss=0.017576 \t time=0.87s\n",
      "Best model: Epoch 13 \t loss=0.017682 \t val_loss=0.017317 \t time=0.86s\n",
      "Best model: Epoch 14 \t loss=0.017413 \t val_loss=0.017172 \t time=0.87s\n",
      "Best model: Epoch 15 \t loss=0.017277 \t val_loss=0.017068 \t time=1.07s\n",
      "Best model: Epoch 16 \t loss=0.017148 \t val_loss=0.017005 \t time=0.91s\n",
      "Best model: Epoch 17 \t loss=0.016898 \t val_loss=0.016945 \t time=0.86s\n",
      "Best model: Epoch 18 \t loss=0.016702 \t val_loss=0.016738 \t time=0.86s\n",
      "Best model: Epoch 19 \t loss=0.016448 \t val_loss=0.016712 \t time=0.88s\n",
      "Best model: Epoch 21 \t loss=0.016312 \t val_loss=0.016602 \t time=0.87s\n",
      "Best model: Epoch 23 \t loss=0.015978 \t val_loss=0.016516 \t time=1.04s\n",
      "Best model: Epoch 24 \t loss=0.015868 \t val_loss=0.016480 \t time=0.95s\n",
      "Best model: Epoch 26 \t loss=0.015581 \t val_loss=0.016478 \t time=1.12s\n",
      "Best model: Epoch 27 \t loss=0.015653 \t val_loss=0.016433 \t time=0.90s\n",
      "Best model: Epoch 28 \t loss=0.015518 \t val_loss=0.016433 \t time=0.86s\n",
      "Best model: Epoch 29 \t loss=0.015479 \t val_loss=0.016413 \t time=0.88s\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Best model: Epoch 34 \t loss=0.014591 \t val_loss=0.016284 \t time=0.86s\n",
      "Best model: Epoch 35 \t loss=0.014365 \t val_loss=0.016185 \t time=0.89s\n",
      "Best model: Epoch 36 \t loss=0.014121 \t val_loss=0.016150 \t time=0.87s\n",
      "Best model: Epoch 38 \t loss=0.013806 \t val_loss=0.016146 \t time=1.12s\n",
      "Best model: Epoch 40 \t loss=0.013604 \t val_loss=0.016141 \t time=0.86s\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Seed 4\n",
      "Fold 1 log loss: 0.016174484102574663\n",
      "Fold 2 log loss: 0.016047175759869827\n",
      "Fold 3 log loss: 0.015928155998706423\n",
      "Fold 4 log loss: 0.016093105821003578\n",
      "Fold 5 log loss: 0.016142314651801725\n",
      "Std of log loss: 8.610756333173562e-05\n",
      "Total log loss: 0.016077045654075823\n"
     ]
    }
   ],
   "source": [
    "seeds = [0,1,2,3,4]\n",
    "fn_train = train.copy() \n",
    "fn_test = test.copy() \n",
    "fn_targets = targets.drop(\"sig_id\", axis=1).copy()\n",
    "\n",
    "fn_train = fn_train[fn_train.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "fn_targets = fn_targets[fn_targets.index.isin(cons_train_index)].copy().reset_index(drop=True).to_numpy()\n",
    "#fn_test = fn_test[fn_test.index.isin(cons_test_index)].copy().reset_index(drop=True).to_numpy()\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "fn_train= ss.fit_transform(fn_train)\n",
    "fn_test = ss.transform(fn_test)\n",
    "\n",
    "pytorch1_oof = np.zeros([len(fn_train),fn_targets.shape[1]])\n",
    "pytorch1_test = np.zeros([len(fn_test),fn_targets.shape[1]])\n",
    "\n",
    "for seed_ in seeds:\n",
    "    oof, oof_targets, pytorch_pred = modelling_torch(fn_train, fn_targets, fn_test, seed_, fn_train.shape[1], fn_targets.shape[1],1)\n",
    "    pytorch1_oof += oof / len(seeds)\n",
    "    pytorch1_test += pytorch_pred / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:18:12.290567Z",
     "iopub.status.busy": "2020-10-10T05:18:12.288766Z",
     "iopub.status.idle": "2020-10-10T05:18:19.232013Z",
     "shell.execute_reply": "2020-10-10T05:18:19.232589Z"
    },
    "papermill": {
     "duration": 7.679552,
     "end_time": "2020-10-10T05:18:19.232765",
     "exception": false,
     "start_time": "2020-10-10T05:18:11.553213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014671957119226644\n"
     ]
    }
   ],
   "source": [
    "check_pytorch1 = targets.copy()\n",
    "check_pytorch1.loc[cons_train_index,target_feats] = pytorch1_oof\n",
    "check_pytorch1.loc[noncons_train_index,target_feats] = 0\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check_pytorch1.iloc[:,1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.855973,
     "end_time": "2020-10-10T05:18:20.953535",
     "exception": false,
     "start_time": "2020-10-10T05:18:20.097562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1st Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.10737,
     "end_time": "2020-10-10T05:18:22.794699",
     "exception": false,
     "start_time": "2020-10-10T05:18:21.687329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.776307,
     "end_time": "2020-10-10T05:18:24.398028",
     "exception": false,
     "start_time": "2020-10-10T05:18:23.621721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:18:26.041113Z",
     "iopub.status.busy": "2020-10-10T05:18:26.040163Z",
     "iopub.status.idle": "2020-10-10T05:18:26.044056Z",
     "shell.execute_reply": "2020-10-10T05:18:26.044639Z"
    },
    "papermill": {
     "duration": 0.812174,
     "end_time": "2020-10-10T05:18:26.044784",
     "exception": false,
     "start_time": "2020-10-10T05:18:25.232610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-4311e65beba2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-4311e65beba2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    self.coef_ = [0 for in range(length)]\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# weight optimization\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self, length):\n",
    "        self.coef_ = [0 for in range(length)]\n",
    "\n",
    "    def _log_loss(self, coef, Xs, y):\n",
    "        X_p = np.zeros_like(Xs[0])\n",
    "        for i in range(len(coef)):\n",
    "            X_p += coef[i] * Xs[i]\n",
    "        return log_loss(np.ravel(y), np.ravel(np.array(X_p)))\n",
    "    \n",
    "    def fit(self, X, y, random_flg = False):\n",
    "        loss_partial = partial(self._log_loss, X=X, y=y)\n",
    "        if random_flg:\n",
    "            initial_coef = [np.random.uniform(0.4,0.5), np.random.uniform(0.5,0.6), np.random.uniform(0.6,0.7)]\n",
    "        else:\n",
    "            initial_coef = [1/length for i in range(length)]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead') #Powell\n",
    "        \n",
    "    def predict(self, X, coef):\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_\n",
    "    \n",
    "best_score = 100\n",
    "#for i in range(10):\n",
    "#    optR = OptimizedRounder()\n",
    "#    optR.fit(, y, random_flg=False)\n",
    "#    coefficients = optR.coefficients()\n",
    "#    score = qwk(new_train.accuracy_group, final_valid_pred)\n",
    "#    print(i, np.sort(coefficients), score)\n",
    "#    if score > best_score:\n",
    "#        best_score = score\n",
    "#        best_coefficients = coefficients\n",
    "#final_test_pred = pd.cut(np.array(test_exp_accuracy).reshape(-1,), [-np.inf] + list(np.sort(best_coefficients)) + [np.inf], labels = [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:18:27.795797Z",
     "iopub.status.busy": "2020-10-10T05:18:27.794380Z",
     "iopub.status.idle": "2020-10-10T05:18:29.277432Z",
     "shell.execute_reply": "2020-10-10T05:18:29.276766Z"
    },
    "papermill": {
     "duration": 2.245815,
     "end_time": "2020-10-10T05:18:29.277553",
     "exception": false,
     "start_time": "2020-10-10T05:18:27.031738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF log loss:  0.014891265208603207\n"
     ]
    }
   ],
   "source": [
    "check = 0.3 * check_xgb1.iloc[:,1:] + 0.7 * check_pytorch1.iloc[:,1:]\n",
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(np.array(check))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-10T05:18:30.844190Z",
     "iopub.status.busy": "2020-10-10T05:18:30.843103Z",
     "iopub.status.idle": "2020-10-10T05:18:34.397133Z",
     "shell.execute_reply": "2020-10-10T05:18:34.395452Z"
    },
    "papermill": {
     "duration": 4.319658,
     "end_time": "2020-10-10T05:18:34.397311",
     "exception": false,
     "start_time": "2020-10-10T05:18:30.077653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub2 = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "sub1.loc[cons_test_index,target_feats] = pytorch1_test\n",
    "sub1.loc[noncons_test_index,target_feats] = 0\n",
    "sub2.loc[cons_test_index,target_feats] = xgb1_test\n",
    "sub2.loc[noncons_test_index,target_feats] = 0\n",
    "\n",
    "sub[target_feats] = 0.3 * sub2.iloc[:,1:] + 0.7 * sub1.iloc[:,1:]\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.733941,
     "end_time": "2020-10-10T05:18:35.860652",
     "exception": false,
     "start_time": "2020-10-10T05:18:35.126711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5564.971074,
   "end_time": "2020-10-10T05:18:37.819689",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-10T03:45:52.848615",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
