{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1c1039",
   "metadata": {
    "papermill": {
     "duration": 0.034336,
     "end_time": "2022-03-13T02:23:08.925979",
     "exception": false,
     "start_time": "2022-03-13T02:23:08.891643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- ref:https://www.kaggle.com/kaerunantoka/h-m-eda-w-pyspark\n",
    "- ref: https://qiita.com/t-yotsu/items/4cabd1ae5406cfd7d741\n",
    "- ref: https://qiita.com/taka4sato/items/4ab2cf9e941599f1c0ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a0195f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-13T02:23:08.998210Z",
     "iopub.status.busy": "2022-03-13T02:23:08.996992Z",
     "iopub.status.idle": "2022-03-13T02:23:59.018037Z",
     "shell.execute_reply": "2022-03-13T02:23:59.016838Z",
     "shell.execute_reply.started": "2022-03-12T23:48:50.280512Z"
    },
    "papermill": {
     "duration": 50.056833,
     "end_time": "2022-03-13T02:23:59.018331",
     "exception": false,
     "start_time": "2022-03-13T02:23:08.961498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3395fc50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:23:59.096626Z",
     "iopub.status.busy": "2022-03-13T02:23:59.095587Z",
     "iopub.status.idle": "2022-03-13T02:24:05.499246Z",
     "shell.execute_reply": "2022-03-13T02:24:05.498073Z",
     "shell.execute_reply.started": "2022-03-13T00:33:59.033680Z"
    },
    "papermill": {
     "duration": 6.446972,
     "end_time": "2022-03-13T02:24:05.499443",
     "exception": false,
     "start_time": "2022-03-13T02:23:59.052471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/13 02:24:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "spark = SparkSession.builder.appName('h-and-m-personalized-fashion-recommendations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7f2a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:05.582503Z",
     "iopub.status.busy": "2022-03-13T02:24:05.581643Z",
     "iopub.status.idle": "2022-03-13T02:24:05.583712Z",
     "shell.execute_reply": "2022-03-13T02:24:05.584494Z",
     "shell.execute_reply.started": "2022-03-12T23:49:37.164410Z"
    },
    "papermill": {
     "duration": 0.047146,
     "end_time": "2022-03-13T02:24:05.584670",
     "exception": false,
     "start_time": "2022-03-13T02:24:05.537524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    TRANSACTION_PATH = '../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'\n",
    "    ARTICLE_PATH = '../input/h-and-m-personalized-fashion-recommendations/articles.csv'\n",
    "    CUSTOMER_PATH = '../input/h-and-m-personalized-fashion-recommendations/customers.csv'\n",
    "    SAMPLE_PATH = '../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv'\n",
    "    IMAGE_PATH = '../input/h-and-m-personalized-fashion-recommendations/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbb233",
   "metadata": {
    "papermill": {
     "duration": 0.042107,
     "end_time": "2022-03-13T02:24:05.674001",
     "exception": false,
     "start_time": "2022-03-13T02:24:05.631894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c6bbfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:05.752420Z",
     "iopub.status.busy": "2022-03-13T02:24:05.751597Z",
     "iopub.status.idle": "2022-03-13T02:24:11.196925Z",
     "shell.execute_reply": "2022-03-13T02:24:11.195475Z",
     "shell.execute_reply.started": "2022-03-12T23:56:30.614587Z"
    },
    "papermill": {
     "duration": 5.485536,
     "end_time": "2022-03-13T02:24:11.197139",
     "exception": false,
     "start_time": "2022-03-13T02:24:05.711603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = spark.read.option('header','true').csv(CFG.TRANSACTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c66510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:11.300291Z",
     "iopub.status.busy": "2022-03-13T02:24:11.299590Z",
     "iopub.status.idle": "2022-03-13T02:24:28.438597Z",
     "shell.execute_reply": "2022-03-13T02:24:28.439166Z",
     "shell.execute_reply.started": "2022-03-12T23:49:42.313396Z"
    },
    "papermill": {
     "duration": 17.1855,
     "end_time": "2022-03-13T02:24:28.439360",
     "exception": false,
     "start_time": "2022-03-13T02:24:11.253860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31788324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16f9fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:28.521932Z",
     "iopub.status.busy": "2022-03-13T02:24:28.520905Z",
     "iopub.status.idle": "2022-03-13T02:24:28.750490Z",
     "shell.execute_reply": "2022-03-13T02:24:28.749543Z",
     "shell.execute_reply.started": "2022-03-12T23:49:56.005611Z"
    },
    "papermill": {
     "duration": 0.271796,
     "end_time": "2022-03-13T02:24:28.750688",
     "exception": false,
     "start_time": "2022-03-13T02:24:28.478892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "|t_dat     |customer_id                                                     |article_id|price               |sales_channel_id|\n",
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "|2018-09-20|000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318|0663713001|0.050830508474576264|2               |\n",
      "|2018-09-20|000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318|0541518023|0.03049152542372881 |2               |\n",
      "|2018-09-20|00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2|0505221004|0.01523728813559322 |2               |\n",
      "|2018-09-20|00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2|0685687003|0.016932203389830508|2               |\n",
      "|2018-09-20|00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2|0685687004|0.016932203389830508|2               |\n",
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e555fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:28.847905Z",
     "iopub.status.busy": "2022-03-13T02:24:28.847215Z",
     "iopub.status.idle": "2022-03-13T02:24:28.858656Z",
     "shell.execute_reply": "2022-03-13T02:24:28.857715Z",
     "shell.execute_reply.started": "2022-03-12T23:49:56.259072Z"
    },
    "papermill": {
     "duration": 0.055709,
     "end_time": "2022-03-13T02:24:28.858886",
     "exception": false,
     "start_time": "2022-03-13T02:24:28.803177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t_dat: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- article_id: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- sales_channel_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9885a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:28.945357Z",
     "iopub.status.busy": "2022-03-13T02:24:28.944733Z",
     "iopub.status.idle": "2022-03-13T02:24:29.095395Z",
     "shell.execute_reply": "2022-03-13T02:24:29.096368Z",
     "shell.execute_reply.started": "2022-03-12T23:49:56.276625Z"
    },
    "papermill": {
     "duration": 0.196119,
     "end_time": "2022-03-13T02:24:29.096622",
     "exception": false,
     "start_time": "2022-03-13T02:24:28.900503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|t_dat     |\n",
      "+----------+\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "|2018-09-20|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(\n",
    "    \"t_dat\"\n",
    ").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e3651e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:29.184133Z",
     "iopub.status.busy": "2022-03-13T02:24:29.183261Z",
     "iopub.status.idle": "2022-03-13T02:24:55.837268Z",
     "shell.execute_reply": "2022-03-13T02:24:55.838077Z",
     "shell.execute_reply.started": "2022-03-13T00:25:00.078607Z"
    },
    "papermill": {
     "duration": 26.701266,
     "end_time": "2022-03-13T02:24:55.838755",
     "exception": false,
     "start_time": "2022-03-13T02:24:29.137489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|sales_channel_id|\n",
      "+----------------+\n",
      "|               1|\n",
      "|               2|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.select('sales_channel_id').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c63281d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:55.989227Z",
     "iopub.status.busy": "2022-03-13T02:24:55.988148Z",
     "iopub.status.idle": "2022-03-13T02:24:56.369641Z",
     "shell.execute_reply": "2022-03-13T02:24:56.368783Z",
     "shell.execute_reply.started": "2022-03-13T00:54:52.794333Z"
    },
    "papermill": {
     "duration": 0.458189,
     "end_time": "2022-03-13T02:24:56.369883",
     "exception": false,
     "start_time": "2022-03-13T02:24:55.911694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  FutureWarning\n",
      "/opt/conda/lib/python3.7/site-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "|t_dat     |customer_id                                                     |article_id|price               |sales_channel_id|\n",
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "|2018-09-20|00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4c73235dccbbc132280|0688873012|0.03049152542372881 |1               |\n",
      "|2018-09-20|00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4c73235dccbbc132280|0501323011|0.053372881355932204|1               |\n",
      "|2018-09-20|00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4c73235dccbbc132280|0688873020|0.03049152542372881 |1               |\n",
      "|2018-09-20|00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4c73235dccbbc132280|0688873011|0.03049152542372881 |1               |\n",
      "|2018-09-20|001127bffdda108579e6cb16080440e89bf1250a776c6e55f56e35e9ee029a8d|0397068015|0.033881355932203386|1               |\n",
      "+----------+----------------------------------------------------------------+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.registerTempTable(\"train_table\") #dataframeにsqlテーブル名を付与\n",
    "sqlContext = SQLContext(spark) #sparksqlを利用するには、sparkcontextだけでなくSQLContextも必要\n",
    "\n",
    "sqlContext.sql(\" SELECT * FROM train_table where sales_channel_id == 1 \").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46cb1807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:24:56.522421Z",
     "iopub.status.busy": "2022-03-13T02:24:56.519587Z",
     "iopub.status.idle": "2022-03-13T02:25:36.719957Z",
     "shell.execute_reply": "2022-03-13T02:25:36.719144Z",
     "shell.execute_reply.started": "2022-03-13T01:18:05.075027Z"
    },
    "papermill": {
     "duration": 40.279287,
     "end_time": "2022-03-13T02:25:36.720163",
     "exception": false,
     "start_time": "2022-03-13T02:24:56.440876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         customer_id|count|\n",
      "+--------------------+-----+\n",
      "|be1981ab818cf4ef6...| 1895|\n",
      "|b4db5e5259234574e...| 1441|\n",
      "|49beaacac0c7801c2...| 1364|\n",
      "|a65f77281a528bf5c...| 1361|\n",
      "|cd04ec2726dd58a8c...| 1237|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"customer_id\").count().sort(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b8808d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:25:36.898378Z",
     "iopub.status.busy": "2022-03-13T02:25:36.897457Z",
     "iopub.status.idle": "2022-03-13T02:26:03.275650Z",
     "shell.execute_reply": "2022-03-13T02:26:03.275022Z",
     "shell.execute_reply.started": "2022-03-13T01:19:24.164740Z"
    },
    "papermill": {
     "duration": 26.470611,
     "end_time": "2022-03-13T02:26:03.275794",
     "exception": false,
     "start_time": "2022-03-13T02:25:36.805183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     t_dat| count|\n",
      "+----------+------+\n",
      "|2019-09-28|198622|\n",
      "|2020-04-11|162799|\n",
      "|2019-11-29|160875|\n",
      "|2018-11-23|142018|\n",
      "|2018-09-29|141700|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"t_dat\").count().sort(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462b7d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:26:03.393476Z",
     "iopub.status.busy": "2022-03-13T02:26:03.392437Z",
     "iopub.status.idle": "2022-03-13T02:26:49.693839Z",
     "shell.execute_reply": "2022-03-13T02:26:49.694651Z",
     "shell.execute_reply.started": "2022-03-13T01:31:40.130635Z"
    },
    "papermill": {
     "duration": 46.362481,
     "end_time": "2022-03-13T02:26:49.694917",
     "exception": false,
     "start_time": "2022-03-13T02:26:03.332436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|         customer_id|         sum(price)|\n",
      "+--------------------+-------------------+\n",
      "|05f65801b9a2d28a5...| 1.7262881355932203|\n",
      "|05f79b715286a38a8...| 0.3919322033898305|\n",
      "|072d11a8c0a1e6d0f...|0.38801694915254237|\n",
      "+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# groupby + aggは使える\n",
    "train.groupBy(\"customer_id\").agg({\"price\": \"sum\"}).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678d82c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:26:49.824012Z",
     "iopub.status.busy": "2022-03-13T02:26:49.823353Z",
     "iopub.status.idle": "2022-03-13T02:27:32.216318Z",
     "shell.execute_reply": "2022-03-13T02:27:32.217102Z",
     "shell.execute_reply.started": "2022-03-13T01:36:24.793882Z"
    },
    "papermill": {
     "duration": 42.460037,
     "end_time": "2022-03-13T02:27:32.217322",
     "exception": false,
     "start_time": "2022-03-13T02:26:49.757285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|         customer_id|     t_dat|count|\n",
      "+--------------------+----------+-----+\n",
      "|00b5bd5358a051556...|2018-09-20|    3|\n",
      "|00be0a263381af381...|2018-09-20|    1|\n",
      "|023b48de81f6af9de...|2018-09-20|    9|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agged_df = train.groupBy(\"customer_id\", \"t_dat\").count()\n",
    "agged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7333485c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:27:32.366633Z",
     "iopub.status.busy": "2022-03-13T02:27:32.365999Z",
     "iopub.status.idle": "2022-03-13T02:29:37.179401Z",
     "shell.execute_reply": "2022-03-13T02:29:37.178271Z",
     "shell.execute_reply.started": "2022-03-13T01:40:19.647611Z"
    },
    "papermill": {
     "duration": 124.892579,
     "end_time": "2022-03-13T02:29:37.180166",
     "exception": true,
     "start_time": "2022-03-13T02:27:32.287587",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 02:28:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/03/13 02:28:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:28:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/03/13 02:29:32 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:32 ERROR Executor: Exception in task 5.0 in stage 30.0 (TID 231)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "22/03/13 02:29:33 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 30.0 (TID 231),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "22/03/13 02:29:33 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:33 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:34 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:34 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:35 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:35 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:35 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:36 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:36 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:36 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n",
      "22/03/13 02:29:36 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@24ade7bf rejected from java.util.concurrent.ThreadPoolExecutor@3ae9f341[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 230]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/03/13 02:29:36 WARN TaskSetManager: Lost task 5.0 in stage 30.0 (TID 231) (4e8d388e4ace executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n",
      "\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\n",
      "22/03/13 02:29:36 ERROR TaskSetManager: Task 5 in stage 30.0 failed 1 times; aborting job\n",
      "22/03/13 02:29:36 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$3665/0x0000000841319040@3aa94df6 rejected from java.util.concurrent.ThreadPoolExecutor@355f97fa[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 231]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:817)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:791)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o85.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 30.0 failed 1 times, most recent failure: Lost task 5.0 in stage 30.0 (TID 231) (4e8d388e4ace executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21/2167182783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t_dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o85.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 30.0 failed 1 times, most recent failure: Lost task 5.0 in stage 30.0 (TID 231) (4e8d388e4ace executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.sortedIterator(UnsafeKVExternalSorter.java:206)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:264)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:224)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:364)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:120)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:94)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$3579/0x000000084140d040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD$$Lambda$2498/0x0000000841031840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2461/0x0000000840ff6c40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n"
     ]
    }
   ],
   "source": [
    "agged_df.groupBy(\"customer_id\").pivot(\"t_dat\").sum(\"count\").fillna(0).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4c5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:03:53.058719Z",
     "iopub.status.busy": "2022-03-13T02:03:53.058410Z",
     "iopub.status.idle": "2022-03-13T02:03:53.130645Z",
     "shell.execute_reply": "2022-03-13T02:03:53.130031Z",
     "shell.execute_reply.started": "2022-03-13T02:03:53.058681Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.withColumn(\"real_price\", train.price * 590).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2621d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:10:56.699880Z",
     "iopub.status.busy": "2022-03-13T02:10:56.699514Z",
     "iopub.status.idle": "2022-03-13T02:10:56.708336Z",
     "shell.execute_reply": "2022-03-13T02:10:56.707199Z",
     "shell.execute_reply.started": "2022-03-13T02:10:56.699841Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146bb92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:21:10.004177Z",
     "iopub.status.busy": "2022-03-13T02:21:10.003871Z",
     "iopub.status.idle": "2022-03-13T02:21:53.374486Z",
     "shell.execute_reply": "2022-03-13T02:21:53.373870Z",
     "shell.execute_reply.started": "2022-03-13T02:21:10.004142Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.select(\"t_dat\", \"customer_id\").write.parquet(\"./parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442de7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8eb62f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T23:56:21.463125Z",
     "iopub.status.busy": "2022-03-12T23:56:21.462658Z",
     "iopub.status.idle": "2022-03-12T23:56:21.713076Z",
     "shell.execute_reply": "2022-03-12T23:56:21.712364Z",
     "shell.execute_reply.started": "2022-03-12T23:56:21.463079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article = spark.read.option('header','true').csv(CFG.ARTICLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cea7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T23:56:37.743570Z",
     "iopub.status.busy": "2022-03-12T23:56:37.743298Z",
     "iopub.status.idle": "2022-03-12T23:56:38.150240Z",
     "shell.execute_reply": "2022-03-12T23:56:38.149512Z",
     "shell.execute_reply.started": "2022-03-12T23:56:37.743541Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9dbf79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T23:58:37.139790Z",
     "iopub.status.busy": "2022-03-12T23:58:37.138962Z",
     "iopub.status.idle": "2022-03-12T23:58:37.397331Z",
     "shell.execute_reply": "2022-03-12T23:58:37.396468Z",
     "shell.execute_reply.started": "2022-03-12T23:58:37.139757Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a110368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T00:27:31.544070Z",
     "iopub.status.busy": "2022-03-13T00:27:31.543825Z",
     "iopub.status.idle": "2022-03-13T00:27:31.550720Z",
     "shell.execute_reply": "2022-03-13T00:27:31.549762Z",
     "shell.execute_reply.started": "2022-03-13T00:27:31.544045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747deed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T00:08:32.664982Z",
     "iopub.status.busy": "2022-03-13T00:08:32.664722Z",
     "iopub.status.idle": "2022-03-13T00:08:32.943338Z",
     "shell.execute_reply": "2022-03-13T00:08:32.941502Z",
     "shell.execute_reply.started": "2022-03-13T00:08:32.664955Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.select(\n",
    "    'article_id',\n",
    "    'index_name'\n",
    ").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f415b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T00:20:09.157992Z",
     "iopub.status.busy": "2022-03-13T00:20:09.157406Z",
     "iopub.status.idle": "2022-03-13T00:20:10.865815Z",
     "shell.execute_reply": "2022-03-13T00:20:10.865199Z",
     "shell.execute_reply.started": "2022-03-13T00:20:09.157960Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.select('index_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a994c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T00:20:53.617862Z",
     "iopub.status.busy": "2022-03-13T00:20:53.616942Z",
     "iopub.status.idle": "2022-03-13T00:20:54.196223Z",
     "shell.execute_reply": "2022-03-13T00:20:54.195312Z",
     "shell.execute_reply.started": "2022-03-13T00:20:53.617797Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.select('department_name').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35468778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T01:10:32.313683Z",
     "iopub.status.busy": "2022-03-13T01:10:32.313407Z",
     "iopub.status.idle": "2022-03-13T01:10:32.600982Z",
     "shell.execute_reply": "2022-03-13T01:10:32.600164Z",
     "shell.execute_reply.started": "2022-03-13T01:10:32.313654Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article.filter(article[\"department_name\"] == \"Jacket Casual\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a2081",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c979ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T01:50:58.573905Z",
     "iopub.status.busy": "2022-03-13T01:50:58.573079Z",
     "iopub.status.idle": "2022-03-13T01:50:58.830660Z",
     "shell.execute_reply": "2022-03-13T01:50:58.830027Z",
     "shell.execute_reply.started": "2022-03-13T01:50:58.573863Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer = spark.read.option('header','true').csv(CFG.CUSTOMER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baceec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T01:51:00.112483Z",
     "iopub.status.busy": "2022-03-13T01:51:00.112247Z",
     "iopub.status.idle": "2022-03-13T01:51:00.229680Z",
     "shell.execute_reply": "2022-03-13T01:51:00.228969Z",
     "shell.execute_reply.started": "2022-03-13T01:51:00.112459Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981e9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T01:58:58.223290Z",
     "iopub.status.busy": "2022-03-13T01:58:58.222996Z",
     "iopub.status.idle": "2022-03-13T01:59:00.250816Z",
     "shell.execute_reply": "2022-03-13T01:59:00.249893Z",
     "shell.execute_reply.started": "2022-03-13T01:58:58.223254Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer.groupby(\"FN\").count().sort(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb09ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T02:00:56.679657Z",
     "iopub.status.busy": "2022-03-13T02:00:56.679353Z",
     "iopub.status.idle": "2022-03-13T02:00:58.154980Z",
     "shell.execute_reply": "2022-03-13T02:00:58.153879Z",
     "shell.execute_reply.started": "2022-03-13T02:00:56.679616Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer.groupby(\"age\").count().sort(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f30228",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 399.080293,
   "end_time": "2022-03-13T02:29:38.222287",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-13T02:22:59.141994",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
