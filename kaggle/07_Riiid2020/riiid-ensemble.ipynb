{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026573,
     "end_time": "2021-01-07T00:08:32.340806",
     "exception": false,
     "start_time": "2021-01-07T00:08:32.314233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- lgb (ver68) + transformer (ver1)\n",
    "- ensemble ratioã€€7:3 \n",
    "- training size 350, boost round 3750 \n",
    "- change train size and boost round from ver 16 *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-07T00:08:32.394737Z",
     "iopub.status.busy": "2021-01-07T00:08:32.393997Z",
     "iopub.status.idle": "2021-01-07T00:09:02.823099Z",
     "shell.execute_reply": "2021-01-07T00:09:02.822430Z"
    },
    "papermill": {
     "duration": 30.457243,
     "end_time": "2021-01-07T00:09:02.823231",
     "exception": false,
     "start_time": "2021-01-07T00:08:32.365988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-07T00:09:02.883754Z",
     "iopub.status.busy": "2021-01-07T00:09:02.883056Z",
     "iopub.status.idle": "2021-01-07T00:09:04.925295Z",
     "shell.execute_reply": "2021-01-07T00:09:04.925875Z"
    },
    "papermill": {
     "duration": 2.077341,
     "end_time": "2021-01-07T00:09:04.926030",
     "exception": false,
     "start_time": "2021-01-07T00:09:02.848689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import datatable as dt\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "import riiideducation\n",
    "from bitarray import bitarray\n",
    "from functools import partial\n",
    "import pickle\n",
    "import math \n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from numba import jit\n",
    "import random\n",
    "\n",
    "tqdm_notebook.pandas(desc=\"progress: \")\n",
    "\n",
    "_ = np.seterr(divide='ignore', invalid='ignore')\n",
    "pd.set_option(\"max_rows\", 100)\n",
    "pd.set_option(\"max_columns\", 100)\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:09:04.989581Z",
     "iopub.status.busy": "2021-01-07T00:09:04.988896Z",
     "iopub.status.idle": "2021-01-07T00:09:04.992248Z",
     "shell.execute_reply": "2021-01-07T00:09:04.991726Z"
    },
    "papermill": {
     "duration": 0.039611,
     "end_time": "2021-01-07T00:09:04.992383",
     "exception": false,
     "start_time": "2021-01-07T00:09:04.952772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_bitarray():\n",
    "    a = bitarray(32737, endian='little')\n",
    "    a.setall(True)   \n",
    "    return a\n",
    "\n",
    "def clear_mem():\n",
    "    %reset -f out\n",
    "    %reset -f in\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:09:05.056697Z",
     "iopub.status.busy": "2021-01-07T00:09:05.055702Z",
     "iopub.status.idle": "2021-01-07T00:09:05.059164Z",
     "shell.execute_reply": "2021-01-07T00:09:05.058506Z"
    },
    "papermill": {
     "duration": 0.036139,
     "end_time": "2021-01-07T00:09:05.059330",
     "exception": false,
     "start_time": "2021-01-07T00:09:05.023191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FULL_TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026744,
     "end_time": "2021-01-07T00:09:05.113441",
     "exception": false,
     "start_time": "2021-01-07T00:09:05.086697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:09:05.174879Z",
     "iopub.status.busy": "2021-01-07T00:09:05.174209Z",
     "iopub.status.idle": "2021-01-07T00:09:05.176830Z",
     "shell.execute_reply": "2021-01-07T00:09:05.177324Z"
    },
    "papermill": {
     "duration": 0.036067,
     "end_time": "2021-01-07T00:09:05.177496",
     "exception": false,
     "start_time": "2021-01-07T00:09:05.141429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_types_dict = {\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32', \n",
    "    'content_id': 'int16', \n",
    "    'answered_correctly': 'int8', \n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool',\n",
    "}\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:09:05.233413Z",
     "iopub.status.busy": "2021-01-07T00:09:05.232767Z",
     "iopub.status.idle": "2021-01-07T00:10:34.424798Z",
     "shell.execute_reply": "2021-01-07T00:10:34.425313Z"
    },
    "papermill": {
     "duration": 89.221778,
     "end_time": "2021-01-07T00:10:34.425520",
     "exception": false,
     "start_time": "2021-01-07T00:09:05.203742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns=set(data_types_dict.keys())).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:10:34.483675Z",
     "iopub.status.busy": "2021-01-07T00:10:34.483004Z",
     "iopub.status.idle": "2021-01-07T00:11:02.544518Z",
     "shell.execute_reply": "2021-01-07T00:11:02.543864Z"
    },
    "papermill": {
     "duration": 28.091323,
     "end_time": "2021-01-07T00:11:02.544651",
     "exception": false,
     "start_time": "2021-01-07T00:10:34.453328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[target] != -1].reset_index(drop=True)\n",
    "\n",
    "train_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "train_df['prior_question_elapsed_time'].fillna(0, inplace=True)\n",
    "\n",
    "train_df = train_df.astype(data_types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:11:02.605473Z",
     "iopub.status.busy": "2021-01-07T00:11:02.604648Z",
     "iopub.status.idle": "2021-01-07T00:11:03.467642Z",
     "shell.execute_reply": "2021-01-07T00:11:03.466484Z"
    },
    "papermill": {
     "duration": 0.896172,
     "end_time": "2021-01-07T00:11:03.467812",
     "exception": false,
     "start_time": "2021-01-07T00:11:02.571640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number skills 13523\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = 180\n",
    "ACCEPTED_USER_CONTENT_SIZE = 4\n",
    "EMBED_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.1\n",
    "\n",
    "skills = train_df[\"content_id\"].unique()\n",
    "n_skill = len(skills)\n",
    "print(\"number skills\", len(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:11:03.529371Z",
     "iopub.status.busy": "2021-01-07T00:11:03.528684Z",
     "iopub.status.idle": "2021-01-07T00:11:18.302112Z",
     "shell.execute_reply": "2021-01-07T00:11:18.301310Z"
    },
    "papermill": {
     "duration": 14.806005,
     "end_time": "2021-01-07T00:11:18.302238",
     "exception": false,
     "start_time": "2021-01-07T00:11:03.496233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 350    \n",
    "train_index = list(train_df.groupby('user_id').tail(train_size).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:11:18.364211Z",
     "iopub.status.busy": "2021-01-07T00:11:18.362967Z",
     "iopub.status.idle": "2021-01-07T00:11:33.622600Z",
     "shell.execute_reply": "2021-01-07T00:11:33.621970Z"
    },
    "papermill": {
     "duration": 15.292956,
     "end_time": "2021-01-07T00:11:33.622735",
     "exception": false,
     "start_time": "2021-01-07T00:11:18.329779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"count\"] = 1\n",
    "\n",
    "# normal cumsum\n",
    "count_array = train_df.groupby(\"user_id\")[\"count\"].cumsum().values\n",
    "count_array = count_array[train_index]\n",
    "train_df.drop(\"count\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:11:34.097906Z",
     "iopub.status.busy": "2021-01-07T00:11:34.097158Z",
     "iopub.status.idle": "2021-01-07T00:18:17.210391Z",
     "shell.execute_reply": "2021-01-07T00:18:17.209216Z"
    },
    "papermill": {
     "duration": 403.559866,
     "end_time": "2021-01-07T00:18:17.210598",
     "exception": false,
     "start_time": "2021-01-07T00:11:33.650732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n",
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n",
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n",
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n"
     ]
    }
   ],
   "source": [
    "timediff_array = train_df.groupby(\"user_id\")[\"timestamp\"].diff().values\n",
    "timediff_array = timediff_array[train_index]\n",
    "clear_mem()\n",
    "\n",
    "timediff2_array = train_df.groupby(\"user_id\")[\"timestamp\"].diff(2).values\n",
    "timediff2_array = timediff2_array[train_index]\n",
    "clear_mem()\n",
    "\n",
    "timediff3_array = train_df.groupby(\"user_id\")[\"timestamp\"].diff(3).values\n",
    "timediff3_array = timediff3_array[train_index]\n",
    "clear_mem()\n",
    "\n",
    "timediff4_array = train_df.groupby(\"user_id\")[\"timestamp\"].diff(4).values\n",
    "timediff4_array = timediff4_array[train_index]\n",
    "clear_mem()\n",
    "\n",
    "user_timestamp_max_dict = train_df.groupby(\"user_id\")[\"timestamp\"].apply(lambda x: x[-4:].values).to_dict(defaultdict(partial(np.ndarray, 0, dtype=\"int64\")))\n",
    "\n",
    "train_df.drop(\"timestamp\", axis=1, inplace=True)\n",
    "\n",
    "timediff_array = np.nan_to_num(timediff_array, nan=-1)\n",
    "timediff2_array = np.nan_to_num(timediff2_array, nan=-1)\n",
    "timediff3_array = np.nan_to_num(timediff3_array, nan=-1)\n",
    "timediff4_array = np.nan_to_num(timediff4_array, nan=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:18:27.652259Z",
     "iopub.status.busy": "2021-01-07T00:18:18.169244Z",
     "iopub.status.idle": "2021-01-07T00:18:27.737971Z",
     "shell.execute_reply": "2021-01-07T00:18:27.737119Z"
    },
    "papermill": {
     "duration": 10.497103,
     "end_time": "2021-01-07T00:18:27.738131",
     "exception": false,
     "start_time": "2021-01-07T00:18:17.241028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_question_elapsed_time_array = train_df.prior_question_elapsed_time.values\n",
    "train_df.drop(\"prior_question_elapsed_time\", axis =1, inplace=True)\n",
    "prior_question_elapsed_time_array = prior_question_elapsed_time_array[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:18:27.816985Z",
     "iopub.status.busy": "2021-01-07T00:18:27.816260Z",
     "iopub.status.idle": "2021-01-07T00:18:42.873000Z",
     "shell.execute_reply": "2021-01-07T00:18:42.873616Z"
    },
    "papermill": {
     "duration": 15.104209,
     "end_time": "2021-01-07T00:18:42.873785",
     "exception": false,
     "start_time": "2021-01-07T00:18:27.769576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\n",
    "    '../input/riiid-test-answer-prediction/questions.csv', \n",
    "    usecols=[0, 3], \n",
    "    dtype={'question_id': 'int16', 'part': 'int8'} \n",
    ")\n",
    "\n",
    "additional_q_df = pd.read_csv('../input/riiid-question-clustering/question_cmnts.csv')\n",
    "questions_df[\"community\"] = additional_q_df[\"community\"].astype('int8')\n",
    "del additional_q_df \n",
    "    \n",
    "train_df = pd.merge(train_df, questions_df, left_on='content_id', right_on='question_id', how='left', right_index=True).reset_index(drop=True)\n",
    "train_df.drop(columns=['question_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:18:42.940980Z",
     "iopub.status.busy": "2021-01-07T00:18:42.940293Z",
     "iopub.status.idle": "2021-01-07T00:18:42.972031Z",
     "shell.execute_reply": "2021-01-07T00:18:42.972594Z"
    },
    "papermill": {
     "duration": 0.06736,
     "end_time": "2021-01-07T00:18:42.972740",
     "exception": false,
     "start_time": "2021-01-07T00:18:42.905380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "community_num = len(questions_df.community.unique())\n",
    "print(community_num)\n",
    "\n",
    "@jit\n",
    "def tag_accuracy(A, C):\n",
    "    ans = []\n",
    "    community_count = [0] * community_num\n",
    "    community_correct = [0] * community_num\n",
    "    for i in range(len(C)):\n",
    "        if community_count[C[i]]==0:\n",
    "            ans.append(-1)\n",
    "        else:\n",
    "            ans.append(community_correct[C[i]]/community_count[C[i]])\n",
    "        community_count[C[i]] +=1\n",
    "        community_correct[C[i]] += A[i]\n",
    "    return np.array(ans)\n",
    "\n",
    "@jit\n",
    "def tag_correct_last(A, C):\n",
    "    community_correct = [0] * community_num\n",
    "    for i in range(len(C)):\n",
    "        community_correct[C[i]] += A[i]\n",
    "    return np.array(community_correct)\n",
    "\n",
    "@jit\n",
    "def tag_count_last(A, C):\n",
    "    community_count = [0] * community_num\n",
    "    for i in range(len(C)):\n",
    "        community_count[C[i]] +=1\n",
    "    return np.array(community_count)\n",
    "\n",
    "def init_dict():\n",
    "    ans = [0] * community_num\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:18:43.037971Z",
     "iopub.status.busy": "2021-01-07T00:18:43.037249Z",
     "iopub.status.idle": "2021-01-07T00:21:06.737124Z",
     "shell.execute_reply": "2021-01-07T00:21:06.736282Z"
    },
    "papermill": {
     "duration": 143.7343,
     "end_time": "2021-01-07T00:21:06.737304",
     "exception": false,
     "start_time": "2021-01-07T00:18:43.003004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "tag_acc_array = train_df.groupby(\"user_id\").apply(lambda x: tag_accuracy(x[\"answered_correctly\"].values, x[\"community\"].values))\n",
    "tag_acc_array = np.hstack(tag_acc_array)\n",
    "tag_acc_array = tag_acc_array[train_index]\n",
    "\n",
    "user_community_count_dict = train_df.groupby(\"user_id\").apply(lambda x: tag_count_last(x[\"answered_correctly\"].values, x[\"community\"].values)).to_dict(defaultdict(init_dict))\n",
    "user_community_correct_dict = train_df.groupby(\"user_id\").apply(lambda x: tag_correct_last(x[\"answered_correctly\"].values, x[\"community\"].values)).to_dict(defaultdict(init_dict))\n",
    "    \n",
    "#\n",
    "community_agg = train_df.groupby('community')[target].agg(['count'])\n",
    "community_count_dict = community_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "community_count_array = train_df['community'].map(community_agg['count']).astype('int32').values\n",
    "del community_agg\n",
    "community_count_array = community_count_array[train_index]\n",
    "    \n",
    "community_array = train_df[\"community\"].values\n",
    "community_array = community_array[train_index]\n",
    "train_df.drop('community', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:21:06.809876Z",
     "iopub.status.busy": "2021-01-07T00:21:06.808461Z",
     "iopub.status.idle": "2021-01-07T00:21:30.868524Z",
     "shell.execute_reply": "2021-01-07T00:21:30.867918Z"
    },
    "papermill": {
     "duration": 24.099464,
     "end_time": "2021-01-07T00:21:30.868652",
     "exception": false,
     "start_time": "2021-01-07T00:21:06.769188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['lag'] = train_df.groupby('user_id')[target].shift()\n",
    "cum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n",
    "user_correctness_array = np.array(cum['cumsum'] / cum['cumcount'])\n",
    "user_correctness_array = user_correctness_array[train_index]\n",
    "train_df.drop(columns=['lag'], inplace=True)\n",
    "del cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:21:30.962524Z",
     "iopub.status.busy": "2021-01-07T00:21:30.960994Z",
     "iopub.status.idle": "2021-01-07T00:22:41.518910Z",
     "shell.execute_reply": "2021-01-07T00:22:41.518331Z"
    },
    "papermill": {
     "duration": 70.619083,
     "end_time": "2021-01-07T00:22:41.519033",
     "exception": false,
     "start_time": "2021-01-07T00:21:30.899950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def part_count_calc(P):\n",
    "    ans = []\n",
    "    part_count = [0] * 8\n",
    "    for i in range(len(P)):\n",
    "        part_count[P[i]] += 1\n",
    "        ans.append(part_count[P[i]])\n",
    "    return np.array(ans)\n",
    "\n",
    "@jit\n",
    "def part_count_dict_calc(P):\n",
    "    part_count = [0] * 8\n",
    "    for i in range(len(P)):\n",
    "        part_count[P[i]] += 1\n",
    "    return np.array(part_count)\n",
    "\n",
    "def part_dict_init():\n",
    "    ans = [0] * 8\n",
    "    return np.array(ans)\n",
    "\n",
    "part_count_array = train_df.groupby(\"user_id\").apply(lambda x: part_count_calc(x[\"part\"].values))\n",
    "part_count_array = np.hstack(part_count_array)\n",
    "part_count_array = part_count_array[train_index]\n",
    "part_ratio_array = part_count_array / count_array\n",
    "\n",
    "user_part_count_dict = train_df.groupby(\"user_id\").apply(lambda x: part_count_dict_calc(x[\"part\"].values)).to_dict(defaultdict(part_dict_init))\n",
    "\n",
    "part_array = train_df.part.values\n",
    "train_df.drop(\"part\", axis=1, inplace=True)\n",
    "part_array = part_array[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:22:41.593195Z",
     "iopub.status.busy": "2021-01-07T00:22:41.592561Z",
     "iopub.status.idle": "2021-01-07T00:23:24.693438Z",
     "shell.execute_reply": "2021-01-07T00:23:24.692565Z"
    },
    "papermill": {
     "duration": 43.142102,
     "end_time": "2021-01-07T00:23:24.693589",
     "exception": false,
     "start_time": "2021-01-07T00:22:41.551487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prior_question_had_explanation_mean\n",
    "train_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift().astype(bool)\n",
    "cum = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\n",
    "prior_question_had_explanation_mean_array = np.array(cum['cumsum'] / cum['cumcount'])\n",
    "prior_question_had_explanation_mean_array = prior_question_had_explanation_mean_array[train_index]\n",
    "\n",
    "user_prior_question_had_explanation_sum_agg = train_df.groupby('user_id')[\"prior_question_had_explanation\"].agg(['sum'])\n",
    "user_prior_question_had_explanation_sum_dict = user_prior_question_had_explanation_sum_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "train_df.drop(columns=['lag'], inplace=True)\n",
    "del cum, user_prior_question_had_explanation_sum_agg\n",
    "\n",
    "prior_question_had_explanation_array = train_df.prior_question_had_explanation.values\n",
    "train_df.drop('prior_question_had_explanation', axis=1, inplace=True)\n",
    "prior_question_had_explanation_array = prior_question_had_explanation_array[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:23:24.772988Z",
     "iopub.status.busy": "2021-01-07T00:23:24.772253Z",
     "iopub.status.idle": "2021-01-07T00:24:25.953345Z",
     "shell.execute_reply": "2021-01-07T00:24:25.953892Z"
    },
    "papermill": {
     "duration": 61.228124,
     "end_time": "2021-01-07T00:24:25.954043",
     "exception": false,
     "start_time": "2021-01-07T00:23:24.725919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n"
     ]
    }
   ],
   "source": [
    "first_attempt_df = pd.read_csv(\"../input/riiid-additional-data/content_first_attempt.csv\")\n",
    "first_attempt_array = first_attempt_df.first_attempt.values\n",
    "train_df[\"first_attempt\"] = first_attempt_array\n",
    "\n",
    "unique_attempt_array= train_df.groupby(\"user_id\")[\"first_attempt\"].cumsum().values\n",
    "train_df[\"unique_attempt\"] = unique_attempt_array\n",
    "user_unique_agg = train_df.groupby('user_id')[\"unique_attempt\"].agg(['max'])\n",
    "user_unique_dict = user_unique_agg['max'].astype('int32').to_dict(defaultdict(int))\n",
    "\n",
    "first_attempt_array = first_attempt_array[train_index]\n",
    "unique_attempt_array = unique_attempt_array[train_index]\n",
    "train_df.drop(['first_attempt', 'unique_attempt'], axis=1, inplace=True)\n",
    "del first_attempt_df, user_unique_agg\n",
    "\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:24:26.029574Z",
     "iopub.status.busy": "2021-01-07T00:24:26.028842Z",
     "iopub.status.idle": "2021-01-07T00:24:40.834814Z",
     "shell.execute_reply": "2021-01-07T00:24:40.834245Z"
    },
    "papermill": {
     "duration": 14.848778,
     "end_time": "2021-01-07T00:24:40.834935",
     "exception": false,
     "start_time": "2021-01-07T00:24:25.986157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n",
      "Flushing output cache (0 entries)\n",
      "Flushing input history\n"
     ]
    }
   ],
   "source": [
    "user_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n",
    "user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "del user_agg['sum']\n",
    "user_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "del user_agg['count']\n",
    "clear_mem()\n",
    "\n",
    "#\n",
    "content_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])\n",
    "content_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "content_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "\n",
    "content_count_array = train_df['content_id'].map(content_agg['count']).astype('int32').values\n",
    "content_id_array = train_df['content_id'].map(content_agg['sum'] / content_agg['count']).values\n",
    "del content_agg\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:24:50.099335Z",
     "iopub.status.busy": "2021-01-07T00:24:50.098473Z",
     "iopub.status.idle": "2021-01-07T00:25:24.038146Z",
     "shell.execute_reply": "2021-01-07T00:25:24.038673Z"
    },
    "papermill": {
     "duration": 43.17064,
     "end_time": "2021-01-07T00:25:24.038878",
     "exception": false,
     "start_time": "2021-01-07T00:24:40.868238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# benefit of solving difficult questions\n",
    "#point_array = 1 / (content_id_array + 0.1)\n",
    "content_id_array = content_id_array[train_index]\n",
    "content_count_array = content_count_array[train_index]\n",
    "\n",
    "with open('../input/riiid-premade-data/got_point_array.pickle','rb') as f:\n",
    "    got_point_array = pickle.load(f)\n",
    "got_point_array = got_point_array[train_index]\n",
    "    \n",
    "with open('../input/riiid-premade-data/user_point_sum_dict.pickle','rb') as f:\n",
    "    user_point_sum_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:25:24.110471Z",
     "iopub.status.busy": "2021-01-07T00:25:24.109748Z",
     "iopub.status.idle": "2021-01-07T00:25:51.696887Z",
     "shell.execute_reply": "2021-01-07T00:25:51.696303Z"
    },
    "papermill": {
     "duration": 27.623584,
     "end_time": "2021-01-07T00:25:51.697023",
     "exception": false,
     "start_time": "2021-01-07T00:25:24.073439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 3.01 s, total: 8.27 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df.drop([\"content_id\"], axis=1, inplace=True)\n",
    "\n",
    "with open('../input/riiid-premade-data/user_content_dict.pickle','rb') as f:\n",
    "    user_content_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:25:51.841267Z",
     "iopub.status.busy": "2021-01-07T00:25:51.840606Z",
     "iopub.status.idle": "2021-01-07T00:26:15.148853Z",
     "shell.execute_reply": "2021-01-07T00:26:15.148194Z"
    },
    "papermill": {
     "duration": 23.417109,
     "end_time": "2021-01-07T00:26:15.148977",
     "exception": false,
     "start_time": "2021-01-07T00:25:51.731868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../input/riiid-premade-data/answered_correctly_last7_array.pickle','rb') as f:\n",
    "    answered_correctly_last7_array = pickle.load(f)\n",
    "answered_correctly_last7_array = answered_correctly_last7_array[train_index]\n",
    "    \n",
    "with open('../input/riiid-premade-data/user_last7_answer_dict.pickle','rb') as f:\n",
    "    user_last7_answer_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:24.912658Z",
     "iopub.status.busy": "2021-01-07T00:26:24.911571Z",
     "iopub.status.idle": "2021-01-07T00:26:24.995097Z",
     "shell.execute_reply": "2021-01-07T00:26:24.994375Z"
    },
    "papermill": {
     "duration": 9.811763,
     "end_time": "2021-01-07T00:26:24.995212",
     "exception": false,
     "start_time": "2021-01-07T00:26:15.183449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answered_correctly_array = train_df[target].values\n",
    "train_df.drop(target, axis=1, inplace=True)\n",
    "answered_correctly_array = answered_correctly_array[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034891,
     "end_time": "2021-01-07T00:26:25.066526",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.031635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:25.147852Z",
     "iopub.status.busy": "2021-01-07T00:26:25.147037Z",
     "iopub.status.idle": "2021-01-07T00:26:25.150721Z",
     "shell.execute_reply": "2021-01-07T00:26:25.150203Z"
    },
    "papermill": {
     "duration": 0.048424,
     "end_time": "2021-01-07T00:26:25.150836",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.102412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:25.231183Z",
     "iopub.status.busy": "2021-01-07T00:26:25.230509Z",
     "iopub.status.idle": "2021-01-07T00:26:25.233886Z",
     "shell.execute_reply": "2021-01-07T00:26:25.234355Z"
    },
    "papermill": {
     "duration": 0.049042,
     "end_time": "2021-01-07T00:26:25.234500",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.185458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "features_dict = {\n",
    "    'content_id': content_id_array,\n",
    "    'prior_question_elapsed_time': prior_question_elapsed_time_array,\n",
    "    'prior_question_had_explanation':  prior_question_had_explanation_array,\n",
    "    'user_correctness': user_correctness_array,\n",
    "    'part': part_array,\n",
    "    'content_count': content_count_array,\n",
    "    'count': count_array,\n",
    "    'first_attempt': first_attempt_array,\n",
    "    'unique_attempt': unique_attempt_array,\n",
    "    'part_count': part_count_array,\n",
    "    'part_ratio': part_ratio_array,\n",
    "    'prior_question_had_explanation_mean': prior_question_had_explanation_mean_array,\n",
    "    'got_point': got_point_array,\n",
    "    'answered_correctly_last7': answered_correctly_last7_array,   \n",
    "    'timediff': timediff_array,\n",
    "    'timediff2': timediff2_array,\n",
    "    'timediff3': timediff3_array,\n",
    "    'timediff4': timediff4_array,\n",
    "    'community': community_array,\n",
    "    'tag_acc': tag_acc_array,\n",
    "    'community_count': community_count_array,\n",
    "}\n",
    "\n",
    "features = list(features_dict.keys())\n",
    "print(len(features))\n",
    "\n",
    "del content_id_array, prior_question_elapsed_time_array, prior_question_had_explanation_array,\n",
    "del user_correctness_array, part_array,\n",
    "del content_count_array, count_array, first_attempt_array, unique_attempt_array,\n",
    "del part_ratio_array, part_count_array,\n",
    "del prior_question_had_explanation_mean_array, got_point_array, \n",
    "del answered_correctly_last7_array\n",
    "del timediff_array, timediff2_array, timediff3_array, timediff4_array\n",
    "del community_array, tag_acc_array, community_count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:25.310837Z",
     "iopub.status.busy": "2021-01-07T00:26:25.310136Z",
     "iopub.status.idle": "2021-01-07T00:26:25.312831Z",
     "shell.execute_reply": "2021-01-07T00:26:25.313324Z"
    },
    "papermill": {
     "duration": 0.044246,
     "end_time": "2021-01-07T00:26:25.313478",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.269232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42726873 22\n"
     ]
    }
   ],
   "source": [
    "if FULL_TRAIN:\n",
    "    print(len(train_index), len(features)+1)\n",
    "else:\n",
    "    print((len(train_index), len(features)+1), (len(valid_index), len(features)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035521,
     "end_time": "2021-01-07T00:26:25.384324",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.348803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:25.462886Z",
     "iopub.status.busy": "2021-01-07T00:26:25.462158Z",
     "iopub.status.idle": "2021-01-07T00:26:25.464623Z",
     "shell.execute_reply": "2021-01-07T00:26:25.465137Z"
    },
    "papermill": {
     "duration": 0.044753,
     "end_time": "2021-01-07T00:26:25.465278",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.420525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'seed': 2020, #42\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05, #0.1\n",
    "    #'max_bin': 800,\n",
    "    'num_leaves': 300, #80\n",
    "    'max_depth': 15,\n",
    "    'subsample': 0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:25.544364Z",
     "iopub.status.busy": "2021-01-07T00:26:25.543703Z",
     "iopub.status.idle": "2021-01-07T00:26:36.838316Z",
     "shell.execute_reply": "2021-01-07T00:26:36.837415Z"
    },
    "papermill": {
     "duration": 11.337533,
     "end_time": "2021-01-07T00:26:36.838456",
     "exception": false,
     "start_time": "2021-01-07T00:26:25.500923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = answered_correctly_array.astype(np.float32)\n",
    "del answered_correctly_array\n",
    "\n",
    "X_train = np.ndarray(shape=(len(train_index), len(features)), dtype=np.float32)\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    X_train[:,idx] = features_dict[feature].astype(np.float32).reshape(-1)\n",
    "    del features_dict[feature]\n",
    "tr_data = lgb.Dataset(X_train, label=y_train)\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T00:26:36.915550Z",
     "iopub.status.busy": "2021-01-07T00:26:36.914905Z",
     "iopub.status.idle": "2021-01-07T03:43:46.402078Z",
     "shell.execute_reply": "2021-01-07T03:43:46.401106Z"
    },
    "papermill": {
     "duration": 11829.528231,
     "end_time": "2021-01-07T03:43:46.402286",
     "exception": false,
     "start_time": "2021-01-07T00:26:36.874055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts\n"
     ]
    }
   ],
   "source": [
    "print(\"training starts\")\n",
    "model = lgb.train(\n",
    "        params, \n",
    "        tr_data, \n",
    "        num_boost_round= 3750,\n",
    "        valid_sets=None, \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036707,
     "end_time": "2021-01-07T03:43:46.479223",
     "exception": false,
     "start_time": "2021-01-07T03:43:46.442516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T03:43:46.609179Z",
     "iopub.status.busy": "2021-01-07T03:43:46.564640Z",
     "iopub.status.idle": "2021-01-07T03:44:03.306438Z",
     "shell.execute_reply": "2021-01-07T03:44:03.305659Z"
    },
    "papermill": {
     "duration": 16.79101,
     "end_time": "2021-01-07T03:44:03.306579",
     "exception": false,
     "start_time": "2021-01-07T03:43:46.515569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size = 200, forward_expansion = 1, bn_size=MAX_SEQ - 1, dropout=0.2):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.lr1 = nn.Linear(state_size, forward_expansion * state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(bn_size)\n",
    "        self.lr2 = nn.Linear(forward_expansion * state_size, state_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.lr1(x))\n",
    "        x = self.bn(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, heads = 8, dropout = DROPOUT, forward_expansion = 1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=heads, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FFN(embed_dim, forward_expansion = forward_expansion, dropout=dropout)\n",
    "        self.layer_normal_2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, value, key, query, att_mask):\n",
    "        att_output, att_weight = self.multi_att(value, key, query, attn_mask=att_mask)\n",
    "        att_output = self.dropout(self.layer_normal(att_output + value))\n",
    "        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.dropout(self.layer_normal_2(x + att_output))\n",
    "        return x.squeeze(-1), att_weight\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, num_layers=1, heads = 8):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_skill, self.embed_dim = n_skill, embed_dim\n",
    "        self.embedding = nn.Embedding(2 * n_skill + 1, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq - 1, embed_dim)\n",
    "        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, forward_expansion = forward_expansion) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, question_ids):\n",
    "        device = x.device\n",
    "        x = self.embedding(x)\n",
    "        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n",
    "        pos_x = self.pos_embedding(pos_id)\n",
    "        x = self.dropout(x + pos_x)\n",
    "        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        e = self.e_embedding(question_ids)\n",
    "        e = e.permute(1, 0, 2)\n",
    "        for layer in self.layers:\n",
    "            att_mask = future_mask(e.size(0)).to(device)\n",
    "            x, att_weight = layer(e, x, x, att_mask=att_mask)\n",
    "            x = x.permute(1, 0, 2)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x, att_weight\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = (np.triu(np.ones([seq_length, seq_length]), k = 1)).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "    \n",
    "class SAKTModel(nn.Module):\n",
    "    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, enc_layers=1, heads = 8):\n",
    "        super(SAKTModel, self).__init__()\n",
    "        self.encoder = Encoder(n_skill, max_seq, embed_dim, dropout, forward_expansion, num_layers=enc_layers)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x, question_ids):\n",
    "        x, att_weight = self.encoder(x, question_ids)\n",
    "        x = self.pred(x)\n",
    "        return x.squeeze(-1), att_weight\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, n_skill, max_seq=100):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.samples, self.user_ids, self.test_df = samples, [x for x in test_df[\"user_id\"].unique()], test_df\n",
    "        self.n_skill, self.max_seq = n_skill, max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "        \n",
    "        user_id = test_info['user_id']\n",
    "        target_id = test_info['content_id']\n",
    "        \n",
    "        content_id_seq = np.zeros(self.max_seq, dtype=int)\n",
    "        answered_correctly_seq = np.zeros(self.max_seq, dtype=int)\n",
    "        \n",
    "        if user_id in self.samples.index:\n",
    "            content_id, answered_correctly = self.samples[user_id]\n",
    "            \n",
    "            seq_len = len(content_id)\n",
    "            \n",
    "            if seq_len >= self.max_seq:\n",
    "                content_id_seq = content_id[-self.max_seq:]\n",
    "                answered_correctly_seq = answered_correctly[-self.max_seq:]\n",
    "            else:\n",
    "                content_id_seq[-seq_len:] = content_id\n",
    "                answered_correctly_seq[-seq_len:] = answered_correctly\n",
    "                \n",
    "        x = content_id_seq[1:].copy()\n",
    "        x += (answered_correctly_seq[1:] == 1) * self.n_skill\n",
    "        \n",
    "        questions = np.append(content_id_seq[2:], [target_id])\n",
    "        \n",
    "        return x, questions\n",
    "\n",
    "def create_model():\n",
    "    return SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_SIZE, forward_expansion=1, enc_layers=1, heads=8, dropout=0.1)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAKT_model = create_model()\n",
    "\n",
    "SAKT_model.load_state_dict(torch.load(\"../input/riiid-transformer-pretrain/sakt.pth\", map_location='cpu'))\n",
    "\n",
    "SAKT_model.to(device)\n",
    "SAKT_model.eval()\n",
    "\n",
    "group = pickle.load(open(\"../input/riiid-transformer-pretrain/group.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T03:44:03.386862Z",
     "iopub.status.busy": "2021-01-07T03:44:03.385901Z",
     "iopub.status.idle": "2021-01-07T03:44:03.389241Z",
     "shell.execute_reply": "2021-01-07T03:44:03.388622Z"
    },
    "papermill": {
     "duration": 0.045838,
     "end_time": "2021-01-07T03:44:03.389371",
     "exception": false,
     "start_time": "2021-01-07T03:44:03.343533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "prior_test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-07T03:44:03.497709Z",
     "iopub.status.busy": "2021-01-07T03:44:03.486919Z",
     "iopub.status.idle": "2021-01-07T03:44:04.822638Z",
     "shell.execute_reply": "2021-01-07T03:44:04.823183Z"
    },
    "papermill": {
     "duration": 1.396454,
     "end_time": "2021-01-07T03:44:04.823329",
     "exception": false,
     "start_time": "2021-01-07T03:44:03.426875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 211 ms, total: 1.62 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    if prior_test_df is not None:\n",
    "        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop=True)\n",
    "        \n",
    "        ######## Transformer State Update\n",
    "        prev_group = prior_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values))\n",
    "        for prev_user_id in prev_group.index:\n",
    "            prev_group_content = prev_group[prev_user_id][0]\n",
    "            prev_group_answered_correctly = prev_group[prev_user_id][1]\n",
    "            if prev_user_id in group.index:\n",
    "                group[prev_user_id] = (np.append(group[prev_user_id][0], prev_group_content), \n",
    "                                       np.append(group[prev_user_id][1], prev_group_answered_correctly))\n",
    "            else:\n",
    "                group[prev_user_id] = (prev_group_content, prev_group_answered_correctly)\n",
    "            \n",
    "            if len(group[prev_user_id][0]) > MAX_SEQ:\n",
    "                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n",
    "                new_group_answered_correctly = group[prev_user_id][1][-MAX_SEQ:]\n",
    "                group[prev_user_id] = (new_group_content, new_group_answered_correctly)\n",
    "        ######### Transformer State Update\n",
    "        \n",
    "        user_ids = prior_test_df['user_id'].values\n",
    "        content_ids = prior_test_df['content_id'].values\n",
    "        targets = prior_test_df[target].values\n",
    "         \n",
    "        for user_id, content_id, answered_correctly, first_attempt_ornot, prior_explanation, prior_point, prior_community in zip(user_ids, content_ids, \n",
    "                                                            targets, \n",
    "                                                            prior_f_attempt_arrays,\n",
    "                                                            p_prior_question_had_explanation,\n",
    "                                                            prior_point_array,\n",
    "                                                            prior_community_arrays):\n",
    "            \n",
    "            user_sum_dict[user_id] += answered_correctly\n",
    "            user_count_dict[user_id] += 1\n",
    "            content_sum_dict[content_id] += answered_correctly\n",
    "            content_count_dict[content_id] += 1\n",
    "            user_unique_dict[user_id] += first_attempt_ornot\n",
    "            user_prior_question_had_explanation_sum_dict[user_id] += prior_explanation\n",
    "            user_point_sum_dict[user_id] += prior_point * answered_correctly\n",
    "            if len(user_last7_answer_dict[user_id])==7:\n",
    "                user_last7_answer_dict[user_id] = np.concatenate([user_last7_answer_dict[user_id],[answered_correctly]])[1:]\n",
    "            else:\n",
    "                user_last7_answer_dict[user_id] = np.concatenate([user_last7_answer_dict[user_id],[answered_correctly]])\n",
    "            \n",
    "            user_community_correct_dict[user_id][prior_community] += answered_correctly\n",
    "            user_community_count_dict[user_id][prior_community] += 1\n",
    "            community_count_dict[prior_community] += 1            \n",
    "            \n",
    "    prior_test_df = test_df.copy()\n",
    "    \n",
    "    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    ####### Transformer\n",
    "    test_dataset = TestDataset(group, test_df, n_skill, max_seq=MAX_SEQ)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_df), shuffle=False)\n",
    "    \n",
    "    item = next(iter(test_dataloader))\n",
    "    x = item[0].to(device).long()\n",
    "    target_id = item[1].to(device).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = SAKT_model(x, target_id)\n",
    "        \n",
    "    output = torch.sigmoid(output)\n",
    "    output = output[:, -1]\n",
    "    SAKT_outs = output.cpu().numpy()\n",
    "    ######## Transformer\n",
    "    \n",
    "    test_df = pd.merge(test_df, questions_df, left_on='content_id', right_on='question_id', how='left', right_index=True).reset_index(drop=True)\n",
    "    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')\n",
    "    test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(0)\n",
    "\n",
    "    p_prior_question_had_explanation = test_df['prior_question_had_explanation'].values\n",
    "    prior_community_arrays = test_df['community'].values\n",
    "    \n",
    "    user_sum = np.zeros(len(test_df), dtype=np.int16)\n",
    "    user_count = np.zeros(len(test_df), dtype=np.int16)\n",
    "    content_sum = np.zeros(len(test_df), dtype=np.int32)\n",
    "    content_count = np.zeros(len(test_df), dtype=np.int32)\n",
    "    part_count = np.zeros(len(test_df), dtype=np.int32)\n",
    "    first_attempt_values = []\n",
    "    user_unique_count = np.zeros(len(test_df), dtype=np.int32)\n",
    "    user_prior_question_had_explanation_sum = np.zeros(len(test_df), dtype=np.int32)\n",
    "    got_point_array = np.zeros(len(test_df), dtype=np.float32)\n",
    "    user_last7_accuracy_array = np.zeros(len(test_df), dtype=np.float32)\n",
    "    timediff_array = np.zeros(len(test_df), dtype = np.int64)\n",
    "    timediff2_array = np.zeros(len(test_df), dtype = np.int64)\n",
    "    timediff3_array = np.zeros(len(test_df), dtype = np.int64)\n",
    "    timediff4_array = np.zeros(len(test_df), dtype = np.int64)\n",
    "    tag_acc_array = np.zeros(len(test_df), dtype=np.float32)\n",
    "    community_count_array = np.zeros(len(test_df), dtype=np.int32)\n",
    "    \n",
    "    for i, (user_id, content_id, timestamp, community, part) in enumerate(zip(test_df['user_id'].values, \n",
    "                                                             test_df['content_id'].values,\n",
    "                                                             test_df['timestamp'].values,\n",
    "                                                             test_df['community'].values,\n",
    "                                                             test_df['part'].values)):\n",
    "        user_sum[i] = user_sum_dict[user_id]\n",
    "        user_count[i] = user_count_dict[user_id]\n",
    "        content_sum[i] = content_sum_dict[content_id]\n",
    "        content_count[i] = content_count_dict[content_id]\n",
    "        part_count[i] = user_part_count_dict[user_id][part] + 1\n",
    "        user_part_count_dict[user_id][part] += 1\n",
    "        first_attempt_values.append(user_content_dict[user_id][content_id])\n",
    "        user_content_dict[user_id][content_id] = False             \n",
    "        user_unique_count[i] = user_unique_dict[user_id]\n",
    "        user_prior_question_had_explanation_sum[i] = user_prior_question_had_explanation_sum_dict[user_id]\n",
    "        got_point_array[i] = user_point_sum_dict[user_id]\n",
    "        \n",
    "        if len(user_last7_answer_dict[user_id])==7:\n",
    "            user_last7_accuracy_array[i] = user_last7_answer_dict[user_id].mean()\n",
    "        else:\n",
    "            user_last7_accuracy_array[i] = np.nan\n",
    "            \n",
    "        if len(user_timestamp_max_dict[user_id]) ==0:\n",
    "            timediff_array[i] = -1\n",
    "            timediff2_array[i] = -1\n",
    "            timediff3_array[i] = -1\n",
    "            timediff4_array[i] = -1\n",
    "            user_timestamp_max_dict[user_id] = np.concatenate([user_timestamp_max_dict[user_id],[timestamp]])\n",
    "            \n",
    "        elif len(user_timestamp_max_dict[user_id]) ==1:\n",
    "            timediff_array[i] = timestamp - user_timestamp_max_dict[user_id][0]\n",
    "            timediff2_array[i] = -1\n",
    "            timediff3_array[i] = -1\n",
    "            timediff4_array[i] = -1\n",
    "            user_timestamp_max_dict[user_id] = np.concatenate([user_timestamp_max_dict[user_id],[timestamp]])\n",
    "            \n",
    "        elif len(user_timestamp_max_dict[user_id]) ==2:\n",
    "            timediff_array[i] = timestamp - user_timestamp_max_dict[user_id][1]\n",
    "            timediff2_array[i] = timestamp - user_timestamp_max_dict[user_id][0]\n",
    "            timediff3_array[i] = -1\n",
    "            timediff4_array[i] = -1\n",
    "            user_timestamp_max_dict[user_id] = np.concatenate([user_timestamp_max_dict[user_id],[timestamp]])  \n",
    "            \n",
    "        elif len(user_timestamp_max_dict[user_id]) ==3:\n",
    "            timediff_array[i] = timestamp - user_timestamp_max_dict[user_id][2]\n",
    "            timediff2_array[i] = timestamp - user_timestamp_max_dict[user_id][1]\n",
    "            timediff3_array[i] = timestamp - user_timestamp_max_dict[user_id][0]\n",
    "            timediff4_array[i] = -1\n",
    "            user_timestamp_max_dict[user_id] = np.concatenate([user_timestamp_max_dict[user_id],[timestamp]]) \n",
    "    \n",
    "        else:\n",
    "            timediff_array[i] = timestamp - user_timestamp_max_dict[user_id][3]\n",
    "            timediff2_array[i] = timestamp - user_timestamp_max_dict[user_id][2]\n",
    "            timediff3_array[i] = timestamp - user_timestamp_max_dict[user_id][1]\n",
    "            timediff4_array[i] = timestamp - user_timestamp_max_dict[user_id][0]\n",
    "            user_timestamp_max_dict[user_id] = np.concatenate([user_timestamp_max_dict[user_id],[timestamp]])[1:]  \n",
    "            \n",
    "        if user_community_count_dict[user_id][community] == 0:\n",
    "            tag_acc_array[i] = -1\n",
    "        else:\n",
    "            tag_acc_array[i] = user_community_correct_dict[user_id][community] / user_community_count_dict[user_id][community]\n",
    "            \n",
    "        community_count_array[i] = community_count_dict[community]\n",
    "        \n",
    "        \n",
    "    test_df['user_correctness'] = user_sum / user_count\n",
    "    test_df['content_count'] = content_count\n",
    "    test_df['content_id'] = content_sum / content_count\n",
    "    test_df['count'] = 1\n",
    "    test_df['count'] = test_df.groupby(\"user_id\")[\"count\"].cumsum()\n",
    "    test_df['count'] += user_count\n",
    "    test_df['part_count'] = part_count\n",
    "    test_df['part_ratio'] = part_count / test_df['count'].values\n",
    "    test_df[\"first_attempt\"] = first_attempt_values\n",
    "    test_df[\"unique_attempt\"] = test_df.groupby(\"user_id\")[\"first_attempt\"].cumsum()\n",
    "    test_df[\"unique_attempt\"] += user_unique_count\n",
    "    test_df['prior_question_had_explanation_mean'] = user_prior_question_had_explanation_sum / user_count\n",
    "    test_df['got_point'] = got_point_array / user_count\n",
    "    test_df['answered_correctly_last7'] = user_last7_accuracy_array\n",
    "    test_df['timediff'] = timediff_array\n",
    "    test_df['timediff2'] = timediff2_array\n",
    "    test_df['timediff3'] = timediff3_array\n",
    "    test_df['timediff4'] = timediff4_array\n",
    "    test_df['tag_acc'] = tag_acc_array\n",
    "    test_df['community_count'] = community_count_array\n",
    "    \n",
    "    prior_f_attempt_arrays = test_df['first_attempt'].values\n",
    "    prior_point_array = 1 / (test_df.content_id.values + 0.1)\n",
    "    \n",
    "    lgb_out = model.predict(test_df[features])\n",
    "    test_df[target] = SAKT_outs * 0.3 + lgb_out * 0.7\n",
    "    env.predict(test_df[['row_id', target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.036528,
     "end_time": "2021-01-07T03:44:04.897563",
     "exception": false,
     "start_time": "2021-01-07T03:44:04.861035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 12938.023307,
   "end_time": "2021-01-07T03:44:05.445407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-07T00:08:27.422100",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
