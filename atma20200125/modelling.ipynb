{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from functools import partial\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from hyperopt import hp, tpe, Trials, fmin, space_eval\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "np.set_printoptions(precision=8)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../20200125atma/input/train.csv\")\n",
    "test = pd.read_csv(\"../../../20200125atma/input/test.csv\")\n",
    "userlog = pd.read_csv(\"../../../20200125atma/input/user_log.csv\")\n",
    "poi = pd.read_csv(\"../../../20200125atma/input/poi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "list_y = []\n",
    "for i in range(13411):\n",
    "    img = cv2.imread('images/'+str(i)+'.png')\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  # HSV 色空間に変換\n",
    "\n",
    "    red = cv2.inRange(hsv, np.array([145, 70, 0]), np.array([180, 255, 255]))\n",
    "    yellow = cv2.inRange(hsv, (15,0,0), (36, 255, 255))\n",
    "    green = cv2.inRange(hsv, np.array([30, 190, 0]), np.array([90, 255, 255]))\n",
    "    blue = cv2.inRange(hsv, np.array([108, 121, 0]), np.array([120, 255, 255]))\n",
    "    white = cv2.inRange(hsv, np.array([108, 21, 0]), np.array([255, 70, 255]))\n",
    "\n",
    "    # 白だけゴミがあるので、収縮演算\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    white = cv2.erode(yellow, kernel)\n",
    "\n",
    "    bin_imgs = {'red': red, 'yellow': yellow, 'green': green,\n",
    "            'blue': blue, 'white': white}\n",
    "\n",
    "    for label, bin_img in bin_imgs.items():\n",
    "        contours, _ = cv2.findContours(bin_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = list(filter(lambda cnt: len(cnt) > 30, contours))\n",
    "        count = len(contours)\n",
    "    \n",
    "        if label == \"yellow\":\n",
    "            list_y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train, test, userlog, poi):\n",
    "    userlog[\"sysname\"][userlog.sysname == \"ANDROID\"] = \"Android\"\n",
    "    userlog[\"lang\"][userlog.lang == \"ja_JP\"] = \"ja_JP\"\n",
    "    poi = poi.rename(columns={\"latitude\": \"store_lat\", \"longitude\": \"store_lon\"})\n",
    "    train = pd.merge(train, poi, on =\"pid\", how = \"left\")\n",
    "    test = pd.merge(test, poi, on =\"pid\", how = \"left\")\n",
    "    userlog = pd.merge(userlog, train[[\"session_id\", \"store_lat\", \"store_lon\", \"radius\"]], on=\"session_id\", how=\"left\")\n",
    "    userlog = pd.merge(userlog, test[[\"session_id\", \"store_lat\", \"store_lon\", \"radius\"]], on=\"session_id\", how=\"left\")\n",
    "    userlog[\"store_lat\"] = np.nanmax(userlog[[\"store_lat_x\", \"store_lat_y\"]], axis=1)\n",
    "    userlog[\"store_lon\"] = np.nanmax(userlog[[\"store_lon_x\", \"store_lon_y\"]], axis=1)\n",
    "    userlog[\"radius\"] = np.nanmax(userlog[[\"radius_x\", \"radius_y\"]], axis=1)\n",
    "    drop_features = [\"store_lat_x\", \"store_lat_y\", \"store_lon_x\", \"store_lon_y\", \"radius_x\", \"radius_y\"]\n",
    "    userlog.drop(drop_features, axis=1, inplace=True)\n",
    "    userlog[\"distance\"] = np.sqrt((userlog[\"latitude\"]- userlog[\"store_lat\"])**2 + (userlog[\"longitude\"]-userlog[\"store_lon\"])** 2 )\n",
    "    userlog[\"time\"] = userlog[\"hour\"].map(str) + str(\":\") + userlog[\"minute\"].map(str) + str(\":\") + userlog[\"second\"].map(str)\n",
    "    userlog[\"time\"] = pd.to_datetime(userlog['time'],format= '%H:%M:%S' )\n",
    "    userlog = userlog.sort_values([\"session_id\", \"hour\", \"minute\", \"second\"]).reset_index(drop=True)\n",
    "    userlog[\"virtual_dis\"] = 6370 * np.arccos(np.sin(userlog[\"latitude\"])*np.sin(userlog[\"store_lat\"]) + np.cos(userlog[\"latitude\"])*np.cos(userlog[\"store_lat\"])*np.cos(userlog[\"longitude\"]-userlog[\"store_lon\"]))\n",
    "    userlog[\"in_store\"] = userlog[\"virtual_dis\"] < userlog[\"radius\"]\n",
    "    #userlog[\"JP\"] = userlog[\"lang\"].apply(lambda x: 1 if \"ja\" in x or \"JP\" in x else 0)\n",
    "    #tmp = userlog.groupby(\"session_id\").agg([\"std\"])[[\"latitude\", \"longitude\", \"categorical_1\", \n",
    "    #                                               \"categorical_2\", \"categorical_3\", \"categorical_4\", \"categorical_5\", \"categorical_6\"]].reset_index(drop=False)\n",
    "    #userlog = pd.merge(userlog, tmp, on=\"session_id\", how=\"left\")\n",
    "    return train, test, userlog, poi\n",
    "train, test, userlog, poi = preprocess(train, test, userlog, poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(train, test, userlog):\n",
    "    os_list =  sorted(list(set(userlog['sysname'].unique())))\n",
    "    os_map = dict(zip(os_list, np.arange(len(os_list))))\n",
    "    userlog[\"sysname\"] = userlog[\"sysname\"].map(os_map)\n",
    "    \n",
    "    lang_list =  sorted(list(set(userlog['lang'].unique())))\n",
    "    lang_map = dict(zip(lang_list, np.arange(len(lang_list))))\n",
    "    userlog[\"lang\"] = userlog[\"lang\"].map(lang_map)\n",
    "    \n",
    "    timezone_list =  sorted(list(set(userlog['timezone'].unique())))\n",
    "    timezone_map = dict(zip(timezone_list, np.arange(len(timezone_list))))\n",
    "    userlog[\"timezone\"] = userlog[\"timezone\"].map(timezone_map)\n",
    "    \n",
    "    #cat1_list =  sorted(list(set(userlog['categorical_1'].unique())))\n",
    "    #cat2_list =  sorted(list(set(userlog['categorical_2'].unique())))\n",
    "    #cat3_list =  sorted(list(set(userlog['categorical_3'].unique())))\n",
    "    #cat4_list =  sorted(list(set(userlog['categorical_4'].unique())))\n",
    "    #cat5_list =  sorted(list(set(userlog['categorical_5'].unique())))\n",
    "    #cat6_list =  sorted(list(set(userlog['categorical_6'].unique())))\n",
    "\n",
    "    return train, test, userlog\n",
    "train, test, userlog = encode(train, test, userlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sysname</th>\n",
       "      <th>optout</th>\n",
       "      <th>lang</th>\n",
       "      <th>timezone</th>\n",
       "      <th>session_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>categorical_1</th>\n",
       "      <th>categorical_2</th>\n",
       "      <th>categorical_3</th>\n",
       "      <th>categorical_4</th>\n",
       "      <th>categorical_5</th>\n",
       "      <th>categorical_6</th>\n",
       "      <th>store_lat</th>\n",
       "      <th>store_lon</th>\n",
       "      <th>radius</th>\n",
       "      <th>distance</th>\n",
       "      <th>time</th>\n",
       "      <th>virtual_dis</th>\n",
       "      <th>in_store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.650300</td>\n",
       "      <td>46.780509</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0003f26df5d8b928b416fba58efd5c91</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>35.651739</td>\n",
       "      <td>46.776134</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>1900-01-01 00:33:31</td>\n",
       "      <td>15.744307</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.650298</td>\n",
       "      <td>46.780512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0003f26df5d8b928b416fba58efd5c91</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>35.651739</td>\n",
       "      <td>46.776134</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>1900-01-01 00:33:31</td>\n",
       "      <td>15.758882</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.650286</td>\n",
       "      <td>46.780524</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0003f26df5d8b928b416fba58efd5c91</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>35.651739</td>\n",
       "      <td>46.776134</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>1900-01-01 02:18:33</td>\n",
       "      <td>15.832117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude  sysname  optout  lang  timezone  \\\n",
       "0  35.650300  46.780509        0       0     9         1   \n",
       "1  35.650298  46.780512        0       0     9         1   \n",
       "2  35.650286  46.780524        0       0     9         1   \n",
       "\n",
       "                         session_id  hour  minute  second  day_of_week  \\\n",
       "0  0003f26df5d8b928b416fba58efd5c91     0      33      31            6   \n",
       "1  0003f26df5d8b928b416fba58efd5c91     0      33      31            6   \n",
       "2  0003f26df5d8b928b416fba58efd5c91     2      18      33            6   \n",
       "\n",
       "   categorical_1  categorical_2  categorical_3  categorical_4  categorical_5  \\\n",
       "0              4              0             44            1.0              1   \n",
       "1              4              1             44            1.0              1   \n",
       "2              4              2             44            1.0              1   \n",
       "\n",
       "   categorical_6  store_lat  store_lon  radius  distance                time  \\\n",
       "0            187  35.651739  46.776134   118.0  0.004606 1900-01-01 00:33:31   \n",
       "1            187  35.651739  46.776134   118.0  0.004609 1900-01-01 00:33:31   \n",
       "2            187  35.651739  46.776134   118.0  0.004624 1900-01-01 02:18:33   \n",
       "\n",
       "   virtual_dis  in_store  \n",
       "0    15.744307      True  \n",
       "1    15.758882      True  \n",
       "2    15.832117      True  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userlog.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logdata(user_sample):\n",
    "    day_of_week_counts = {\"day\"+str(day) : 0 for day in range(7)}\n",
    "    #cat1_counts = {\"cat1_\"+str(ele) : 0 for ele in cat1_list}\n",
    "    #cat2_counts = {\"cat2_\"+str(ele) : 0 for ele in cat2_list}\n",
    "    #cat3_counts = {\"cat3_\"+str(ele) : 0 for ele in cat3_list}\n",
    "    #cat4_counts = {\"cat4_\"+str(ele) : 0 for ele in cat4_list}\n",
    "    #cat5_counts = {\"cat5_\"+str(ele) : 0 for ele in cat5_list}\n",
    "    #cat6_counts = {\"cat6_\"+str(ele) : 0 for ele in cat6_list}\n",
    "\n",
    "    all_results = []\n",
    "    features = {\"accumulated_actions\":0}\n",
    "    features[\"accumulated_actions\"] = user_sample.shape[0]\n",
    "    features[\"session_id\"] = user_sample.iloc[0][\"session_id\"]\n",
    "    features[\"OS\"] = user_sample.iloc[0][\"sysname\"]\n",
    "    features[\"lang\"] = user_sample.iloc[0][\"lang\"]\n",
    "    features[\"timezone\"] = user_sample.iloc[0][\"timezone\"]\n",
    "    features[\"optout_count\"] = np.sum(user_sample[\"optout\"])\n",
    "    distance = np.array(user_sample[\"distance\"])\n",
    "    features[\"max_dist\"] = np.nanmax(distance)\n",
    "    features[\"min_dist\"] = np.nanmin(distance)\n",
    "    features[\"std_dist\"] = np.nanstd(distance)\n",
    "    #features[\"mean_dist\"] = np.nanmean(distance)\n",
    "    #features[\"sum_dist\"] = np.nanmean(distance)\n",
    "    #features[\"max_lat\"] = np.max(user_sample[\"latitude\"])\n",
    "    #features[\"min_lat\"] = np.min(user_sample[\"latitude\"])\n",
    "    #features[\"max_lon\"] = np.max(user_sample[\"longitude\"])\n",
    "    #features[\"min_lon\"] = np.min(user_sample[\"longitude\"])\n",
    "    #features[\"range\"] = (features[\"max_lat\"] - features[\"min_lat\"]) * (features[\"max_lon\"]- features[\"min_lon\"])\n",
    "    #features[\"JP\"] = user_sample.iloc[0][\"JP\"]\n",
    "    \n",
    "    n_of_days = Counter(user_sample['day_of_week']) \n",
    "    for key in n_of_days.keys():\n",
    "        day_of_week_counts[\"day\"+str(key)] += n_of_days[key]\n",
    "    features.update(day_of_week_counts)\n",
    "    \n",
    "    #vir_distance = np.array(user_sample[\"virtual_dis\"])\n",
    "    #features[\"max_vir\"] = np.nanmax(vir_distance)\n",
    "    #features[\"min_vir\"] = np.nanmin(vir_distance)\n",
    "    #features[\"std_vir\"] = np.nanstd(vir_distance)\n",
    "    #features[\"mean_vir\"] = np.nanmean(vir_distance)\n",
    "    features[\"in_store\"] = np.sum(user_sample[\"in_store\"])\n",
    "    #features[\"cat4\"] = np.sum(user_sample[\"cat4\"])\n",
    "    \n",
    "    #features[\"mean_lat\"] = user_sample.iloc[0][\"'latitude', 'mean'\"]\n",
    "    #features[\"std_lat\"] = user_sample.iloc[0][\"latitude\"][\"std\"]\n",
    "    #features[\"mean_lon\"] = user_sample.iloc[0][\"longitude\"][\"mean\"]\n",
    "    #features[\"std_lon\"] = user_sample.iloc[0][\"longitude\"][\"mean\"]\n",
    "    #feautres[\"mean_cat5\"] = user_sample.iloc[0][\"categorical_5\"][\"mean\"]\n",
    "    #features[\"std_cat5\"] = user_sample.iloc[0][\"latitude\"][\"mean\"][\"categorical_6\"][\"std\"]\n",
    "    #features[\"mean_cat6\"] = user_sample.iloc[0][\"categorical_5\"][\"mean\"]\n",
    "    #features[\"std_cat6\"] = user_sample.iloc[0][\"categorical_6\"][\"std\"]\n",
    "    \n",
    "    #diff = np.array(user_sample['time'].diff(1) / np.timedelta64(1,'s'))\n",
    "    #features[\"max_diff\"] = np.nanmax(diff)\n",
    "    #features[\"min_diff\"] = np.nanmin(diff)\n",
    "    #features[\"std_diff\"] = np.nanstd(diff)\n",
    "    #features[\"mean_diff\"] = np.nanmean(diff)\n",
    "    \n",
    "    #features[\"catogorical_1_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_1\"])][\"categorical_1\"]))\n",
    "    #features[\"catogorical_2_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_2\"])][\"categorical_2\"]))\n",
    "    #features[\"catogorical_3_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_3\"])][\"categorical_3\"]))\n",
    "    #features[\"catogorical_4_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_4\"])][\"categorical_4\"]))\n",
    "    #features[\"catogorical_5_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_5\"])][\"categorical_5\"]))\n",
    "    #features[\"catogorical_6_kind\"] = len(np.unique(user_sample[~np.isnan(user_sample[\"categorical_6\"])][\"categorical_6\"]))\n",
    "    \n",
    "    #n_of_cat = Counter(user_sample['categorical_1']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat1_counts[\"cat1_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat1_counts)\n",
    "    #n_of_cat = Counter(user_sample['categorical_2']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat2_counts[\"cat2_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat2_counts)\n",
    "    #n_of_cat = Counter(user_sample['categorical_3']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat3_counts[\"cat3_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat3_counts)\n",
    "    #n_of_cat = Counter(user_sample['categorical_4']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat4_counts[\"cat4_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat4_counts)\n",
    "    #n_of_cat = Counter(user_sample['categorical_5']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat5_counts[\"cat5_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat5_counts)\n",
    "    #n_of_cat = Counter(user_sample['categorical_6']) \n",
    "    #for key in n_of_cat.keys():\n",
    "    #    cat6_counts[\"cat6_\"+str(key)] += n_of_cat[key]\n",
    "    #features.update(cat6_counts)\n",
    "    \n",
    "    all_results.append(features)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d803482e7a4f69b8d497f6199e4ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='session_id', max=5601, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_log_info(userlog):\n",
    "    compiled_log = []\n",
    "\n",
    "    for i, (ses_id, user_sample) in tqdm(enumerate(userlog.groupby('session_id', sort=False)), total=userlog.session_id.nunique(), desc='session_id', position=0):\n",
    "        compiled_log += get_logdata(user_sample)\n",
    "        reduced_log = pd.DataFrame(compiled_log)\n",
    "    return reduced_log \n",
    "reduced_log = get_log_info(userlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(train, test, reduced_log):\n",
    "    drop_features = [\"imid\", \"pid\", \"session_id\"]\n",
    "    new_train = pd.merge(train, reduced_log, on =\"session_id\", how = \"left\")\n",
    "    new_test = pd.merge(test, reduced_log, on =\"session_id\", how = \"left\")\n",
    "    new_train.drop(drop_features, axis=1, inplace=True)\n",
    "    new_test.drop(drop_features, axis=1, inplace=True)\n",
    "    new_train[\"yellow_in_pic\"] = list_y[:6612]\n",
    "    new_test[\"yellow_in_pic\"] = list_y[6612:]\n",
    "    print(new_train.shape, new_test.shape)\n",
    "    return new_train, new_test\n",
    "new_train, new_test = postprocess(train, test, reduced_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['lang', 'OS']\n",
    "\n",
    "lgbm_params = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial','learning_rate': 0.017891320270412462,'max_depth': 5, 'random_seed':42,\n",
    " 'min_data_in_leaf': 8,'min_sum_hessian_in_leaf': 17,'num_leaves': 17}\n",
    "    \n",
    "lgbm_params2 = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial', 'bagging_fraction': 0.6633984686903678, 'bagging_freq': 9,\n",
    " 'colsample_bytree': 0.9881097320572887,'feature_fraction': 0.6691096601215081,\n",
    " 'learning_rate': 0.02861754102536491,\n",
    " 'max_depth': 25,\n",
    " 'min_data_in_leaf': 101,\n",
    " 'min_sum_hessian_in_leaf': 9,\n",
    " 'num_leaves': 59}\n",
    "\n",
    "lgbm_params3 = {'objective': 'binary','eval_metric': 'auc','metric': 'auc', 'boosting_type': 'gbdt',\n",
    " 'tree_learner': 'serial', 'bagging_fraction': 0.4522886818593178,\n",
    " 'bagging_freq': 3,'colsample_bytree': 0.5872391314121622,\n",
    " 'feature_fraction': 0.9884182286365535,'learning_rate': 0.09482045811322946,\n",
    " 'max_depth': 24,'min_data_in_leaf': 88,'min_sum_hessian_in_leaf': 13, 'num_leaves': 29}\n",
    "\n",
    "def modelling(new_train, new_test, lgbm_params):\n",
    "    X_train = new_train.drop(['target'],axis=1).copy()\n",
    "    y_train = new_train.target.copy()\n",
    "    \n",
    "    remove_features = []\n",
    "    for i in X_train.columns:\n",
    "        if (X_train[i].std() == 0) and i not in remove_features:\n",
    "            remove_features.append(i)\n",
    "    X_train = X_train.drop(remove_features, axis=1)\n",
    "    X_test = new_test.copy()\n",
    "    X_test = X_test.drop(remove_features, axis=1)\n",
    "\n",
    "    n_folds=5\n",
    "    skf=StratifiedKFold(n_splits = n_folds)\n",
    "    models = []\n",
    "    \n",
    "    valid = np.array([])\n",
    "    real = np.array([])\n",
    "    evals_result = {}\n",
    "    features_list = [i for i in X_train.columns]\n",
    "    feature_importance_df = pd.DataFrame(features_list, columns=[\"Feature\"])\n",
    "    for i , (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(\"Fold \"+str(i+1))\n",
    "        X_train2 = X_train.iloc[train_index,:]\n",
    "        y_train2 = y_train.iloc[train_index]\n",
    "\n",
    "        X_test2 = X_train.iloc[test_index,:]\n",
    "        y_test2 = y_train.iloc[test_index]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "        lgb_eval = lgb.Dataset(X_test2, y_test2, reference=lgb_train)\n",
    "        clf = lgb.train(lgbm_params, lgb_train,valid_sets=[lgb_train, lgb_eval],\n",
    "            num_boost_round=10000,early_stopping_rounds=100,verbose_eval = 500, categorical_feature = categoricals)\n",
    "        valid_predict = clf.predict(X_test2, num_iteration = clf.best_iteration)\n",
    "        valid = np.concatenate([valid, valid_predict])\n",
    "        real = np.concatenate([real, y_test2])\n",
    "        feature_importance_df[\"Fold_\"+str(i+1)] = clf.feature_importance()\n",
    "\n",
    "        models.append(clf)\n",
    "        \n",
    "    feature_importance_df[\"Average\"] = np.mean(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "    feature_importance_df[\"Std\"] = np.std(feature_importance_df.iloc[:,1:n_folds+1], axis=1)\n",
    "    feature_importance_df[\"Cv\"] = feature_importance_df[\"Std\"] / feature_importance_df[\"Average\"]\n",
    "\n",
    "    roc = roc_auc_score(real, valid)\n",
    "    print(\"ROC = {}\".format(roc_auc_score(real, valid)))\n",
    "    print(confusion_matrix(real, np.round(valid)))\n",
    "    pred_value = np.zeros(X_test.shape[0])\n",
    "    for model in models:\n",
    "        pred_value += model.predict(X_test, num_iteration = model.best_iteration) / len(models)\n",
    "    return roc, pred_value, feature_importance_df\n",
    "    \n",
    "roc, pred_value, feature_importance_df = modelling(new_train, new_test, lgbm_params)\n",
    "roc, pred_value2, feature_importance_df = modelling(new_train, new_test, lgbm_params2)\n",
    "#roc, pred_value3, _ = modelling(new_train, new_test, lgbm_params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Fold_1</th>\n",
       "      <th>Fold_2</th>\n",
       "      <th>Fold_3</th>\n",
       "      <th>Fold_4</th>\n",
       "      <th>Fold_5</th>\n",
       "      <th>Average</th>\n",
       "      <th>Std</th>\n",
       "      <th>Cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>min_dist</td>\n",
       "      <td>106</td>\n",
       "      <td>175</td>\n",
       "      <td>90</td>\n",
       "      <td>82</td>\n",
       "      <td>360</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104.010769</td>\n",
       "      <td>0.639673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>radius</td>\n",
       "      <td>96</td>\n",
       "      <td>149</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>366</td>\n",
       "      <td>147.4</td>\n",
       "      <td>113.735834</td>\n",
       "      <td>0.771614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in_store</td>\n",
       "      <td>92</td>\n",
       "      <td>160</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>327</td>\n",
       "      <td>139.2</td>\n",
       "      <td>101.009703</td>\n",
       "      <td>0.725644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>store_lon</td>\n",
       "      <td>99</td>\n",
       "      <td>133</td>\n",
       "      <td>53</td>\n",
       "      <td>36</td>\n",
       "      <td>361</td>\n",
       "      <td>136.4</td>\n",
       "      <td>117.380748</td>\n",
       "      <td>0.860563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_lat</td>\n",
       "      <td>70</td>\n",
       "      <td>113</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>390</td>\n",
       "      <td>130.6</td>\n",
       "      <td>132.442591</td>\n",
       "      <td>1.014109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max_dist</td>\n",
       "      <td>95</td>\n",
       "      <td>123</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>321</td>\n",
       "      <td>125.8</td>\n",
       "      <td>102.094858</td>\n",
       "      <td>0.811565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>std_dist</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "      <td>44</td>\n",
       "      <td>344</td>\n",
       "      <td>121.4</td>\n",
       "      <td>112.987787</td>\n",
       "      <td>0.930707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>accumulated_actions</td>\n",
       "      <td>71</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>304</td>\n",
       "      <td>105.2</td>\n",
       "      <td>101.515319</td>\n",
       "      <td>0.964975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yellow_in_pic</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>103</td>\n",
       "      <td>52.6</td>\n",
       "      <td>27.111621</td>\n",
       "      <td>0.515430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OS</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "      <td>26.6</td>\n",
       "      <td>15.932357</td>\n",
       "      <td>0.598961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>type</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>14.2</td>\n",
       "      <td>9.987993</td>\n",
       "      <td>0.703380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>day6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.696938</td>\n",
       "      <td>1.336085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>day2</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.454211</td>\n",
       "      <td>0.586080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>day4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.006659</td>\n",
       "      <td>1.252775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.521363</td>\n",
       "      <td>1.760682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lang</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>day3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>day1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>day0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>day5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>optout_count</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>timezone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Feature  Fold_1  Fold_2  Fold_3  Fold_4  Fold_5  Average  \\\n",
       "0              min_dist     106     175      90      82     360    162.6   \n",
       "1                radius      96     149      64      62     366    147.4   \n",
       "2              in_store      92     160      53      64     327    139.2   \n",
       "3             store_lon      99     133      53      36     361    136.4   \n",
       "4             store_lat      70     113      37      43     390    130.6   \n",
       "5              max_dist      95     123      45      45     321    125.8   \n",
       "6              std_dist      58     102      59      44     344    121.4   \n",
       "7   accumulated_actions      71      85      32      34     304    105.2   \n",
       "8         yellow_in_pic      45      55      35      25     103     52.6   \n",
       "9                    OS      14      28      17      17      57     26.6   \n",
       "10                 type      16       7       7       8      33     14.2   \n",
       "11                 day6       1       2       8       4      40     11.0   \n",
       "12                 day2       7      13       1       5      12      7.6   \n",
       "13                 day4       1       3       0       0       8      2.4   \n",
       "14                 name       0       0       0       1       9      2.0   \n",
       "15                 lang       2       2       0       0       3      1.4   \n",
       "16                 day3       0       0       0       4       0      0.8   \n",
       "17                 day1       0       0       0       0       3      0.6   \n",
       "18                 day0       0       0       0       0       2      0.4   \n",
       "19                 day5       0       0       0       0       0      0.0   \n",
       "20         optout_count       0       0       0       0       0      0.0   \n",
       "21             timezone       0       0       0       0       0      0.0   \n",
       "\n",
       "           Std        Cv  \n",
       "0   104.010769  0.639673  \n",
       "1   113.735834  0.771614  \n",
       "2   101.009703  0.725644  \n",
       "3   117.380748  0.860563  \n",
       "4   132.442591  1.014109  \n",
       "5   102.094858  0.811565  \n",
       "6   112.987787  0.930707  \n",
       "7   101.515319  0.964975  \n",
       "8    27.111621  0.515430  \n",
       "9    15.932357  0.598961  \n",
       "10    9.987993  0.703380  \n",
       "11   14.696938  1.336085  \n",
       "12    4.454211  0.586080  \n",
       "13    3.006659  1.252775  \n",
       "14    3.521363  1.760682  \n",
       "15    1.200000  0.857143  \n",
       "16    1.600000  2.000000  \n",
       "17    1.200000  2.000000  \n",
       "18    0.800000  2.000000  \n",
       "19    0.000000       NaN  \n",
       "20    0.000000       NaN  \n",
       "21    0.000000       NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df.sort_values(\"Cv\", ascending = True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_feat = list(feature_importance_df[feature_importance_df.Cv <= 1.1][\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "def my_hyperopt(X, Y):\n",
    "    def para_tuning_obj(params):\n",
    "        params = {\n",
    "        'boosting_type': 'gbdt', \n",
    "        'metric': 'auc', \n",
    "        'objective': 'binary', \n",
    "        'eval_metric': 'auc', \n",
    "        \"tree_learner\": \"serial\",\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'bagging_freq': int(params['bagging_freq']),\n",
    "        'bagging_fraction': float(params['bagging_fraction']),\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'feature_fraction': float(params['feature_fraction']),\n",
    "        'learning_rate': float(params['learning_rate']),\n",
    "        'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "        'min_sum_hessian_in_leaf': int(params['min_sum_hessian_in_leaf']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "}\n",
    "    \n",
    "        real = np.array([])\n",
    "        pred = np.array([])\n",
    "        skf = StratifiedKFold(n_splits=5)\n",
    "        for trn_idx, val_idx in skf.split(X, Y):\n",
    "            x_train, x_val = X.iloc[trn_idx, :], X.iloc[val_idx, :]\n",
    "            y_train, y_val = Y.iloc[trn_idx], Y.iloc[val_idx]\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            val_set = lgb.Dataset(x_val, y_val)\n",
    "        \n",
    "            clf = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                         valid_sets = [train_set, val_set], verbose_eval = 300)\n",
    "            pred = np.concatenate((pred, np.array(clf.predict(x_val, num_iteration = clf.best_iteration))), axis=0) \n",
    "            real = np.concatenate((real, np.array(y_val)), axis=0) \n",
    "        score = roc_auc_score(real, pred)\n",
    "    \n",
    "        return -score\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    space ={\n",
    "        'max_depth': hp.quniform('max_depth', 1, 30, 1),\n",
    "        'bagging_freq': hp.quniform('bagging_freq', 1, 10, 1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.2, 1.0),\n",
    "        'num_leaves': hp.quniform('num_leaves', 8, 128, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.2, 1.0),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
    "        'min_data_in_leaf': hp.quniform('min_data_in_leaf', 8, 128, 1),\n",
    "        'min_sum_hessian_in_leaf': hp.quniform('min_sum_hessian_in_leaf', 5, 30, 1),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0)\n",
    "    }\n",
    "\n",
    "    best = fmin(para_tuning_obj, space = space, algo=tpe.suggest, max_evals=10, trials=trials, verbose=1)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    return best_params\n",
    "\n",
    "#X = new_train.drop(['target'],axis=1).copy()\n",
    "#Y = new_train.target.copy()\n",
    "#my_hyperopt(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'store_lat', 'store_lon', 'radius', 'type', 'name', 'OS',\n",
      "       'accumulated_actions', 'day0', 'day1', 'day2', 'day3', 'day4', 'day5',\n",
      "       'day6', 'in_store', 'lang', 'max_dist', 'min_dist', 'optout_count',\n",
      "       'std_dist', 'timezone', 'yellow_in_pic', 'mean_lat', 'std_lat'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(new_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (pred_value + pred_value2) / 2\n",
    "sample_submission = pd.read_csv(\"atmacup3_sample_submission.csv\")\n",
    "sample_submission[\"target\"] = final_pred\n",
    "sample_submission.to_csv(\"atmacup3_sample_submission\"+str(roc)+\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = userlog[userlog.session_id == train.iloc[4854][\"session_id\"]].copy().reset_index(drop=True)  # train.iloc[0][\"session_id\"]でのuserlog\n",
    "#user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上位解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3rd\n",
    "- lightgbm\n",
    "- pidごとのsession_idのユニーク数がきいた →理由は不明\n",
    "- 一番近い点にいた時間\n",
    "- 近い点が何個あったか\n",
    "- 緯度経度を標準化してから距離を計算した\n",
    "- stratifiedgroupkfold\n",
    "- parameterは、データは基本的なものだったので、max_depthは4くらいで浅めにした。\n",
    "- rank averageとそうでないものの2種類\n",
    "- \n",
    "- 2nd\n",
    "- cat 8 gbm 2\n",
    "- session中の最も近づいた時の距離\n",
    "- 一定の距離にいた時間\n",
    "- pidに対するtarget_encodingがきいた\n",
    "- depth4にしただけで、素のモデルで攻めた\n",
    "- early_stoppingをやめて、固定のエポックでやめる形にした\n",
    "- dataに同一ユーザが実は同じユーザがいた\n",
    "- \n",
    "- 1st\n",
    "- 店舗に一番近い、２番目に近いデータの内容を付け加えた\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
