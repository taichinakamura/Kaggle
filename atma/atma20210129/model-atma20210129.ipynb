{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022843,
     "end_time": "2021-03-01T11:51:44.642396",
     "exception": false,
     "start_time": "2021-03-01T11:51:44.619553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- go back to ver 65\n",
    "- change max_depth to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:51:44.690481Z",
     "iopub.status.busy": "2021-03-01T11:51:44.689877Z",
     "iopub.status.idle": "2021-03-01T11:51:58.401990Z",
     "shell.execute_reply": "2021-03-01T11:51:58.401336Z"
    },
    "papermill": {
     "duration": 13.738503,
     "end_time": "2021-03-01T11:51:58.402184",
     "exception": false,
     "start_time": "2021-03-01T11:51:44.663681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\r\n",
      "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.55.1)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.7.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.19.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting jpholiday\r\n",
      "  Downloading jpholiday-0.1.5-py3-none-any.whl (8.7 kB)\r\n",
      "Installing collected packages: jpholiday\r\n",
      "Successfully installed jpholiday-0.1.5\r\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet\n",
    "!pip install jpholiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-01T11:51:58.461759Z",
     "iopub.status.busy": "2021-03-01T11:51:58.459361Z",
     "iopub.status.idle": "2021-03-01T11:52:09.709183Z",
     "shell.execute_reply": "2021-03-01T11:52:09.708189Z"
    },
    "papermill": {
     "duration": 11.280831,
     "end_time": "2021-03-01T11:52:09.709320",
     "exception": false,
     "start_time": "2021-03-01T11:51:58.428489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import jpholiday\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from catboost import CatBoost, CatBoostClassifier, CatBoostRegressor, Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"max_rows\", 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:52:09.768439Z",
     "iopub.status.busy": "2021-03-01T11:52:09.767934Z",
     "iopub.status.idle": "2021-03-01T11:52:09.771728Z",
     "shell.execute_reply": "2021-03-01T11:52:09.771272Z"
    },
    "papermill": {
     "duration": 0.036464,
     "end_time": "2021-03-01T11:52:09.771851",
     "exception": false,
     "start_time": "2021-03-01T11:52:09.735387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NFOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:52:09.832517Z",
     "iopub.status.busy": "2021-03-01T11:52:09.831982Z",
     "iopub.status.idle": "2021-03-01T11:53:03.972898Z",
     "shell.execute_reply": "2021-03-01T11:53:03.971957Z"
    },
    "papermill": {
     "duration": 54.175645,
     "end_time": "2021-03-01T11:53:03.973045",
     "exception": false,
     "start_time": "2021-03-01T11:52:09.797400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIR = '../input/atma-retail20210129/'\n",
    "log_df = pd.read_csv(DIR+'carlog.csv', dtype={ 'value_1': str }, parse_dates=['date'])\n",
    "test_df = pd.read_csv(DIR+'test.csv')\n",
    "meta_df = pd.read_csv(DIR+\"meta.csv\")\n",
    "display_action_id = pd.read_csv(DIR+\"display_action_id.csv\")\n",
    "product_master_df = pd.read_csv(DIR+\"product_master.csv\", dtype={ 'JAN': str })\n",
    "user_master = pd.read_csv(DIR+\"user_master.csv\")\n",
    "submission = pd.read_csv(DIR+\"atmaCup9__sample_submission.csv\")\n",
    "test_sessions = test_df['session_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:04.029491Z",
     "iopub.status.busy": "2021-03-01T11:53:04.027742Z",
     "iopub.status.idle": "2021-03-01T11:53:04.030104Z",
     "shell.execute_reply": "2021-03-01T11:53:04.030490Z"
    },
    "papermill": {
     "duration": 0.031715,
     "end_time": "2021-03-01T11:53:04.030616",
     "exception": false,
     "start_time": "2021-03-01T11:53:03.998901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_actions = list(display_action_id.display_action_id.unique())\n",
    "del display_action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:04.085553Z",
     "iopub.status.busy": "2021-03-01T11:53:04.084368Z",
     "iopub.status.idle": "2021-03-01T11:53:04.086875Z",
     "shell.execute_reply": "2021-03-01T11:53:04.087296Z"
    },
    "papermill": {
     "duration": 0.032205,
     "end_time": "2021-03-01T11:53:04.087414",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.055209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_CATEGORIES = [\n",
    "    # お酒に関するもの\n",
    "    'ビール系__RTD', 'ビール系__ビール系', 'ビール系__ノンアルコール',\n",
    "    \n",
    "    # お菓子に関するもの\n",
    "    'スナック・キャンディー__スナック', \n",
    "    'チョコ・ビスクラ__チョコレート', \n",
    "    'スナック・キャンディー__ガム', \n",
    "    'スナック・キャンディー__シリアル',\n",
    "    'アイスクリーム__ノベルティー', \n",
    "    '和菓子__米菓',\n",
    "    \n",
    "    # 飲料に関するもの\n",
    "    '水・炭酸水__大型PET（炭酸水）',\n",
    "    '水・炭酸水__小型PET（炭酸水）',\n",
    "    '缶飲料__コーヒー（缶）',\n",
    "    '小型PET__コーヒー（小型PET）',\n",
    "    '大型PET__無糖茶（大型PET）',\n",
    "    \n",
    "    # 麺類\n",
    "    '麺類__カップ麺',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:04.700734Z",
     "iopub.status.busy": "2021-03-01T11:53:04.697551Z",
     "iopub.status.idle": "2021-03-01T11:53:04.702892Z",
     "shell.execute_reply": "2021-03-01T11:53:04.703304Z"
    },
    "papermill": {
     "duration": 0.591184,
     "end_time": "2021-03-01T11:53:04.703450",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.112266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 'ビール系__RTD')\n",
      "(173, 'ビール系__ビール系')\n",
      "(172, 'ビール系__ノンアルコール')\n",
      "(114, 'スナック・キャンディー__スナック')\n",
      "(134, 'チョコ・ビスクラ__チョコレート')\n",
      "(110, 'スナック・キャンディー__ガム')\n",
      "(113, 'スナック・キャンディー__シリアル')\n",
      "(38, 'アイスクリーム__ノベルティー')\n",
      "(376, '和菓子__米菓')\n",
      "(537, '水・炭酸水__大型PET（炭酸水）')\n",
      "(539, '水・炭酸水__小型PET（炭酸水）')\n",
      "(629, '缶飲料__コーヒー（缶）')\n",
      "(467, '小型PET__コーヒー（小型PET）')\n",
      "(435, '大型PET__無糖茶（大型PET）')\n",
      "(768, '麺類__カップ麺')\n"
     ]
    }
   ],
   "source": [
    "cat2id = dict(zip(product_master_df['category_name'], product_master_df['category_id']))\n",
    "TARGET_IDS = pd.Series(TARGET_CATEGORIES).map(cat2id).values.tolist()\n",
    "category_id2code = dict(zip(TARGET_IDS, TARGET_CATEGORIES))\n",
    "\n",
    "for x in zip(TARGET_IDS, TARGET_CATEGORIES):\n",
    "    print(x)\n",
    "\n",
    "def only_purchase_records(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx = input_df['kind_1'] == '商品'\n",
    "    out_df = input_df[idx].reset_index(drop=True)\n",
    "    return out_df\n",
    "\n",
    "def create_payment(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ログデータから session_id / JAN ごとの購買情報に変換します.\n",
    "\n",
    "    Args:\n",
    "        input_df:\n",
    "            レジカートログデータ\n",
    "\n",
    "    Returns:\n",
    "        session_id, JAN, n_items (合計購買数) の DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 購買情報は商品のものだけ.\n",
    "    out_df = only_purchase_records(input_df)\n",
    "    out_df = out_df.groupby(['session_id', 'value_1'])['n_items'].sum().reset_index()\n",
    "    out_df = out_df.rename(columns={\n",
    "        'value_1': 'JAN'\n",
    "    })\n",
    "    return out_df\n",
    "\n",
    "def annot_category(input_df: pd.DataFrame,\n",
    "                   master_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    カテゴリ ID をひも付けます.\n",
    "\n",
    "    Args:\n",
    "        input_df:\n",
    "            変換するデータ.\n",
    "            `value_1`  or `JAN` を column として持っている必要があります.\n",
    "        master_df:\n",
    "            商品マスタのデータフレーム\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    input_df = input_df.rename(columns={'value_1': 'JAN'})\n",
    "    out_df = pd.merge(input_df['JAN'],\n",
    "                      master_df[['JAN', 'category_id']], on='JAN', how='left')\n",
    "    return out_df['category_id']\n",
    "\n",
    "def only_payment_session_record(input_log_df):\n",
    "    \"\"\"支払いが紐づくセッションへ絞り込みを行なう\"\"\"\n",
    "    payed_sessions = input_log_df[input_log_df['is_payment'] == 1]['session_id'].unique()\n",
    "    idx = input_log_df['session_id'].isin(payed_sessions)\n",
    "    out_df = input_log_df[idx].reset_index(drop=True)\n",
    "    return out_df\n",
    "\n",
    "def create_target_from_log(log_df: pd.DataFrame,\n",
    "                           product_master_df: pd.DataFrame,\n",
    "                          only_payment=True):\n",
    "\n",
    "    if only_payment:\n",
    "        log_df = only_payment_session_record(log_df)\n",
    "    pay_df = create_payment(log_df)\n",
    "    pay_df['category_id'] = annot_category(pay_df, master_df=product_master_df)\n",
    "\n",
    "    # null の category を削除. JAN が紐付かない時に発生する.\n",
    "    idx_null = pay_df['category_id'].isnull()\n",
    "    pay_df = pay_df[~idx_null].reset_index(drop=True)\n",
    "    # Nullが混じっている時 float になるため int へ明示的に戻す\n",
    "    pay_df['category_id'] = pay_df['category_id'].astype(int)\n",
    "\n",
    "    idx = pay_df['category_id'].isin(TARGET_IDS)\n",
    "    target_df = pd.pivot_table(data=pay_df[idx],\n",
    "                               index='session_id',\n",
    "                               columns='category_id',\n",
    "                               values='n_items',\n",
    "                               aggfunc='sum')\n",
    "\n",
    "    sessions = sorted(log_df['session_id'].unique())\n",
    "    print(len(sessions))\n",
    "    target_df = target_df.reindex(sessions)\n",
    "    target_df = target_df.fillna(0).astype(int)\n",
    "    return target_df, pay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:04.767526Z",
     "iopub.status.busy": "2021-03-01T11:53:04.767017Z",
     "iopub.status.idle": "2021-03-01T11:53:04.770293Z",
     "shell.execute_reply": "2021-03-01T11:53:04.770703Z"
    },
    "papermill": {
     "duration": 0.041924,
     "end_time": "2021-03-01T11:53:04.770896",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.728972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_encoding(X_train, y_train, X_test, col_name, target_name, user_df, replace=True, option = \"mean\"):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    rare_users = list(user_df[user_df[\"total_appearance\"]<10][\"user_id\"])\n",
    "\n",
    "    Xy = pd.DataFrame({'trans_col': X_train[col_name], 'target': y_train})\n",
    "    \n",
    "    if option == \"sum\":\n",
    "        target_mean_all = Xy.groupby('trans_col')['target'].sum()\n",
    "    else:\n",
    "        target_mean_all = Xy.groupby('trans_col')['target'].mean()  \n",
    "        \n",
    "    if replace:\n",
    "        X_test[col_name+str(target_name)] = X_test[col_name].map(target_mean_all)\n",
    "        X_test.loc[X_test.user_id.isin(rare_users), col_name+str(target_name)] = -1\n",
    "        X_test[col_name+str(target_name)].fillna(-1, inplace=True)\n",
    "    else:\n",
    "        X_test[f'te_{col_name}_{target_name}'] = X_test[col_name].map(target_mean_all)\n",
    "        X_test.loc[X_test.user_id.isin(rare_users), f'te_{col_name}_{target_name}'] = -1\n",
    "        X_test[f'te_{col_name}_{target_name}'].fillna(-1, inplace=True)\n",
    "\n",
    "    oof_target = np.zeros(X_train.shape[0])\n",
    "    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
    "    for idx_1, idx_2 in kf.split(X_train, y_train):\n",
    "        if option == \"sum\":\n",
    "            target_mean = Xy.iloc[idx_1, :].groupby('trans_col')['target'].sum()\n",
    "        else:\n",
    "            target_mean = Xy.iloc[idx_1, :].groupby('trans_col')['target'].mean()\n",
    "            \n",
    "        oof_target[idx_2] = X_train[col_name].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    if replace:\n",
    "        X_train[col_name+str(target_name)] = oof_target\n",
    "        X_train.loc[X_train.user_id.isin(rare_users), col_name+str(target_name)] = -1\n",
    "        X_train[col_name+str(target_name)].fillna(-1, inplace=True)\n",
    "\n",
    "    else:\n",
    "        X_train[f'te_{col_name}_{target_name}'] = oof_target\n",
    "        X_train.loc[X_train.user_id.isin(rare_users), f'te_{col_name}_{target_name}'] = -1\n",
    "        X_train[f'te_{col_name}_{target_name}'].fillna(-1, inplace=True)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:04.836162Z",
     "iopub.status.busy": "2021-03-01T11:53:04.835468Z",
     "iopub.status.idle": "2021-03-01T11:53:04.838850Z",
     "shell.execute_reply": "2021-03-01T11:53:04.838402Z"
    },
    "papermill": {
     "duration": 0.043017,
     "end_time": "2021-03-01T11:53:04.838964",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.795947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_encoding_with_covid(X_train, y_train, X_test, col_name, target_name, user_df, replace=True, option = \"mean\", when=\"after\"):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    rare_users = list(user_df[user_df[\"total_appearance\"]<10][\"user_id\"])\n",
    "    \n",
    "    if when == \"after\":\n",
    "        cons_index = X_train[X_train.date_date>=\"2020-03-01\"].index\n",
    "    else:\n",
    "        cons_index = X_train[X_train.date_date<\"2020-03-01\"].index\n",
    "        \n",
    "    Xy = pd.DataFrame({'trans_col': X_train.loc[cons_index][col_name], 'target': y_train.loc[cons_index]})\n",
    "    \n",
    "    if option == \"sum\":\n",
    "        target_mean_all = Xy.groupby('trans_col')['target'].sum()\n",
    "    else:\n",
    "        target_mean_all = Xy.groupby('trans_col')['target'].mean()  \n",
    "        \n",
    "    if replace:\n",
    "        X_test[col_name+str(target_name)+\"_\"+str(when)] = X_test[col_name].map(target_mean_all)\n",
    "        X_test.loc[X_test.user_id.isin(rare_users), col_name+str(target_name)+\"_\"+str(when)] = -1\n",
    "        X_test[col_name+str(target_name)+\"_\"+str(when)].fillna(-1, inplace=True)\n",
    "    else:\n",
    "        X_test[f'te_{col_name}_{target_name}_{when}'] = X_test[col_name].map(target_mean_all)\n",
    "        X_test.loc[X_test.user_id.isin(rare_users), f'te_{col_name}_{target_name}_{when}'] = -1\n",
    "        X_test[f'te_{col_name}_{target_name}_{when}'].fillna(-1, inplace=True)\n",
    "\n",
    "    oof_target = np.zeros(X_train.shape[0])\n",
    "    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n",
    "    for idx_1, idx_2 in kf.split(X_train, y_train):\n",
    "        idx_1 = list(set(idx_1) & set(cons_index))\n",
    "        Xy = pd.DataFrame({'trans_col': X_train.loc[idx_1][col_name], 'target': y_train.loc[idx_1]})\n",
    "        if option == \"sum\":\n",
    "            target_mean = Xy.groupby('trans_col')['target'].sum()\n",
    "        else:\n",
    "            target_mean = Xy.groupby('trans_col')['target'].mean()\n",
    "            \n",
    "        oof_target[idx_2] = X_train[col_name].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    if replace:\n",
    "        X_train[col_name+str(target_name)+\"_\"+str(when)] = oof_target\n",
    "        X_train.loc[X_train.user_id.isin(rare_users), col_name+str(target_name)+\"_\"+str(when)] = -1\n",
    "        X_train[col_name+str(target_name)+\"_\"+str(when)].fillna(-1, inplace=True)\n",
    "\n",
    "    else:\n",
    "        X_train[f'te_{col_name}_{target_name}_{when}'] = oof_target\n",
    "        X_train.loc[X_train.user_id.isin(rare_users), f'te_{col_name}_{target_name}_{when}'] = -1\n",
    "        X_train[f'te_{col_name}_{target_name}_{when}'].fillna(-1, inplace=True)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026311,
     "end_time": "2021-03-01T11:53:04.890858",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.864547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:05.141936Z",
     "iopub.status.busy": "2021-03-01T11:53:05.139753Z",
     "iopub.status.idle": "2021-03-01T11:53:05.150237Z",
     "shell.execute_reply": "2021-03-01T11:53:05.149769Z"
    },
    "papermill": {
     "duration": 0.233962,
     "end_time": "2021-03-01T11:53:05.150366",
     "exception": false,
     "start_time": "2021-03-01T11:53:04.916404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_data = meta_df[\"user_id\"].value_counts().reset_index()\n",
    "user_data.columns = [\"user_id\", \"total_appearance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:05.208220Z",
     "iopub.status.busy": "2021-03-01T11:53:05.207529Z",
     "iopub.status.idle": "2021-03-01T11:53:14.491766Z",
     "shell.execute_reply": "2021-03-01T11:53:14.491138Z"
    },
    "papermill": {
     "duration": 9.315736,
     "end_time": "2021-03-01T11:53:14.491985",
     "exception": false,
     "start_time": "2021-03-01T11:53:05.176249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user data\n",
    "for id_ in all_actions:\n",
    "    id_count_session = list(log_df[log_df[\"display_action_id\"]==id_][\"session_id\"].unique())\n",
    "    id_meta = meta_df[meta_df.session_id.isin(id_count_session)][\"user_id\"].value_counts().reset_index()\n",
    "    id_meta.columns = [\"user_id\", \"count\"+str(id_)]\n",
    "    user_data = pd.merge(user_data, id_meta, on=\"user_id\", how=\"left\").fillna(0)\n",
    "    user_data[\"ratio_action\"+str(id_)] =  user_data[\"count\"+str(id_)] / user_data[\"total_appearance\"]\n",
    "    del user_data[\"count\"+str(id_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:14.604846Z",
     "iopub.status.busy": "2021-03-01T11:53:14.604009Z",
     "iopub.status.idle": "2021-03-01T11:53:23.121644Z",
     "shell.execute_reply": "2021-03-01T11:53:23.121143Z"
    },
    "papermill": {
     "duration": 8.60013,
     "end_time": "2021-03-01T11:53:23.121792",
     "exception": false,
     "start_time": "2021-03-01T11:53:14.521662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def isBizDay(DATE):\n",
    "    Date = datetime.date(int(DATE[0:4]), int(DATE[5:7]), int(DATE[8:10]))\n",
    "    if jpholiday.is_holiday(Date):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "meta_df[\"dayofweek\"] = pd.to_datetime(meta_df[\"date\"]).dt.dayofweek\n",
    "\n",
    "meta_df[\"tmp_date\"] = pd.to_datetime(meta_df[\"date\"])\n",
    "meta_df[\"days_from_prev\"] = meta_df.groupby(\"user_id\")[\"tmp_date\"].diff().dt.days\n",
    "del meta_df[\"tmp_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:23.179095Z",
     "iopub.status.busy": "2021-03-01T11:53:23.178137Z",
     "iopub.status.idle": "2021-03-01T11:53:30.628553Z",
     "shell.execute_reply": "2021-03-01T11:53:30.628089Z"
    },
    "papermill": {
     "duration": 7.480743,
     "end_time": "2021-03-01T11:53:30.628737",
     "exception": false,
     "start_time": "2021-03-01T11:53:23.147994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 完全なデータを持っているログに絞る\n",
    "test_sessions = test_df['session_id'].unique()\n",
    "idx_test = log_df['session_id'].isin(test_sessions)\n",
    "whole_log_df = log_df[~idx_test].reset_index(drop=True)\n",
    "payment_session_df = only_payment_session_record(whole_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:32.276670Z",
     "iopub.status.busy": "2021-03-01T11:53:32.275753Z",
     "iopub.status.idle": "2021-03-01T11:53:37.000272Z",
     "shell.execute_reply": "2021-03-01T11:53:37.000739Z"
    },
    "papermill": {
     "duration": 6.345527,
     "end_time": "2021-03-01T11:53:37.000914",
     "exception": false,
     "start_time": "2021-03-01T11:53:30.655387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 商品購買の最後(max spend time)が10分より大きいセッションを取り出す\n",
    "is_item_record = payment_session_df['kind_1'] == '商品'\n",
    "max_payed_time = payment_session_df[is_item_record].groupby('session_id')['spend_time'].max()\n",
    "max_payed_time_over_10min = max_payed_time[max_payed_time > 10 * 60]\n",
    "\n",
    "train_sessions = max_payed_time_over_10min.index.tolist()\n",
    "train_whole_log_df = payment_session_df[payment_session_df['session_id'].isin(train_sessions)].reset_index(drop=True)\n",
    "\n",
    "del max_payed_time, max_payed_time_over_10min, payment_session_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:37.075080Z",
     "iopub.status.busy": "2021-03-01T11:53:37.074016Z",
     "iopub.status.idle": "2021-03-01T11:53:42.579717Z",
     "shell.execute_reply": "2021-03-01T11:53:42.579239Z"
    },
    "papermill": {
     "duration": 5.546752,
     "end_time": "2021-03-01T11:53:42.579848",
     "exception": false,
     "start_time": "2021-03-01T11:53:37.033096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ユーザーごとに10分以上の買い物でどれくらいの金額を使うのか、どれくらいの個数買うか、どれくらい時間を費やしているのか\n",
    "train_whole_log_df[\"cost\"] = train_whole_log_df[\"number_1\"] * train_whole_log_df[\"unit_price\"]\n",
    "\n",
    "total_cost_per_session = train_whole_log_df[is_item_record].groupby(\"session_id\")[\"cost\"].sum()\n",
    "total_cost_per_session = pd.merge(total_cost_per_session, meta_df[[\"user_id\", \"session_id\"]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "total_nitems_per_session = train_whole_log_df[is_item_record].groupby(\"session_id\")[\"n_items\"].sum()\n",
    "total_nitems_per_session = pd.merge(total_nitems_per_session, meta_df[[\"user_id\", \"session_id\"]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "total_spend_time_per_session = train_whole_log_df[is_item_record].groupby(\"session_id\")[\"spend_time\"].sum()\n",
    "total_spend_time_per_session = pd.merge(total_spend_time_per_session, meta_df[[\"user_id\", \"session_id\"]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "user_data = pd.merge(user_data, total_cost_per_session.groupby(\"user_id\")[\"cost\"].agg([\"mean\", \"std\"]).add_prefix(\"total_cost_\"),\n",
    "                    on=\"user_id\", how=\"left\").fillna(-1)\n",
    "\n",
    "user_data = pd.merge(user_data, total_nitems_per_session.groupby(\"user_id\")[\"n_items\"].agg([\"mean\", \"std\"]).add_prefix(\"total_nitems_\"),\n",
    "                    on=\"user_id\", how=\"left\").fillna(-1)\n",
    "\n",
    "user_data = pd.merge(user_data, total_spend_time_per_session.groupby(\"user_id\")[\"spend_time\"].agg([\"mean\", \"std\"]).add_prefix(\"spend_time_\"),\n",
    "                    on=\"user_id\", how=\"left\").fillna(-1)\n",
    "\n",
    "for i in user_data.columns: \n",
    "    if i not in [\"user_id\", \"total_appearance\"]:\n",
    "        user_data.loc[user_data.total_appearance<10, i] = -1\n",
    "        \n",
    "del total_cost_per_session, total_nitems_per_session, total_spend_time_per_session, is_item_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:42.640842Z",
     "iopub.status.busy": "2021-03-01T11:53:42.640033Z",
     "iopub.status.idle": "2021-03-01T11:53:42.981417Z",
     "shell.execute_reply": "2021-03-01T11:53:42.980937Z"
    },
    "papermill": {
     "duration": 0.37463,
     "end_time": "2021-03-01T11:53:42.981553",
     "exception": false,
     "start_time": "2021-03-01T11:53:42.606923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "time_elasped_count = meta_df['time_elapsed'].value_counts(normalize=True)\n",
    "\n",
    "train_time_elapsed = np.random.choice(time_elasped_count.index.astype(int), \n",
    "                                      p=time_elasped_count.values, \n",
    "                                      size=len(train_sessions))\n",
    "train_meta_df = pd.DataFrame({\n",
    "    'session_id': train_sessions,\n",
    "    'time_elapsed': train_time_elapsed\n",
    "})\n",
    "\n",
    "train_meta_df = pd.merge(train_meta_df, \n",
    "                         meta_df.drop(columns=['time_elapsed']), \n",
    "                         on='session_id', \n",
    "                         how='left')\n",
    "del train_time_elapsed, train_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:43.074474Z",
     "iopub.status.busy": "2021-03-01T11:53:43.073528Z",
     "iopub.status.idle": "2021-03-01T11:53:49.050896Z",
     "shell.execute_reply": "2021-03-01T11:53:49.049756Z"
    },
    "papermill": {
     "duration": 6.04303,
     "end_time": "2021-03-01T11:53:49.051068",
     "exception": false,
     "start_time": "2021-03-01T11:53:43.008038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_df = pd.merge(train_whole_log_df[['session_id', 'spend_time']], train_meta_df, on='session_id', how='left')\n",
    "idx_show = _df['spend_time'] <= _df['time_elapsed'] * 60\n",
    "\n",
    "del _df\n",
    "\n",
    "train_public_df = train_whole_log_df[idx_show].reset_index(drop=True)\n",
    "train_private_df = train_whole_log_df[~idx_show].reset_index(drop=True)\n",
    "\n",
    "del idx_show, train_whole_log_df\n",
    "\n",
    "# テストのログデータと合わせて推論時に見ても良いログ `public_log_df` として保存しておく\n",
    "public_log_df = pd.concat([\n",
    "    train_public_df, log_df[log_df['session_id'].isin(test_sessions)]\n",
    "], axis=0, ignore_index=True)\n",
    "# meta に紐づく情報は後でよく使うので, テストデータにも meta 情報をマージしておきます. \n",
    "# train_meata_df / test_meta_df が今後特徴を作る上で key になるデータになります。\n",
    "test_meta_df = pd.merge(test_df, meta_df, on='session_id', how='left')\n",
    "\n",
    "del log_df, train_public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:50.135258Z",
     "iopub.status.busy": "2021-03-01T11:53:50.134198Z",
     "iopub.status.idle": "2021-03-01T11:53:57.952738Z",
     "shell.execute_reply": "2021-03-01T11:53:57.951723Z"
    },
    "papermill": {
     "duration": 8.871066,
     "end_time": "2021-03-01T11:53:57.952889",
     "exception": false,
     "start_time": "2021-03-01T11:53:49.081823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366478\n"
     ]
    }
   ],
   "source": [
    "train_target_df, _  = create_target_from_log(train_private_df, \n",
    "                                             product_master_df=product_master_df,\n",
    "                                            only_payment=False)\n",
    "\n",
    "train_target_df = train_target_df.reset_index(drop=True)\n",
    "train_target_df[train_target_df >= 1] = 1\n",
    "train_target_df[train_target_df <= 0] = 0\n",
    "\n",
    "del train_private_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026549,
     "end_time": "2021-03-01T11:53:58.006491",
     "exception": false,
     "start_time": "2021-03-01T11:53:57.979942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:58.099065Z",
     "iopub.status.busy": "2021-03-01T11:53:58.085037Z",
     "iopub.status.idle": "2021-03-01T11:53:58.101380Z",
     "shell.execute_reply": "2021-03-01T11:53:58.100966Z"
    },
    "papermill": {
     "duration": 0.068101,
     "end_time": "2021-03-01T11:53:58.101496",
     "exception": false,
     "start_time": "2021-03-01T11:53:58.033395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AbstractBaseBlock:\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class MetaInformationBlock(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        use_columns = [\n",
    "            'hour', 'register_number', 'time_elapsed'\n",
    "        ]\n",
    "        return input_df[use_columns].copy()\n",
    "    \n",
    "class DateBlock(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        is_holiday = input_df['date'].apply(lambda x: isBizDay(x))\n",
    "        date = pd.to_datetime(input_df['date'])\n",
    "\n",
    "        out_df = pd.DataFrame({\n",
    "            'date': date,\n",
    "            'dayofweek': date.dt.dayofweek,\n",
    "            'day': date.dt.day,\n",
    "            'year': date.dt.year,\n",
    "            'month': date.dt.month,\n",
    "            'is_holiday': is_holiday,\n",
    "        })\n",
    "\n",
    "        # 金曜日の夜はお祭り騒ぎ\n",
    "        out_df['hanakin'] = np.where((date.dt.dayofweek == 4) & (input_df['hour'] > 17), 1, 0)\n",
    "        return out_df.add_prefix('date_')\n",
    "    \n",
    "class CountEncodingBlock(AbstractBaseBlock):\n",
    "    \"\"\"CountEncodingを行なう block\"\"\"\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        vc = input_df[self.column].value_counts()\n",
    "        self.count_ = vc\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.DataFrame()\n",
    "        out_df[self.column] = input_df[self.column].map(self.count_)\n",
    "        return out_df.add_prefix('CE_')\n",
    "    \n",
    "class HourActionPortfolioBlock(AbstractBaseBlock):\n",
    "    \"\"\"時間ごとの `display_action_id` の出現回数を紐付ける block. \"\"\"\n",
    "    def fit(self, input_df, y=None):\n",
    "        _df = pd.pivot_table(\n",
    "               data=whole_log_df, \n",
    "              index='hour',\n",
    "              columns=whole_log_df['display_action_id'].map(display_action2name),\n",
    "              values='session_id',\n",
    "              aggfunc='count').fillna(0)\n",
    "\n",
    "        self.pivot_df_ = _df\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.merge(input_df['hour'], \n",
    "                          self.pivot_df_, \n",
    "                          on='hour', how='left').drop(columns=['hour'])\n",
    "        return out_df.add_prefix('hour_ratio=')\n",
    "    \n",
    "class UserHistoryBlock(AbstractBaseBlock):\n",
    "    \"\"\"ユーザーの購買履歴を部門名ごとに集計したベクトルを付与する特徴量 block\"\"\"\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        purchase_df = only_purchase_records(whole_log_df)\n",
    "        purchase_df = purchase_df.rename(columns={ 'value_1': 'JAN' })\n",
    "        category = annot_category(purchase_df, product_master_df)\n",
    "        idx_null = category.isnull()  # JAN が紐付かないやつ\n",
    "\n",
    "        # target の情報はリークになる可能性があるので削除する\n",
    "        idx_none_target = ~category.isin(TARGET_IDS)\n",
    "\n",
    "        # 商品マスタの部門名を取り出して集計\n",
    "        bumon_name = pd.merge(purchase_df['JAN'], \n",
    "                              product_master_df[['JAN', '部門名']], on='JAN', how='left')['部門名']\n",
    "\n",
    "        _df = pd.pivot_table(data=purchase_df[idx_none_target], \n",
    "               index='user_id', \n",
    "               columns=bumon_name[idx_none_target],\n",
    "              values='n_items',\n",
    "              aggfunc='sum')\\\n",
    "                .fillna(0)\n",
    "        \n",
    "        # ユーザーごとに平均化. \n",
    "        _df = _df.div(_df.sum(axis=1), axis=0)\n",
    "        \n",
    "        self.agg_df_ = _df\n",
    "        del _df\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.merge(input_df['user_id'], self.agg_df_, on='user_id', how='left').drop(columns=['user_id'])\n",
    "        out_df = out_df.fillna(0)\n",
    "        return out_df.add_prefix('ratio_部門名=')\n",
    "    \n",
    "def join(df):\n",
    "    x = [str(e) for e in list(df)]\n",
    "    return \" \".join(x)\n",
    "    \n",
    "class PublicLogBlock(AbstractBaseBlock):\n",
    "    \"\"\"見えているログに関する特徴量\"\"\"\n",
    "    def fit(self, input_df, y=None):\n",
    "        self.agg_df_ = pd.concat([\n",
    "            # 買っている商品の数\n",
    "            public_log_df.groupby('session_id')['n_items'].sum().rename('total_items'),\n",
    "            # 買っている商品 (JANレベル) のユニーク数\n",
    "            public_log_df[public_log_df['kind_1'] == '商品'].groupby('session_id')['value_1'].nunique().rename('JAN_nunique'),\n",
    "        ], axis=1)        \n",
    "        \n",
    "        # public_logの範囲で特定カテゴリ商品を買っているか\n",
    "        tmp = pd.merge(public_log_df[public_log_df['kind_1'] == '商品'][[\"session_id\", \"value_1\"]], \n",
    "                       product_master_df[[\"JAN\", \"category_id\"]], \n",
    "                       left_on=\"value_1\",\n",
    "                       right_on=\"JAN\",how=\"left\")\n",
    "        \n",
    "        # association analysis + targets themselves\n",
    "        for cat in [114, 134, 135, 136, 143, 207, 209, 210, 368, 370, 376, 508, 509, 587, 716, 720, 724, 768,\n",
    "                   171, 173, 172, 110, 113, 38, 537, 539, 629, 467, 435]:\n",
    "            self.agg_df_[\"buy_catCD\"+str(cat)] = tmp.groupby('session_id').apply(lambda x: cat in x[\"category_id\"].values)\n",
    "        del tmp\n",
    "        \n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.merge(input_df['session_id'], self.agg_df_, on='session_id', how='left').drop(columns=['session_id'])\n",
    "        out_df = out_df.fillna(0)\n",
    "        return out_df.add_prefix('pub_log=')\n",
    "    \n",
    "class UserInfoBlock(AbstractBaseBlock):\n",
    "    def fit(self, input_df, y=None):\n",
    "        user_hour_count = pd.pivot_table(data=meta_df,\n",
    "                                         index='user_id', \n",
    "                                         values=\"date\", \n",
    "                                         columns='hour', \n",
    "                                         aggfunc='count').fillna(0).add_prefix('visit_hour=')\n",
    "        self.user_hour_count = user_hour_count\n",
    "        \n",
    "        user_dayofweek_count = pd.pivot_table(meta_df, \n",
    "                       index=\"user_id\", \n",
    "                       columns=\"dayofweek\",\n",
    "                       values=\"date\", \n",
    "                       aggfunc=\"count\").fillna(0).add_prefix(\"user_dayofweek=\")\n",
    "        self.user_dayofweek_count = user_dayofweek_count\n",
    "                \n",
    "        return self.transform(input_df)\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.merge(input_df['user_id'], self.user_hour_count, on='user_id', how='left')\n",
    "        out_df = pd.merge(out_df, self.user_dayofweek_count, on='user_id', how='left')\n",
    "        out_df = pd.merge(out_df, user_master, on='user_id', how='left')\n",
    "        out_df = pd.merge(out_df, user_data, on='user_id', how='left')\n",
    "        return out_df\n",
    "    \n",
    "class BeforeBuyIntervalBlock(AbstractBaseBlock):\n",
    "    def __init__(self, category_id):\n",
    "        self.category_id = category_id\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        log_df = pd.concat([public_log_df, whole_log_df], ignore_index=True)\n",
    "        jans = product_master_df[product_master_df['category_id'] == self.category_id]['JAN'].unique()\n",
    "        x = log_df[(log_df['kind_1'] == '商品') & (log_df['value_1'].isin(jans))].groupby('session_id')['n_items'].sum()\n",
    "\n",
    "        buy_sessions = x[x > 0].index\n",
    "        df = meta_df[['session_id', 'user_id', 'date']].copy()\n",
    "\n",
    "        df['buy'] = df['session_id'].isin(buy_sessions).astype(int)\n",
    "        out_df = pd.DataFrame()\n",
    "\n",
    "        # 過去平均何回買っているか\n",
    "        df['past_avg_buy'] = (df.groupby('user_id')['buy'].cumsum() - df['buy']) / (df.groupby('user_id').cumcount() + 1)\n",
    "\n",
    "        # 直前買ってから何日か\n",
    "        _x = pd.concat([\n",
    "            df[['user_id', 'date']],\n",
    "            (df.groupby('user_id')['buy'].cumsum() - df['buy'])\n",
    "        ], axis=1)\n",
    "        _x = _x.merge(_x.groupby(['user_id', 'buy'])['date'].first().rename('first_date'), on=['user_id', 'buy'], how='left')\n",
    "        _x = _x.merge(_x.groupby('user_id')['date'].first().rename('user_first_date'), on='user_id', how='left')\n",
    "        _x[\"date\"] = pd.to_datetime(_x[\"date\"])\n",
    "        _x[\"first_date\"] = pd.to_datetime(_x[\"first_date\"])\n",
    "        from_before_buy = np.where(_x['first_date'] == _x['user_first_date'], None, (_x['date'] - _x['first_date']).dt.days)\n",
    "        df['days_before_buy'] = from_before_buy\n",
    "        self.agg_df_ = df\n",
    "        \n",
    "        return self.transform(input_df)\n",
    "    \n",
    "    def transform(self, input_df):\n",
    "        out_df = pd.merge(input_df['session_id'], \n",
    "                          self.agg_df_, \n",
    "                          on='session_id', \n",
    "                          how='left').drop(columns=['session_id', 'user_id', 'date', 'buy'])\n",
    "        return out_df.add_prefix('BBInterval_{}_'.format(self.category_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T11:53:58.163001Z",
     "iopub.status.busy": "2021-03-01T11:53:58.161939Z",
     "iopub.status.idle": "2021-03-01T12:04:16.038531Z",
     "shell.execute_reply": "2021-03-01T12:04:16.038031Z"
    },
    "papermill": {
     "duration": 617.910159,
     "end_time": "2021-03-01T12:04:16.038704",
     "exception": false,
     "start_time": "2021-03-01T11:53:58.128545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_blocks = [\n",
    "    #*[CountEncodingBlock(column=c) for c in ['user_id', 'register_number']],\n",
    "    DateBlock(),\n",
    "    PublicLogBlock(),\n",
    "    MetaInformationBlock(),\n",
    "    UserHistoryBlock(),\n",
    "    UserInfoBlock(),\n",
    "    *[BeforeBuyIntervalBlock(i) for i in TARGET_IDS]\n",
    "]\n",
    "\n",
    "feat_train_df = pd.DataFrame()\n",
    "for block in feature_blocks:\n",
    "    out_i = block.fit(train_meta_df)\n",
    "    assert len(train_meta_df) == len(out_i), block\n",
    "    feat_train_df = pd.concat([feat_train_df, out_i], axis=1)\n",
    "del train_meta_df\n",
    "\n",
    "feat_test_df = pd.DataFrame()\n",
    "for block in feature_blocks:\n",
    "    out_i = block.transform(test_meta_df)\n",
    "    assert len(test_meta_df) == len(out_i), block\n",
    "    feat_test_df = pd.concat([feat_test_df, out_i], axis=1)\n",
    "del test_meta_df\n",
    "\n",
    "del meta_df, public_log_df, user_master, feature_blocks, whole_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:04:16.100439Z",
     "iopub.status.busy": "2021-03-01T12:04:16.098969Z",
     "iopub.status.idle": "2021-03-01T12:04:40.967095Z",
     "shell.execute_reply": "2021-03-01T12:04:40.966611Z"
    },
    "papermill": {
     "duration": 24.900151,
     "end_time": "2021-03-01T12:04:40.967226",
     "exception": false,
     "start_time": "2021-03-01T12:04:16.067075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in train_target_df.columns:\n",
    "    feat_train_df, feat_test_df = target_encoding(feat_train_df, train_target_df[j], feat_test_df, \n",
    "                                                  \"user_id\", j, user_data, False, \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:04:41.026577Z",
     "iopub.status.busy": "2021-03-01T12:04:41.025444Z",
     "iopub.status.idle": "2021-03-01T12:05:25.156396Z",
     "shell.execute_reply": "2021-03-01T12:05:25.155916Z"
    },
    "papermill": {
     "duration": 44.162111,
     "end_time": "2021-03-01T12:05:25.156534",
     "exception": false,
     "start_time": "2021-03-01T12:04:40.994423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in train_target_df.columns:\n",
    "    feat_train_df, feat_test_df = target_encoding_with_covid(feat_train_df, train_target_df[j], feat_test_df, \n",
    "                                                  \"user_id\", j, user_data, False, \"mean\", \"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:05:25.225127Z",
     "iopub.status.busy": "2021-03-01T12:05:25.223237Z",
     "iopub.status.idle": "2021-03-01T12:05:25.323662Z",
     "shell.execute_reply": "2021-03-01T12:05:25.323010Z"
    },
    "papermill": {
     "duration": 0.136762,
     "end_time": "2021-03-01T12:05:25.323816",
     "exception": false,
     "start_time": "2021-03-01T12:05:25.187054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del feat_train_df[\"user_id\"], feat_test_df[\"user_id\"], feat_train_df[\"date_date\"], feat_test_df[\"date_date\"], user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:05:25.388443Z",
     "iopub.status.busy": "2021-03-01T12:05:25.387787Z",
     "iopub.status.idle": "2021-03-01T12:05:25.392774Z",
     "shell.execute_reply": "2021-03-01T12:05:25.392250Z"
    },
    "papermill": {
     "duration": 0.039007,
     "end_time": "2021-03-01T12:05:25.392882",
     "exception": false,
     "start_time": "2021-03-01T12:05:25.353875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date_dayofweek', 'date_day', 'date_year', 'date_month', 'date_is_holiday', 'date_hanakin', 'pub_log=total_items', 'pub_log=JAN_nunique', 'pub_log=buy_catCD114', 'pub_log=buy_catCD134', 'pub_log=buy_catCD135', 'pub_log=buy_catCD136', 'pub_log=buy_catCD143', 'pub_log=buy_catCD207', 'pub_log=buy_catCD209', 'pub_log=buy_catCD210', 'pub_log=buy_catCD368', 'pub_log=buy_catCD370', 'pub_log=buy_catCD376', 'pub_log=buy_catCD508', 'pub_log=buy_catCD509', 'pub_log=buy_catCD587', 'pub_log=buy_catCD716', 'pub_log=buy_catCD720', 'pub_log=buy_catCD724', 'pub_log=buy_catCD768', 'pub_log=buy_catCD171', 'pub_log=buy_catCD173', 'pub_log=buy_catCD172', 'pub_log=buy_catCD110', 'pub_log=buy_catCD113', 'pub_log=buy_catCD38', 'pub_log=buy_catCD537', 'pub_log=buy_catCD539', 'pub_log=buy_catCD629', 'pub_log=buy_catCD467', 'pub_log=buy_catCD435', 'hour', 'register_number', 'time_elapsed', 'ratio_部門名=100円均一', 'ratio_部門名=AV家電', 'ratio_部門名=おもちゃ', 'ratio_部門名=たばこ', 'ratio_部門名=アイスクリーム', 'ratio_部門名=インテリア', 'ratio_部門名=オーラルケア', 'ratio_部門名=カスタマー', 'ratio_部門名=カー用品', 'ratio_部門名=キッズアウター', 'ratio_部門名=キッズインナー', 'ratio_部門名=キッチン', 'ratio_部門名=ゴルフ', 'ratio_部門名=サニタリー', 'ratio_部門名=サンダル', 'ratio_部門名=シーズン', 'ratio_部門名=スナック・キャンディー', 'ratio_部門名=スポーツ', 'ratio_部門名=スマホパーツ', 'ratio_部門名=セルフ化粧品', 'ratio_部門名=ソックス', 'ratio_部門名=チョコ・ビスクラ', 'ratio_部門名=デザート', 'ratio_部門名=トイレタリー用品', 'ratio_部門名=ハードリカー', 'ratio_部門名=パジャマ', 'ratio_部門名=パンスト', 'ratio_部門名=パーツ', 'ratio_部門名=ビジネスウェア', 'ratio_部門名=ビジネスグッズ', 'ratio_部門名=ファブリックケア', 'ratio_部門名=ヘアカラー', 'ratio_部門名=ヘアケア', 'ratio_部門名=ペット水物', 'ratio_部門名=ペット用品', 'ratio_部門名=ペーパー', 'ratio_部門名=ホールセールパン', 'ratio_部門名=メンズアウター', 'ratio_部門名=メンズアクセサリー', 'ratio_部門名=メンズアクティブ', 'ratio_部門名=メンズインナー', 'ratio_部門名=メンズシューズ', 'ratio_部門名=メンズビューティー', 'ratio_部門名=レジャー', 'ratio_部門名=レジ袋', 'ratio_部門名=レディースアウター', 'ratio_部門名=レディースアクセサリー', 'ratio_部門名=レディースアクティブ', 'ratio_部門名=レディースインナー', 'ratio_部門名=レディースシューズ', 'ratio_部門名=ワークウェア', 'ratio_部門名=ワークグッズ', 'ratio_部門名=ワークシューズ', 'ratio_部門名=乳製品', 'ratio_部門名=住居補修用品', 'ratio_部門名=健康・理美容家電', 'ratio_部門名=健康食品', 'ratio_部門名=健康飲料', 'ratio_部門名=冷凍', 'ratio_部門名=冷凍食品', 'ratio_部門名=冷惣菜', 'ratio_部門名=制度化粧品', 'ratio_部門名=加工', 'ratio_部門名=加工肉', 'ratio_部門名=医薬品', 'ratio_部門名=台所消耗', 'ratio_部門名=和惣菜', 'ratio_部門名=和日配', 'ratio_部門名=和洋菓子', 'ratio_部門名=和菓子', 'ratio_部門名=和酒', 'ratio_部門名=和風調味料', 'ratio_部門名=嗜好品', 'ratio_部門名=園芸用品', 'ratio_部門名=基礎化粧品', 'ratio_部門名=塩干', 'ratio_部門名=塩干(イン加工)', 'ratio_部門名=大型PET', 'ratio_部門名=季節家電', 'ratio_部門名=季節雑貨', 'ratio_部門名=寝具', 'ratio_部門名=寿司(惣菜)', 'ratio_部門名=寿司(鮮魚)', 'ratio_部門名=小型PET', 'ratio_部門名=情報家電', 'ratio_部門名=指定ゴミ袋', 'ratio_部門名=文具', 'ratio_部門名=日配飲料', 'ratio_部門名=未登録等', 'ratio_部門名=果物', 'ratio_部門名=水・炭酸水', 'ratio_部門名=洋風調味料', 'ratio_部門名=洗剤・芳香', 'ratio_部門名=洗面浴用用品', 'ratio_部門名=温惣菜', 'ratio_部門名=漬物', 'ratio_部門名=牛肉', 'ratio_部門名=犬フード', 'ratio_部門名=猫フード', 'ratio_部門名=玉子', 'ratio_部門名=珍味', 'ratio_部門名=生活家電', 'ratio_部門名=生魚', 'ratio_部門名=産直', 'ratio_部門名=穀物', 'ratio_部門名=米', 'ratio_部門名=米飯(麺)', 'ratio_部門名=練り製品', 'ratio_部門名=缶飲料', 'ratio_部門名=羊・馬肉', 'ratio_部門名=自転車', 'ratio_部門名=虫ケア', 'ratio_部門名=行楽用品', 'ratio_部門名=衛生', 'ratio_部門名=調理品', 'ratio_部門名=調理家電', 'ratio_部門名=調理素材', 'ratio_部門名=豚肉', 'ratio_部門名=軽家具・収納', 'ratio_部門名=農産加工', 'ratio_部門名=野菜', 'ratio_部門名=釣具', 'ratio_部門名=雑誌･CD', 'ratio_部門名=鶏肉', 'ratio_部門名=麺類', 'visit_hour=0', 'visit_hour=1', 'visit_hour=2', 'visit_hour=3', 'visit_hour=4', 'visit_hour=5', 'visit_hour=6', 'visit_hour=7', 'visit_hour=8', 'visit_hour=9', 'visit_hour=10', 'visit_hour=11', 'visit_hour=12', 'visit_hour=13', 'visit_hour=14', 'visit_hour=15', 'visit_hour=16', 'visit_hour=17', 'visit_hour=18', 'visit_hour=19', 'visit_hour=20', 'visit_hour=21', 'visit_hour=22', 'visit_hour=23', 'user_dayofweek=0', 'user_dayofweek=1', 'user_dayofweek=2', 'user_dayofweek=3', 'user_dayofweek=4', 'user_dayofweek=5', 'user_dayofweek=6', 'age', 'gender', 'total_appearance', 'ratio_action8', 'ratio_action9', 'ratio_action11', 'ratio_action14', 'ratio_action20', 'ratio_action22', 'ratio_action23', 'ratio_action25', 'ratio_action28', 'ratio_action36', 'ratio_action41', 'ratio_action57', 'ratio_action59', 'ratio_action62', 'ratio_action63', 'ratio_action64', 'ratio_action73', 'ratio_action77', 'ratio_action78', 'ratio_action79', 'ratio_action99', 'ratio_action105', 'ratio_action111', 'ratio_action117', 'ratio_action123', 'ratio_action136', 'ratio_action137', 'ratio_action141', 'ratio_action170', 'ratio_action186', 'ratio_action187', 'ratio_action190', 'ratio_action194', 'ratio_action199', 'ratio_action202', 'ratio_action203', 'ratio_action204', 'ratio_action205', 'ratio_action207', 'ratio_action209', 'ratio_action216', 'ratio_action218', 'ratio_action225', 'total_cost_mean', 'total_cost_std', 'total_nitems_mean', 'total_nitems_std', 'spend_time_mean', 'spend_time_std', 'BBInterval_171_past_avg_buy', 'BBInterval_171_days_before_buy', 'BBInterval_173_past_avg_buy', 'BBInterval_173_days_before_buy', 'BBInterval_172_past_avg_buy', 'BBInterval_172_days_before_buy', 'BBInterval_114_past_avg_buy', 'BBInterval_114_days_before_buy', 'BBInterval_134_past_avg_buy', 'BBInterval_134_days_before_buy', 'BBInterval_110_past_avg_buy', 'BBInterval_110_days_before_buy', 'BBInterval_113_past_avg_buy', 'BBInterval_113_days_before_buy', 'BBInterval_38_past_avg_buy', 'BBInterval_38_days_before_buy', 'BBInterval_376_past_avg_buy', 'BBInterval_376_days_before_buy', 'BBInterval_537_past_avg_buy', 'BBInterval_537_days_before_buy', 'BBInterval_539_past_avg_buy', 'BBInterval_539_days_before_buy', 'BBInterval_629_past_avg_buy', 'BBInterval_629_days_before_buy', 'BBInterval_467_past_avg_buy', 'BBInterval_467_days_before_buy', 'BBInterval_435_past_avg_buy', 'BBInterval_435_days_before_buy', 'BBInterval_768_past_avg_buy', 'BBInterval_768_days_before_buy', 'te_user_id_38', 'te_user_id_110', 'te_user_id_113', 'te_user_id_114', 'te_user_id_134', 'te_user_id_171', 'te_user_id_172', 'te_user_id_173', 'te_user_id_376', 'te_user_id_435', 'te_user_id_467', 'te_user_id_537', 'te_user_id_539', 'te_user_id_629', 'te_user_id_768', 'te_user_id_38_after', 'te_user_id_110_after', 'te_user_id_113_after', 'te_user_id_114_after', 'te_user_id_134_after', 'te_user_id_171_after', 'te_user_id_172_after', 'te_user_id_173_after', 'te_user_id_376_after', 'te_user_id_435_after', 'te_user_id_467_after', 'te_user_id_537_after', 'te_user_id_539_after', 'te_user_id_629_after', 'te_user_id_768_after']\n"
     ]
    }
   ],
   "source": [
    "features = [i for i in feat_train_df.columns]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:05:25.461698Z",
     "iopub.status.busy": "2021-03-01T12:05:25.460401Z",
     "iopub.status.idle": "2021-03-01T12:06:00.572561Z",
     "shell.execute_reply": "2021-03-01T12:06:00.573271Z"
    },
    "papermill": {
     "duration": 35.150832,
     "end_time": "2021-03-01T12:06:00.573499",
     "exception": false,
     "start_time": "2021-03-01T12:05:25.422667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((366478, 15), (366478, 308), (56486, 308))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not to increase memory use during training\n",
    "feat_train_np = np.ndarray(shape=(len(feat_train_df), len(features)), dtype=np.float32)\n",
    "feat_test_np = np.ndarray(shape=(len(feat_test_df), len(features)), dtype=np.float32)\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    feat_train_np[:,idx] = feat_train_df[feature].astype(np.float32)\n",
    "    feat_test_np[:,idx] = feat_test_df[feature].astype(np.float32)\n",
    "    del feat_train_df[feature], feat_test_df[feature]\n",
    "    \n",
    "feat_train_df = feat_train_np\n",
    "feat_test_df = feat_test_np\n",
    "train_target_df = train_target_df.to_numpy()\n",
    "\n",
    "train_target_df.shape, feat_train_df.shape, feat_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028415,
     "end_time": "2021-03-01T12:06:00.639500",
     "exception": false,
     "start_time": "2021-03-01T12:06:00.611085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# xgb modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:06:00.704773Z",
     "iopub.status.busy": "2021-03-01T12:06:00.703889Z",
     "iopub.status.idle": "2021-03-01T12:06:00.729831Z",
     "shell.execute_reply": "2021-03-01T12:06:00.729294Z"
    },
    "papermill": {
     "duration": 0.061832,
     "end_time": "2021-03-01T12:06:00.729941",
     "exception": false,
     "start_time": "2021-03-01T12:06:00.668109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('classify',\n",
       "                 MultiOutputClassifier(estimator=XGBClassifier(base_score=None,\n",
       "                                                               booster=None,\n",
       "                                                               colsample_bylevel=None,\n",
       "                                                               colsample_bynode=None,\n",
       "                                                               colsample_bytree=None,\n",
       "                                                               eval_metric='auc',\n",
       "                                                               gamma=3.6975,\n",
       "                                                               gpu_id=None,\n",
       "                                                               importance_type='gain',\n",
       "                                                               interaction_constraints=None,\n",
       "                                                               learning_rate=0.1,\n",
       "                                                               max_delta_step=2.0706,\n",
       "                                                               max_depth=7,\n",
       "                                                               min_child_weight=31.58,\n",
       "                                                               missing=nan,\n",
       "                                                               monotone_constraints=None,\n",
       "                                                               n_estimators=166,\n",
       "                                                               n_jobs=None,\n",
       "                                                               num_parallel_tree=None,\n",
       "                                                               random_state=None,\n",
       "                                                               reg_alpha=None,\n",
       "                                                               reg_lambda=None,\n",
       "                                                               scale_pos_weight=None,\n",
       "                                                               subsample=None,\n",
       "                                                               tree_method='gpu_hist',\n",
       "                                                               validate_parameters=None,\n",
       "                                                               verbosity=None)))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n",
    "\n",
    "clf = Pipeline([('classify', classifier)\n",
    "               ])\n",
    "\n",
    "params = {#'classify__estimator__colsample_bytree': 0.7522,\n",
    "          'classify__estimator__gamma': 3.6975,\n",
    "          'classify__estimator__learning_rate': 0.1,\n",
    "          'classify__estimator__max_delta_step': 2.0706,\n",
    "          'classify__estimator__max_depth': 7,\n",
    "          'classify__estimator__min_child_weight': 31.5800,\n",
    "          'classify__estimator__n_estimators': 166,\n",
    "          #'classify__estimator__subsample': 0.8,\n",
    "          'classify__estimator__eval_metric': \"auc\",\n",
    "         }\n",
    "\n",
    "clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:06:00.797640Z",
     "iopub.status.busy": "2021-03-01T12:06:00.796188Z",
     "iopub.status.idle": "2021-03-01T12:14:16.966585Z",
     "shell.execute_reply": "2021-03-01T12:14:16.967017Z"
    },
    "papermill": {
     "duration": 496.208751,
     "end_time": "2021-03-01T12:14:16.967184",
     "exception": false,
     "start_time": "2021-03-01T12:06:00.758433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold:  0\n",
      "0.8414680919409944\n",
      "Starting fold:  1\n",
      "0.8404503498337793\n",
      "Starting fold:  2\n",
      "0.8424446670756048\n",
      "Starting fold:  3\n",
      "0.8426184665345091\n",
      "Starting fold:  4\n",
      "0.8421301780800833\n",
      "[0.8414680919409944, 0.8404503498337793, 0.8424446670756048, 0.8426184665345091, 0.8421301780800833]\n",
      "Mean OOF loss across folds 0.8418223506929943\n",
      "STD OOF loss across folds 0.000790407671328354\n",
      "OOF score:  0.8417740594612987\n"
     ]
    }
   ],
   "source": [
    "xgb_oof_preds = np.zeros((feat_train_df.shape[0], train_target_df.shape[1]))\n",
    "xgb_test_preds = np.zeros((feat_test_df.shape[0], train_target_df.shape[1]))\n",
    "oof_losses = []\n",
    "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=0, shuffle=True)\n",
    "for fn, (trn_idx, val_idx) in enumerate(mskf.split(feat_train_df, train_target_df)):\n",
    "    print('Starting fold: ', fn)\n",
    "    X_train, X_val = feat_train_df[trn_idx,:], feat_train_df[val_idx,:]\n",
    "    y_train, y_val = train_target_df[trn_idx,:], train_target_df[val_idx,:]\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "    xgb_oof_preds[val_idx] = val_preds\n",
    "    \n",
    "    score = roc_auc_score(y_val, val_preds, average='macro')\n",
    "    print(score)\n",
    "    oof_losses.append(score)\n",
    "    preds = clf.predict_proba(feat_test_df)\n",
    "\n",
    "    preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "    xgb_test_preds += preds / NFOLDS\n",
    "    \n",
    "print(oof_losses)\n",
    "print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "print('STD OOF loss across folds', np.std(oof_losses))\n",
    "print('OOF score: ', roc_auc_score(train_target_df, xgb_oof_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:17.049017Z",
     "iopub.status.busy": "2021-03-01T12:14:17.048251Z",
     "iopub.status.idle": "2021-03-01T12:14:18.755141Z",
     "shell.execute_reply": "2021-03-01T12:14:18.754661Z"
    },
    "papermill": {
     "duration": 1.743263,
     "end_time": "2021-03-01T12:14:18.755259",
     "exception": false,
     "start_time": "2021-03-01T12:14:17.011996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/54562464/can-i-show-feature-importance-for-multioutputclassifier\n",
    "feat_impts = [] \n",
    "for model in classifier.estimators_:\n",
    "    feat_impts.append(model.feature_importances_)\n",
    "\n",
    "feature_importance_values = np.mean(feat_impts, axis=0)\n",
    "feature_importance_values = pd.DataFrame(feature_importance_values)\n",
    "feature_importance_values.index = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:18.831556Z",
     "iopub.status.busy": "2021-03-01T12:14:18.831072Z",
     "iopub.status.idle": "2021-03-01T12:14:18.844379Z",
     "shell.execute_reply": "2021-03-01T12:14:18.843716Z"
    },
    "papermill": {
     "duration": 0.05714,
     "end_time": "2021-03-01T12:14:18.844490",
     "exception": false,
     "start_time": "2021-03-01T12:14:18.787350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_dayofweek</th>\n",
       "      <th>date_day</th>\n",
       "      <th>date_year</th>\n",
       "      <th>date_month</th>\n",
       "      <th>date_is_holiday</th>\n",
       "      <th>date_hanakin</th>\n",
       "      <th>pub_log=total_items</th>\n",
       "      <th>pub_log=JAN_nunique</th>\n",
       "      <th>pub_log=buy_catCD114</th>\n",
       "      <th>pub_log=buy_catCD134</th>\n",
       "      <th>...</th>\n",
       "      <th>te_user_id_171_after</th>\n",
       "      <th>te_user_id_172_after</th>\n",
       "      <th>te_user_id_173_after</th>\n",
       "      <th>te_user_id_376_after</th>\n",
       "      <th>te_user_id_435_after</th>\n",
       "      <th>te_user_id_467_after</th>\n",
       "      <th>te_user_id_537_after</th>\n",
       "      <th>te_user_id_539_after</th>\n",
       "      <th>te_user_id_629_after</th>\n",
       "      <th>te_user_id_768_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.00336</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_dayofweek  date_day  date_year  date_month  date_is_holiday  \\\n",
       "0        0.003499  0.003001   0.004578    0.003964         0.002588   \n",
       "\n",
       "   date_hanakin  pub_log=total_items  pub_log=JAN_nunique  \\\n",
       "0      0.001057              0.00336             0.003498   \n",
       "\n",
       "   pub_log=buy_catCD114  pub_log=buy_catCD134  ...  te_user_id_171_after  \\\n",
       "0              0.004369              0.002233  ...              0.002723   \n",
       "\n",
       "   te_user_id_172_after  te_user_id_173_after  te_user_id_376_after  \\\n",
       "0              0.002492              0.002994              0.003255   \n",
       "\n",
       "   te_user_id_435_after  te_user_id_467_after  te_user_id_537_after  \\\n",
       "0              0.003023              0.003285              0.002678   \n",
       "\n",
       "   te_user_id_539_after  te_user_id_629_after  te_user_id_768_after  \n",
       "0              0.003222              0.002696              0.003078  \n",
       "\n",
       "[1 rows x 308 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(feature_importance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032455,
     "end_time": "2021-03-01T12:14:18.909738",
     "exception": false,
     "start_time": "2021-03-01T12:14:18.877283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# catboost modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:18.979678Z",
     "iopub.status.busy": "2021-03-01T12:14:18.979009Z",
     "iopub.status.idle": "2021-03-01T12:14:18.981843Z",
     "shell.execute_reply": "2021-03-01T12:14:18.981359Z"
    },
    "papermill": {
     "duration": 0.03911,
     "end_time": "2021-03-01T12:14:18.981952",
     "exception": false,
     "start_time": "2021-03-01T12:14:18.942842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier = MultiOutputClassifier(CatBoostClassifier(task_type='GPU'))\n",
    "\n",
    "# clf = Pipeline([('classify', classifier)\n",
    "#                ])\n",
    "\n",
    "# params = {'classify__estimator__learning_rate': 0.1,\n",
    "#           'classify__estimator__depth': 6, \n",
    "#           'classify__estimator__l2_leaf_reg': 3, \n",
    "#           'classify__estimator__loss_function': 'Logloss', \n",
    "#           'classify__estimator__eval_metric': 'AUC', \n",
    "#           'classify__estimator__iterations': 100,\n",
    "#           'classify__estimator__od_type': 'Iter', \n",
    "#           'classify__estimator__boosting_type': 'Plain', \n",
    "#           'classify__estimator__bootstrap_type': 'Bernoulli', \n",
    "#           'classify__estimator__allow_const_label': True, \n",
    "#           'classify__estimator__random_state': 0,\n",
    "#           'classify__estimator__verbose': 0\n",
    "#          }\n",
    "\n",
    "# clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:19.050263Z",
     "iopub.status.busy": "2021-03-01T12:14:19.049575Z",
     "iopub.status.idle": "2021-03-01T12:14:19.052556Z",
     "shell.execute_reply": "2021-03-01T12:14:19.052149Z"
    },
    "papermill": {
     "duration": 0.0386,
     "end_time": "2021-03-01T12:14:19.052681",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.014081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cat_oof_preds = np.zeros((feat_train_df.shape[0], train_target_df.shape[1]))\n",
    "# cat_test_preds = np.zeros((feat_test_df.shape[0], train_target_df.shape[1]))\n",
    "# oof_losses = []\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=0, shuffle=True)\n",
    "# for fn, (trn_idx, val_idx) in enumerate(mskf.split(feat_train_df, train_target_df)):\n",
    "#     print('Starting fold: ', fn)\n",
    "#     X_train, X_val = feat_train_df[trn_idx,:], feat_train_df[val_idx,:]\n",
    "#     y_train, y_val = train_target_df[trn_idx,:], train_target_df[val_idx,:]\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "#     val_preds = np.array(val_preds) \n",
    "#     val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "#     cat_oof_preds[val_idx] = val_preds\n",
    "    \n",
    "#     score = roc_auc_score(y_val, val_preds, average='macro')\n",
    "#     print(score)\n",
    "#     oof_losses.append(score)\n",
    "#     preds = clf.predict_proba(feat_test_df)    \n",
    "#     preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "#     cat_test_preds += preds / NFOLDS\n",
    "    \n",
    "# print(oof_losses)\n",
    "# print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "# print('STD OOF loss across folds', np.std(oof_losses))\n",
    "# print('OOF score: ', roc_auc_score(train_target_df, cat_oof_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032114,
     "end_time": "2021-03-01T12:14:19.117543",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.085429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tabnet modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:19.185757Z",
     "iopub.status.busy": "2021-03-01T12:14:19.185125Z",
     "iopub.status.idle": "2021-03-01T12:14:19.188135Z",
     "shell.execute_reply": "2021-03-01T12:14:19.187724Z"
    },
    "papermill": {
     "duration": 0.03856,
     "end_time": "2021-03-01T12:14:19.188241",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.149681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LogitsLogLoss(Metric):\n",
    "#     def __init__(self):\n",
    "#         self._name = \"logits_ll\"\n",
    "#         self._maximize = False\n",
    "\n",
    "#     def __call__(self, y_true, y_pred):\n",
    "\n",
    "#         logits = 1 / (1 + np.exp(-y_pred))\n",
    "        \n",
    "#         aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n",
    "#         return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:19.256974Z",
     "iopub.status.busy": "2021-03-01T12:14:19.256328Z",
     "iopub.status.idle": "2021-03-01T12:14:19.258855Z",
     "shell.execute_reply": "2021-03-01T12:14:19.259290Z"
    },
    "papermill": {
     "duration": 0.038819,
     "end_time": "2021-03-01T12:14:19.259405",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.220586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class AUC(Metric):\n",
    "#     def __init__(self):\n",
    "#         self._name = \"auc_ll\"\n",
    "#         self._maximize = True\n",
    "\n",
    "#     def __call__(self, y_true, y_pred):\n",
    "#         logits = 1 / (1 + np.exp(-y_pred))\n",
    "#         auc = roc_auc_score(y_true, logits, average=\"macro\")\n",
    "#         return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:19.328897Z",
     "iopub.status.busy": "2021-03-01T12:14:19.328213Z",
     "iopub.status.idle": "2021-03-01T12:14:19.331186Z",
     "shell.execute_reply": "2021-03-01T12:14:19.330754Z"
    },
    "papermill": {
     "duration": 0.039624,
     "end_time": "2021-03-01T12:14:19.331286",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.291662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAX_EPOCH=200\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# def seed_everything(seed_value):\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "#     if torch.cuda.is_available(): \n",
    "#         torch.cuda.manual_seed(seed_value)\n",
    "#         torch.cuda.manual_seed_all(seed_value)\n",
    "#         torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "# def modelling_tabnet(tr, target, te, sample_seed):\n",
    "#     seed_everything(sample_seed) \n",
    "#     tabnet_params = dict(n_d=8, n_a=8, n_steps=1, gamma=1.3, seed = sample_seed,\n",
    "#                      lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n",
    "#                      optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "#                      mask_type='entmax',\n",
    "#                      scheduler_params=dict(mode=\"min\",\n",
    "#                                            patience=5,\n",
    "#                                            min_lr=1e-5,\n",
    "#                                            factor=0.9,),\n",
    "#                      scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "#                      verbose=10,\n",
    "#                      )\n",
    "#     test_cv_preds = []\n",
    "\n",
    "#     oof_preds = np.zeros([len(tr),target.shape[1]])\n",
    "#     scores = []\n",
    "#     NB_SPLITS = 5\n",
    "#     mskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\n",
    "#     for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(tr, target)):\n",
    "#         print(\"FOLDS : \", fold_nb+1)\n",
    "\n",
    "#         ## model\n",
    "#         X_train, y_train = tr[train_idx, :], target[train_idx, :]\n",
    "#         X_val, y_val = tr[val_idx, :], target[val_idx, :]\n",
    "#         model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "#         model.fit(X_train=X_train,\n",
    "#               y_train=y_train,\n",
    "#               eval_set=[(X_val, y_val)],\n",
    "#               eval_name = [\"val\"],\n",
    "#               eval_metric = [\"auc_ll\"],\n",
    "#               max_epochs=MAX_EPOCH,\n",
    "#               patience=20, batch_size=1024, virtual_batch_size=128,\n",
    "#               num_workers=1, drop_last=False,\n",
    "#               # use binary cross entropy as this is not a regression problem\n",
    "#               loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "    \n",
    "#         preds_val = model.predict(X_val)\n",
    "#         # Apply sigmoid to the predictions\n",
    "#         preds =  1 / (1 + np.exp(-preds_val))\n",
    "#         score = np.max(model.history[\"val_auc_ll\"])\n",
    "#         oof_preds[val_idx,:] = preds\n",
    "#         scores.append(score)\n",
    "\n",
    "#         # preds on test\n",
    "#         preds_test = model.predict(te)\n",
    "#         test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n",
    "        \n",
    "#     test_preds_all = np.stack(test_cv_preds)\n",
    "#     aucs = []\n",
    "#     for task_id in range(15):\n",
    "#         aucs.append(roc_auc_score(y_true=target[:, task_id],y_score=oof_preds[:, task_id]))\n",
    "#     print(f\"Overall AUC : {np.mean(aucs)}\")\n",
    "#     return oof_preds, test_preds_all\n",
    "\n",
    "# tabnet_oof = np.zeros([feat_train_df.shape[0], len(TARGET_CATEGORIES)])\n",
    "# tabnet_pred = np.zeros([feat_test_df.shape[0], len(TARGET_CATEGORIES)])\n",
    "\n",
    "# seeds = [0]\n",
    "# for seed_ in seeds:\n",
    "#     oof_preds, test_preds_all = modelling_tabnet(feat_train_df, train_target_df, feat_test_df, seed_)\n",
    "#     tabnet_oof += oof_preds / len(seeds)\n",
    "#     tabnet_pred += test_preds_all.mean(axis=0) / len(seeds)\n",
    "    \n",
    "# print('OOF score: ', roc_auc_score(train_target_df, tabnet_oof, average='macro'))\n",
    "# score = roc_auc_score(train_target_df, oof_preds, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032928,
     "end_time": "2021-03-01T12:14:19.397003",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.364075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T12:14:19.468407Z",
     "iopub.status.busy": "2021-03-01T12:14:19.467327Z",
     "iopub.status.idle": "2021-03-01T12:14:21.443754Z",
     "shell.execute_reply": "2021-03-01T12:14:21.442692Z"
    },
    "papermill": {
     "duration": 2.014028,
     "end_time": "2021-03-01T12:14:21.443920",
     "exception": false,
     "start_time": "2021-03-01T12:14:19.429892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.iloc[:,:] = xgb_test_preds\n",
    "submission.to_csv(\"atmacup9_\"+str(score)[:-10]+\"_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.032977,
     "end_time": "2021-03-01T12:14:21.510492",
     "exception": false,
     "start_time": "2021-03-01T12:14:21.477515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1365.256604,
   "end_time": "2021-03-01T12:14:24.809856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-01T11:51:39.553252",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
