{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020716,
     "end_time": "2021-03-22T11:57:48.994002",
     "exception": false,
     "start_time": "2021-03-22T11:57:48.973286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- introduce seed for reproducibility in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-22T11:57:49.041283Z",
     "iopub.status.busy": "2021-03-22T11:57:49.040572Z",
     "iopub.status.idle": "2021-03-22T11:57:55.499151Z",
     "shell.execute_reply": "2021-03-22T11:57:55.498348Z"
    },
    "papermill": {
     "duration": 6.485789,
     "end_time": "2021-03-22T11:57:55.499378",
     "exception": false,
     "start_time": "2021-03-22T11:57:49.013589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from numba import jit\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "sys.path.append('../input/multilabelstraifier/')\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"max_rows\", 110)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:57:55.546092Z",
     "iopub.status.busy": "2021-03-22T11:57:55.545425Z",
     "iopub.status.idle": "2021-03-22T11:57:55.548266Z",
     "shell.execute_reply": "2021-03-22T11:57:55.547731Z"
    },
    "papermill": {
     "duration": 0.028227,
     "end_time": "2021-03-22T11:57:55.548374",
     "exception": false,
     "start_time": "2021-03-22T11:57:55.520147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 完全な再現はできないが、ブレは抑えることができる\n",
    "XLA_ACCELERATE = True\n",
    "tf.config.optimizer.set_jit(XLA_ACCELERATE)\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        inter_op_parallelism_threads=1, intra_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=graph, config=session_conf)\n",
    "\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:57:55.599193Z",
     "iopub.status.busy": "2021-03-22T11:57:55.598656Z",
     "iopub.status.idle": "2021-03-22T11:58:48.562028Z",
     "shell.execute_reply": "2021-03-22T11:58:48.561032Z"
    },
    "papermill": {
     "duration": 52.994038,
     "end_time": "2021-03-22T11:58:48.562176",
     "exception": false,
     "start_time": "2021-03-22T11:57:55.568138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIR = '../input/atma-retail20210129/'\n",
    "log_df = pd.read_csv(DIR+'carlog.csv', dtype={ 'value_1': str }, parse_dates=['date'])\n",
    "test_df = pd.read_csv(DIR+'test.csv')\n",
    "meta_df = pd.read_csv(DIR+\"meta.csv\")\n",
    "display_action_id = pd.read_csv(DIR+\"display_action_id.csv\")\n",
    "product_master_df = pd.read_csv(DIR+\"product_master.csv\", dtype={ 'JAN': str })\n",
    "user_master = pd.read_csv(DIR+\"user_master.csv\")\n",
    "submission = pd.read_csv(DIR+\"atmaCup9__sample_submission.csv\")\n",
    "test_sessions = test_df['session_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:48.651933Z",
     "iopub.status.busy": "2021-03-22T11:58:48.651041Z",
     "iopub.status.idle": "2021-03-22T11:58:48.995939Z",
     "shell.execute_reply": "2021-03-22T11:58:48.995458Z"
    },
    "papermill": {
     "duration": 0.412247,
     "end_time": "2021-03-22T11:58:48.996075",
     "exception": false,
     "start_time": "2021-03-22T11:58:48.583828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_df[\"dayofweek\"] = pd.to_datetime(meta_df[\"date\"]).dt.dayofweek\n",
    "meta_df[\"year\"] = pd.to_datetime(meta_df[\"date\"]).dt.year\n",
    "#meta_df[\"month\"] = pd.to_datetime(meta_df[\"date\"]).dt.month\n",
    "#meta_df[\"day\"] = pd.to_datetime(meta_df[\"date\"]).dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.042253Z",
     "iopub.status.busy": "2021-03-22T11:58:49.041518Z",
     "iopub.status.idle": "2021-03-22T11:58:49.060240Z",
     "shell.execute_reply": "2021-03-22T11:58:49.060694Z"
    },
    "papermill": {
     "duration": 0.044755,
     "end_time": "2021-03-22T11:58:49.060890",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.016135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number register_number 224\n"
     ]
    }
   ],
   "source": [
    "register_num_id2code = dict()\n",
    "for ind, i in enumerate(meta_df[\"register_number\"].unique()):\n",
    "    register_num_id2code[i] = ind\n",
    "meta_df[\"register_number\"] = meta_df[\"register_number\"].map(register_num_id2code)\n",
    "n_register = meta_df[\"register_number\"].nunique() + 1\n",
    "print(\"number register_number\", n_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.106302Z",
     "iopub.status.busy": "2021-03-22T11:58:49.104603Z",
     "iopub.status.idle": "2021-03-22T11:58:49.106957Z",
     "shell.execute_reply": "2021-03-22T11:58:49.107346Z"
    },
    "papermill": {
     "duration": 0.026302,
     "end_time": "2021-03-22T11:58:49.107461",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.081159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SEQ = 64\n",
    "NFOLDS = 5\n",
    "target_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.152923Z",
     "iopub.status.busy": "2021-03-22T11:58:49.152209Z",
     "iopub.status.idle": "2021-03-22T11:58:49.168357Z",
     "shell.execute_reply": "2021-03-22T11:58:49.168762Z"
    },
    "papermill": {
     "duration": 0.040356,
     "end_time": "2021-03-22T11:58:49.168890",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.128534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number skills 773 0 771\n"
     ]
    }
   ],
   "source": [
    "n_category_id_class = product_master_df[\"category_id\"].nunique() + 1\n",
    "print(\"number skills\", n_category_id_class, product_master_df[\"category_id\"].min(), product_master_df[\"category_id\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.216279Z",
     "iopub.status.busy": "2021-03-22T11:58:49.214558Z",
     "iopub.status.idle": "2021-03-22T11:58:49.216880Z",
     "shell.execute_reply": "2021-03-22T11:58:49.217270Z"
    },
    "papermill": {
     "duration": 0.027816,
     "end_time": "2021-03-22T11:58:49.217381",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.189565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_CATEGORIES = [\n",
    "    # お酒に関するもの\n",
    "    'ビール系__RTD', 'ビール系__ビール系', 'ビール系__ノンアルコール',\n",
    "    \n",
    "    # お菓子に関するもの\n",
    "    'スナック・キャンディー__スナック', \n",
    "    'チョコ・ビスクラ__チョコレート', \n",
    "    'スナック・キャンディー__ガム', \n",
    "    'スナック・キャンディー__シリアル',\n",
    "    'アイスクリーム__ノベルティー', \n",
    "    '和菓子__米菓',\n",
    "    \n",
    "    # 飲料に関するもの\n",
    "    '水・炭酸水__大型PET（炭酸水）',\n",
    "    '水・炭酸水__小型PET（炭酸水）',\n",
    "    '缶飲料__コーヒー（缶）',\n",
    "    '小型PET__コーヒー（小型PET）',\n",
    "    '大型PET__無糖茶（大型PET）',\n",
    "    \n",
    "    # 麺類\n",
    "    '麺類__カップ麺',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.811152Z",
     "iopub.status.busy": "2021-03-22T11:58:49.808291Z",
     "iopub.status.idle": "2021-03-22T11:58:49.817826Z",
     "shell.execute_reply": "2021-03-22T11:58:49.818239Z"
    },
    "papermill": {
     "duration": 0.58026,
     "end_time": "2021-03-22T11:58:49.818378",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.238118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 'ビール系__RTD')\n",
      "(173, 'ビール系__ビール系')\n",
      "(172, 'ビール系__ノンアルコール')\n",
      "(114, 'スナック・キャンディー__スナック')\n",
      "(134, 'チョコ・ビスクラ__チョコレート')\n",
      "(110, 'スナック・キャンディー__ガム')\n",
      "(113, 'スナック・キャンディー__シリアル')\n",
      "(38, 'アイスクリーム__ノベルティー')\n",
      "(376, '和菓子__米菓')\n",
      "(537, '水・炭酸水__大型PET（炭酸水）')\n",
      "(539, '水・炭酸水__小型PET（炭酸水）')\n",
      "(629, '缶飲料__コーヒー（缶）')\n",
      "(467, '小型PET__コーヒー（小型PET）')\n",
      "(435, '大型PET__無糖茶（大型PET）')\n",
      "(768, '麺類__カップ麺')\n"
     ]
    }
   ],
   "source": [
    "cat2id = dict(zip(product_master_df['category_name'], product_master_df['category_id']))\n",
    "TARGET_IDS = pd.Series(TARGET_CATEGORIES).map(cat2id).values.tolist()\n",
    "category_id2code = dict(zip(TARGET_IDS, TARGET_CATEGORIES))\n",
    "\n",
    "for x in zip(TARGET_IDS, TARGET_CATEGORIES):\n",
    "    print(x)\n",
    "\n",
    "def only_purchase_records(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx = input_df['kind_1'] == '商品'\n",
    "    out_df = input_df[idx].reset_index(drop=True)\n",
    "    return out_df\n",
    "\n",
    "def create_payment(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ログデータから session_id / JAN ごとの購買情報に変換します.\n",
    "\n",
    "    Args:\n",
    "        input_df:\n",
    "            レジカートログデータ\n",
    "\n",
    "    Returns:\n",
    "        session_id, JAN, n_items (合計購買数) の DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 購買情報は商品のものだけ.\n",
    "    out_df = only_purchase_records(input_df)\n",
    "    out_df = out_df.groupby(['session_id', 'value_1'])['n_items'].sum().reset_index()\n",
    "    out_df = out_df.rename(columns={\n",
    "        'value_1': 'JAN'\n",
    "    })\n",
    "    return out_df\n",
    "\n",
    "def annot_category(input_df: pd.DataFrame,\n",
    "                   master_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    カテゴリ ID をひも付けます.\n",
    "\n",
    "    Args:\n",
    "        input_df:\n",
    "            変換するデータ.\n",
    "            `value_1`  or `JAN` を column として持っている必要があります.\n",
    "        master_df:\n",
    "            商品マスタのデータフレーム\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    input_df = input_df.rename(columns={'value_1': 'JAN'})\n",
    "    out_df = pd.merge(input_df['JAN'],\n",
    "                      master_df[['JAN', 'category_id']], on='JAN', how='left')\n",
    "    return out_df['category_id']\n",
    "\n",
    "def only_payment_session_record(input_log_df):\n",
    "    \"\"\"支払いが紐づくセッションへ絞り込みを行なう\"\"\"\n",
    "    payed_sessions = input_log_df[input_log_df['is_payment'] == 1]['session_id'].unique()\n",
    "    idx = input_log_df['session_id'].isin(payed_sessions)\n",
    "    out_df = input_log_df[idx].reset_index(drop=True)\n",
    "    return out_df\n",
    "\n",
    "def create_target_from_log(log_df: pd.DataFrame,\n",
    "                           product_master_df: pd.DataFrame,\n",
    "                          only_payment=True):\n",
    "\n",
    "    if only_payment:\n",
    "        log_df = only_payment_session_record(log_df)\n",
    "    pay_df = create_payment(log_df)\n",
    "    pay_df['category_id'] = annot_category(pay_df, master_df=product_master_df)\n",
    "\n",
    "    # null の category を削除. JAN が紐付かない時に発生する.\n",
    "    idx_null = pay_df['category_id'].isnull()\n",
    "    pay_df = pay_df[~idx_null].reset_index(drop=True)\n",
    "    # Nullが混じっている時 float になるため int へ明示的に戻す\n",
    "    pay_df['category_id'] = pay_df['category_id'].astype(int)\n",
    "\n",
    "    idx = pay_df['category_id'].isin(TARGET_IDS)\n",
    "    target_df = pd.pivot_table(data=pay_df[idx],\n",
    "                               index='session_id',\n",
    "                               columns='category_id',\n",
    "                               values='n_items',\n",
    "                               aggfunc='sum')\n",
    "\n",
    "    sessions = sorted(log_df['session_id'].unique())\n",
    "    print(len(sessions))\n",
    "    target_df = target_df.reindex(sessions)\n",
    "    target_df = target_df.fillna(0).astype(int)\n",
    "    return target_df, pay_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023605,
     "end_time": "2021-03-22T11:58:49.863323",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.839718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:49.909623Z",
     "iopub.status.busy": "2021-03-22T11:58:49.909124Z",
     "iopub.status.idle": "2021-03-22T11:58:58.150533Z",
     "shell.execute_reply": "2021-03-22T11:58:58.150082Z"
    },
    "papermill": {
     "duration": 8.266226,
     "end_time": "2021-03-22T11:58:58.150661",
     "exception": false,
     "start_time": "2021-03-22T11:58:49.884435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 完全なデータを持っているログに絞る\n",
    "test_sessions = test_df['session_id'].unique()\n",
    "idx_test = log_df['session_id'].isin(test_sessions)\n",
    "whole_log_df = log_df[~idx_test].reset_index(drop=True)\n",
    "payment_session_df = only_payment_session_record(whole_log_df)\n",
    "del whole_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:58:59.782771Z",
     "iopub.status.busy": "2021-03-22T11:58:59.781900Z",
     "iopub.status.idle": "2021-03-22T11:59:05.148066Z",
     "shell.execute_reply": "2021-03-22T11:59:05.148496Z"
    },
    "papermill": {
     "duration": 6.976471,
     "end_time": "2021-03-22T11:59:05.148662",
     "exception": false,
     "start_time": "2021-03-22T11:58:58.172191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 商品購買の最後(max spend time)が10分より大きいセッションを取り出す\n",
    "is_item_record = payment_session_df['kind_1'] == '商品'\n",
    "max_payed_time = payment_session_df[is_item_record].groupby('session_id')['spend_time'].max()\n",
    "max_payed_time_over_10min = max_payed_time[max_payed_time > 10 * 60]\n",
    "\n",
    "train_sessions = max_payed_time_over_10min.index.tolist()\n",
    "train_whole_log_df = payment_session_df[payment_session_df['session_id'].isin(train_sessions)].reset_index(drop=True)\n",
    "\n",
    "del max_payed_time, max_payed_time_over_10min, payment_session_df, is_item_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:05.199500Z",
     "iopub.status.busy": "2021-03-22T11:59:05.198760Z",
     "iopub.status.idle": "2021-03-22T11:59:05.545198Z",
     "shell.execute_reply": "2021-03-22T11:59:05.544695Z"
    },
    "papermill": {
     "duration": 0.374693,
     "end_time": "2021-03-22T11:59:05.545328",
     "exception": false,
     "start_time": "2021-03-22T11:59:05.170635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 103\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "time_elasped_count = meta_df['time_elapsed'].value_counts(normalize=True)\n",
    "\n",
    "train_time_elapsed = np.random.choice(time_elasped_count.index.astype(int), \n",
    "                                      p=time_elasped_count.values, \n",
    "                                      size=len(train_sessions))\n",
    "train_meta_df = pd.DataFrame({\n",
    "    'session_id': train_sessions,\n",
    "    'time_elapsed': train_time_elapsed\n",
    "})\n",
    "\n",
    "train_meta_df = pd.merge(train_meta_df, \n",
    "                         meta_df.drop(columns=['time_elapsed']), \n",
    "                         on='session_id', \n",
    "                         how='left')\n",
    "del train_time_elapsed, train_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:05.627617Z",
     "iopub.status.busy": "2021-03-22T11:59:05.626737Z",
     "iopub.status.idle": "2021-03-22T11:59:11.322912Z",
     "shell.execute_reply": "2021-03-22T11:59:11.322222Z"
    },
    "papermill": {
     "duration": 5.756062,
     "end_time": "2021-03-22T11:59:11.323043",
     "exception": false,
     "start_time": "2021-03-22T11:59:05.566981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_df = pd.merge(train_whole_log_df[['session_id', 'spend_time']], train_meta_df, on='session_id', how='left')\n",
    "idx_show = _df['spend_time'] <= _df['time_elapsed'] * 60\n",
    "\n",
    "del _df\n",
    "\n",
    "train_public_df = train_whole_log_df[idx_show].reset_index(drop=True)\n",
    "train_private_df = train_whole_log_df[~idx_show].reset_index(drop=True)\n",
    "\n",
    "del idx_show, train_whole_log_df\n",
    "\n",
    "# テストのログデータと合わせて推論時に見ても良いログ `public_log_df` として保存しておく\n",
    "public_log_df = pd.concat([\n",
    "    train_public_df, log_df[log_df['session_id'].isin(test_sessions)]\n",
    "], axis=0, ignore_index=True)\n",
    "# meta に紐づく情報は後でよく使うので, テストデータにも meta 情報をマージしておきます. \n",
    "# train_meata_df / test_meta_df が今後特徴を作る上で key になるデータになります。\n",
    "test_meta_df = pd.merge(test_df, meta_df, on='session_id', how='left')\n",
    "\n",
    "del log_df, train_public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:12.320869Z",
     "iopub.status.busy": "2021-03-22T11:59:12.320010Z",
     "iopub.status.idle": "2021-03-22T11:59:20.944444Z",
     "shell.execute_reply": "2021-03-22T11:59:20.943979Z"
    },
    "papermill": {
     "duration": 9.599468,
     "end_time": "2021-03-22T11:59:20.944563",
     "exception": false,
     "start_time": "2021-03-22T11:59:11.345095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366478\n"
     ]
    }
   ],
   "source": [
    "train_target_df, _  = create_target_from_log(train_private_df, \n",
    "                                             product_master_df=product_master_df,\n",
    "                                            only_payment=False)\n",
    "\n",
    "train_target_df[train_target_df >= 1] = 1\n",
    "train_target_df[train_target_df <= 0] = 0\n",
    "\n",
    "del train_private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:21.310429Z",
     "iopub.status.busy": "2021-03-22T11:59:21.309630Z",
     "iopub.status.idle": "2021-03-22T11:59:26.109098Z",
     "shell.execute_reply": "2021-03-22T11:59:26.108021Z"
    },
    "papermill": {
     "duration": 5.142578,
     "end_time": "2021-03-22T11:59:26.109241",
     "exception": false,
     "start_time": "2021-03-22T11:59:20.966663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_log_df = public_log_df[public_log_df['kind_1'] == '商品'].reset_index(drop=True)\n",
    "public_log_df = pd.merge(public_log_df, product_master_df[[\"JAN\", \"category_id\"]], \n",
    "                         left_on=\"value_1\", right_on=\"JAN\",how=\"left\")[[\"session_id\", \"value_1\", \"category_id\"]]\n",
    "public_log_df[\"category_id\"] += 1\n",
    "public_log_df[\"value_1\"] = public_log_df[\"value_1\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:26.161420Z",
     "iopub.status.busy": "2021-03-22T11:59:26.160585Z",
     "iopub.status.idle": "2021-03-22T11:59:26.433349Z",
     "shell.execute_reply": "2021-03-22T11:59:26.433774Z"
    },
    "papermill": {
     "duration": 0.301513,
     "end_time": "2021-03-22T11:59:26.433918",
     "exception": false,
     "start_time": "2021-03-22T11:59:26.132405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number jans 47005 1 47004\n"
     ]
    }
   ],
   "source": [
    "jan2code = dict()\n",
    "for ind, i in enumerate(public_log_df[\"value_1\"].unique()):\n",
    "    jan2code[i] = ind + 1\n",
    "public_log_df[\"value_1\"] = public_log_df[\"value_1\"].map(jan2code)\n",
    "\n",
    "n_jan_class = public_log_df[\"value_1\"].nunique() + 1\n",
    "print(\"number jans\", n_jan_class, public_log_df[\"value_1\"].min(), public_log_df[\"value_1\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T11:59:26.488562Z",
     "iopub.status.busy": "2021-03-22T11:59:26.488057Z",
     "iopub.status.idle": "2021-03-22T12:00:29.691155Z",
     "shell.execute_reply": "2021-03-22T12:00:29.690650Z"
    },
    "papermill": {
     "duration": 63.235014,
     "end_time": "2021-03-22T12:00:29.691299",
     "exception": false,
     "start_time": "2021-03-22T11:59:26.456285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def make_category(a):\n",
    "    a = np.nan_to_num(a, nan=0)\n",
    "    if len(a) < MAX_SEQ:\n",
    "        a = np.concatenate([[0]*(MAX_SEQ-len(a)), a])\n",
    "    else:\n",
    "        a = a[:MAX_SEQ]\n",
    "    return a\n",
    "\n",
    "@jit\n",
    "def make_invisible_category(a):\n",
    "    return np.array([0]*(MAX_SEQ))\n",
    "\n",
    "def make_data(public_log_df, feature):\n",
    "    group = public_log_df.groupby('session_id').apply(lambda r: make_category(r[feature].values))\n",
    "\n",
    "    train_group = group[group.index.isin(train_meta_df.session_id.unique())]\n",
    "    test_group = group[group.index.isin(test_meta_df.session_id.unique())]\n",
    "\n",
    "    train_inv_session = set(train_meta_df.session_id) - set(train_group.index)\n",
    "    test_inv_session = set(test_meta_df.session_id) - set(test_group.index)\n",
    "\n",
    "    train_inv_group = meta_df[meta_df.session_id.isin(train_inv_session)].groupby('session_id').apply(lambda x: make_invisible_category(x[\"user_id\"].values))\n",
    "    test_inv_group = meta_df[meta_df.session_id.isin(test_inv_session)].groupby('session_id').apply(lambda x: make_invisible_category(x[\"user_id\"].values))\n",
    "\n",
    "    train_group = pd.concat([train_group, train_inv_group]).sort_index()\n",
    "    test_group = pd.concat([test_group, test_inv_group]).sort_index()\n",
    "    return train_group, test_group\n",
    "\n",
    "train_group, test_group = make_data(public_log_df, \"category_id\")\n",
    "train_action_group, test_action_group = make_data(public_log_df, \"value_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:00:29.813828Z",
     "iopub.status.busy": "2021-03-22T12:00:29.812642Z",
     "iopub.status.idle": "2021-03-22T12:00:29.815552Z",
     "shell.execute_reply": "2021-03-22T12:00:29.815144Z"
    },
    "papermill": {
     "duration": 0.101347,
     "end_time": "2021-03-22T12:00:29.815664",
     "exception": false,
     "start_time": "2021-03-22T12:00:29.714317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_only_users = set(test_meta_df.user_id) - set(train_meta_df.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:00:29.902816Z",
     "iopub.status.busy": "2021-03-22T12:00:29.901926Z",
     "iopub.status.idle": "2021-03-22T12:00:30.261585Z",
     "shell.execute_reply": "2021-03-22T12:00:30.260853Z"
    },
    "papermill": {
     "duration": 0.423676,
     "end_time": "2021-03-22T12:00:30.261776",
     "exception": false,
     "start_time": "2021-03-22T12:00:29.838100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_sorted_index = train_meta_df.sort_values([\"user_id\",\"session_id\"]).index\n",
    "original_index = [i for i in range(train_meta_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:00:30.528146Z",
     "iopub.status.busy": "2021-03-22T12:00:30.526858Z",
     "iopub.status.idle": "2021-03-22T12:00:30.528845Z",
     "shell.execute_reply": "2021-03-22T12:00:30.527499Z"
    },
    "papermill": {
     "duration": 0.230111,
     "end_time": "2021-03-22T12:00:30.529021",
     "exception": false,
     "start_time": "2021-03-22T12:00:30.298910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del meta_df, display_action_id, product_master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:00:30.675793Z",
     "iopub.status.busy": "2021-03-22T12:00:30.674664Z",
     "iopub.status.idle": "2021-03-22T12:01:14.490363Z",
     "shell.execute_reply": "2021-03-22T12:01:14.491051Z"
    },
    "papermill": {
     "duration": 43.926392,
     "end_time": "2021-03-22T12:01:14.491250",
     "exception": false,
     "start_time": "2021-03-22T12:00:30.564858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def calc_seq_target(c):\n",
    "    target_nums = 15\n",
    "    ans = np.ones([target_length, target_nums]) * -1\n",
    "    ans = ans.reshape(1,target_length, target_nums)\n",
    "    for i in range(1,len(c)):\n",
    "        tmp = np.concatenate([ans[-1,1:,:], np.array(c[i-1,:]).reshape(1,target_nums)]).reshape(1,target_length, target_nums)\n",
    "        ans = np.vstack([ans, tmp])\n",
    "    return ans\n",
    "\n",
    "@jit\n",
    "def calc_test_seq_target(c):\n",
    "    target_nums = 15\n",
    "    ans = np.ones([target_length, target_nums]) * -1\n",
    "    ans = np.concatenate([ans, np.array(c)])\n",
    "    return ans[-target_length:]\n",
    "\n",
    "@jit\n",
    "def calc_test_only_seq_target():\n",
    "    target_nums = 15\n",
    "    ans = np.ones([target_length, target_nums]) * -1\n",
    "    return ans\n",
    "\n",
    "def make_seq_target(df):\n",
    "    df_ = df.reset_index().copy()\n",
    "    df_ = df_.reindex(index = time_sorted_index) # timeでソートしてgroupbyかける\n",
    "    df_ = pd.merge(df_, train_meta_df[[\"user_id\", \"session_id\"]], on=\"session_id\", how=\"left\")\n",
    "    values = df_.groupby('user_id').apply(lambda x: calc_seq_target(x[TARGET_IDS].values))\n",
    "    values = np.vstack(values)\n",
    "    vlaues = values[original_index] #もとのindex順に戻す\n",
    "    return values\n",
    "\n",
    "def make_test_seq_target(df):\n",
    "    df_ = df.reset_index().copy()\n",
    "    df_ = df_.reindex(index = time_sorted_index)\n",
    "    df_ = pd.merge(df_, train_meta_df[[\"user_id\", \"session_id\"]], on=\"session_id\", how=\"left\")\n",
    "    values = df_.groupby('user_id').apply(lambda x: calc_test_seq_target(x[TARGET_IDS].values))\n",
    "    tmp = pd.DataFrame(test_only_users)\n",
    "    tmp.columns = [\"user_id\"]\n",
    "    values2 = tmp.groupby('user_id').apply(lambda x: calc_test_only_seq_target())\n",
    "    values = pd.concat([values, values2])\n",
    "    return values\n",
    "\n",
    "seq_target = make_seq_target(train_target_df)\n",
    "\n",
    "test_seq_target_tmp = make_test_seq_target(train_target_df)\n",
    "test_seq_target = pd.merge(test_meta_df, test_seq_target_tmp.reset_index(), on=[\"user_id\"], how=\"left\")[0]\n",
    "del test_seq_target_tmp\n",
    "print(test_seq_target.isnull().sum())\n",
    "test_seq_target = np.vstack(test_seq_target).reshape(-1, target_length, 15)\n",
    "#https://stackoverflow.com/questions/42170682/retain-dataframes-index-when-using-groupby-apply-to-generate-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:14.627181Z",
     "iopub.status.busy": "2021-03-22T12:01:14.626361Z",
     "iopub.status.idle": "2021-03-22T12:01:15.385415Z",
     "shell.execute_reply": "2021-03-22T12:01:15.384932Z"
    },
    "papermill": {
     "duration": 0.82505,
     "end_time": "2021-03-22T12:01:15.385558",
     "exception": false,
     "start_time": "2021-03-22T12:01:14.560508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_cat_df = pd.merge(train_meta_df, user_master, on=\"user_id\", how=\"left\")[[\"user_id\",\"register_number\"]]\n",
    "test_cat_df = pd.merge(test_meta_df, user_master, on=\"user_id\", how=\"left\")[[\"user_id\",\"register_number\"]]\n",
    "\n",
    "train_meta_df = pd.merge(train_meta_df, user_master, on=\"user_id\", how=\"left\")[[\"age\", \"gender\", \"time_elapsed\",\n",
    "                                                                                \"dayofweek\", \"hour\", \"year\"]] \n",
    "\n",
    "test_meta_df = pd.merge(test_meta_df, user_master, on=\"user_id\", how=\"left\")[[\"age\", \"gender\", \"time_elapsed\",\n",
    "                                                                              \"dayofweek\", \"hour\", \"year\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:15.723095Z",
     "iopub.status.busy": "2021-03-22T12:01:15.721448Z",
     "iopub.status.idle": "2021-03-22T12:01:15.813268Z",
     "shell.execute_reply": "2021-03-22T12:01:15.812520Z"
    },
    "papermill": {
     "duration": 0.395186,
     "end_time": "2021-03-22T12:01:15.813401",
     "exception": false,
     "start_time": "2021-03-22T12:01:15.418215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "tmp = pd.concat([train_cat_df, test_cat_df])\n",
    "le.fit(tmp[\"user_id\"])\n",
    "train_cat_df[\"user_id\"] = le.transform(train_cat_df[\"user_id\"])\n",
    "test_cat_df[\"user_id\"] = le.transform(test_cat_df[\"user_id\"])\n",
    "n_user = tmp[\"user_id\"].nunique() + 1\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:15.870285Z",
     "iopub.status.busy": "2021-03-22T12:01:15.868665Z",
     "iopub.status.idle": "2021-03-22T12:01:15.917813Z",
     "shell.execute_reply": "2021-03-22T12:01:15.917313Z"
    },
    "papermill": {
     "duration": 0.080979,
     "end_time": "2021-03-22T12:01:15.917947",
     "exception": false,
     "start_time": "2021-03-22T12:01:15.836968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe(df, columns):\n",
    "    tmp = df.copy()\n",
    "    tmp = pd.get_dummies(tmp, columns=columns)\n",
    "    return tmp\n",
    "\n",
    "train_meta_df = fe(train_meta_df, [\"time_elapsed\", \"dayofweek\"])\n",
    "test_meta_df = fe(test_meta_df, [\"time_elapsed\", \"dayofweek\"])\n",
    "train_meta_df[\"age\"] = train_meta_df[\"age\"]/100\n",
    "test_meta_df[\"age\"] = test_meta_df[\"age\"]/100\n",
    "train_meta_df[\"hour\"] = train_meta_df[\"hour\"]/24\n",
    "test_meta_df[\"hour\"] = test_meta_df[\"hour\"]/24\n",
    "train_meta_df[\"year2019\"] = (train_meta_df[\"year\"] == 2019).astype(int)\n",
    "test_meta_df[\"year2019\"] = (test_meta_df[\"year\"] == 2019).astype(int)\n",
    "train_meta_df[\"year2020\"] = (train_meta_df[\"year\"] == 2020).astype(int)\n",
    "test_meta_df[\"year2020\"] = (test_meta_df[\"year\"] == 2020).astype(int)\n",
    "del train_meta_df[\"year\"], test_meta_df[\"year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023565,
     "end_time": "2021-03-22T12:01:15.965602",
     "exception": false,
     "start_time": "2021-03-22T12:01:15.942037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:16.102378Z",
     "iopub.status.busy": "2021-03-22T12:01:16.077092Z",
     "iopub.status.idle": "2021-03-22T12:01:17.802397Z",
     "shell.execute_reply": "2021-03-22T12:01:17.801932Z"
    },
    "papermill": {
     "duration": 1.813207,
     "end_time": "2021-03-22T12:01:17.802536",
     "exception": false,
     "start_time": "2021-03-22T12:01:15.989329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57001209/creating-numpy-matrix-from-nested-arrays-in-a-list\n",
    "train_group = np.vstack(np.ravel(train_group))\n",
    "test_group = np.vstack(np.ravel(test_group))\n",
    "train_action_group = np.vstack(np.ravel(train_action_group))\n",
    "test_action_group = np.vstack(np.ravel(test_action_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:17.864376Z",
     "iopub.status.busy": "2021-03-22T12:01:17.863420Z",
     "iopub.status.idle": "2021-03-22T12:01:21.878195Z",
     "shell.execute_reply": "2021-03-22T12:01:21.877696Z"
    },
    "papermill": {
     "duration": 4.051744,
     "end_time": "2021-03-22T12:01:21.878314",
     "exception": false,
     "start_time": "2021-03-22T12:01:17.826570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not to increase memory use during training\n",
    "train_meta_np = np.ndarray(shape=(len(train_meta_df), train_meta_df.shape[1]), dtype=np.float32)\n",
    "test_meta_np = np.ndarray(shape=(len(test_meta_df), test_meta_df.shape[1]), dtype=np.float32)\n",
    "train_cat_np = np.ndarray(shape=(len(train_cat_df), train_cat_df.shape[1]), dtype=np.float32)\n",
    "test_cat_np = np.ndarray(shape=(len(test_cat_df), test_cat_df.shape[1]), dtype=np.float32)\n",
    "\n",
    "for idx in range(train_meta_df.shape[1]):\n",
    "    train_meta_np[:,idx] = train_meta_df.iloc[:,idx].astype(np.float32)\n",
    "\n",
    "for idx in range(test_meta_df.shape[1]):\n",
    "    test_meta_np[:,idx] = test_meta_df.iloc[:,idx].astype(np.float32)\n",
    "\n",
    "for idx in range(train_cat_df.shape[1]):\n",
    "    train_cat_np[:,idx] = train_cat_df.iloc[:,idx].astype(np.float32)\n",
    "    test_cat_np[:,idx] = test_cat_df.iloc[:,idx].astype(np.float32)\n",
    "\n",
    "del train_meta_df, test_meta_df, train_cat_df, test_cat_df\n",
    "\n",
    "train_meta_df = train_meta_np.copy()\n",
    "test_meta_df = test_meta_np.copy()\n",
    "train_cat_df = train_cat_np.copy()\n",
    "test_cat_df = test_cat_np.copy()\n",
    "del train_meta_np, test_meta_np, train_cat_np, test_cat_np\n",
    "\n",
    "seq_target_np = np.ndarray(shape=(len(seq_target), seq_target.shape[1], seq_target.shape[2]), dtype=np.float32)\n",
    "for idx in range(seq_target.shape[1]):\n",
    "    for idx2 in range(seq_target.shape[2]):\n",
    "        seq_target_np[:, idx, idx2] = seq_target[:, idx, idx2].astype(np.float32)\n",
    "del seq_target\n",
    "seq_target = seq_target_np.copy()\n",
    "del seq_target_np\n",
    "\n",
    "test_seq_target_np = np.ndarray(shape=(len(test_seq_target), test_seq_target.shape[1], \n",
    "                                       test_seq_target.shape[2]), dtype=np.float32)\n",
    "for idx in range(test_seq_target.shape[1]):\n",
    "    for idx2 in range(test_seq_target.shape[2]):\n",
    "        test_seq_target_np[:, idx, idx2] = test_seq_target[:, idx, idx2].astype(np.float32)\n",
    "del test_seq_target\n",
    "test_seq_target = test_seq_target_np.copy()\n",
    "del test_seq_target_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:21.931231Z",
     "iopub.status.busy": "2021-03-22T12:01:21.930190Z",
     "iopub.status.idle": "2021-03-22T12:01:21.948096Z",
     "shell.execute_reply": "2021-03-22T12:01:21.947681Z"
    },
    "papermill": {
     "duration": 0.045997,
     "end_time": "2021-03-22T12:01:21.948206",
     "exception": false,
     "start_time": "2021-03-22T12:01:21.902209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_target_df = train_target_df.reset_index(drop=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:22.001557Z",
     "iopub.status.busy": "2021-03-22T12:01:22.000782Z",
     "iopub.status.idle": "2021-03-22T12:01:22.004805Z",
     "shell.execute_reply": "2021-03-22T12:01:22.004397Z"
    },
    "papermill": {
     "duration": 0.032828,
     "end_time": "2021-03-22T12:01:22.004912",
     "exception": false,
     "start_time": "2021-03-22T12:01:21.972084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366478, 15)\n",
      "(366478, 64) (366478, 64) (366478, 16) (366478, 32, 15)\n",
      "(56486, 64) (56486, 64) (56486, 16) (56486, 32, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_target_df.shape)\n",
    "print(train_group.shape, train_action_group.shape, train_meta_df.shape, seq_target.shape)\n",
    "print(test_group.shape, test_action_group.shape, test_meta_df.shape, test_seq_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024305,
     "end_time": "2021-03-22T12:01:22.053424",
     "exception": false,
     "start_time": "2021-03-22T12:01:22.029119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# keras transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:22.106158Z",
     "iopub.status.busy": "2021-03-22T12:01:22.105549Z",
     "iopub.status.idle": "2021-03-22T12:01:22.108510Z",
     "shell.execute_reply": "2021-03-22T12:01:22.108110Z"
    },
    "papermill": {
     "duration": 0.031079,
     "end_time": "2021-03-22T12:01:22.108613",
     "exception": false,
     "start_time": "2021-03-22T12:01:22.077534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_nums = n_register\n",
    "user_nums = n_user\n",
    "embed_dim = 64  # Embedding size for each token\n",
    "target_embed_dim = 15 # embedding size for each target seq\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 128 # Hidden layer size in feed forward network inside transformer\n",
    "vocab_size = n_category_id_class \n",
    "vocab_size2 = n_jan_class\n",
    "maxlen = MAX_SEQ \n",
    "BATCH_SIZE = 1024\n",
    "TRAIN_EPOCH = 8\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:22.166849Z",
     "iopub.status.busy": "2021-03-22T12:01:22.166138Z",
     "iopub.status.idle": "2021-03-22T12:01:22.168573Z",
     "shell.execute_reply": "2021-03-22T12:01:22.169019Z"
    },
    "papermill": {
     "duration": 0.036516,
     "end_time": "2021-03-22T12:01:22.169132",
     "exception": false,
     "start_time": "2021-03-22T12:01:22.132616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "# https://www.kaggle.com/gogo827jz/moa-lstm-pure-transformer-fast-and-not-bad\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=DROPOUT):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:22.231062Z",
     "iopub.status.busy": "2021-03-22T12:01:22.230366Z",
     "iopub.status.idle": "2021-03-22T12:01:22.233117Z",
     "shell.execute_reply": "2021-03-22T12:01:22.232692Z"
    },
    "papermill": {
     "duration": 0.039987,
     "end_time": "2021-03-22T12:01:22.233218",
     "exception": false,
     "start_time": "2021-03-22T12:01:22.193231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(maxlen, category_size, action_size, user_size, register_size, embed_dim):\n",
    "    _input_aux = Input((test_meta_df.shape[1],))  \n",
    "    x_aux = Dense(16, activation=\"relu\")(_input_aux)\n",
    "    x_aux = Dropout(DROPOUT)(x_aux)\n",
    "\n",
    "    inputs = Input(shape=(maxlen,))        \n",
    "    x_category = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(inputs)\n",
    "    x_category = TransformerBlock(embed_dim, num_heads, ff_dim)(x_category)\n",
    "    x_category = TransformerBlock(embed_dim, num_heads, ff_dim)(x_category)\n",
    "    x_category = TransformerBlock(embed_dim, num_heads, ff_dim)(x_category)\n",
    "    x_category = LSTM(128)(x_category)\n",
    "    x_category = Dropout(DROPOUT)(x_category)\n",
    "\n",
    "    inputs_for_action = Input(shape=(maxlen,))\n",
    "    x_action = TokenAndPositionEmbedding(maxlen, action_size, embed_dim)(inputs_for_action)\n",
    "    x_action = TransformerBlock(embed_dim, num_heads, ff_dim)(x_action)\n",
    "    x_action = TransformerBlock(embed_dim, num_heads, ff_dim)(x_action)\n",
    "    x_action = TransformerBlock(embed_dim, num_heads, ff_dim)(x_action)\n",
    "    x_action = LSTM(128)(x_action)\n",
    "    x_action = Dropout(DROPOUT)(x_action)\n",
    "    \n",
    "    inputs_for_target_seq = Input(shape=(target_length,15,))\n",
    "    x_target_seq = TransformerBlock(target_embed_dim, num_heads, ff_dim)(inputs_for_target_seq)\n",
    "    x_target_seq = LSTM(32)(x_target_seq)\n",
    "    x_target_seq = Dropout(DROPOUT)(x_target_seq)\n",
    "    \n",
    "    inputs_for_register = Input(shape=(1,))\n",
    "    x_register = Embedding(input_dim=register_size, output_dim=64)(inputs_for_register)\n",
    "    x_register = GlobalMaxPooling1D()(x_register)\n",
    "    \n",
    "    inputs_for_user = Input(shape=(1,))\n",
    "    x_user = Embedding(input_dim=user_size, output_dim=64)(inputs_for_user)\n",
    "    x_user = GlobalMaxPooling1D()(x_user)\n",
    "    \n",
    "    x = Concatenate()([x_category, x_action])  \n",
    "    x = Concatenate()([x, x_target_seq])\n",
    "    x = Concatenate()([x, x_register])\n",
    "    x = Concatenate()([x, x_user])\n",
    "    x = Concatenate()([x, x_aux])\n",
    "    \n",
    "    x = Dense(128+128+test_meta_df.shape[1], activation=\"relu\")(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    outputs = Dense(len(TARGET_CATEGORIES), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs, inputs_for_action, inputs_for_target_seq, _input_aux, inputs_for_register, inputs_for_user],\n",
    "                  outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T12:01:22.301246Z",
     "iopub.status.busy": "2021-03-22T12:01:22.300608Z",
     "iopub.status.idle": "2021-03-22T13:20:49.693080Z",
     "shell.execute_reply": "2021-03-22T13:20:49.693966Z"
    },
    "papermill": {
     "duration": 4767.436457,
     "end_time": "2021-03-22T13:20:49.694139",
     "exception": false,
     "start_time": "2021-03-22T12:01:22.257682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold:  0\n",
      "Epoch 1/8\n",
      "287/287 [==============================] - 144s 412ms/step - loss: 0.2558 - auc: 0.5119 - val_loss: 0.2066 - val_auc: 0.6448\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.64484, saving model to transformer_0.hdf5\n",
      "Epoch 2/8\n",
      "287/287 [==============================] - 125s 435ms/step - loss: 0.1991 - auc: 0.6762 - val_loss: 0.1927 - val_auc: 0.7272\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.64484 to 0.72723, saving model to transformer_0.hdf5\n",
      "Epoch 3/8\n",
      "287/287 [==============================] - 111s 388ms/step - loss: 0.1795 - auc: 0.7767 - val_loss: 0.1896 - val_auc: 0.7546\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.72723 to 0.75463, saving model to transformer_0.hdf5\n",
      "Epoch 4/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1682 - auc: 0.8178 - val_loss: 0.1899 - val_auc: 0.7631\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.75463 to 0.76306, saving model to transformer_0.hdf5\n",
      "Epoch 5/8\n",
      "287/287 [==============================] - 111s 388ms/step - loss: 0.1618 - auc: 0.8394 - val_loss: 0.1926 - val_auc: 0.7693\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.76306 to 0.76934, saving model to transformer_0.hdf5\n",
      "Epoch 6/8\n",
      "287/287 [==============================] - 111s 386ms/step - loss: 0.1558 - auc: 0.8543 - val_loss: 0.1960 - val_auc: 0.7675\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.76934\n",
      "Epoch 7/8\n",
      "287/287 [==============================] - 111s 386ms/step - loss: 0.1488 - auc: 0.8674 - val_loss: 0.2043 - val_auc: 0.7605\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.76934\n",
      "Epoch 8/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1424 - auc: 0.8772 - val_loss: 0.2123 - val_auc: 0.7567\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.76934\n",
      "0.7728541856802419\n",
      "Starting fold:  1\n",
      "Epoch 1/8\n",
      "287/287 [==============================] - 135s 403ms/step - loss: 0.2556 - auc: 0.5119 - val_loss: 0.2071 - val_auc: 0.6398\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.63978, saving model to transformer_1.hdf5\n",
      "Epoch 2/8\n",
      "287/287 [==============================] - 121s 421ms/step - loss: 0.1998 - auc: 0.6657 - val_loss: 0.1933 - val_auc: 0.7284\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.63978 to 0.72841, saving model to transformer_1.hdf5\n",
      "Epoch 3/8\n",
      "287/287 [==============================] - 111s 388ms/step - loss: 0.1796 - auc: 0.7751 - val_loss: 0.1891 - val_auc: 0.7543\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.72841 to 0.75429, saving model to transformer_1.hdf5\n",
      "Epoch 4/8\n",
      "287/287 [==============================] - 111s 386ms/step - loss: 0.1687 - auc: 0.8166 - val_loss: 0.1900 - val_auc: 0.7648\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.75429 to 0.76484, saving model to transformer_1.hdf5\n",
      "Epoch 5/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1624 - auc: 0.8387 - val_loss: 0.1914 - val_auc: 0.7677\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.76484 to 0.76765, saving model to transformer_1.hdf5\n",
      "Epoch 6/8\n",
      "287/287 [==============================] - 111s 386ms/step - loss: 0.1561 - auc: 0.8533 - val_loss: 0.1952 - val_auc: 0.7691\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.76765 to 0.76914, saving model to transformer_1.hdf5\n",
      "Epoch 7/8\n",
      "287/287 [==============================] - 111s 388ms/step - loss: 0.1499 - auc: 0.8652 - val_loss: 0.2013 - val_auc: 0.7658\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.76914\n",
      "Epoch 8/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1435 - auc: 0.8747 - val_loss: 0.2125 - val_auc: 0.7552\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.76914\n",
      "0.7737871983372706\n",
      "Starting fold:  2\n",
      "Epoch 1/8\n",
      "287/287 [==============================] - 137s 411ms/step - loss: 0.2553 - auc: 0.5124 - val_loss: 0.2067 - val_auc: 0.6497\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.64969, saving model to transformer_2.hdf5\n",
      "Epoch 2/8\n",
      "287/287 [==============================] - 126s 441ms/step - loss: 0.1995 - auc: 0.6755 - val_loss: 0.1932 - val_auc: 0.7291\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.64969 to 0.72907, saving model to transformer_2.hdf5\n",
      "Epoch 3/8\n",
      "287/287 [==============================] - 112s 391ms/step - loss: 0.1797 - auc: 0.7732 - val_loss: 0.1897 - val_auc: 0.7535\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.72907 to 0.75352, saving model to transformer_2.hdf5\n",
      "Epoch 4/8\n",
      "287/287 [==============================] - 112s 392ms/step - loss: 0.1689 - auc: 0.8158 - val_loss: 0.1897 - val_auc: 0.7632\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.75352 to 0.76317, saving model to transformer_2.hdf5\n",
      "Epoch 5/8\n",
      "287/287 [==============================] - 112s 392ms/step - loss: 0.1632 - auc: 0.8367 - val_loss: 0.1918 - val_auc: 0.7678\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.76317 to 0.76783, saving model to transformer_2.hdf5\n",
      "Epoch 6/8\n",
      "287/287 [==============================] - 113s 393ms/step - loss: 0.1579 - auc: 0.8516 - val_loss: 0.1953 - val_auc: 0.7681\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.76783 to 0.76812, saving model to transformer_2.hdf5\n",
      "Epoch 7/8\n",
      "287/287 [==============================] - 112s 392ms/step - loss: 0.1514 - auc: 0.8637 - val_loss: 0.2003 - val_auc: 0.7663\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.76812\n",
      "Epoch 8/8\n",
      "287/287 [==============================] - 112s 392ms/step - loss: 0.1449 - auc: 0.8735 - val_loss: 0.2081 - val_auc: 0.7612\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.76812\n",
      "0.7725605452131913\n",
      "Starting fold:  3\n",
      "Epoch 1/8\n",
      "287/287 [==============================] - 135s 404ms/step - loss: 0.2553 - auc: 0.5136 - val_loss: 0.2064 - val_auc: 0.6472\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.64718, saving model to transformer_3.hdf5\n",
      "Epoch 2/8\n",
      "287/287 [==============================] - 121s 423ms/step - loss: 0.1992 - auc: 0.6767 - val_loss: 0.1920 - val_auc: 0.7316\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.64718 to 0.73156, saving model to transformer_3.hdf5\n",
      "Epoch 3/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1786 - auc: 0.7777 - val_loss: 0.1887 - val_auc: 0.7565\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.73156 to 0.75650, saving model to transformer_3.hdf5\n",
      "Epoch 4/8\n",
      "287/287 [==============================] - 112s 389ms/step - loss: 0.1684 - auc: 0.8182 - val_loss: 0.1891 - val_auc: 0.7659\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.75650 to 0.76585, saving model to transformer_3.hdf5\n",
      "Epoch 5/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1626 - auc: 0.8380 - val_loss: 0.1919 - val_auc: 0.7692\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.76585 to 0.76923, saving model to transformer_3.hdf5\n",
      "Epoch 6/8\n",
      "287/287 [==============================] - 111s 386ms/step - loss: 0.1567 - auc: 0.8534 - val_loss: 0.1962 - val_auc: 0.7683\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.76923\n",
      "Epoch 7/8\n",
      "287/287 [==============================] - 111s 387ms/step - loss: 0.1496 - auc: 0.8662 - val_loss: 0.2021 - val_auc: 0.7642\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.76923\n",
      "Epoch 8/8\n",
      "287/287 [==============================] - 112s 390ms/step - loss: 0.1430 - auc: 0.8755 - val_loss: 0.2124 - val_auc: 0.7574\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.76923\n",
      "0.7731492413965936\n",
      "Starting fold:  4\n",
      "Epoch 1/8\n",
      "287/287 [==============================] - 139s 417ms/step - loss: 0.2558 - auc: 0.5129 - val_loss: 0.2063 - val_auc: 0.6445\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.64447, saving model to transformer_4.hdf5\n",
      "Epoch 2/8\n",
      "287/287 [==============================] - 123s 429ms/step - loss: 0.1992 - auc: 0.6735 - val_loss: 0.1933 - val_auc: 0.7322\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.64447 to 0.73216, saving model to transformer_4.hdf5\n",
      "Epoch 3/8\n",
      "287/287 [==============================] - 114s 398ms/step - loss: 0.1791 - auc: 0.7773 - val_loss: 0.1893 - val_auc: 0.7529\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.73216 to 0.75290, saving model to transformer_4.hdf5\n",
      "Epoch 4/8\n",
      "287/287 [==============================] - 114s 397ms/step - loss: 0.1685 - auc: 0.8165 - val_loss: 0.1896 - val_auc: 0.7647\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.75290 to 0.76471, saving model to transformer_4.hdf5\n",
      "Epoch 5/8\n",
      "287/287 [==============================] - 114s 397ms/step - loss: 0.1623 - auc: 0.8383 - val_loss: 0.1916 - val_auc: 0.7692\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.76471 to 0.76923, saving model to transformer_4.hdf5\n",
      "Epoch 6/8\n",
      "287/287 [==============================] - 114s 397ms/step - loss: 0.1562 - auc: 0.8544 - val_loss: 0.1966 - val_auc: 0.7673\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.76923\n",
      "Epoch 7/8\n",
      "287/287 [==============================] - 114s 397ms/step - loss: 0.1491 - auc: 0.8671 - val_loss: 0.2018 - val_auc: 0.7629\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.76923\n",
      "Epoch 8/8\n",
      "287/287 [==============================] - 115s 399ms/step - loss: 0.1430 - auc: 0.8754 - val_loss: 0.2149 - val_auc: 0.7550\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.76923\n",
      "0.7729363235441882\n",
      "[0.7728541856802419, 0.7737871983372706, 0.7725605452131913, 0.7731492413965936, 0.7729363235441882]\n",
      "Mean OOF loss across folds 0.7730574988342971\n",
      "STD OOF loss across folds 0.0004108188538642188\n"
     ]
    }
   ],
   "source": [
    "gru_oof_preds = np.zeros((train_group.shape[0], train_target_df.shape[1]))\n",
    "gru_test_preds = np.zeros((test_group.shape[0], train_target_df.shape[1]))\n",
    "oof_losses = []\n",
    "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=0, shuffle=True)\n",
    "X_test = tf.convert_to_tensor(test_group, dtype=tf.float32)\n",
    "X_test_action = tf.convert_to_tensor(test_action_group, dtype=tf.float32)\n",
    "X_test_seq_tar = tf.convert_to_tensor(test_seq_target, dtype=tf.float32)\n",
    "meta_X_test = tf.convert_to_tensor(test_meta_df, dtype=tf.float32)\n",
    "\n",
    "user_X_test = tf.convert_to_tensor(test_cat_df[:,0], dtype=tf.float32)\n",
    "reg_X_test = tf.convert_to_tensor(test_cat_df[:,1], dtype=tf.float32)\n",
    "\n",
    "for fn, (trn_idx, val_idx) in enumerate(mskf.split(train_group, train_target_df)):\n",
    "    print('Starting fold: ', fn)    \n",
    "    X_train, X_val = train_group[trn_idx,:], train_group[val_idx,:]\n",
    "    X_train_action, X_val_action = train_action_group[trn_idx,:], train_action_group[val_idx,:]\n",
    "    X_train_seq_tar, X_val_seq_tar = seq_target[trn_idx], seq_target[val_idx]\n",
    "    meta_X_train, meta_X_val = train_meta_df[trn_idx,:], train_meta_df[val_idx,:]\n",
    "    user_X_train, user_X_val = train_cat_df[trn_idx,0], train_cat_df[val_idx,0]\n",
    "    reg_X_train, reg_X_val = train_cat_df[trn_idx,1], train_cat_df[val_idx,1]\n",
    "    \n",
    "    y_train, y_val = train_target_df[trn_idx,:], train_target_df[val_idx,:]\n",
    "    \n",
    "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "    X_train_action = tf.convert_to_tensor(X_train_action, dtype=tf.float32)\n",
    "    X_val_action = tf.convert_to_tensor(X_val_action, dtype=tf.float32)   \n",
    "    X_train_seq_tar = tf.convert_to_tensor(X_train_seq_tar, dtype=tf.float32)\n",
    "    X_val_seq_tar = tf.convert_to_tensor(X_val_seq_tar, dtype=tf.float32)  \n",
    "    meta_X_train = tf.convert_to_tensor(meta_X_train, dtype=tf.float32)\n",
    "    meta_X_val = tf.convert_to_tensor(meta_X_val, dtype=tf.float32)\n",
    "    user_X_train = tf.convert_to_tensor(user_X_train, dtype=tf.float32)\n",
    "    user_X_val = tf.convert_to_tensor(user_X_val, dtype=tf.float32)\n",
    "    reg_X_train = tf.convert_to_tensor(reg_X_train, dtype=tf.float32)\n",
    "    reg_X_val = tf.convert_to_tensor(reg_X_val, dtype=tf.float32)\n",
    "    \n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "    \n",
    "    # modelling\n",
    "    set_seed()\n",
    "    model = create_model(maxlen, vocab_size, vocab_size2, user_nums, register_nums, embed_dim)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC(multi_label=True)])    \n",
    "    model.fit([X_train, X_train_action, X_train_seq_tar, meta_X_train, reg_X_train, user_X_train], y_train, \n",
    "              validation_data = ([X_val, X_val_action, X_val_seq_tar, meta_X_val, reg_X_val, user_X_val], y_val),\n",
    "              epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, verbose=1,\n",
    "              callbacks = [\n",
    "                          ReduceLROnPlateau(monitor = 'val_auc', factor = 0.1, patience = 3, verbose = 1, \n",
    "                           min_delta = 1e-4, mode = 'max'),\n",
    "                          ModelCheckpoint(filepath=f'transformer_{fn}.hdf5', verbose = 1, \n",
    "                           save_best_only = True, save_weights_only=True, mode = 'max', monitor = 'val_auc')\n",
    "                          ]\n",
    "              )\n",
    "    #\n",
    "\n",
    "    model.load_weights(f'transformer_{fn}.hdf5')\n",
    "    gru_preds = model.predict([X_val, X_val_action, X_val_seq_tar,  meta_X_val, reg_X_val, user_X_val], batch_size=BATCH_SIZE)\n",
    "    gru_oof_preds[val_idx] = gru_preds\n",
    "    score = roc_auc_score(y_val, gru_preds, average='macro')\n",
    "    print(score)\n",
    "    oof_losses.append(score)\n",
    "    preds = model.predict([X_test, X_test_action, X_test_seq_tar, meta_X_test, reg_X_test, user_X_test], batch_size=BATCH_SIZE)\n",
    "\n",
    "    gru_test_preds += preds / NFOLDS\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "print(oof_losses)\n",
    "print('Mean OOF loss across folds', np.mean(oof_losses))\n",
    "print('STD OOF loss across folds', np.std(oof_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.061624,
     "end_time": "2021-03-22T13:20:56.259994",
     "exception": false,
     "start_time": "2021-03-22T13:20:53.198370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 287/287 [==============================] - 148s 466ms/step - loss: 0.2558 - auc: 0.5117 - val_loss: 0.2065 - val_auc: 0.6458\n",
    "- 287/287 [==============================] - 152s 473ms/step - loss: 0.2558 - auc: 0.5117 - val_loss: 0.2065 - val_auc: 0.6446\n",
    "- 287/287 [==============================] - 146s 464ms/step - loss: 0.2558 - auc: 0.5118 - val_loss: 0.2064 - val_auc: 0.6455\n",
    "- 追加導入後\n",
    "- 287/287 [==============================] - 147s 417ms/step - loss: 0.2558 - auc: 0.5118 - val_loss: 0.2066 - val_auc: 0.6448\n",
    "- 287/287 [==============================] - 159s 441ms/step - loss: 0.2558 - auc: 0.5119 - val_loss: 0.2064 - val_auc: 0.6455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.150515,
     "end_time": "2021-03-22T13:21:02.699763",
     "exception": false,
     "start_time": "2021-03-22T13:20:59.549248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T13:21:08.767075Z",
     "iopub.status.busy": "2021-03-22T13:21:08.765916Z",
     "iopub.status.idle": "2021-03-22T13:21:10.982729Z",
     "shell.execute_reply": "2021-03-22T13:21:10.982267Z"
    },
    "papermill": {
     "duration": 5.299111,
     "end_time": "2021-03-22T13:21:10.982900",
     "exception": false,
     "start_time": "2021-03-22T13:21:05.683789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.iloc[:,:] = gru_test_preds\n",
    "submission.to_csv(\"atmacup9_\"+str(score)[:-10]+\"_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.021675,
     "end_time": "2021-03-22T13:21:16.970256",
     "exception": false,
     "start_time": "2021-03-22T13:21:13.948581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5019.832521,
   "end_time": "2021-03-22T13:21:24.093194",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-22T11:57:44.260673",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
